# ============================================================================
# Paths Configuration
# ============================================================================
schema_version: 2

# Centralized directory structure for the entire ML pipeline.
# 
# This file defines all directory paths and file naming patterns used across
# the pipeline. All path resolution should go through src/orchestration/paths.py
# to ensure consistency.
#
# Usage Example:
#   from orchestration.paths import resolve_output_path
#   hpo_dir = resolve_output_path(ROOT_DIR, CONFIG_DIR, "hpo")
#   # Returns: ROOT_DIR / "outputs" / "hpo"
#
# ============================================================================

# ----------------------------------------------------------------------------
# Project Configuration
# ----------------------------------------------------------------------------
# Single source of truth for project name and identifiers.
# This is referenced throughout the config to avoid duplication.
#
# NOTE: For paths that contain the project name as part of a string,
# the project name is hardcoded but should match this value.
# Consider using a post-processing step to replace placeholders if needed.
#
project:
  name: &project_name "resume-ner-azureml"  # Project name used in paths and identifiers

# ----------------------------------------------------------------------------
# Base Directories
# ----------------------------------------------------------------------------
# Top-level directories relative to project root.
# These are the main directories in the repository structure.
#
# Example structure:
#   project_root/
#     ├── outputs/          # All training outputs
#     ├── notebooks/        # Jupyter notebooks
#     ├── config/           # Configuration files
#     ├── src/              # Source code
#     ├── tests/            # Test files
#     └── mlruns/           # Local MLflow runs (if using file store)
#
base:
  outputs: "outputs"      # Main output directory for all training artifacts
  notebooks: "notebooks"  # Jupyter notebooks directory
  config: "config"        # Configuration files directory
  src: "src"              # Source code directory
  tests: "tests"          # Test files directory
  mlruns: "mlruns"        # Local MLflow runs (file store, optional)

# ----------------------------------------------------------------------------
# Environment-specific overrides (shallow, keyed by storage_env)
# ----------------------------------------------------------------------------
env_overrides:
  colab:
    base:
      # NOTE: Project name "resume-ner-azureml" should match project.name above
      outputs: "/content/drive/MyDrive/resume-ner-azureml/outputs"
  azureml:
    base:
      outputs: "/mnt/outputs"
  kaggle:
    base:
      outputs: "/kaggle/working/outputs"

# ----------------------------------------------------------------------------
# Output Subdirectories
# ----------------------------------------------------------------------------
# Subdirectories within outputs/ for different pipeline stages.
# All paths are relative to outputs/.
#
# Example structure:
#   outputs/
#     ├── hpo/                    # Hyperparameter optimization results
#     ├── final_training/          # Final production training runs
#     │   └── distilbert_20251228_000723/
#     │       ├── checkpoint/
#     │       └── metrics.json
#     ├── conversion/             # ONNX model conversions
#     │   └── distilbert_20251228_001226/
#     │       └── model_int8.onnx
#     └── cache/                  # Cache files (see cache section below)
#
outputs:
  # HPO (Hyperparameter Optimization) outputs
  # Structure: outputs/hpo/{environment}/{model}/trial_{n}_{run_id}/
  #   - For k-fold CV: trial_{n}_{run_id}/cv/fold{k}/checkpoint/
  #   - For refit training: trial_{n}_{run_id}/refit/checkpoint/
  #   - For single training (no CV): trial_{n}_{run_id}/checkpoint/
  hpo: "hpo"                      # HPO sweep results: outputs/hpo/{environment}/{model}/trial_{n}_{run_id}/
  hpo_tests: "hpo_tests"          # HPO test/smoke run outputs
  
  # Benchmarking outputs
  benchmarking: "benchmarking"    # Benchmarking results: outputs/benchmarking/{environment}/{model}/trial_{trial_id}/
  
  # Training outputs
  final_training: "final_training"        # Final training: outputs/final_training/{environment}/{model}/spec_{spec_fp}_exec_{exec_fp}/v{variant}/
  dry_run: "dry_run"                      # Dry run/test training outputs
  
  # Model conversion outputs
  conversion: "conversion"        # ONNX conversions: outputs/conversion/{environment}/{model}/{parent_training_id}/conv_{conv_fp}/
  
  # Best model selection outputs
  best_model_selection: "best_model_selection"  # Best model selection checkpoints: outputs/best_model_selection/{environment}/{model}/sel_{study_hash[:8]}_{trial_hash[:8]}/
  
  # Cache directories (for metadata and cache files)
  cache: "cache"                  # Cache files: outputs/cache/{cache_type}/
  
  # Test outputs
  e2e_test: "e2e_test"           # End-to-end test outputs
  pytest_logs: "pytest_logs"     # Pytest log files

# ----------------------------------------------------------------------------
# Cache Subdirectories
# ----------------------------------------------------------------------------
# Subdirectories within outputs/cache/ for different cache types.
# These store metadata about training runs, best configurations, etc.
#
# Example structure:
#   outputs/cache/
#     ├── best_configurations/    # Best HPO configuration cache
#     │   ├── latest_best_configuration.json
#     │   ├── index.json
#     │   └── best_config_distilbert_trial_2_20251227_220407.json
#     └── final_training/         # Final training run cache
#         ├── latest_final_training_cache.json
#         ├── final_training_index.json
#         └── final_training_distilbert_20251228_000723_20251228_001000.json
#
cache:
  best_configurations: "best_configurations"  # Best HPO config cache files
  final_training: "final_training"           # Final training run cache files
  best_model_selection: "best_model_selection"  # Best model selection cache files
  
  # Other cache types can be added here as needed
  # Example:
  # conversion_cache: "conversion_cache"     # Conversion metadata cache

# ----------------------------------------------------------------------------
# File Naming Patterns
# ----------------------------------------------------------------------------
# Standard filenames used throughout the pipeline.
# These are the actual filenames (not paths) for common files.
#
# Examples:
#   - metrics.json: Training metrics saved in each run directory
#   - benchmark.json: Benchmark results for inference speed
#   - checkpoint/: Directory containing model checkpoints
#
files:
  # Standard output files (saved in run directories)
  metrics: "metrics.json"           # Training metrics: {output_dir}/metrics.json
  benchmark: "benchmark.json"        # Benchmark results: {output_dir}/benchmark.json
  checkpoint_dir: "checkpoint"       # Checkpoint directory: {output_dir}/checkpoint/
  
  # Cache files - dual file strategy (see cache_strategies section)
  # These files are stored in outputs/cache/{cache_type}/
  cache:
    # Best configuration cache files
    # Location: outputs/cache/best_configurations/
    best_config_latest: "latest_best_configuration.json"  # Pointer to latest best config
    best_config_index: "index.json"                       # Index of all best configs
    
    # Final training cache files
    # Location: outputs/cache/final_training/
    final_training_latest: "latest_final_training_cache.json"  # Pointer to latest training run
    final_training_index: "final_training_index.json"          # Index of all training runs
    
    # Best model selection cache files
    # Location: outputs/cache/best_model_selection/
    best_model_selection_latest: "latest_best_model_selection_cache.json"  # Pointer to latest selection
    best_model_selection_index: "best_model_selection_index.json"          # Index of all selections
    
    # Other cache files
    conversion_cache: "conversion_cache.json"  # Conversion metadata (legacy location: notebooks/)

# ----------------------------------------------------------------------------
# Directory Patterns (with placeholders)
# ----------------------------------------------------------------------------
# Patterns for generating directory and file names with dynamic values.
# Placeholders are replaced at runtime with actual values.
#
# Placeholders for v2 patterns (fingerprint-based):
#   - {storage_env}: Logical storage environment for outputs
#                    (e.g., "local", "colab", "kaggle", "azureml")
#   - {model}: Model backbone name (e.g., "distilbert", "distilroberta")
#   - {spec8}: Short specification fingerprint (spec_fp[:8])
#   - {exec8}: Short execution fingerprint (exec_fp[:8])
#   - {variant}: Variant number for final_training (e.g., 1, 2, 3)
#   - {conv8}: Short conversion fingerprint (conv_fp[:8])
#   - {study8}: Short study hash (study_key_hash[:8]) for HPO/benchmarking
#   - {trial8}: Short trial hash (trial_key_hash[:8]) for HPO/benchmarking
#   - {bench8}: Optional short benchmark-config hash for benchmarking
#
# Placeholders for cache file patterns:
#   - {backbone}: Model backbone name (e.g., "distilbert", "distilroberta")
#   - {run_id}: Unique run identifier (e.g., "20251228_000723")
#   - {trial}: Trial identifier (e.g., "trial_2", "trial_5")
#   - {timestamp}: Timestamp string (e.g., "20251228_001000")
#
# Examples (v2 patterns):
#   final_training_v2: "{storage_env}/{model}/spec-{spec8}_exec-{exec8}/v{variant}"
#   With storage_env="local", model="distilbert", spec8="abc12345", exec8="xyz789ab", variant=1
#   Result: "local/distilbert/spec-abc12345_exec-xyz789ab/v1"
#   Full path: outputs/final_training/local/distilbert/spec-abc12345_exec-xyz789ab/v1/
#
patterns:
  # Cache file patterns (still used by cache system)
  # Best configuration cache file pattern
  # Example: best_config_distilbert_trial_2_20251227_220407.json
  # Location: outputs/cache/best_configurations/
  best_config_file: "best_config_{backbone}_{trial}_{timestamp}.json"
  
  # Final training cache file pattern
  # Example: final_training_distilbert_20251228_000723_20251228_001000.json
  # Location: outputs/cache/final_training/
  final_training_cache_file: "final_training_{backbone}_{run_id}_{timestamp}.json"
  
  # Best model selection cache file pattern
  # Example: best_model_selection_distilbert_experiment_name_cachekey_20251228_001000.json
  # Location: outputs/cache/best_model_selection/
  best_model_selection_cache_file: "best_model_selection_{backbone}_{identifier}_{timestamp}.json"

  # Fingerprint-based paths (v2) - used by build_output_path()
  # These patterns are used by build_output_path() in naming_centralized.py
  # to generate fingerprint-based, storage-env-aware output paths.
  # Pattern placeholders are replaced at runtime with values from NamingContext.
  #
  # Final training with fingerprints (short hashes)
  # Example resolved fragment:
  #   "local/distilbert/spec-abc12345_exec-xyz789ab/v1"
  final_training_v2: "{storage_env}/{model}/spec-{spec8}_exec-{exec8}/v{variant}"
  
  # Conversion with structured lineage (no parent_training_id)
  # Uses the same spec/exec/variant lineage as final_training plus a conv hash.
  conversion_v2: "{storage_env}/{model}/spec-{spec8}_exec-{exec8}/v{variant}/conv-{conv8}"
  
  # Best config with short spec hash (cache layout)
  best_config_v2: "{model}/spec-{spec8}"
  
  # HPO v2 with study/trial short hashes
  # Stable structure:
  #   outputs/hpo/{storage_env}/{model}/study-{study8}/trial-{trial8}/...
  # where study8 = study_key_hash[:8], trial8 = trial_key_hash[:8]
  # NOTE: When study/trial hashes are not available, code falls back to the
  # legacy trial_id-based layout for backward compatibility.
  hpo_v2: "{storage_env}/{model}/study-{study8}/trial-{trial8}"
  
  # Benchmarking v2 with study/trial + optional benchmark-config hash
  # Stable structure:
  #   outputs/benchmarking/{storage_env}/{model}/study-{study8}/trial-{trial8}/bench-{bench8}/...
  # where bench8 is derived from benchmark_config_hash (if used).
  # When hashes are not available, code falls back to the legacy layout.
  benchmarking_v2: "{storage_env}/{model}/study-{study8}/trial-{trial8}/bench-{bench8}"

# ----------------------------------------------------------------------------
# Cache File Strategies
# ----------------------------------------------------------------------------
# Configuration for the "dual file strategy" used for caching training metadata.
#
# Dual File Strategy Overview:
#   The dual file strategy maintains three types of files:
#   1. Timestamped files: Historical records with full metadata
#      Example: best_config_distilbert_trial_2_20251227_220407.json
#   2. Latest pointer: Points to the most recent timestamped file
#      Example: latest_best_configuration.json (contains copy of latest data)
#   3. Index file: List of all cache entries for browsing/searching
#      Example: index.json (contains metadata for all entries)
#
# Benefits:
#   - Easy access: Use "latest" file for most common case
#   - History preservation: All timestamped files kept for audit trail
#   - Searchable: Index allows finding specific runs by identifier/timestamp
#   - No overwrites: Each save creates new timestamped file
#
# Usage in Code:
#   from orchestration.paths import load_cache_file, save_cache_with_dual_strategy
#   
#   # Save cache (creates timestamped + updates latest + updates index)
#   save_cache_with_dual_strategy(
#       root_dir=ROOT_DIR, config_dir=CONFIG_DIR,
#       cache_type="final_training", data={...},
#       backbone="distilbert", identifier="20251228_000723", timestamp="20251228_001000"
#   )
#   
#   # Load latest cache
#   cache = load_cache_file(ROOT_DIR, CONFIG_DIR, "final_training", use_latest=True)
#   
#   # Load specific run
#   cache = load_cache_file(ROOT_DIR, CONFIG_DIR, "final_training", 
#                          specific_identifier="20251228_000723")
#
# Default cache strategy settings (applied to all strategies unless overridden)
# These anchors are used to avoid duplication across cache strategies
cache_strategy_defaults: &cache_strategy_defaults
  strategy: "dual"
  timestamped: &cache_timestamped_defaults
    enabled: true
    keep_all: true
    max_files: null  # null = keep all, or set number (e.g., 50) to limit
  latest: &cache_latest_defaults
    enabled: true
    include_timestamped_ref: true
  index: &cache_index_defaults
    enabled: true
    max_entries: 20  # Keep last N entries in index (oldest entries removed)
    include_metadata: true  # Include full metadata in index entries

cache_strategies:
  # Best configurations cache (from HPO selection)
  best_configurations:
    <<: *cache_strategy_defaults
    timestamped:
      <<: *cache_timestamped_defaults
      pattern: "best_config_{backbone}_{trial}_{timestamp}.json"
    latest:
      <<: *cache_latest_defaults
      filename: "latest_best_configuration.json"
    index:
      <<: *cache_index_defaults
      filename: "index.json"
  
  # Final training cache strategy
  # Stores metadata about final training runs (checkpoint paths, metrics, config)
  final_training:
    <<: *cache_strategy_defaults
    timestamped:
      <<: *cache_timestamped_defaults
      pattern: "final_training_{backbone}_{run_id}_{timestamp}.json"
    latest:
      <<: *cache_latest_defaults
      filename: "latest_final_training_cache.json"
    index:
      <<: *cache_index_defaults
      filename: "final_training_index.json"
      # max_entries and include_metadata inherited from cache_index_defaults
  
  # Best model selection cache strategy
  # Stores metadata about best model selection results (run_id, metrics, config)
  best_model_selection:
    <<: *cache_strategy_defaults
    timestamped:
      <<: *cache_timestamped_defaults
      pattern: "best_model_selection_{backbone}_{identifier}_{timestamp}.json"
    latest:
      <<: *cache_latest_defaults
      filename: "latest_best_model_selection_cache.json"
    index:
      <<: *cache_index_defaults
      filename: "best_model_selection_index.json"
      # max_entries and include_metadata inherited from cache_index_defaults

# ----------------------------------------------------------------------------
# Repository Root Detection Configuration
# ----------------------------------------------------------------------------
# Configuration for unified repository root detection.
# 
# This section defines how to detect the repository root directory across
# different platforms and environments. It reuses markers from the `base.*`
# section (single source of truth) and adds platform-specific search locations.
#
# Key Principles:
#   - Markers derived from `base.*` section (no duplication)
#   - Platform repo locations ≠ output locations (outputs in env_overrides)
#   - Focus only on repo root discovery, not output routing
#
# Usage:
#   from infrastructure.paths.repo import detect_repo_root
#   root_dir = detect_repo_root()
#
repository_root:
  # Derive markers from existing base section (single source of truth)
  # Uses base.config, base.src, etc. - no duplication
  markers_from_base: true
  
  # Optional extra markers for stricter validation
  # At least one of these should exist to prevent false positives
  extra_markers:
    - ".git"
    - "pyproject.toml"
    - "setup.cfg"
  
  # Workspace candidates (for VS Code/Cursor workspaces)
  # These are checked when running in local environments
  # NOTE: Project name "resume-ner-azureml" should match project.name above
  workspace_candidates:
    - "/workspaces/resume-ner-azureml"
    - "/workspace/resume-ner-azureml"
  
  # Platform-specific REPO locations (NOT outputs - those are in env_overrides)
  # These are where the repository code itself lives, not where outputs go
  # Output routing is handled separately via env_overrides
  # NOTE: Project name "resume-ner-azureml" should match project.name above
  platform_candidates:
    colab:
      - "/content/resume-ner-azureml"
    kaggle:
      - "/kaggle/working/resume-ner-azureml"
    azureml:
      - "/mnt/resume-ner-azureml"
  
  # Search strategy configuration
  search:
    max_depth: 10  # Maximum depth to search up directory tree
    fallback_to_cwd: true  # Fallback to current working directory if not found
    warn_on_fallback: true  # Log warning when falling back to cwd
  
  # Optional caching
  cache:
    enabled: true  # Cache detected repository root (per process)

# ----------------------------------------------------------------------------
# Google Drive Backup Configuration (Colab only)
# ----------------------------------------------------------------------------
# Configuration for Google Drive backup when running in Google Colab.
# The backup mirrors the entire local outputs/ directory structure.
#
# Local structure (v2 with environment):
#   outputs/
#     ├── hpo/{environment}/{model}/trial_{trial_id}/...
#     ├── benchmarking/{environment}/{model}/trial_{trial_id}/...
#     ├── final_training/{environment}/{model}/spec_{spec_fp}_exec_{exec_fp}/v{variant}/...
#     ├── conversion/{environment}/{model}/{parent_training_id}/conv_{conv_fp}/...
#     └── cache/
#         ├── best_configurations/{model}/spec_{spec_fp}/...
#         └── final_training/...
#
# Drive structure (mirrors local exactly):
#   /content/drive/MyDrive/{backup_base_dir}/outputs/
#     ├── hpo/{environment}/{model}/trial_{trial_id}/...
#     ├── benchmarking/{environment}/{model}/trial_{trial_id}/...
#     ├── final_training/{environment}/{model}/spec_{spec_fp}_exec_{exec_fp}/v{variant}/...
#     ├── conversion/{environment}/{model}/{parent_training_id}/conv_{conv_fp}/...
#     └── cache/
#         ├── best_configurations/{model}/spec_{spec_fp}/...
#         └── final_training/...
#
# Usage:
#   from orchestration.paths import get_drive_backup_path, get_drive_backup_base
#   drive_path = get_drive_backup_path(ROOT_DIR, CONFIG_DIR, local_output_path)
#   # Example: Returns /content/drive/MyDrive/resume-ner-azureml/outputs/hpo/colab/distilbert/trial_0_20260101_161725/refit/checkpoint/
#
drive:
  # Google Drive mount point (standard Colab location)
  # Full path will be: {mount_point}/MyDrive/{backup_base_dir}
  mount_point: "/content/drive"
  
  # Base backup directory in Google Drive
  # Full path: {mount_point}/MyDrive/{backup_base_dir}
  # Changed from "resume-ner-checkpoints" to "resume-ner-azureml" to preserve project structure
  # Uses project.name from project section (single source of truth)
  backup_base_dir: *project_name  # References project.name anchor

# ----------------------------------------------------------------------------
# Path normalization policy (for filesystem safety)
# ----------------------------------------------------------------------------
normalize_paths:
  replace:
    "/": "_"
    "\\": "_"
    "-": "_"
    " ": "_"
    ":": "_"
    "*": "_"
    "?": "_"
    "\"": "_"
    "<": "_"
    ">": "_"
    "|": "_"
  lowercase: false
  max_component_length: 255
  max_path_length: 260
