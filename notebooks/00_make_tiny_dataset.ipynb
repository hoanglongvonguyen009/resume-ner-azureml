{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create Tiny Smoke Dataset\n",
        "\n",
        "This notebook creates a **tiny subset** of the main dataset and a matching data config YAML.\n",
        "\n",
        "- Input dataset: `../dataset/train.json`\n",
        "- Output dataset: `../dataset_tiny/{train.json, validation.json}`\n",
        "- Output config: `../config/data/resume_tiny.yaml`\n",
        "\n",
        "After running this once, you can point `01_orchestrate_training.ipynb` at `resume_tiny.yaml`\n",
        "when you want to exercise the full orchestration on a tiny dataset for speed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raw train path: /workspaces/resume-ner-azureml/dataset/train.json\n",
            "Tiny dataset directory: /workspaces/resume-ner-azureml/dataset_tiny\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "import yaml\n",
        "\n",
        "RAW_DATA_DIR = Path(\"../dataset\")\n",
        "RAW_TRAIN = RAW_DATA_DIR / \"train.json\"\n",
        "\n",
        "TINY_DATA_DIR = Path(\"../dataset_tiny\")\n",
        "TINY_TRAIN = TINY_DATA_DIR / \"train.json\"\n",
        "TINY_VAL = TINY_DATA_DIR / \"validation.json\"\n",
        "\n",
        "# How many samples to keep for the tiny smoke dataset\n",
        "TINY_TRAIN_SAMPLES = 8\n",
        "TINY_VAL_SAMPLES = 2\n",
        "\n",
        "print(\"Raw train path:\", RAW_TRAIN.resolve())\n",
        "print(\"Tiny dataset directory:\", TINY_DATA_DIR.resolve())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote tiny train (8 samples, max 1500 chars) to ../dataset_tiny/train.json\n",
            "Wrote tiny validation (2 samples, max 1500 chars) to ../dataset_tiny/validation.json\n"
          ]
        }
      ],
      "source": [
        "# Build tiny train/validation JSON files\n",
        "if not RAW_TRAIN.exists():\n",
        "    raise FileNotFoundError(f\"Raw train.json not found at {RAW_TRAIN}\")\n",
        "\n",
        "with RAW_TRAIN.open(\"r\", encoding=\"utf-8\") as f:\n",
        "    full_train = json.load(f)\n",
        "\n",
        "if not isinstance(full_train, list) or not full_train:\n",
        "    raise ValueError(\"Expected train.json to be a non-empty list of samples\")\n",
        "\n",
        "TINY_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Filter to samples with reasonably short text to keep tokenization/training fast\n",
        "MAX_CHARS = 1500\n",
        "short_samples = []\n",
        "for sample in full_train:\n",
        "    text = sample.get(\"text\", \"\")\n",
        "    if isinstance(text, str) and len(text) <= MAX_CHARS:\n",
        "        short_samples.append(sample)\n",
        "\n",
        "if len(short_samples) < TINY_TRAIN_SAMPLES + TINY_VAL_SAMPLES:\n",
        "    raise ValueError(\n",
        "        f\"Not enough short samples (<= {MAX_CHARS} chars). \"\n",
        "        f\"Found {len(short_samples)}, need at least {TINY_TRAIN_SAMPLES + TINY_VAL_SAMPLES}.\"\n",
        "    )\n",
        "\n",
        "train_slice = short_samples[:TINY_TRAIN_SAMPLES]\n",
        "val_slice = short_samples[TINY_TRAIN_SAMPLES:TINY_TRAIN_SAMPLES + TINY_VAL_SAMPLES]\n",
        "\n",
        "with TINY_TRAIN.open(\"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(train_slice, f, ensure_ascii=False, indent=2)\n",
        "with TINY_VAL.open(\"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(val_slice, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"Wrote tiny train ({len(train_slice)} samples, max {MAX_CHARS} chars) to {TINY_TRAIN}\")\n",
        "print(f\"Wrote tiny validation ({len(val_slice)} samples, max {MAX_CHARS} chars) to {TINY_VAL}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote tiny data config to: /workspaces/resume-ner-azureml/config/data/resume_tiny.yaml\n",
            "Config contents:\n",
            " name: resume-ner-data-tiny-short\n",
            "version: v2\n",
            "description: Tiny smoke-test subset of Resume NER dataset (short-text version for\n",
            "  fast orchestration tests)\n",
            "schema:\n",
            "  format: json\n",
            "  annotation_format: character_spans\n",
            "  entity_types:\n",
            "  - SKILL\n",
            "  - EDUCATION\n",
            "  - DESIGNATION\n",
            "  - EXPERIENCE\n",
            "  - NAME\n",
            "  - EMAIL\n",
            "  - PHONE\n",
            "  - LOCATION\n",
            "  stats:\n",
            "    median_sentence_length: 19\n",
            "    mean_sentence_length: 20\n",
            "    p95_sentence_length: 40\n",
            "    suggested_sequence_length: 40\n",
            "    entity_density: 0.35\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create a tiny data config YAML by copying resume_v1.yaml\n",
        "BASE_CONFIG_PATH = Path(\"../config/data/resume_v1.yaml\")\n",
        "TINY_CONFIG_PATH = Path(\"../config/data/resume_tiny.yaml\")\n",
        "\n",
        "if not BASE_CONFIG_PATH.exists():\n",
        "    raise FileNotFoundError(f\"Base data config not found: {BASE_CONFIG_PATH}\")\n",
        "\n",
        "with BASE_CONFIG_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
        "    base_cfg = yaml.safe_load(f)\n",
        "\n",
        "# Override name/version/description for the tiny dataset\n",
        "# Bump version whenever the tiny dataset generation logic changes materially\n",
        "base_cfg[\"name\"] = \"resume-ner-data-tiny-short\"\n",
        "base_cfg[\"version\"] = \"v2\"\n",
        "base_cfg[\"description\"] = \"Tiny smoke-test subset of Resume NER dataset (short-text version for fast orchestration tests)\"\n",
        "\n",
        "with TINY_CONFIG_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
        "    yaml.safe_dump(base_cfg, f, sort_keys=False)\n",
        "\n",
        "print(\"Wrote tiny data config to:\", TINY_CONFIG_PATH.resolve())\n",
        "print(\"Config contents:\\n\", yaml.safe_dump(base_cfg, sort_keys=False))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
