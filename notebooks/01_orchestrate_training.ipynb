{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Training Orchestration\n",
    "\n",
    "This notebook orchestrates all training activities without performing local computation.\n",
    "\n",
    "## Overview\n",
    "\n",
    "- **Step 1**: Load Centralized Configs\n",
    "- **Step 2**: Data Ingestion & Versioning (Asset Layer)\n",
    "- **Step 3**: Environment Definition\n",
    "- **Step 4**: The Dry Run\n",
    "- **Step 5**: The Sweep (HPO)\n",
    "- **Step 6**: Best Configuration Selection (Automated)\n",
    "- **Step 7**: Final Training (Post-HPO, Single Run)\n",
    "\n",
    "## Important\n",
    "\n",
    "- This notebook **only submits and monitors Azure ML jobs**\n",
    "- **No training logic** is executed locally\n",
    "- All computation happens remotely on Azure ML compute\n",
    "- The notebook must be **re-runnable end-to-end**\n",
    "\n",
    "## Platform Adapter Architecture\n",
    "\n",
    "The training and conversion scripts (`src/train.py` and `src/convert_to_onnx.py`) use a **platform adapter pattern** that automatically detects the execution environment (Azure ML vs local) and adapts accordingly:\n",
    "\n",
    "- **Output paths**: Automatically resolves Azure ML output directories via `AZURE_ML_OUTPUT_*` environment variables\n",
    "- **Logging**: Handles both MLflow and Azure ML native logging seamlessly\n",
    "- **MLflow context**: Manages MLflow runs appropriately for each platform\n",
    "- **Checkpoint resolution**: Handles Azure ML mounted inputs and local file paths\n",
    "\n",
    "This architecture allows the same code to run consistently on both Azure ML and local setups. When jobs run in Azure ML, the adapters automatically detect the Azure ML environment and use Azure-specific implementations. See `docs/PLATFORM_ADAPTER_ARCHITECTURE.md` for details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.1: Load Centralized Configs\n",
    "\n",
    "Load and validate all configuration files. Configs are immutable and will be logged with each job for reproducibility.\n",
    "\n",
    "**Note**: The training and conversion scripts executed by these jobs use platform adapters that automatically handle Azure ML-specific concerns (output paths, logging, MLflow context) without requiring explicit configuration in this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install azureml-mlflow --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Ensure we can import the orchestration package and shared utilities\n",
    "import sys\n",
    "ROOT_DIR = Path(\"..\").resolve()\n",
    "SRC_DIR = ROOT_DIR / \"src\"\n",
    "sys.path.append(str(ROOT_DIR))\n",
    "sys.path.append(str(SRC_DIR))\n",
    "\n",
    "from shared.yaml_utils import load_yaml\n",
    "from shared.json_cache import save_json, load_json\n",
    "from orchestration import (\n",
    "    STAGE_SMOKE,\n",
    "    STAGE_HPO,\n",
    "    STAGE_TRAINING,\n",
    "    EXPERIMENT_NAME,\n",
    "    MODEL_NAME,\n",
    "    PROD_STAGE,\n",
    "    build_aml_experiment_name,\n",
    ")\n",
    "from orchestration.config_loader import (\n",
    "    ExperimentConfig,\n",
    "    create_config_metadata,\n",
    "    load_all_configs,\n",
    "    load_experiment_config,\n",
    "    compute_config_hashes,\n",
    "    snapshot_configs,\n",
    "    validate_config_immutability,\n",
    ")\n",
    "\n",
    "\n",
    "env_path = Path(\"../config.env\")\n",
    "if env_path.exists():\n",
    "    load_dotenv(env_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_DIR = Path(\"../config\")\n",
    "\n",
    "# Experiment selection (switch to try different data/model/HPO/env combos)\n",
    "# The concrete experiment definition lives in config/experiment/<EXPERIMENT_NAME>.yaml\n",
    "\n",
    "# Resolve experiment-level config into concrete file paths\n",
    "experiment_config: ExperimentConfig = load_experiment_config(CONFIG_DIR, EXPERIMENT_NAME)\n",
    "configs = load_all_configs(experiment_config)\n",
    "config_hashes = compute_config_hashes(configs)\n",
    "\n",
    "# Immutable snapshots for runtime mutation checks\n",
    "original_configs = snapshot_configs(configs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse shared immutability validator from orchestration package\n",
    "validate_config_immutability(configs, original_configs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_workspace_name(configs: Dict[str, Any]) -> str:\n",
    "    \"\"\"Resolve the Azure ML workspace name from configuration files.\n",
    "\n",
    "    Order of precedence:\n",
    "    1. ``config/infrastructure.yaml`` (``workspace.name``)\n",
    "    2. ``config/env/azure.yaml`` (``workspace.name`` under ``env`` config)\n",
    "\n",
    "    This function is pure: it only reads configuration objects and files,\n",
    "    and does not perform any network or Azure ML operations.\n",
    "    \"\"\"\n",
    "    infrastructure_config_path = Path(\"../config/infrastructure.yaml\")\n",
    "    if infrastructure_config_path.exists():\n",
    "        infrastructure_config = load_yaml(infrastructure_config_path)\n",
    "        return infrastructure_config[\"workspace\"][\"name\"]\n",
    "\n",
    "    env_workspace = configs[\"env\"].get(\"workspace\", {}).get(\"name\")\n",
    "    if env_workspace:\n",
    "        return env_workspace\n",
    "\n",
    "    raise ValueError(\n",
    "        \"Workspace name must be configured in either \"\n",
    "        \"config/infrastructure.yaml (workspace.name) or config/env/azure.yaml (workspace.name).\"\n",
    "    )\n",
    "\n",
    "\n",
    "def create_ml_client(configs: Dict[str, Any]) -> MLClient:\n",
    "    \"\"\"Create an MLClient instance for the configured Azure ML workspace.\n",
    "\n",
    "    This function is responsible for reading required environment variables\n",
    "    and instantiating the Azure ML client. It assumes that configuration\n",
    "    loading has already completed.\n",
    "    \"\"\"\n",
    "    subscription_id = os.getenv(\"AZURE_SUBSCRIPTION_ID\")\n",
    "    resource_group = os.getenv(\"AZURE_RESOURCE_GROUP\")\n",
    "\n",
    "    if not subscription_id or not resource_group:\n",
    "        raise ValueError(\"AZURE_SUBSCRIPTION_ID and AZURE_RESOURCE_GROUP must be set\")\n",
    "\n",
    "    workspace_name = get_workspace_name(configs)\n",
    "    credential = DefaultAzureCredential()\n",
    "    return MLClient(\n",
    "        credential=credential,\n",
    "        subscription_id=subscription_id,\n",
    "        resource_group_name=resource_group,\n",
    "        workspace_name=workspace_name,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate MLClient for the configured workspace\n",
    "ml_client = create_ml_client(configs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All configs and their hashes will be attached to each Azure ML job for full reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build config metadata for job tagging using shared helper from\n",
    "# `orchestration.config_loader`.\n",
    "config_metadata = create_config_metadata(configs, config_hashes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.2: Data Ingestion & Versioning (Asset Layer)\n",
    "\n",
    "Upload dataset to Blob Storage and register as an Azure ML Data Asset for versioned, immutable data access.\n",
    "\n",
    "**Note**: The training script accepts data asset paths and can work with both Azure ML data assets (when running in Azure ML) and local file paths (when running locally), thanks to the platform adapter architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orchestration.data_assets import (\n",
    "    resolve_dataset_path,\n",
    "    register_data_asset,\n",
    "    ensure_data_asset_uploaded,\n",
    "    build_data_asset_reference,\n",
    ")\n",
    "\n",
    "# Resolve local dataset path from data config (configs[\"data\"][\"local_path\"])\n",
    "DATASET_LOCAL_PATH = resolve_dataset_path(configs[\"data\"])\n",
    "DATA_ASSET_NAME = configs[\"data\"][\"name\"]\n",
    "DATA_ASSET_VERSION = configs[\"data\"][\"version\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ASSET_OVERRIDE_PATH = None\n",
    "blob_uri = DATA_ASSET_OVERRIDE_PATH or str(DATASET_LOCAL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_asset = register_data_asset(\n",
    "    ml_client=ml_client,\n",
    "    name=DATA_ASSET_NAME,\n",
    "    version=DATA_ASSET_VERSION,\n",
    "    uri=blob_uri,\n",
    "    description=configs[\"data\"][\"description\"],\n",
    ")\n",
    "\n",
    "# Best-effort upload of local content to the resolved data asset\n",
    "data_asset = ensure_data_asset_uploaded(\n",
    "    ml_client=ml_client,\n",
    "    data_asset=data_asset,\n",
    "    local_path=DATASET_LOCAL_PATH,\n",
    "    description=configs[\"data\"][\"description\"],\n",
    ")\n",
    "\n",
    "# Build shared references for downstream jobs\n",
    "asset_paths = build_data_asset_reference(ml_client, data_asset)\n",
    "asset_reference = asset_paths[\"asset_uri\"]\n",
    "datastore_path = asset_paths[\"datastore_path\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Troubleshooting\n",
    "\n",
    "If you encounter `ScriptExecution.StreamAccess.NotFound`, verify that:\n",
    "1. Compute cluster has managed identity assigned\n",
    "2. Managed identity has \"Storage Blob Data Reader\" role on storage account\n",
    "3. Storage account firewall allows Azure services\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data asset info to a JSON file\n",
    "data_asset_cache_file = Path(\"data_asset_cache.json\")\n",
    "\n",
    "if \"data_asset\" in globals() and data_asset is not None:\n",
    "    data_asset_info = {\n",
    "        \"name\": data_asset.name,\n",
    "        \"version\": data_asset.version,\n",
    "        \"asset_paths\": asset_paths,\n",
    "    }\n",
    "\n",
    "    save_json(data_asset_cache_file, data_asset_info)\n",
    "    print(\n",
    "        f\"Saved data asset: {data_asset_info['name']} {data_asset_info['version']} \"\n",
    "        f\"to {data_asset_cache_file}\"\n",
    "    )\n",
    "else:\n",
    "    print(\"No data asset to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logged Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orchestration.data_assets import build_data_asset_reference\n",
    "\n",
    "# Try to reload from cache\n",
    "data_asset_cache_file = Path(\"data_asset_cache.json\")\n",
    "\n",
    "data_asset_info = load_json(data_asset_cache_file, default=None)\n",
    "\n",
    "if data_asset_info is None:\n",
    "    print(\n",
    "        f\"Cache file {data_asset_cache_file} not found. \"\n",
    "        \"Will need to register data asset.\"\n",
    "    )\n",
    "    data_asset = None\n",
    "else:\n",
    "    try:\n",
    "        # Reload Data asset object from ML client\n",
    "        data_asset = ml_client.data.get(\n",
    "            name=data_asset_info[\"name\"],\n",
    "            version=data_asset_info[\"version\"],\n",
    "        )\n",
    "\n",
    "        # Rebuild asset_paths if they were saved, otherwise regenerate them\n",
    "        asset_paths = data_asset_info.get(\"asset_paths\") or build_data_asset_reference(\n",
    "            ml_client, data_asset\n",
    "        )\n",
    "\n",
    "        asset_reference = asset_paths[\"asset_uri\"]\n",
    "        datastore_path = asset_paths[\"datastore_path\"]\n",
    "\n",
    "        print(f\"Loaded data asset: {data_asset.name} v{data_asset.version}\")\n",
    "        print(f\"Asset URI: {asset_reference}\")\n",
    "        print(\"Skipping data asset registration - using cached asset\")\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            \"Warning: Could not load data asset \"\n",
    "            f\"{data_asset_info['name']} v{data_asset_info['version']}: {e}\"\n",
    "        )\n",
    "        print(\"Will need to register data asset again\")\n",
    "        data_asset = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.3: Environment Definition\n",
    "\n",
    "Define a stable execution environment (Docker image + Conda dependencies) for consistent behavior across all training jobs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orchestration.environment import (\n",
    "    build_environment_config,\n",
    "    create_training_environment,\n",
    "    prepare_environment_image,\n",
    ")\n",
    "\n",
    "# Build environment configuration from env.yaml (with sensible defaults)\n",
    "env_config = build_environment_config(CONFIG_DIR, configs[\"env\"])\n",
    "\n",
    "# Materialize or fetch the Azure ML Environment\n",
    "training_environment = create_training_environment(ml_client, env_config)\n",
    "\n",
    "# Trigger a small warm-up job so the image is built/cached before real work\n",
    "prepare_environment_image(\n",
    "    ml_client=ml_client,\n",
    "    environment=training_environment,\n",
    "    compute_cluster=configs[\"env\"][\"compute\"][\"training_cluster\"],\n",
    "    env_config=env_config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save environment info to a JSON file\n",
    "env_cache_file = Path(\"training_environment_cache.json\")\n",
    "\n",
    "if 'training_environment' in globals() and training_environment is not None:\n",
    "    env_data = {\n",
    "        \"name\": training_environment.name,\n",
    "        \"version\": training_environment.version,\n",
    "    }\n",
    "\n",
    "    save_json(env_cache_file, env_data)\n",
    "    print(f\"Saved training environment: {env_data['name']} v{env_data['version']} to {env_cache_file}\")\n",
    "else:\n",
    "    print(\"No training environment to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logged Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to reload from cache\n",
    "env_cache_file = Path(\"training_environment_cache.json\")\n",
    "\n",
    "env_data = load_json(env_cache_file, default=None)\n",
    "\n",
    "if env_data is None:\n",
    "    print(f\"Cache file {env_cache_file} not found. Will need to create environment.\")\n",
    "    training_environment = None\n",
    "else:\n",
    "    try:\n",
    "        # Reload Environment object from ML client\n",
    "        training_environment = ml_client.environments.get(\n",
    "            name=env_data[\"name\"],\n",
    "            version=env_data[\"version\"]\n",
    "        )\n",
    "        print(f\"Loaded training environment: {training_environment.name} v{training_environment.version}\")\n",
    "        print(\"Skipping environment setup - using cached environment\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load environment {env_data['name']} v{env_data['version']}: {e}\")\n",
    "        print(\"Will need to create environment again\")\n",
    "        training_environment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.4: The Dry Run\n",
    "\n",
    "Submit a minimal sweep job using `smoke.yaml` to validate the sweep mechanism and pipeline integrity before launching the production HPO sweep.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orchestration.jobs import (\n",
    "    create_dry_run_sweep_job_for_backbone,\n",
    "    submit_and_wait_for_job,\n",
    "    validate_sweep_job,\n",
    ")\n",
    "\n",
    "TRAINING_SCRIPT_PATH = Path(\"../src/train.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_cluster_name = configs[\"env\"][\"compute\"][\"training_cluster\"]\n",
    "\n",
    "try:\n",
    "    compute_cluster = ml_client.compute.get(compute_cluster_name)\n",
    "    if compute_cluster.provisioning_state != \"Succeeded\":\n",
    "        raise ValueError(f\"Compute cluster not ready: {compute_cluster.provisioning_state}\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Compute cluster '{compute_cluster_name}' not accessible: {e}\")\n",
    "\n",
    "stage_name = STAGE_SMOKE\n",
    "smoke_hpo_config = configs[\"hpo\"]\n",
    "\n",
    "# Backbones are controlled by the HPO config file (single source of truth)\n",
    "backbone_values = smoke_hpo_config[\"search_space\"][\"backbone\"][\"values\"]\n",
    "\n",
    "dry_run_sweep_jobs = {}\n",
    "\n",
    "for backbone in backbone_values:\n",
    "    aml_experiment_name = build_aml_experiment_name(\n",
    "        experiment_name=experiment_config.name,\n",
    "        stage=stage_name,\n",
    "        backbone=backbone,\n",
    "    )\n",
    "    dry_run_sweep_jobs[backbone] = create_dry_run_sweep_job_for_backbone(\n",
    "        script_path=TRAINING_SCRIPT_PATH,\n",
    "        data_asset=data_asset,\n",
    "        environment=training_environment,\n",
    "        compute_cluster=compute_cluster_name,\n",
    "        backbone=backbone,\n",
    "        smoke_hpo_config=smoke_hpo_config,\n",
    "        configs=configs,\n",
    "        config_metadata=config_metadata,\n",
    "        aml_experiment_name=aml_experiment_name,\n",
    "        stage=stage_name,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for backbone, sweep_job in dry_run_sweep_jobs.items():\n",
    "    completed_job = submit_and_wait_for_job(ml_client, sweep_job)\n",
    "    validate_sweep_job(\n",
    "        job=completed_job,\n",
    "        backbone=backbone,\n",
    "        job_type=\"Dry run sweep\",\n",
    "        ml_client=ml_client,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.5: The Sweep (HPO)\n",
    "\n",
    "Submit a hyperparameter optimization sweep to systematically search for the best model configuration.\n",
    "\n",
    "**Note**: Currently using `smoke.yaml` for demonstration purposes (CPU-only setup). For production with GPU, switch to `prod.yaml` in the configuration.\n",
    "\n",
    "**Platform Adapter Note**: Each training trial in the sweep automatically uses the platform adapter to handle Azure ML-specific concerns. The adapter ensures consistent behavior across all trials regardless of the execution environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orchestration.jobs import (\n",
    "    create_hpo_sweep_job_for_backbone,\n",
    "    submit_and_wait_for_job,\n",
    "    validate_sweep_job,\n",
    ")\n",
    "\n",
    "TRAINING_SCRIPT_PATH = Path(\"../src/train.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_cluster_name = configs[\"env\"][\"compute\"][\"training_cluster\"]\n",
    "\n",
    "try:\n",
    "    compute_cluster = ml_client.compute.get(compute_cluster_name)\n",
    "    if compute_cluster.provisioning_state != \"Succeeded\":\n",
    "        raise ValueError(f\"Compute cluster not ready: {compute_cluster.provisioning_state}\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Compute cluster '{compute_cluster_name}' not accessible: {e}\")\n",
    "\n",
    "stage_name = STAGE_HPO\n",
    "hpo_config = configs[\"hpo\"]\n",
    "backbone_values = hpo_config[\"search_space\"][\"backbone\"][\"values\"]\n",
    "hpo_sweep_jobs = {}\n",
    "\n",
    "for backbone in backbone_values:\n",
    "    aml_experiment_name = build_aml_experiment_name(\n",
    "        experiment_name=experiment_config.name,\n",
    "        stage=stage_name,\n",
    "        backbone=backbone,\n",
    "    )\n",
    "    hpo_sweep_jobs[backbone] = create_hpo_sweep_job_for_backbone(\n",
    "        script_path=TRAINING_SCRIPT_PATH,\n",
    "        data_asset=data_asset,\n",
    "        environment=training_environment,\n",
    "        compute_cluster=compute_cluster_name,\n",
    "        hpo_config=hpo_config,\n",
    "        backbone=backbone,\n",
    "        aml_experiment_name=aml_experiment_name,\n",
    "        stage=stage_name,\n",
    "        configs=configs,\n",
    "        config_metadata=config_metadata,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo_completed_jobs = {}\n",
    "\n",
    "for backbone, sweep_job in hpo_sweep_jobs.items():\n",
    "    completed_job = submit_and_wait_for_job(ml_client, sweep_job)\n",
    "    validate_sweep_job(\n",
    "        job=completed_job,\n",
    "        backbone=backbone,\n",
    "        job_type=\"HPO sweep\",\n",
    "        min_expected_trials=2,\n",
    "        ml_client=ml_client,\n",
    "    )\n",
    "    hpo_completed_jobs[backbone] = completed_job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save HPO job references to a JSON file\n",
    "hpo_jobs_cache_file = Path(\"hpo_completed_jobs_cache.json\")\n",
    "\n",
    "if hpo_completed_jobs:\n",
    "    hpo_jobs_data = {\n",
    "        backbone: {\n",
    "            \"job_name\": job.name,\n",
    "            \"job_id\": job.id,\n",
    "        }\n",
    "        for backbone, job in hpo_completed_jobs.items()\n",
    "    }\n",
    "\n",
    "    save_json(hpo_jobs_cache_file, hpo_jobs_data)\n",
    "    print(f\"Saved {len(hpo_jobs_data)} HPO job references to {hpo_jobs_cache_file}\")\n",
    "else:\n",
    "    print(\"No HPO completed jobs to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logged Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to reload HPO jobs from cache\n",
    "hpo_jobs_cache_file = Path(\"hpo_completed_jobs_cache.json\")\n",
    "\n",
    "hpo_jobs_data = load_json(hpo_jobs_cache_file, default=None)\n",
    "\n",
    "if hpo_jobs_data is None:\n",
    "    print(f\"Cache file {hpo_jobs_cache_file} not found. Will need to run HPO.\")\n",
    "    hpo_completed_jobs = {}\n",
    "else:\n",
    "    hpo_completed_jobs = {}\n",
    "    for backbone, job_info in hpo_jobs_data.items():\n",
    "        try:\n",
    "            job = ml_client.jobs.get(job_info[\"job_name\"])\n",
    "            hpo_completed_jobs[backbone] = job\n",
    "            print(f\"Loaded HPO job for {backbone}: {job.name} (status: {job.status})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load job {job_info['job_name']} for {backbone}: {e}\")\n",
    "\n",
    "    if hpo_completed_jobs:\n",
    "        print(f\"\\nSuccessfully reloaded {len(hpo_completed_jobs)} HPO completed jobs from cache\")\n",
    "    else:\n",
    "        print(\"No valid jobs found in cache, will need to run HPO again\")\n",
    "        hpo_completed_jobs = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.6: Best Configuration Selection (Automated)\n",
    "\n",
    "Programmatically select the best configuration from all HPO sweep runs across all backbone models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orchestration.jobs import select_best_configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best configuration from all HPO sweep runs\n",
    "best_configuration = select_best_configuration(\n",
    "    ml_client=ml_client,\n",
    "    hpo_completed_jobs=hpo_completed_jobs,\n",
    "    hpo_config=configs[\"hpo\"],\n",
    "    dataset_version=configs[\"data\"][\"version\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best configuration to a JSON file\n",
    "best_config_cache_file = Path(\"best_configuration_cache.json\")\n",
    "\n",
    "if \"best_configuration\" in globals() and best_configuration is not None:\n",
    "    # best_configuration contains trial_name, trial_id, backbone, hyperparameters, metrics, etc.\n",
    "    # All of these are JSON-serializable\n",
    "    save_json(best_config_cache_file, best_configuration)\n",
    "    print(f\"Saved best configuration to {best_config_cache_file}\")\n",
    "    print(f\"  Backbone: {best_configuration.get('backbone')}\")\n",
    "    print(f\"  Best metric value: {best_configuration.get('selection_criteria', {}).get('best_value')}\")\n",
    "else:\n",
    "    print(\"No best configuration to save\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logged Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to reload from cache\n",
    "best_config_cache_file = Path(\"best_configuration_cache.json\")\n",
    "\n",
    "best_configuration = load_json(best_config_cache_file, default=None)\n",
    "\n",
    "if best_configuration is None:\n",
    "    print(f\"Cache file {best_config_cache_file} not found. Will need to run Step P1-3.6.\")\n",
    "else:\n",
    "    print(f\"Loaded best configuration from cache:\")\n",
    "    print(f\"  Backbone: {best_configuration.get('backbone')}\")\n",
    "    print(f\"  Trial: {best_configuration.get('trial_name')}\")\n",
    "    print(f\"  Best metric value: {best_configuration.get('selection_criteria', {}).get('best_value')}\")\n",
    "    print(f\"  Dataset version: {best_configuration.get('dataset_version')}\")\n",
    "    print(f\"\\nSkipping best configuration selection - using cached result\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.7: Final Training (Post-HPO, Single Run)\n",
    "\n",
    "Train the final production model using the best configuration from HPO with stable, controlled conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orchestration.jobs import (\n",
    "    build_final_training_config,\n",
    "    create_final_training_job,\n",
    "    validate_final_training_job,\n",
    "    submit_and_wait_for_job\n",
    ")\n",
    "\n",
    "TRAINING_SCRIPT_PATH = Path(\"../src/train.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build final training config from best HPO result + train.yaml defaults\n",
    "final_training_config = build_final_training_config(best_configuration, configs[\"train\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_cluster_name = configs[\"env\"][\"compute\"][\"training_cluster\"]\n",
    "\n",
    "try:\n",
    "    compute_cluster = ml_client.compute.get(compute_cluster_name)\n",
    "    if compute_cluster.provisioning_state != \"Succeeded\":\n",
    "        raise ValueError(f\"Compute cluster not ready: {compute_cluster.provisioning_state}\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Compute cluster '{compute_cluster_name}' not accessible: {e}\")\n",
    "    \n",
    "# Create and submit final training job\n",
    "stage_name = STAGE_TRAINING\n",
    "aml_experiment_name = build_aml_experiment_name(\n",
    "    experiment_name=experiment_config.name,\n",
    "    stage=stage_name,\n",
    "    backbone=final_training_config[\"backbone\"],\n",
    ")\n",
    "\n",
    "final_training_tags = {\n",
    "    **config_metadata,\n",
    "    \"job_type\": \"final_training\",\n",
    "    \"backbone\": final_training_config[\"backbone\"],\n",
    "    \"best_trial\": best_configuration[\"trial_name\"],\n",
    "    \"best_metric_value\": str(best_configuration[\"selection_criteria\"][\"best_value\"]),\n",
    "    \"stage\": stage_name,\n",
    "}\n",
    "\n",
    "final_training_job = create_final_training_job(\n",
    "    script_path=TRAINING_SCRIPT_PATH,\n",
    "    data_asset=data_asset,\n",
    "    environment=training_environment,\n",
    "    compute_cluster=compute_cluster_name,\n",
    "    final_config=final_training_config,\n",
    "    aml_experiment_name=aml_experiment_name,\n",
    "    tags=final_training_tags,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit and validate final training job\n",
    "final_training_completed_job = submit_and_wait_for_job(ml_client, final_training_job)\n",
    "validate_final_training_job(final_training_completed_job)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_training_cache_file = Path(\"final_training_job_cache.json\")\n",
    "\n",
    "if \"final_training_completed_job\" in globals() and final_training_completed_job is not None:\n",
    "    data = {\n",
    "        \"job_name\": final_training_completed_job.name,\n",
    "        \"job_id\": final_training_completed_job.id,\n",
    "    }\n",
    "    save_json(final_training_cache_file, data)\n",
    "    print(f\"Saved final training job reference to {final_training_cache_file}\")\n",
    "else:\n",
    "    print(\"No final training job to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logged Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_training_cache_file = Path(\"final_training_job_cache.json\")\n",
    "\n",
    "data = load_json(final_training_cache_file, default=None)\n",
    "\n",
    "if data is None:\n",
    "    print(f\"Cache file {final_training_cache_file} not found. Will need to run Step P1-3.7: Final Training.\")\n",
    "    final_training_completed_job = None\n",
    "else:\n",
    "    try:\n",
    "        final_training_completed_job = ml_client.jobs.get(data[\"job_name\"])\n",
    "        print(f\"Loaded final training job: {final_training_completed_job.name} (status: {final_training_completed_job.status})\")\n",
    "        \n",
    "        # Validate that the job has a checkpoint output\n",
    "        if not hasattr(final_training_completed_job, \"outputs\") or \"checkpoint\" not in final_training_completed_job.outputs:\n",
    "            print(f\"\\n⚠️  WARNING: Training job {final_training_completed_job.name} does not have a 'checkpoint' output.\")\n",
    "            print(\"   This job cannot be used for model conversion.\")\n",
    "            print(\"   Please re-run Step P1-3.7: Final Training to generate a new job with checkpoint output.\")\n",
    "            final_training_completed_job = None\n",
    "        else:\n",
    "            print(\"✓ Training job has checkpoint output\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not reload final training job {data['job_name']}: {e}\")\n",
    "        final_training_completed_job = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-4: Model Conversion & Optimization\n",
    "\n",
    "Convert the final training checkpoint to an optimized ONNX model (int8 quantized) for production inference.\n",
    "\n",
    "**Platform Adapter Note**: The conversion script (`src/convert_to_onnx.py`) uses the platform adapter to:\n",
    "- Resolve checkpoint paths from Azure ML mounted inputs\n",
    "- Handle output paths for the ONNX model (via `AZURE_ML_OUTPUT_onnx_model`)\n",
    "- Manage logging and MLflow context appropriately\n",
    "\n",
    "The adapter automatically detects the Azure ML environment and uses the appropriate implementations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_training_completed_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orchestration.jobs import (\n",
    "    get_checkpoint_output_from_training_job,\n",
    "    create_conversion_job,\n",
    "    validate_conversion_job,\n",
    "    submit_and_wait_for_job,\n",
    ")\n",
    "\n",
    "CONVERSION_SCRIPT_PATH = Path(\"../src/convert_to_onnx.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guard: ensure final_training_completed_job is set and has checkpoint output\n",
    "if \"final_training_completed_job\" not in globals() or final_training_completed_job is None:\n",
    "    raise ValueError(\n",
    "        \"final_training_completed_job is not set. \"\n",
    "        \"Please run Step P1-3.7: Final Training first, or ensure the cached job has a checkpoint output.\"\n",
    "    )\n",
    "\n",
    "# Guard: ensure ml_client is defined (required for fetching checkpoint data asset)\n",
    "if \"ml_client\" not in globals() or ml_client is None:\n",
    "    raise ValueError(\n",
    "        \"ml_client is not defined. \"\n",
    "        \"Please run the cells that set up ml_client (Step P1-3.1) before running this cell.\"\n",
    "    )\n",
    "\n",
    "checkpoint_output = get_checkpoint_output_from_training_job(final_training_completed_job, ml_client=ml_client)\n",
    "print(f\"✓ Retrieved checkpoint output: {checkpoint_output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_cluster_name = configs[\"env\"][\"compute\"][\"conversion_cluster\"]\n",
    "conversion_experiment_name = configs[\"env\"][\"logging\"][\"experiment_name\"]\n",
    "\n",
    "conversion_tags = {\n",
    "    **config_metadata,\n",
    "    \"job_type\": \"model_conversion\",\n",
    "    \"backbone\": best_configuration[\"backbone\"],\n",
    "    \"source_training_job\": final_training_completed_job.name,\n",
    "    \"quantization\": \"int8\",\n",
    "}\n",
    "\n",
    "conversion_job = create_conversion_job(\n",
    "    script_path=CONVERSION_SCRIPT_PATH,\n",
    "    checkpoint_uri=str(checkpoint_output),\n",
    "    environment=training_environment,\n",
    "    compute_cluster=conversion_cluster_name,\n",
    "    backbone=best_configuration[\"backbone\"],\n",
    "    experiment_name=conversion_experiment_name,\n",
    "    tags=conversion_tags,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_completed_job = submit_and_wait_for_job(ml_client, conversion_job)\n",
    "validate_conversion_job(conversion_completed_job, ml_client=ml_client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_cache_file = Path(\"conversion_job_cache.json\")\n",
    "\n",
    "if \"conversion_completed_job\" in globals() and conversion_completed_job is not None:\n",
    "    data = {\n",
    "        \"job_name\": conversion_completed_job.name,\n",
    "        \"job_id\": conversion_completed_job.id,\n",
    "    }\n",
    "    save_json(conversion_cache_file, data)\n",
    "    print(f\"Saved conversion job reference to {conversion_cache_file}\")\n",
    "else:\n",
    "    print(\"No conversion job to save\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logged Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_cache_file = Path(\"conversion_job_cache.json\")\n",
    "\n",
    "data = load_json(conversion_cache_file, default=None)\n",
    "\n",
    "if data is None:\n",
    "    print(f\"Cache file {conversion_cache_file} not found. Will need to run Step P1-4: Model Conversion.\")\n",
    "    conversion_completed_job = None\n",
    "else:\n",
    "    try:\n",
    "        conversion_completed_job = ml_client.jobs.get(data[\"job_name\"])\n",
    "        print(f\"Loaded conversion job: {conversion_completed_job.name} (status: {conversion_completed_job.status})\")\n",
    "        \n",
    "        # Validate that the job has an onnx_model output\n",
    "        if not hasattr(conversion_completed_job, \"outputs\") or \"onnx_model\" not in conversion_completed_job.outputs:\n",
    "            print(f\"\\n⚠️  WARNING: Conversion job {conversion_completed_job.name} does not have an 'onnx_model' output.\")\n",
    "            print(\"   This job cannot be used for model registration.\")\n",
    "            print(\"   Please re-run Step P1-4: Model Conversion to generate a new job with ONNX model output.\")\n",
    "            conversion_completed_job = None\n",
    "        else:\n",
    "            print(\"✓ Conversion job has ONNX model output\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not reload conversion job {data['job_name']}: {e}\")\n",
    "        conversion_completed_job = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-5: Model Registration (The Handover)\n",
    "\n",
    "Register the optimized ONNX model in Azure ML Model Registry with full metadata for production deployment.\n",
    "\n",
    "**Platform Adapter Note**: The conversion job's ONNX model output is automatically handled by the platform adapter. The model path is resolved from the Azure ML job output and registered in the model registry with full traceability back to the training and conversion jobs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Model\n",
    "from azure.core.exceptions import ResourceNotFoundError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onnx_model_path(conversion_job: Job) -> str:\n",
    "    \"\"\"\n",
    "    Get ONNX model path from completed conversion job.\n",
    "    \n",
    "    Args:\n",
    "        conversion_job: Completed conversion job\n",
    "        \n",
    "    Returns:\n",
    "        str: ONNX model path (Azure ML datastore URI)\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If ONNX model not found in job outputs\n",
    "    \"\"\"\n",
    "    if not hasattr(conversion_job, \"outputs\") or not conversion_job.outputs:\n",
    "        raise ValueError(\"Conversion job produced no outputs\")\n",
    "    \n",
    "    if \"onnx_model\" not in conversion_job.outputs:\n",
    "        raise ValueError(\"Conversion job missing 'onnx_model' output\")\n",
    "    \n",
    "    onnx_output = conversion_job.outputs[\"onnx_model\"]\n",
    "    \n",
    "    if hasattr(onnx_output, \"path\"):\n",
    "        return onnx_output.path\n",
    "    elif isinstance(onnx_output, str):\n",
    "        return onnx_output\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected ONNX output type: {type(onnx_output)}\")\n",
    "\n",
    "\n",
    "onnx_model_path = get_onnx_model_path(conversion_completed_job)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_model_version(best_config: Dict[str, Any], config_hashes: Dict[str, str]) -> str:\n",
    "    \"\"\"\n",
    "    Compute deterministic model version from configuration hashes.\n",
    "    \n",
    "    Args:\n",
    "        best_config: Best configuration from HPO selection\n",
    "        config_hashes: Configuration hashes dictionary\n",
    "        \n",
    "    Returns:\n",
    "        str: Model version string\n",
    "    \"\"\"\n",
    "    version_components = [\n",
    "        config_hashes[\"data\"],\n",
    "        config_hashes[\"model\"],\n",
    "        config_hashes[\"train\"],\n",
    "        best_config[\"backbone\"],\n",
    "    ]\n",
    "    version_str = \"_\".join(version_components)\n",
    "    version_hash = hashlib.sha256(version_str.encode()).hexdigest()[:CONFIG_HASH_LENGTH]\n",
    "    return f\"v{version_hash}\"\n",
    "\n",
    "\n",
    "model_version = compute_model_version(best_configuration, config_hashes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_production_model(\n",
    "    ml_client: MLClient,\n",
    "    model_name: str,\n",
    "    model_version: str,\n",
    "    model_path: str,\n",
    "    best_config: Dict[str, Any],\n",
    "    configs: Dict[str, Any],\n",
    "    config_metadata: Dict[str, str],\n",
    ") -> Model:\n",
    "    \"\"\"\n",
    "    Register optimized ONNX model in Azure ML Model Registry.\n",
    "    \n",
    "    Args:\n",
    "        ml_client: MLClient instance\n",
    "        model_name: Model name in registry\n",
    "        model_version: Model version\n",
    "        model_path: Path to ONNX model (Azure ML datastore URI)\n",
    "        best_config: Best configuration from HPO selection\n",
    "        configs: Configuration dictionaries\n",
    "        config_metadata: Configuration metadata for tagging\n",
    "        \n",
    "    Returns:\n",
    "        Model: Registered model instance\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If model path is invalid\n",
    "    \"\"\"\n",
    "    if not model_path or not model_path.endswith(\".onnx\"):\n",
    "        raise ValueError(f\"Invalid ONNX model path: {model_path}\")\n",
    "    \n",
    "    selection_criteria = best_config[\"selection_criteria\"]\n",
    "    \n",
    "    model_description = (\n",
    "        f\"Production ONNX model for Resume NER. \"\n",
    "        f\"Backbone: {selection_criteria['backbone']}, \"\n",
    "        f\"Metric: {selection_criteria['metric']}={selection_criteria['best_value']:.4f}\"\n",
    "    )\n",
    "    \n",
    "    model_tags = {\n",
    "        **config_metadata,\n",
    "        \"stage\": PROD_STAGE,\n",
    "        \"backbone\": selection_criteria[\"backbone\"],\n",
    "        \"metric\": selection_criteria[\"metric\"],\n",
    "        \"metric_value\": str(selection_criteria[\"best_value\"]),\n",
    "        \"dataset_version\": best_config[\"dataset_version\"],\n",
    "        \"model_format\": \"onnx\",\n",
    "        \"quantization\": \"int8\",\n",
    "        \"source_training_job\": final_training_completed_job.name,\n",
    "        \"source_conversion_job\": conversion_completed_job.name,\n",
    "    }\n",
    "    \n",
    "    model = Model(\n",
    "        name=model_name,\n",
    "        version=model_version,\n",
    "        description=model_description,\n",
    "        path=model_path,\n",
    "        tags=model_tags,\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        existing_model = ml_client.models.get(name=model_name, version=model_version)\n",
    "        return existing_model\n",
    "    except ResourceNotFoundError:\n",
    "        return ml_client.models.create_or_update(model)\n",
    "\n",
    "\n",
    "registered_model = register_production_model(\n",
    "    ml_client=ml_client,\n",
    "    model_name=MODEL_NAME,\n",
    "    model_version=model_version,\n",
    "    model_path=onnx_model_path,\n",
    "    best_config=best_configuration,\n",
    "    configs=configs,\n",
    "    config_metadata=config_metadata,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_registered_model(model: Model) -> None:\n",
    "    \"\"\"\n",
    "    Validate registered model has required metadata and tags.\n",
    "    \n",
    "    Args:\n",
    "        model: Registered model instance\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If validation fails\n",
    "    \"\"\"\n",
    "    required_tags = [\"stage\", \"backbone\", \"metric\", \"dataset_version\"]\n",
    "    for tag in required_tags:\n",
    "        if tag not in model.tags:\n",
    "            raise ValueError(f\"Registered model missing required tag: {tag}\")\n",
    "    \n",
    "    if model.tags.get(\"stage\") != PROD_STAGE:\n",
    "        raise ValueError(f\"Model stage must be '{PROD_STAGE}', got: {model.tags.get('stage')}\")\n",
    "    \n",
    "    if not model.path or not model.path.endswith(\".onnx\"):\n",
    "        raise ValueError(f\"Invalid model path: {model.path}\")\n",
    "\n",
    "\n",
    "validate_registered_model(registered_model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
