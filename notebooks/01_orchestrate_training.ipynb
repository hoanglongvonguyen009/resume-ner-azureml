{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Training Orchestration\n",
    "\n",
    "This notebook orchestrates all training activities without performing local computation.\n",
    "\n",
    "## Overview\n",
    "\n",
    "- **Step 1**: Load Centralized Configs\n",
    "- **Step 2**: Data Ingestion & Versioning (Asset Layer)\n",
    "- **Step 3**: Environment Definition\n",
    "- **Step 4**: The Dry Run\n",
    "- **Step 5**: The Sweep (HPO)\n",
    "- **Step 6**: Best Configuration Selection (Automated)\n",
    "- **Step 7**: Final Training (Post-HPO, Single Run)\n",
    "\n",
    "## Important\n",
    "\n",
    "- This notebook **only submits and monitors Azure ML jobs**\n",
    "- **No training logic** is executed locally\n",
    "- All computation happens remotely on Azure ML compute\n",
    "- The notebook must be **re-runnable end-to-end**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.1: Load Centralized Configs\n",
    "\n",
    "Load and validate all configuration files. Configs are immutable and will be logged with each job for reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install azureml-mlflow --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Ensure we can import the orchestration package and shared utilities\n",
    "import sys\n",
    "ROOT_DIR = Path(\"..\").resolve()\n",
    "SRC_DIR = ROOT_DIR / \"src\"\n",
    "sys.path.append(str(ROOT_DIR))\n",
    "sys.path.append(str(SRC_DIR))\n",
    "\n",
    "from shared.yaml_utils import load_yaml\n",
    "from shared.json_cache import save_json, load_json\n",
    "from orchestration import (\n",
    "    STAGE_SMOKE,\n",
    "    STAGE_HPO,\n",
    "    STAGE_TRAINING,\n",
    "    EXPERIMENT_NAME,\n",
    "    MODEL_NAME,\n",
    "    PROD_STAGE,\n",
    "    build_aml_experiment_name,\n",
    ")\n",
    "from orchestration.config_loader import (\n",
    "    ExperimentConfig,\n",
    "    create_config_metadata,\n",
    "    load_all_configs,\n",
    "    load_experiment_config,\n",
    "    compute_config_hashes,\n",
    "    snapshot_configs,\n",
    "    validate_config_immutability,\n",
    ")\n",
    "\n",
    "\n",
    "env_path = Path(\"../config.env\")\n",
    "if env_path.exists():\n",
    "    load_dotenv(env_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_DIR = Path(\"../config\")\n",
    "\n",
    "# Experiment selection (switch to try different data/model/HPO/env combos)\n",
    "# The concrete experiment definition lives in config/experiment/<EXPERIMENT_NAME>.yaml\n",
    "\n",
    "# Resolve experiment-level config into concrete file paths\n",
    "experiment_config: ExperimentConfig = load_experiment_config(CONFIG_DIR, EXPERIMENT_NAME)\n",
    "configs = load_all_configs(experiment_config)\n",
    "config_hashes = compute_config_hashes(configs)\n",
    "\n",
    "# Immutable snapshots for runtime mutation checks\n",
    "original_configs = snapshot_configs(configs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse shared immutability validator from orchestration package\n",
    "validate_config_immutability(configs, original_configs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_workspace_name(configs: Dict[str, Any]) -> str:\n",
    "    \"\"\"Resolve the Azure ML workspace name from configuration files.\n",
    "\n",
    "    Order of precedence:\n",
    "    1. ``config/infrastructure.yaml`` (``workspace.name``)\n",
    "    2. ``config/env/azure.yaml`` (``workspace.name`` under ``env`` config)\n",
    "\n",
    "    This function is pure: it only reads configuration objects and files,\n",
    "    and does not perform any network or Azure ML operations.\n",
    "    \"\"\"\n",
    "    infrastructure_config_path = Path(\"../config/infrastructure.yaml\")\n",
    "    if infrastructure_config_path.exists():\n",
    "        infrastructure_config = load_yaml(infrastructure_config_path)\n",
    "        return infrastructure_config[\"workspace\"][\"name\"]\n",
    "\n",
    "    env_workspace = configs[\"env\"].get(\"workspace\", {}).get(\"name\")\n",
    "    if env_workspace:\n",
    "        return env_workspace\n",
    "\n",
    "    raise ValueError(\n",
    "        \"Workspace name must be configured in either \"\n",
    "        \"config/infrastructure.yaml (workspace.name) or config/env/azure.yaml (workspace.name).\"\n",
    "    )\n",
    "\n",
    "\n",
    "def create_ml_client(configs: Dict[str, Any]) -> MLClient:\n",
    "    \"\"\"Create an MLClient instance for the configured Azure ML workspace.\n",
    "\n",
    "    This function is responsible for reading required environment variables\n",
    "    and instantiating the Azure ML client. It assumes that configuration\n",
    "    loading has already completed.\n",
    "    \"\"\"\n",
    "    subscription_id = os.getenv(\"AZURE_SUBSCRIPTION_ID\")\n",
    "    resource_group = os.getenv(\"AZURE_RESOURCE_GROUP\")\n",
    "\n",
    "    if not subscription_id or not resource_group:\n",
    "        raise ValueError(\"AZURE_SUBSCRIPTION_ID and AZURE_RESOURCE_GROUP must be set\")\n",
    "\n",
    "    workspace_name = get_workspace_name(configs)\n",
    "    credential = DefaultAzureCredential()\n",
    "    return MLClient(\n",
    "        credential=credential,\n",
    "        subscription_id=subscription_id,\n",
    "        resource_group_name=resource_group,\n",
    "        workspace_name=workspace_name,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class DeploymentTemplateOperations: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate MLClient for the configured workspace\n",
    "ml_client = create_ml_client(configs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All configs and their hashes will be attached to each Azure ML job for full reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build config metadata for job tagging using shared helper from\n",
    "# `orchestration.config_loader`.\n",
    "config_metadata = create_config_metadata(configs, config_hashes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.2: Data Ingestion & Versioning (Asset Layer)\n",
    "\n",
    "Upload dataset to Blob Storage and register as an Azure ML Data Asset for versioned, immutable data access.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orchestration.data_assets import (\n",
    "    resolve_dataset_path,\n",
    "    register_data_asset,\n",
    "    ensure_data_asset_uploaded,\n",
    "    build_data_asset_reference,\n",
    ")\n",
    "\n",
    "# Resolve local dataset path from data config (configs[\"data\"][\"local_path\"])\n",
    "DATASET_LOCAL_PATH = resolve_dataset_path(configs[\"data\"])\n",
    "DATA_ASSET_NAME = configs[\"data\"][\"name\"]\n",
    "DATA_ASSET_VERSION = configs[\"data\"][\"version\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ASSET_OVERRIDE_PATH = None\n",
    "blob_uri = DATA_ASSET_OVERRIDE_PATH or str(DATASET_LOCAL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_asset = register_data_asset(\n",
    "    ml_client=ml_client,\n",
    "    name=DATA_ASSET_NAME,\n",
    "    version=DATA_ASSET_VERSION,\n",
    "    uri=blob_uri,\n",
    "    description=configs[\"data\"][\"description\"],\n",
    ")\n",
    "\n",
    "# Best-effort upload of local content to the resolved data asset\n",
    "data_asset = ensure_data_asset_uploaded(\n",
    "    ml_client=ml_client,\n",
    "    data_asset=data_asset,\n",
    "    local_path=DATASET_LOCAL_PATH,\n",
    "    description=configs[\"data\"][\"description\"],\n",
    ")\n",
    "\n",
    "# Build shared references for downstream jobs\n",
    "asset_paths = build_data_asset_reference(ml_client, data_asset)\n",
    "asset_reference = asset_paths[\"asset_uri\"]\n",
    "datastore_path = asset_paths[\"datastore_path\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Troubleshooting\n",
    "\n",
    "If you encounter `ScriptExecution.StreamAccess.NotFound`, verify that:\n",
    "1. Compute cluster has managed identity assigned\n",
    "2. Managed identity has \"Storage Blob Data Reader\" role on storage account\n",
    "3. Storage account firewall allows Azure services\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data asset info to a JSON file\n",
    "data_asset_cache_file = Path(\"data_asset_cache.json\")\n",
    "\n",
    "if \"data_asset\" in globals() and data_asset is not None:\n",
    "    data_asset_info = {\n",
    "        \"name\": data_asset.name,\n",
    "        \"version\": data_asset.version,\n",
    "        \"asset_paths\": asset_paths,\n",
    "    }\n",
    "\n",
    "    save_json(data_asset_cache_file, data_asset_info)\n",
    "    print(\n",
    "        f\"Saved data asset: {data_asset_info['name']} v{data_asset_info['version']} \"\n",
    "        f\"to {data_asset_cache_file}\"\n",
    "    )\n",
    "else:\n",
    "    print(\"No data asset to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logged Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data asset: resume-ner-data-tiny-short vv2.1\n",
      "Asset URI: azureml:resume-ner-data-tiny-short:v2.1\n",
      "Skipping data asset registration - using cached asset\n"
     ]
    }
   ],
   "source": [
    "from orchestration.data_assets import build_data_asset_reference\n",
    "\n",
    "# Try to reload from cache\n",
    "data_asset_cache_file = Path(\"data_asset_cache.json\")\n",
    "\n",
    "data_asset_info = load_json(data_asset_cache_file, default=None)\n",
    "\n",
    "if data_asset_info is None:\n",
    "    print(\n",
    "        f\"Cache file {data_asset_cache_file} not found. \"\n",
    "        \"Will need to register data asset.\"\n",
    "    )\n",
    "    data_asset = None\n",
    "else:\n",
    "    try:\n",
    "        # Reload Data asset object from ML client\n",
    "        data_asset = ml_client.data.get(\n",
    "            name=data_asset_info[\"name\"],\n",
    "            version=data_asset_info[\"version\"],\n",
    "        )\n",
    "\n",
    "        # Rebuild asset_paths if they were saved, otherwise regenerate them\n",
    "        asset_paths = data_asset_info.get(\"asset_paths\") or build_data_asset_reference(\n",
    "            ml_client, data_asset\n",
    "        )\n",
    "\n",
    "        asset_reference = asset_paths[\"asset_uri\"]\n",
    "        datastore_path = asset_paths[\"datastore_path\"]\n",
    "\n",
    "        print(f\"Loaded data asset: {data_asset.name} v{data_asset.version}\")\n",
    "        print(f\"Asset URI: {asset_reference}\")\n",
    "        print(\"Skipping data asset registration - using cached asset\")\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            \"Warning: Could not load data asset \"\n",
    "            f\"{data_asset_info['name']} v{data_asset_info['version']}: {e}\"\n",
    "        )\n",
    "        print(\"Will need to register data asset again\")\n",
    "        data_asset = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.3: Environment Definition\n",
    "\n",
    "Define a stable execution environment (Docker image + Conda dependencies) for consistent behavior across all training jobs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orchestration.environment import (\n",
    "    build_environment_config,\n",
    "    create_training_environment,\n",
    "    prepare_environment_image,\n",
    ")\n",
    "\n",
    "# Build environment configuration from env.yaml (with sensible defaults)\n",
    "env_config = build_environment_config(CONFIG_DIR, configs[\"env\"])\n",
    "\n",
    "# Materialize or fetch the Azure ML Environment\n",
    "training_environment = create_training_environment(ml_client, env_config)\n",
    "\n",
    "# Trigger a small warm-up job so the image is built/cached before real work\n",
    "prepare_environment_image(\n",
    "    ml_client=ml_client,\n",
    "    environment=training_environment,\n",
    "    compute_cluster=configs[\"env\"][\"compute\"][\"training_cluster\"],\n",
    "    env_config=env_config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save environment info to a JSON file\n",
    "env_cache_file = Path(\"training_environment_cache.json\")\n",
    "\n",
    "if 'training_environment' in globals() and training_environment is not None:\n",
    "    env_data = {\n",
    "        \"name\": training_environment.name,\n",
    "        \"version\": training_environment.version,\n",
    "    }\n",
    "\n",
    "    save_json(env_cache_file, env_data)\n",
    "    print(f\"Saved training environment: {env_data['name']} v{env_data['version']} to {env_cache_file}\")\n",
    "else:\n",
    "    print(\"No training environment to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logged Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training environment: resume-ner-training vv8dba3aeec663aed1\n",
      "Skipping environment setup - using cached environment\n"
     ]
    }
   ],
   "source": [
    "# Try to reload from cache\n",
    "env_cache_file = Path(\"training_environment_cache.json\")\n",
    "\n",
    "env_data = load_json(env_cache_file, default=None)\n",
    "\n",
    "if env_data is None:\n",
    "    print(f\"Cache file {env_cache_file} not found. Will need to create environment.\")\n",
    "    training_environment = None\n",
    "else:\n",
    "    try:\n",
    "        # Reload Environment object from ML client\n",
    "        training_environment = ml_client.environments.get(\n",
    "            name=env_data[\"name\"],\n",
    "            version=env_data[\"version\"]\n",
    "        )\n",
    "        print(f\"Loaded training environment: {training_environment.name} v{training_environment.version}\")\n",
    "        print(\"Skipping environment setup - using cached environment\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load environment {env_data['name']} v{env_data['version']}: {e}\")\n",
    "        print(\"Will need to create environment again\")\n",
    "        training_environment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.4: The Dry Run\n",
    "\n",
    "Submit a minimal sweep job using `smoke.yaml` to validate the sweep mechanism and pipeline integrity before launching the production HPO sweep.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/current/lib/python3.12/site-packages/mlflow/__init__.py:41: UserWarning: Versions of mlflow (3.7.0) and child packages mlflow-skinny (3.5.0) are different. This may lead to unexpected behavior. Please install the same version of all MLflow packages.\n",
      "  mlflow.mismatch._check_version_mismatch()\n"
     ]
    }
   ],
   "source": [
    "from orchestration.jobs import (\n",
    "    create_dry_run_sweep_job_for_backbone,\n",
    "    submit_and_wait_for_job,\n",
    "    validate_sweep_job,\n",
    ")\n",
    "\n",
    "TRAINING_SCRIPT_PATH = Path(\"../src/train.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_cluster_name = configs[\"env\"][\"compute\"][\"training_cluster\"]\n",
    "\n",
    "try:\n",
    "    compute_cluster = ml_client.compute.get(compute_cluster_name)\n",
    "    if compute_cluster.provisioning_state != \"Succeeded\":\n",
    "        raise ValueError(f\"Compute cluster not ready: {compute_cluster.provisioning_state}\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Compute cluster '{compute_cluster_name}' not accessible: {e}\")\n",
    "\n",
    "stage_name = STAGE_SMOKE\n",
    "smoke_hpo_config = configs[\"hpo\"]\n",
    "\n",
    "# Backbones are controlled by the HPO config file (single source of truth)\n",
    "backbone_values = smoke_hpo_config[\"search_space\"][\"backbone\"][\"values\"]\n",
    "\n",
    "dry_run_sweep_jobs = {}\n",
    "\n",
    "for backbone in backbone_values:\n",
    "    aml_experiment_name = build_aml_experiment_name(\n",
    "        experiment_name=experiment_config.name,\n",
    "        stage=stage_name,\n",
    "        backbone=backbone,\n",
    "    )\n",
    "    dry_run_sweep_jobs[backbone] = create_dry_run_sweep_job_for_backbone(\n",
    "        script_path=TRAINING_SCRIPT_PATH,\n",
    "        data_asset=data_asset,\n",
    "        environment=training_environment,\n",
    "        compute_cluster=compute_cluster_name,\n",
    "        backbone=backbone,\n",
    "        smoke_hpo_config=smoke_hpo_config,\n",
    "        configs=configs,\n",
    "        config_metadata=config_metadata,\n",
    "        aml_experiment_name=aml_experiment_name,\n",
    "        stage=stage_name,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading resume-ner-azureml (0.18 MBs): 100%|██████████| 175712/175712 [00:04<00:00, 43424.45it/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: sad_bulb_c9r89q0mdr\n",
      "Web View: https://ml.azure.com/runs/sad_bulb_c9r89q0mdr?wsid=/subscriptions/a23fa87c-802c-4fdf-9e59-e3d7969bcf31/resourcegroups/resume_ner_2025-12-14-13-17-35/workspaces/resume-ner-ws\n",
      "\n",
      "Streaming azureml-logs/hyperdrive.txt\n",
      "=====================================\n",
      "\n",
      "[2025-12-18T16:11:29.6309489Z][GENERATOR][DEBUG]Sampled 2 jobs from search space \n",
      "[2025-12-18T16:11:29.9570062Z][SCHEDULER][INFO]Scheduling job, id='sad_bulb_c9r89q0mdr_0' \n",
      "[2025-12-18T16:11:29.9996376Z][SCHEDULER][INFO]Scheduling job, id='sad_bulb_c9r89q0mdr_1' \n",
      "[2025-12-18T16:11:30.5442603Z][SCHEDULER][INFO]Successfully scheduled a job. Id='sad_bulb_c9r89q0mdr_0' \n",
      "[2025-12-18T16:11:30.5739676Z][SCHEDULER][INFO]Successfully scheduled a job. Id='sad_bulb_c9r89q0mdr_1' \n",
      "[2025-12-18T16:12:00.1524946Z][GENERATOR][DEBUG]Setting all jobs generated as True, reason : Max number of jobs reached \n",
      "[2025-12-18T16:17:00.6132038Z][CONTROLLER][INFO]Changing Run Status from Running to Failed \n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: sad_bulb_c9r89q0mdr\n",
      "Web View: https://ml.azure.com/runs/sad_bulb_c9r89q0mdr?wsid=/subscriptions/a23fa87c-802c-4fdf-9e59-e3d7969bcf31/resourcegroups/resume_ner_2025-12-14-13-17-35/workspaces/resume-ner-ws\n"
     ]
    },
    {
     "ename": "JobException",
     "evalue": "Exception : \n {\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Execution failed. User process 'python' exited with status code 1. Please check log file 'user_logs/std_log.txt' for error details. Error:   File \\\"/mnt/azureml/cr/j/cc455629b37c4e028d114af0d5e16e02/exe/wd/src/train.py\\\", line 159\\n    log_metrics(output_dir, metrics)\\nIndentationError: unexpected indent\\n User errors were found in at least one of the child runs.\",\n        \"message_parameters\": {},\n        \"details\": []\n    },\n    \"time\": \"0001-01-01T00:00:00.000Z\"\n} ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJobException\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m backbone, sweep_job \u001b[38;5;129;01min\u001b[39;00m dry_run_sweep_jobs.items():\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     completed_job = \u001b[43msubmit_and_wait_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[43mml_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msweep_job\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     validate_sweep_job(\n\u001b[32m      4\u001b[39m         job=completed_job,\n\u001b[32m      5\u001b[39m         backbone=backbone,\n\u001b[32m      6\u001b[39m         job_type=\u001b[33m\"\u001b[39m\u001b[33mDry run sweep\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m         ml_client=ml_client,\n\u001b[32m      8\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/resume-ner-azureml/src/orchestration/jobs/runtime.py:24\u001b[39m, in \u001b[36msubmit_and_wait_for_job\u001b[39m\u001b[34m(ml_client, job)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[33;03mSubmit a job and block until it completes or fails.\u001b[39;00m\n\u001b[32m     12\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m \u001b[33;03m    RuntimeError: If the job terminates with a non-``Completed`` status.\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     23\u001b[39m submitted = ml_client.jobs.create_or_update(job)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43mml_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjobs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubmitted\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m completed = ml_client.jobs.get(submitted.name)\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m completed.status != \u001b[33m\"\u001b[39m\u001b[33mCompleted\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/current/lib/python3.12/site-packages/azure/core/tracing/decorator.py:138\u001b[39m, in \u001b[36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    136\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m span_attributes.items():\n\u001b[32m    137\u001b[39m                 span.add_attribute(key, value)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# Native path\u001b[39;00m\n\u001b[32m    141\u001b[39m     config = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/current/lib/python3.12/site-packages/azure/ai/ml/_telemetry/activity.py:288\u001b[39m, in \u001b[36mmonitor_with_activity.<locals>.monitor.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m tracer.start_as_current_span(ACTIVITY_SPAN):\n\u001b[32m    285\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m log_activity(\n\u001b[32m    286\u001b[39m             logger.package_logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f.\u001b[34m__name__\u001b[39m, activity_type, custom_dimensions\n\u001b[32m    287\u001b[39m         ):\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(logger, \u001b[33m\"\u001b[39m\u001b[33mpackage_logger\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m log_activity(logger.package_logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f.\u001b[34m__name__\u001b[39m, activity_type, custom_dimensions):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/current/lib/python3.12/site-packages/azure/ai/ml/operations/_job_operations.py:858\u001b[39m, in \u001b[36mJobOperations.stream\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    855\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_pipeline_child_job(job_object):\n\u001b[32m    856\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PipelineChildJobError(job_id=job_object.id)\n\u001b[32m--> \u001b[39m\u001b[32m858\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stream_logs_until_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_runs_operations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    860\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjob_object\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    861\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_datastore_operations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    862\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequests_pipeline\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_requests_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/current/lib/python3.12/site-packages/azure/ai/ml/operations/_job_ops_helper.py:334\u001b[39m, in \u001b[36mstream_logs_until_completion\u001b[39m\u001b[34m(run_operations, job_resource, datastore_operations, raise_exception_on_failed_job, requests_pipeline)\u001b[39m\n\u001b[32m    332\u001b[39m         file_handle.write(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m JobException(\n\u001b[32m    335\u001b[39m             message=\u001b[33m\"\u001b[39m\u001b[33mException : \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.format(json.dumps(error, indent=\u001b[32m4\u001b[39m)),\n\u001b[32m    336\u001b[39m             target=ErrorTarget.JOB,\n\u001b[32m    337\u001b[39m             no_personal_data_message=\u001b[33m\"\u001b[39m\u001b[33mException raised on failed job.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    338\u001b[39m             error_category=ErrorCategory.SYSTEM_ERROR,\n\u001b[32m    339\u001b[39m         )\n\u001b[32m    341\u001b[39m file_handle.write(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    342\u001b[39m file_handle.flush()\n",
      "\u001b[31mJobException\u001b[39m: Exception : \n {\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Execution failed. User process 'python' exited with status code 1. Please check log file 'user_logs/std_log.txt' for error details. Error:   File \\\"/mnt/azureml/cr/j/cc455629b37c4e028d114af0d5e16e02/exe/wd/src/train.py\\\", line 159\\n    log_metrics(output_dir, metrics)\\nIndentationError: unexpected indent\\n User errors were found in at least one of the child runs.\",\n        \"message_parameters\": {},\n        \"details\": []\n    },\n    \"time\": \"0001-01-01T00:00:00.000Z\"\n} "
     ]
    }
   ],
   "source": [
    "for backbone, sweep_job in dry_run_sweep_jobs.items():\n",
    "    completed_job = submit_and_wait_for_job(ml_client, sweep_job)\n",
    "    validate_sweep_job(\n",
    "        job=completed_job,\n",
    "        backbone=backbone,\n",
    "        job_type=\"Dry run sweep\",\n",
    "        ml_client=ml_client,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.5: The Sweep (HPO)\n",
    "\n",
    "Submit a hyperparameter optimization sweep to systematically search for the best model configuration.\n",
    "\n",
    "**Note**: Currently using `smoke.yaml` for demonstration purposes (CPU-only setup). For production with GPU, switch to `prod.yaml` in the configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orchestration.jobs import (\n",
    "    create_hpo_sweep_job_for_backbone,\n",
    "    submit_and_wait_for_job,\n",
    "    validate_sweep_job,\n",
    ")\n",
    "\n",
    "TRAINING_SCRIPT_PATH = Path(\"../src/train.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_cluster_name = configs[\"env\"][\"compute\"][\"training_cluster\"]\n",
    "\n",
    "try:\n",
    "    compute_cluster = ml_client.compute.get(compute_cluster_name)\n",
    "    if compute_cluster.provisioning_state != \"Succeeded\":\n",
    "        raise ValueError(f\"Compute cluster not ready: {compute_cluster.provisioning_state}\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Compute cluster '{compute_cluster_name}' not accessible: {e}\")\n",
    "\n",
    "stage_name = STAGE_HPO\n",
    "hpo_config = configs[\"hpo\"]\n",
    "backbone_values = hpo_config[\"search_space\"][\"backbone\"][\"values\"]\n",
    "hpo_sweep_jobs = {}\n",
    "\n",
    "for backbone in backbone_values:\n",
    "    aml_experiment_name = build_aml_experiment_name(\n",
    "        experiment_name=experiment_config.name,\n",
    "        stage=stage_name,\n",
    "        backbone=backbone,\n",
    "    )\n",
    "    hpo_sweep_jobs[backbone] = create_hpo_sweep_job_for_backbone(\n",
    "        script_path=TRAINING_SCRIPT_PATH,\n",
    "        data_asset=data_asset,\n",
    "        environment=training_environment,\n",
    "        compute_cluster=compute_cluster_name,\n",
    "        hpo_config=hpo_config,\n",
    "        backbone=backbone,\n",
    "        aml_experiment_name=aml_experiment_name,\n",
    "        stage=stage_name,\n",
    "        configs=configs,\n",
    "        config_metadata=config_metadata,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading resume-ner-azureml (0.17 MBs): 100%|██████████| 174399/174399 [00:03<00:00, 43623.66it/s]\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: frosty_nail_dh6526y3kd\n",
      "Web View: https://ml.azure.com/runs/frosty_nail_dh6526y3kd?wsid=/subscriptions/a23fa87c-802c-4fdf-9e59-e3d7969bcf31/resourcegroups/resume_ner_2025-12-14-13-17-35/workspaces/resume-ner-ws\n",
      "\n",
      "Streaming azureml-logs/hyperdrive.txt\n",
      "=====================================\n",
      "\n",
      "[2025-12-18T16:25:07.2153867Z][GENERATOR][DEBUG]Sampled 2 jobs from search space \n",
      "[2025-12-18T16:25:07.5647646Z][SCHEDULER][INFO]Scheduling job, id='frosty_nail_dh6526y3kd_0' \n",
      "[2025-12-18T16:25:07.6659486Z][SCHEDULER][INFO]Scheduling job, id='frosty_nail_dh6526y3kd_1' \n",
      "[2025-12-18T16:25:08.2223044Z][SCHEDULER][INFO]Successfully scheduled a job. Id='frosty_nail_dh6526y3kd_1' \n",
      "[2025-12-18T16:25:08.2783031Z][SCHEDULER][INFO]Successfully scheduled a job. Id='frosty_nail_dh6526y3kd_0' \n",
      "[2025-12-18T16:25:37.7306481Z][GENERATOR][DEBUG]Setting all jobs generated as True, reason : Max number of jobs reached \n",
      "[2025-12-18T16:34:40.5879583Z][CONTROLLER][INFO]Changing Run Status from Running to Completed \n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: frosty_nail_dh6526y3kd\n",
      "Web View: https://ml.azure.com/runs/frosty_nail_dh6526y3kd?wsid=/subscriptions/a23fa87c-802c-4fdf-9e59-e3d7969bcf31/resourcegroups/resume_ner_2025-12-14-13-17-35/workspaces/resume-ner-ws\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hpo_completed_jobs = {}\n",
    "\n",
    "for backbone, sweep_job in hpo_sweep_jobs.items():\n",
    "    completed_job = submit_and_wait_for_job(ml_client, sweep_job)\n",
    "    validate_sweep_job(\n",
    "        job=completed_job,\n",
    "        backbone=backbone,\n",
    "        job_type=\"HPO sweep\",\n",
    "        min_expected_trials=2,\n",
    "        ml_client=ml_client,\n",
    "    )\n",
    "    hpo_completed_jobs[backbone] = completed_job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1 HPO job references to hpo_completed_jobs_cache.json\n"
     ]
    }
   ],
   "source": [
    "# Save HPO job references to a JSON file\n",
    "hpo_jobs_cache_file = Path(\"hpo_completed_jobs_cache.json\")\n",
    "\n",
    "if hpo_completed_jobs:\n",
    "    hpo_jobs_data = {\n",
    "        backbone: {\n",
    "            \"job_name\": job.name,\n",
    "            \"job_id\": job.id,\n",
    "        }\n",
    "        for backbone, job in hpo_completed_jobs.items()\n",
    "    }\n",
    "\n",
    "    save_json(hpo_jobs_cache_file, hpo_jobs_data)\n",
    "    print(f\"Saved {len(hpo_jobs_data)} HPO job references to {hpo_jobs_cache_file}\")\n",
    "else:\n",
    "    print(\"No HPO completed jobs to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logged Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HPO job for deberta: frosty_nail_dh6526y3kd (status: Completed)\n",
      "\n",
      "Successfully reloaded 1 HPO completed jobs from cache\n"
     ]
    }
   ],
   "source": [
    "# Try to reload HPO jobs from cache\n",
    "hpo_jobs_cache_file = Path(\"hpo_completed_jobs_cache.json\")\n",
    "\n",
    "hpo_jobs_data = load_json(hpo_jobs_cache_file, default=None)\n",
    "\n",
    "if hpo_jobs_data is None:\n",
    "    print(f\"Cache file {hpo_jobs_cache_file} not found. Will need to run HPO.\")\n",
    "    hpo_completed_jobs = {}\n",
    "else:\n",
    "    hpo_completed_jobs = {}\n",
    "    for backbone, job_info in hpo_jobs_data.items():\n",
    "        try:\n",
    "            job = ml_client.jobs.get(job_info[\"job_name\"])\n",
    "            hpo_completed_jobs[backbone] = job\n",
    "            print(f\"Loaded HPO job for {backbone}: {job.name} (status: {job.status})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load job {job_info['job_name']} for {backbone}: {e}\")\n",
    "\n",
    "    if hpo_completed_jobs:\n",
    "        print(f\"\\nSuccessfully reloaded {len(hpo_completed_jobs)} HPO completed jobs from cache\")\n",
    "    else:\n",
    "        print(\"No valid jobs found in cache, will need to run HPO again\")\n",
    "        hpo_completed_jobs = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.6: Best Configuration Selection (Automated)\n",
    "\n",
    "Programmatically select the best configuration from all HPO sweep runs across all backbone models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orchestration.jobs import select_best_configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best configuration from all HPO sweep runs\n",
    "best_configuration = select_best_configuration(\n",
    "    ml_client=ml_client,\n",
    "    hpo_completed_jobs=hpo_completed_jobs,\n",
    "    hpo_config=configs[\"hpo\"],\n",
    "    dataset_version=configs[\"data\"][\"version\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best configuration to best_configuration_cache.json\n",
      "  Backbone: deberta\n",
      "  Best metric value: 0.056306306306306314\n"
     ]
    }
   ],
   "source": [
    "# Save best configuration to a JSON file\n",
    "best_config_cache_file = Path(\"best_configuration_cache.json\")\n",
    "\n",
    "if \"best_configuration\" in globals() and best_configuration is not None:\n",
    "    # best_configuration contains trial_name, trial_id, backbone, hyperparameters, metrics, etc.\n",
    "    # All of these are JSON-serializable\n",
    "    save_json(best_config_cache_file, best_configuration)\n",
    "    print(f\"Saved best configuration to {best_config_cache_file}\")\n",
    "    print(f\"  Backbone: {best_configuration.get('backbone')}\")\n",
    "    print(f\"  Best metric value: {best_configuration.get('selection_criteria', {}).get('best_value')}\")\n",
    "else:\n",
    "    print(\"No best configuration to save\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logged Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best configuration from cache:\n",
      "  Backbone: deberta\n",
      "  Trial: frosty_nail_dh6526y3kd_1\n",
      "  Best metric value: 0.056306306306306314\n",
      "  Dataset version: v2.1\n",
      "\n",
      "Skipping best configuration selection - using cached result\n"
     ]
    }
   ],
   "source": [
    "# Try to reload from cache\n",
    "best_config_cache_file = Path(\"best_configuration_cache.json\")\n",
    "\n",
    "best_configuration = load_json(best_config_cache_file, default=None)\n",
    "\n",
    "if best_configuration is None:\n",
    "    print(f\"Cache file {best_config_cache_file} not found. Will need to run Step P1-3.6.\")\n",
    "else:\n",
    "    print(f\"Loaded best configuration from cache:\")\n",
    "    print(f\"  Backbone: {best_configuration.get('backbone')}\")\n",
    "    print(f\"  Trial: {best_configuration.get('trial_name')}\")\n",
    "    print(f\"  Best metric value: {best_configuration.get('selection_criteria', {}).get('best_value')}\")\n",
    "    print(f\"  Dataset version: {best_configuration.get('dataset_version')}\")\n",
    "    print(f\"\\nSkipping best configuration selection - using cached result\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.7: Final Training (Post-HPO, Single Run)\n",
    "\n",
    "Train the final production model using the best configuration from HPO with stable, controlled conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orchestration.jobs import (\n",
    "    build_final_training_config,\n",
    "    create_final_training_job,\n",
    "    validate_final_training_job,\n",
    "    submit_and_wait_for_job\n",
    ")\n",
    "\n",
    "TRAINING_SCRIPT_PATH = Path(\"../src/train.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build final training config from best HPO result + train.yaml defaults\n",
    "final_training_config = build_final_training_config(best_configuration, configs[\"train\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'backbone': 'deberta',\n",
       " 'learning_rate': '1.0000330443059944',\n",
       " 'dropout': '0.2561414191558018',\n",
       " 'weight_decay': '1.0205121578437533',\n",
       " 'batch_size': 4,\n",
       " 'epochs': 2,\n",
       " 'random_seed': 42,\n",
       " 'early_stopping_enabled': False,\n",
       " 'use_combined_data': True}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_cluster_name = configs[\"env\"][\"compute\"][\"training_cluster\"]\n",
    "\n",
    "try:\n",
    "    compute_cluster = ml_client.compute.get(compute_cluster_name)\n",
    "    if compute_cluster.provisioning_state != \"Succeeded\":\n",
    "        raise ValueError(f\"Compute cluster not ready: {compute_cluster.provisioning_state}\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Compute cluster '{compute_cluster_name}' not accessible: {e}\")\n",
    "    \n",
    "# Create and submit final training job\n",
    "stage_name = STAGE_TRAINING\n",
    "aml_experiment_name = build_aml_experiment_name(\n",
    "    experiment_name=experiment_config.name,\n",
    "    stage=stage_name,\n",
    "    backbone=final_training_config[\"backbone\"],\n",
    ")\n",
    "\n",
    "final_training_tags = {\n",
    "    **config_metadata,\n",
    "    \"job_type\": \"final_training\",\n",
    "    \"backbone\": final_training_config[\"backbone\"],\n",
    "    \"best_trial\": best_configuration[\"trial_name\"],\n",
    "    \"best_metric_value\": str(best_configuration[\"selection_criteria\"][\"best_value\"]),\n",
    "    \"stage\": stage_name,\n",
    "}\n",
    "\n",
    "final_training_job = create_final_training_job(\n",
    "    script_path=TRAINING_SCRIPT_PATH,\n",
    "    data_asset_datastore_path=datastore_path,\n",
    "    environment=training_environment,\n",
    "    compute_cluster=compute_cluster_name,\n",
    "    final_config=final_training_config,\n",
    "    aml_experiment_name=aml_experiment_name,\n",
    "    tags=final_training_tags,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Warning: the provided asset name 'resume-ner-training' will not be used for anonymous registration\n",
      "Warning: the provided asset name 'resume-ner-training' will not be used for anonymous registration\n",
      "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: frosty_frog_2hmh6dqqjk\n",
      "Web View: https://ml.azure.com/runs/frosty_frog_2hmh6dqqjk?wsid=/subscriptions/a23fa87c-802c-4fdf-9e59-e3d7969bcf31/resourcegroups/resume_ner_2025-12-14-13-17-35/workspaces/resume-ner-ws\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: frosty_frog_2hmh6dqqjk\n",
      "Web View: https://ml.azure.com/runs/frosty_frog_2hmh6dqqjk?wsid=/subscriptions/a23fa87c-802c-4fdf-9e59-e3d7969bcf31/resourcegroups/resume_ner_2025-12-14-13-17-35/workspaces/resume-ner-ws\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Submit and validate final training job\n",
    "final_training_completed_job = submit_and_wait_for_job(ml_client, final_training_job)\n",
    "validate_final_training_job(final_training_completed_job)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved final training job reference to final_training_job_cache.json\n"
     ]
    }
   ],
   "source": [
    "final_training_cache_file = Path(\"final_training_job_cache.json\")\n",
    "\n",
    "if \"final_training_completed_job\" in globals() and final_training_completed_job is not None:\n",
    "    data = {\n",
    "        \"job_name\": final_training_completed_job.name,\n",
    "        \"job_id\": final_training_completed_job.id,\n",
    "    }\n",
    "    save_json(final_training_cache_file, data)\n",
    "    print(f\"Saved final training job reference to {final_training_cache_file}\")\n",
    "else:\n",
    "    print(\"No final training job to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logged Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded final training job: frosty_frog_2hmh6dqqjk (status: Completed)\n",
      "✓ Training job has checkpoint output\n"
     ]
    }
   ],
   "source": [
    "final_training_cache_file = Path(\"final_training_job_cache.json\")\n",
    "\n",
    "data = load_json(final_training_cache_file, default=None)\n",
    "\n",
    "if data is None:\n",
    "    print(f\"Cache file {final_training_cache_file} not found. Will need to run Step P1-3.7: Final Training.\")\n",
    "    final_training_completed_job = None\n",
    "else:\n",
    "    try:\n",
    "        final_training_completed_job = ml_client.jobs.get(data[\"job_name\"])\n",
    "        print(f\"Loaded final training job: {final_training_completed_job.name} (status: {final_training_completed_job.status})\")\n",
    "        \n",
    "        # Validate that the job has a checkpoint output\n",
    "        if not hasattr(final_training_completed_job, \"outputs\") or \"checkpoint\" not in final_training_completed_job.outputs:\n",
    "            print(f\"\\n⚠️  WARNING: Training job {final_training_completed_job.name} does not have a 'checkpoint' output.\")\n",
    "            print(\"   This job cannot be used for model conversion.\")\n",
    "            print(\"   Please re-run Step P1-3.7: Final Training to generate a new job with checkpoint output.\")\n",
    "            final_training_completed_job = None\n",
    "        else:\n",
    "            print(\"✓ Training job has checkpoint output\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not reload final training job {data['job_name']}: {e}\")\n",
    "        final_training_completed_job = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-4: Model Conversion & Optimization\n",
    "\n",
    "Convert the final training checkpoint to an optimized ONNX model (int8 quantized) for production inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>resume_ner_baseline-training-deberta</td><td>frosty_frog_2hmh6dqqjk</td><td>command</td><td>Completed</td><td><a href=\"https://ml.azure.com/runs/frosty_frog_2hmh6dqqjk?wsid=/subscriptions/a23fa87c-802c-4fdf-9e59-e3d7969bcf31/resourcegroups/resume_ner_2025-12-14-13-17-35/workspaces/resume-ner-ws&amp;tid=e7572e92-7aee-4713-a3c4-ba64888ad45f\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
      ],
      "text/plain": [
       "Command({'parameters': {'learning_rate': '1.0000330443059944', 'batch_size': '4', 'dropout': '0.2561414191558018', 'weight_decay': '1.0205121578437533', 'epochs': '2', 'backbone': 'microsoft/deberta-v3-base'}, 'init': False, 'name': 'frosty_frog_2hmh6dqqjk', 'type': 'command', 'status': 'Completed', 'log_files': None, 'description': 'Final production training with best HPO configuration', 'tags': {'data_config_hash': '2f21eff8258574d6', 'model_config_hash': '5f90a66353401b44', 'train_config_hash': 'f0c2caf728759868', 'hpo_config_hash': 'b28114c649d43a67', 'env_config_hash': '3e54b931c7640cf2', 'data_version': 'v2.1', 'model_backbone': 'distilbert-base-uncased', 'job_type': 'final_training', 'backbone': 'deberta', 'best_trial': 'frosty_nail_dh6526y3kd_1', 'best_metric_value': '0.056306306306306314', 'stage': 'training', '_aml_system_ComputeTargetStatus': '{\"AllocationState\":\"steady\",\"PreparingNodeCount\":0,\"RunningNodeCount\":0,\"CurrentNodeCount\":2}'}, 'properties': {'mlflow.source.git.repoURL': 'https://github.com/longdang193/resume-ner-azureml', 'mlflow.source.git.branch': 'main', 'mlflow.source.git.commit': '06f27069354f43c94b2638785680e5075d5bee9e', 'azureml.git.dirty': 'True', '_azureml.ComputeTargetType': 'amlctrain', '_azureml.ClusterName': 'cpu-cluster', 'ContentSnapshotId': 'e3897058-3841-4a8a-91be-fd04d8b194f2', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json', 'StartTimeUtc': '2025-12-18 16:38:15', 'EndTimeUtc': '2025-12-18 16:39:45'}, 'print_as_yaml': False, 'id': '/subscriptions/a23fa87c-802c-4fdf-9e59-e3d7969bcf31/resourceGroups/resume_ner_2025-12-14-13-17-35/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/jobs/frosty_frog_2hmh6dqqjk', 'Resource__source_path': '', 'base_path': '/workspaces/resume-ner-azureml/notebooks', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x77b731cf4110>, 'serialize': <msrest.serialization.Serializer object at 0x77b733f05700>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': 'final-training', 'experiment_name': 'resume_ner_baseline-training-deberta', 'compute': 'cpu-cluster', 'services': {'Tracking': {'endpoint': 'azureml://japanwest.api.azureml.ms/mlflow/v1.0/subscriptions/a23fa87c-802c-4fdf-9e59-e3d7969bcf31/resourceGroups/resume_ner_2025-12-14-13-17-35/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws?', 'type': 'Tracking'}, 'Studio': {'endpoint': 'https://ml.azure.com/runs/frosty_frog_2hmh6dqqjk?wsid=/subscriptions/a23fa87c-802c-4fdf-9e59-e3d7969bcf31/resourcegroups/resume_ner_2025-12-14-13-17-35/workspaces/resume-ner-ws&tid=e7572e92-7aee-4713-a3c4-ba64888ad45f', 'type': 'Studio'}}, 'comment': None, 'job_inputs': {'data': {'type': 'uri_folder', 'path': 'azureml://datastores/workspaceblobstore/paths/LocalUpload/3232563f3982da9961ea618d65a32a106230b55248141ce91a9158d93c07e092/dataset_tiny', 'mode': 'ro_mount'}}, 'job_outputs': {'checkpoint': {'type': 'uri_folder', 'mode': 'rw_mount'}, 'default': {'type': 'uri_folder', 'path': 'azureml://datastores/workspaceartifactstore/ExperimentRun/dcid.frosty_frog_2hmh6dqqjk', 'mode': 'rw_mount'}}, 'inputs': {'data': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x77b731cf93d0>}, 'outputs': {'checkpoint': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x77b733f06030>, 'default': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x77b733f06120>}, 'component': CommandComponent({'latest_version': None, 'intellectual_property': None, 'auto_increment_version': True, 'source': 'REMOTE.WORKSPACE.JOB', 'is_anonymous': False, 'auto_delete_setting': None, 'name': 'frosty_frog_2hmh6dqqjk', 'description': 'Final production training with best HPO configuration', 'tags': {'data_config_hash': '2f21eff8258574d6', 'model_config_hash': '5f90a66353401b44', 'train_config_hash': 'f0c2caf728759868', 'hpo_config_hash': 'b28114c649d43a67', 'env_config_hash': '3e54b931c7640cf2', 'data_version': 'v2.1', 'model_backbone': 'distilbert-base-uncased', 'job_type': 'final_training', 'backbone': 'deberta', 'best_trial': 'frosty_nail_dh6526y3kd_1', 'best_metric_value': '0.056306306306306314', 'stage': 'training', '_aml_system_ComputeTargetStatus': '{\"AllocationState\":\"steady\",\"PreparingNodeCount\":0,\"RunningNodeCount\":0,\"CurrentNodeCount\":2}'}, 'properties': {}, 'print_as_yaml': False, 'id': None, 'Resource__source_path': None, 'base_path': '/workspaces/resume-ner-azureml/notebooks', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x77b731cf4110>, 'serialize': <msrest.serialization.Serializer object at 0x77b733f06e10>, 'command': 'python src/train.py --data-asset ${{inputs.data}} --config-dir config --backbone deberta --learning-rate 1.0000330443059944 --batch-size 4 --dropout 0.2561414191558018 --weight-decay 1.0205121578437533 --epochs 2 --random-seed 42 --early-stopping-enabled false --use-combined-data true', 'code': '/subscriptions/a23fa87c-802c-4fdf-9e59-e3d7969bcf31/resourceGroups/resume_ner_2025-12-14-13-17-35/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/codes/2494586b-546b-4e8c-941f-2fdf276084a0/versions/1', 'environment_variables': {}, 'environment': '/subscriptions/a23fa87c-802c-4fdf-9e59-e3d7969bcf31/resourceGroups/resume_ner_2025-12-14-13-17-35/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/environments/resume-ner-training/versions/v8dba3aeec663aed1', 'distribution': None, 'resources': None, 'queue_settings': None, 'version': None, 'schema': None, 'type': 'command', 'display_name': 'final-training', 'is_deterministic': True, 'inputs': {'data': {'type': 'uri_folder', 'path': 'azureml://datastores/workspaceblobstore/paths/LocalUpload/3232563f3982da9961ea618d65a32a106230b55248141ce91a9158d93c07e092/dataset_tiny', 'mode': 'ro_mount'}}, 'outputs': {'checkpoint': {'type': 'uri_folder', 'mode': 'rw_mount'}, 'default': {'type': 'uri_folder', 'path': 'azureml://datastores/workspaceartifactstore/ExperimentRun/dcid.frosty_frog_2hmh6dqqjk', 'mode': 'rw_mount'}}, 'yaml_str': None, 'other_parameter': {'status': 'Completed', 'parameters': {'learning_rate': '1.0000330443059944', 'batch_size': '4', 'dropout': '0.2561414191558018', 'weight_decay': '1.0205121578437533', 'epochs': '2', 'backbone': 'microsoft/deberta-v3-base'}}, 'additional_includes': []}), 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': {'Tracking': {'endpoint': 'azureml://japanwest.api.azureml.ms/mlflow/v1.0/subscriptions/a23fa87c-802c-4fdf-9e59-e3d7969bcf31/resourceGroups/resume_ner_2025-12-14-13-17-35/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws?', 'type': 'Tracking'}, 'Studio': {'endpoint': 'https://ml.azure.com/runs/frosty_frog_2hmh6dqqjk?wsid=/subscriptions/a23fa87c-802c-4fdf-9e59-e3d7969bcf31/resourcegroups/resume_ner_2025-12-14-13-17-35/workspaces/resume-ner-ws&tid=e7572e92-7aee-4713-a3c4-ba64888ad45f', 'type': 'Studio'}}, 'status': 'Completed', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x77b731cf4110>}, 'instance_id': 'f42493dc-bc8a-4116-9f1c-32f06ad0b993', 'source': 'BUILDER', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': 'resume-ner-training:v8dba3aeec663aed1', 'resources': {'instance_count': 1, 'shm_size': '2g'}, 'queue_settings': {'job_tier': 'null'}, 'parent_job_name': None, 'swept': False})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_training_completed_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orchestration.jobs import (\n",
    "    get_checkpoint_output_from_training_job,\n",
    "    create_conversion_job,\n",
    "    validate_conversion_job,\n",
    "    submit_and_wait_for_job,\n",
    ")\n",
    "\n",
    "CONVERSION_SCRIPT_PATH = Path(\"../src/convert_to_onnx.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Retrieved checkpoint output: azureml:azureml_frosty_frog_2hmh6dqqjk_output_data_checkpoint:1\n"
     ]
    }
   ],
   "source": [
    "# Guard: ensure final_training_completed_job is set and has checkpoint output\n",
    "if \"final_training_completed_job\" not in globals() or final_training_completed_job is None:\n",
    "    raise ValueError(\n",
    "        \"final_training_completed_job is not set. \"\n",
    "        \"Please run Step P1-3.7: Final Training first, or ensure the cached job has a checkpoint output.\"\n",
    "    )\n",
    "\n",
    "# Guard: ensure ml_client is defined (required for fetching checkpoint data asset)\n",
    "if \"ml_client\" not in globals() or ml_client is None:\n",
    "    raise ValueError(\n",
    "        \"ml_client is not defined. \"\n",
    "        \"Please run the cells that set up ml_client (Step P1-3.1) before running this cell.\"\n",
    "    )\n",
    "\n",
    "checkpoint_output = get_checkpoint_output_from_training_job(final_training_completed_job, ml_client=ml_client)\n",
    "print(f\"✓ Retrieved checkpoint output: {checkpoint_output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_cluster_name = configs[\"env\"][\"compute\"][\"conversion_cluster\"]\n",
    "conversion_experiment_name = configs[\"env\"][\"logging\"][\"experiment_name\"]\n",
    "\n",
    "conversion_tags = {\n",
    "    **config_metadata,\n",
    "    \"job_type\": \"model_conversion\",\n",
    "    \"backbone\": best_configuration[\"backbone\"],\n",
    "    \"source_training_job\": final_training_completed_job.name,\n",
    "    \"quantization\": \"int8\",\n",
    "}\n",
    "\n",
    "conversion_job = create_conversion_job(\n",
    "    script_path=CONVERSION_SCRIPT_PATH,\n",
    "    checkpoint_uri=str(checkpoint_output),\n",
    "    environment=training_environment,\n",
    "    compute_cluster=conversion_cluster_name,\n",
    "    backbone=best_configuration[\"backbone\"],\n",
    "    experiment_name=conversion_experiment_name,\n",
    "    tags=conversion_tags,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: the provided asset name 'resume-ner-training' will not be used for anonymous registration\n",
      "Warning: the provided asset name 'resume-ner-training' will not be used for anonymous registration\n",
      "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: zen_basil_1wgpj6vq1b\n",
      "Web View: https://ml.azure.com/runs/zen_basil_1wgpj6vq1b?wsid=/subscriptions/a23fa87c-802c-4fdf-9e59-e3d7969bcf31/resourcegroups/resume_ner_2025-12-14-13-17-35/workspaces/resume-ner-ws\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: zen_basil_1wgpj6vq1b\n",
      "Web View: https://ml.azure.com/runs/zen_basil_1wgpj6vq1b?wsid=/subscriptions/a23fa87c-802c-4fdf-9e59-e3d7969bcf31/resourcegroups/resume_ner_2025-12-14-13-17-35/workspaces/resume-ner-ws\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conversion_completed_job = submit_and_wait_for_job(ml_client, conversion_job)\n",
    "validate_conversion_job(conversion_completed_job, ml_client=ml_client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved conversion job reference to conversion_job_cache.json\n"
     ]
    }
   ],
   "source": [
    "conversion_cache_file = Path(\"conversion_job_cache.json\")\n",
    "\n",
    "if \"conversion_completed_job\" in globals() and conversion_completed_job is not None:\n",
    "    data = {\n",
    "        \"job_name\": conversion_completed_job.name,\n",
    "        \"job_id\": conversion_completed_job.id,\n",
    "    }\n",
    "    save_json(conversion_cache_file, data)\n",
    "    print(f\"Saved conversion job reference to {conversion_cache_file}\")\n",
    "else:\n",
    "    print(\"No conversion job to save\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logged Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded conversion job: zen_basil_1wgpj6vq1b (status: Completed)\n",
      "✓ Conversion job has ONNX model output\n"
     ]
    }
   ],
   "source": [
    "conversion_cache_file = Path(\"conversion_job_cache.json\")\n",
    "\n",
    "data = load_json(conversion_cache_file, default=None)\n",
    "\n",
    "if data is None:\n",
    "    print(f\"Cache file {conversion_cache_file} not found. Will need to run Step P1-4: Model Conversion.\")\n",
    "    conversion_completed_job = None\n",
    "else:\n",
    "    try:\n",
    "        conversion_completed_job = ml_client.jobs.get(data[\"job_name\"])\n",
    "        print(f\"Loaded conversion job: {conversion_completed_job.name} (status: {conversion_completed_job.status})\")\n",
    "        \n",
    "        # Validate that the job has an onnx_model output\n",
    "        if not hasattr(conversion_completed_job, \"outputs\") or \"onnx_model\" not in conversion_completed_job.outputs:\n",
    "            print(f\"\\n⚠️  WARNING: Conversion job {conversion_completed_job.name} does not have an 'onnx_model' output.\")\n",
    "            print(\"   This job cannot be used for model registration.\")\n",
    "            print(\"   Please re-run Step P1-4: Model Conversion to generate a new job with ONNX model output.\")\n",
    "            conversion_completed_job = None\n",
    "        else:\n",
    "            print(\"✓ Conversion job has ONNX model output\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not reload conversion job {data['job_name']}: {e}\")\n",
    "        conversion_completed_job = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-5: Model Registration (The Handover)\n",
    "\n",
    "Register the optimized ONNX model in Azure ML Model Registry with full metadata for production deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Model\n",
    "from azure.core.exceptions import ResourceNotFoundError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onnx_model_path(conversion_job: Job) -> str:\n",
    "    \"\"\"\n",
    "    Get ONNX model path from completed conversion job.\n",
    "    \n",
    "    Args:\n",
    "        conversion_job: Completed conversion job\n",
    "        \n",
    "    Returns:\n",
    "        str: ONNX model path (Azure ML datastore URI)\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If ONNX model not found in job outputs\n",
    "    \"\"\"\n",
    "    if not hasattr(conversion_job, \"outputs\") or not conversion_job.outputs:\n",
    "        raise ValueError(\"Conversion job produced no outputs\")\n",
    "    \n",
    "    if \"onnx_model\" not in conversion_job.outputs:\n",
    "        raise ValueError(\"Conversion job missing 'onnx_model' output\")\n",
    "    \n",
    "    onnx_output = conversion_job.outputs[\"onnx_model\"]\n",
    "    \n",
    "    if hasattr(onnx_output, \"path\"):\n",
    "        return onnx_output.path\n",
    "    elif isinstance(onnx_output, str):\n",
    "        return onnx_output\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected ONNX output type: {type(onnx_output)}\")\n",
    "\n",
    "\n",
    "onnx_model_path = get_onnx_model_path(conversion_completed_job)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_model_version(best_config: Dict[str, Any], config_hashes: Dict[str, str]) -> str:\n",
    "    \"\"\"\n",
    "    Compute deterministic model version from configuration hashes.\n",
    "    \n",
    "    Args:\n",
    "        best_config: Best configuration from HPO selection\n",
    "        config_hashes: Configuration hashes dictionary\n",
    "        \n",
    "    Returns:\n",
    "        str: Model version string\n",
    "    \"\"\"\n",
    "    version_components = [\n",
    "        config_hashes[\"data\"],\n",
    "        config_hashes[\"model\"],\n",
    "        config_hashes[\"train\"],\n",
    "        best_config[\"backbone\"],\n",
    "    ]\n",
    "    version_str = \"_\".join(version_components)\n",
    "    version_hash = hashlib.sha256(version_str.encode()).hexdigest()[:CONFIG_HASH_LENGTH]\n",
    "    return f\"v{version_hash}\"\n",
    "\n",
    "\n",
    "model_version = compute_model_version(best_configuration, config_hashes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_production_model(\n",
    "    ml_client: MLClient,\n",
    "    model_name: str,\n",
    "    model_version: str,\n",
    "    model_path: str,\n",
    "    best_config: Dict[str, Any],\n",
    "    configs: Dict[str, Any],\n",
    "    config_metadata: Dict[str, str],\n",
    ") -> Model:\n",
    "    \"\"\"\n",
    "    Register optimized ONNX model in Azure ML Model Registry.\n",
    "    \n",
    "    Args:\n",
    "        ml_client: MLClient instance\n",
    "        model_name: Model name in registry\n",
    "        model_version: Model version\n",
    "        model_path: Path to ONNX model (Azure ML datastore URI)\n",
    "        best_config: Best configuration from HPO selection\n",
    "        configs: Configuration dictionaries\n",
    "        config_metadata: Configuration metadata for tagging\n",
    "        \n",
    "    Returns:\n",
    "        Model: Registered model instance\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If model path is invalid\n",
    "    \"\"\"\n",
    "    if not model_path or not model_path.endswith(\".onnx\"):\n",
    "        raise ValueError(f\"Invalid ONNX model path: {model_path}\")\n",
    "    \n",
    "    selection_criteria = best_config[\"selection_criteria\"]\n",
    "    \n",
    "    model_description = (\n",
    "        f\"Production ONNX model for Resume NER. \"\n",
    "        f\"Backbone: {selection_criteria['backbone']}, \"\n",
    "        f\"Metric: {selection_criteria['metric']}={selection_criteria['best_value']:.4f}\"\n",
    "    )\n",
    "    \n",
    "    model_tags = {\n",
    "        **config_metadata,\n",
    "        \"stage\": PROD_STAGE,\n",
    "        \"backbone\": selection_criteria[\"backbone\"],\n",
    "        \"metric\": selection_criteria[\"metric\"],\n",
    "        \"metric_value\": str(selection_criteria[\"best_value\"]),\n",
    "        \"dataset_version\": best_config[\"dataset_version\"],\n",
    "        \"model_format\": \"onnx\",\n",
    "        \"quantization\": \"int8\",\n",
    "        \"source_training_job\": final_training_completed_job.name,\n",
    "        \"source_conversion_job\": conversion_completed_job.name,\n",
    "    }\n",
    "    \n",
    "    model = Model(\n",
    "        name=model_name,\n",
    "        version=model_version,\n",
    "        description=model_description,\n",
    "        path=model_path,\n",
    "        tags=model_tags,\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        existing_model = ml_client.models.get(name=model_name, version=model_version)\n",
    "        return existing_model\n",
    "    except ResourceNotFoundError:\n",
    "        return ml_client.models.create_or_update(model)\n",
    "\n",
    "\n",
    "registered_model = register_production_model(\n",
    "    ml_client=ml_client,\n",
    "    model_name=MODEL_NAME,\n",
    "    model_version=model_version,\n",
    "    model_path=onnx_model_path,\n",
    "    best_config=best_configuration,\n",
    "    configs=configs,\n",
    "    config_metadata=config_metadata,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_registered_model(model: Model) -> None:\n",
    "    \"\"\"\n",
    "    Validate registered model has required metadata and tags.\n",
    "    \n",
    "    Args:\n",
    "        model: Registered model instance\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If validation fails\n",
    "    \"\"\"\n",
    "    required_tags = [\"stage\", \"backbone\", \"metric\", \"dataset_version\"]\n",
    "    for tag in required_tags:\n",
    "        if tag not in model.tags:\n",
    "            raise ValueError(f\"Registered model missing required tag: {tag}\")\n",
    "    \n",
    "    if model.tags.get(\"stage\") != PROD_STAGE:\n",
    "        raise ValueError(f\"Model stage must be '{PROD_STAGE}', got: {model.tags.get('stage')}\")\n",
    "    \n",
    "    if not model.path or not model.path.endswith(\".onnx\"):\n",
    "        raise ValueError(f\"Invalid model path: {model.path}\")\n",
    "\n",
    "\n",
    "validate_registered_model(registered_model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
