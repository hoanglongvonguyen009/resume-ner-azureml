{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Training Orchestration (Google Colab & Kaggle)\n",
    "\n",
    "This notebook orchestrates all training activities for **Google Colab or Kaggle execution** with GPU compute support.\n",
    "\n",
    "## Overview\n",
    "\n",
    "- **Step 1**: Repository Setup & Environment Configuration\n",
    "- **Step 2**: Load Centralized Configs\n",
    "- **Step 3**: Verify Local Dataset (from data config)\n",
    "- **Step 4**: Setup Local Environment\n",
    "- **Step 5**: The Dry Run\n",
    "- **Step 6**: The Sweep (HPO) - Local with Optuna\n",
    "- **Step 5.5**: Benchmarking Best Trials (NEW)\n",
    "- **Step 7**: Best Configuration Selection (Automated)\n",
    "- **Step 8**: Final Training (Post-HPO, Single Run)\n",
    "- **Step 9**: Model Conversion & Optimization\n",
    "\n",
    "## Important\n",
    "\n",
    "- This notebook **executes training in Google Colab or Kaggle** (not on Azure ML)\n",
    "- All computation happens on the platform's GPU\n",
    "- **Storage & Persistence**:\n",
    "  - **Google Colab**: Checkpoints are automatically saved to Google Drive for persistence across sessions\n",
    "  - **Kaggle**: Outputs in `/kaggle/working/` are automatically persisted - no manual backup needed\n",
    "- The notebook must be **re-runnable end-to-end**\n",
    "- Uses the dataset path specified in the data config (from `config/data/*.yaml`), typically pointing to a local folder included in the repository\n",
    "- **Session Management**:\n",
    "  - **Colab**: Sessions timeout after 12-24 hours (depending on Colab plan). Checkpoints are saved to Drive automatically.\n",
    "  - **Kaggle**: Sessions have time limits based on your plan. All outputs are automatically saved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Detection\n",
    "\n",
    "The notebook automatically detects the execution environment (local, Google Colab, or Kaggle) and adapts its behavior accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Detected environment: LOCAL\n",
      "Platform: local\n",
      "Base directory: Will use current working directory\n",
      "Backup enabled: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect execution environment\n",
    "IN_COLAB = \"COLAB_GPU\" in os.environ or \"COLAB_TPU\" in os.environ\n",
    "IN_KAGGLE = \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
    "IS_LOCAL = not IN_COLAB and not IN_KAGGLE\n",
    "\n",
    "# Set platform-specific constants\n",
    "if IN_COLAB:\n",
    "    PLATFORM = \"colab\"\n",
    "    BASE_DIR = Path(\"/content\")\n",
    "    BACKUP_ENABLED = True\n",
    "elif IN_KAGGLE:\n",
    "    PLATFORM = \"kaggle\"\n",
    "    BASE_DIR = Path(\"/kaggle/working\")\n",
    "    BACKUP_ENABLED = False\n",
    "else:\n",
    "    PLATFORM = \"local\"\n",
    "    BASE_DIR = None  # Will use Path.cwd() instead\n",
    "    BACKUP_ENABLED = False\n",
    "\n",
    "print(f\"✓ Detected environment: {PLATFORM.upper()}\")\n",
    "print(f\"Platform: {PLATFORM}\")\n",
    "if BASE_DIR:\n",
    "    print(f\"Base directory: {BASE_DIR}\")\n",
    "else:\n",
    "    print(f\"Base directory: Will use current working directory\")\n",
    "print(f\"Backup enabled: {BACKUP_ENABLED}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Repository Setup\n",
    "\n",
    "**Note**: Repository setup is only needed for Colab/Kaggle environments. Local environments should already have the repository cloned.\n",
    "\n",
    "### For Colab/Kaggle: Clone from Git or Upload Files\n",
    "\n",
    "Choose one of the following options:\n",
    "\n",
    "**Option A: Clone from Git (Recommended)**\n",
    "\n",
    "If your repository is on GitHub/GitLab, clone it:\n",
    "\n",
    "**For Google Colab:**\n",
    "```python\n",
    "!git clone -b feature/google-colab-compute https://github.com/longdang193/resume-ner-azureml.git /content/resume-ner-azureml\n",
    "```\n",
    "\n",
    "**For Kaggle:**\n",
    "```python\n",
    "!git clone -b feature/google-colab-compute https://github.com/longdang193/resume-ner-azureml.git /kaggle/working/resume-ner-azureml\n",
    "```\n",
    "\n",
    "**Option B: Upload Files**\n",
    "\n",
    "**For Google Colab:**\n",
    "1. Use the Colab file browser (folder icon on left sidebar)\n",
    "2. Upload your project files to `/content/resume-ner-azureml/`\n",
    "3. Ensure the directory structure matches: `src/`, `config/`, `notebooks/`, etc.\n",
    "\n",
    "**For Kaggle:**\n",
    "1. Use the Kaggle file browser (Data tab)\n",
    "2. Upload your project files to `/kaggle/working/resume-ner-azureml/`\n",
    "3. Ensure the directory structure matches: `src/`, `config/`, `notebooks/`, etc.\n",
    "\n",
    "### For Local: Repository Already Exists\n",
    "\n",
    "Local environments should have the repository already cloned. The notebook will automatically detect the repository location.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Local environment detected - assuming repository already exists\n"
     ]
    }
   ],
   "source": [
    "# Repository setup - only needed for Colab/Kaggle\n",
    "if not IS_LOCAL:\n",
    "    if IN_KAGGLE:\n",
    "        # For Kaggle\n",
    "        !git clone -b feature/google-colab-compute https://github.com/longdang193/resume-ner-azureml.git /kaggle/working/resume-ner-azureml\n",
    "    elif IN_COLAB:\n",
    "        # For Google Colab\n",
    "        !git clone -b feature/google-colab-compute https://github.com/longdang193/resume-ner-azureml.git /content/resume-ner-azureml\n",
    "else:\n",
    "    print(\"✓ Local environment detected - assuming repository already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Repository Setup\n",
    "\n",
    "Verify the repository structure exists:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Repository found at: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\n",
      "✓ Required directories found: ['src', 'config', 'notebooks']\n",
      "Notebook directory: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\notebooks\n",
      "Project root: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\n",
      "Source directory: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\src\n",
      "Config directory: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\config\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Unified path setup for all environments\n",
    "if IS_LOCAL:\n",
    "    # Local: assume notebook is in notebooks/ directory\n",
    "    NOTEBOOK_DIR = Path.cwd()\n",
    "    ROOT_DIR = NOTEBOOK_DIR.parent\n",
    "else:\n",
    "    # Colab/Kaggle: use fixed paths\n",
    "    ROOT_DIR = BASE_DIR / \"resume-ner-azureml\"\n",
    "\n",
    "SRC_DIR = ROOT_DIR / \"src\"\n",
    "CONFIG_DIR = ROOT_DIR / \"config\"\n",
    "NOTEBOOK_DIR = ROOT_DIR / \"notebooks\"\n",
    "\n",
    "# Verify repository structure\n",
    "if not ROOT_DIR.exists():\n",
    "    if IS_LOCAL:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Repository not found at {ROOT_DIR}\\n\"\n",
    "            f\"Please ensure you're running this notebook from the notebooks/ directory of the repository.\"\n",
    "        )\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Repository not found at {ROOT_DIR}\\n\"\n",
    "            f\"Please run Step 2 to clone or upload the repository.\"\n",
    "        )\n",
    "\n",
    "required_dirs = [\"src\", \"config\", \"notebooks\"]\n",
    "missing_dirs = [d for d in required_dirs if not (ROOT_DIR / d).exists()]\n",
    "\n",
    "if missing_dirs:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Missing required directories: {missing_dirs}\\n\"\n",
    "        f\"Please ensure the repository structure is correct.\"\n",
    "    )\n",
    "\n",
    "# Add to Python path\n",
    "sys.path.insert(0, str(ROOT_DIR))\n",
    "sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "print(f\"✓ Repository found at: {ROOT_DIR}\")\n",
    "print(f\"✓ Required directories found: {required_dirs}\")\n",
    "print(f\"Notebook directory: {NOTEBOOK_DIR}\")\n",
    "print(f\"Project root: {ROOT_DIR}\")\n",
    "print(f\"Source directory: {SRC_DIR}\")\n",
    "print(f\"Config directory: {CONFIG_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Install Dependencies\n",
    "\n",
    "**For Local**: Use conda environment (instructions below).  \n",
    "**For Colab/Kaggle**: Install packages via pip (automated below).\n",
    "\n",
    "### Local Environment Setup\n",
    "\n",
    "For local execution, create and activate a conda environment:\n",
    "\n",
    "1. Open a terminal in the project root\n",
    "2. Create the conda environment: `conda env create -f config/environment/conda.yaml`\n",
    "3. Activate: `conda activate resume-ner-training`\n",
    "4. Restart the kernel after activation\n",
    "\n",
    "### Colab/Kaggle: Automated Installation\n",
    "\n",
    "PyTorch is usually pre-installed in Colab/Kaggle, but we'll verify and install other required packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1\n",
      "CUDA available: True\n",
      "Visible GPUs: 1\n",
      "  GPU 0: Quadro T1000\n",
      "✓ PyTorch version meets requirements\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check PyTorch version and GPU availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    device_count = torch.cuda.device_count()\n",
    "    print(f\"Visible GPUs: {device_count}\")\n",
    "    for i in range(device_count):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "# Verify PyTorch version meets requirements (>=2.6.0)\n",
    "torch_version = tuple(map(int, torch.__version__.split('.')[:2]))\n",
    "if torch_version < (2, 6):\n",
    "    print(f\"⚠ Warning: PyTorch {torch.__version__} may not meet requirements (>=2.6.0)\")\n",
    "    if not IS_LOCAL:\n",
    "        print(\"Consider upgrading: !pip install torch>=2.6.0 --upgrade\")\n",
    "else:\n",
    "    print(\"✓ PyTorch version meets requirements\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For local environment, please:\n",
      "1. Create conda environment: conda env create -f config/environment/conda.yaml\n",
      "2. Activate: conda activate resume-ner-training\n",
      "3. Restart kernel after activation\n",
      "\n",
      "If you've already done this, you can continue to the next cell.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages - only for Colab/Kaggle\n",
    "if IS_LOCAL:\n",
    "    print(\"For local environment, please:\")\n",
    "    print(\"1. Create conda environment: conda env create -f config/environment/conda.yaml\")\n",
    "    print(\"2. Activate: conda activate resume-ner-training\")\n",
    "    print(\"3. Restart kernel after activation\")\n",
    "    print(\"\\nIf you've already done this, you can continue to the next cell.\")\n",
    "else:\n",
    "    # Core ML libraries\n",
    "    %pip install transformers>=4.35.0,<5.0.0 --quiet\n",
    "    %pip install safetensors>=0.4.0 --quiet\n",
    "    %pip install datasets>=2.12.0 --quiet\n",
    "\n",
    "    # ML utilities\n",
    "    %pip install numpy>=1.24.0,<2.0.0 --quiet\n",
    "    %pip install pandas>=2.0.0 --quiet\n",
    "    %pip install scikit-learn>=1.3.0 --quiet\n",
    "\n",
    "    # Utilities\n",
    "    %pip install pyyaml>=6.0 --quiet\n",
    "    %pip install tqdm>=4.65.0 --quiet\n",
    "    %pip install seqeval>=1.2.2 --quiet\n",
    "    %pip install sentencepiece>=0.1.99 --quiet\n",
    "\n",
    "    # Experiment tracking\n",
    "    %pip install mlflow --quiet\n",
    "    %pip install optuna --quiet\n",
    "\n",
    "    # ONNX support\n",
    "    %pip install onnxruntime --quiet\n",
    "    %pip install onnx>=1.16.0 --quiet\n",
    "    %pip install onnxscript>=0.1.0 --quiet\n",
    "\n",
    "    print(\"✓ All dependencies installed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Setup Paths and Import Paths\n",
    "\n",
    "Python paths are already configured in Step 2. This section verifies the setup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform: local\n",
      "Base directory: Will use current working directory\n",
      "Backup enabled: False\n"
     ]
    }
   ],
   "source": [
    "# Environment detection and platform configuration\n",
    "# Note: This cell is a duplicate of Cell 2. If Cell 2 was already executed, these variables are already set.\n",
    "# This cell ensures they're set even if Cell 2 was skipped.\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect execution environment\n",
    "IN_COLAB = \"COLAB_GPU\" in os.environ or \"COLAB_TPU\" in os.environ\n",
    "IN_KAGGLE = \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
    "IS_LOCAL = not IN_COLAB and not IN_KAGGLE\n",
    "\n",
    "# Set platform-specific constants (only if not already set)\n",
    "if 'PLATFORM' not in globals():\n",
    "    if IN_COLAB:\n",
    "        PLATFORM = \"colab\"\n",
    "        BASE_DIR = Path(\"/content\")\n",
    "        BACKUP_ENABLED = True\n",
    "        print(\"✓ Detected: Google Colab environment\")\n",
    "    elif IN_KAGGLE:\n",
    "        PLATFORM = \"kaggle\"\n",
    "        BASE_DIR = Path(\"/kaggle/working\")\n",
    "        BACKUP_ENABLED = False  # Kaggle outputs are automatically persisted\n",
    "        print(\"✓ Detected: Kaggle environment\")\n",
    "    else:\n",
    "        PLATFORM = \"local\"\n",
    "        BASE_DIR = None  # Will use Path.cwd() instead\n",
    "        BACKUP_ENABLED = False\n",
    "        print(\"✓ Detected: Local environment\")\n",
    "\n",
    "if 'PLATFORM' in globals():\n",
    "    print(f\"Platform: {PLATFORM}\")\n",
    "    if BASE_DIR:\n",
    "        print(f\"Base directory: {BASE_DIR}\")\n",
    "    else:\n",
    "        print(f\"Base directory: Will use current working directory\")\n",
    "    print(f\"Backup enabled: {BACKUP_ENABLED}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook directory: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\notebooks\n",
      "Project root: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\n",
      "Source directory: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\src\n",
      "Config directory: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\config\n",
      "Platform: local\n",
      "In Colab: False\n",
      "In Kaggle: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup paths (ROOT_DIR should be set in Cell 2)\n",
    "# If not, set it here\n",
    "if 'ROOT_DIR' not in globals():\n",
    "    if IN_COLAB:\n",
    "        ROOT_DIR = Path(\"/content/resume-ner-azureml\")\n",
    "    elif IN_KAGGLE:\n",
    "        ROOT_DIR = Path(\"/kaggle/working/resume-ner-azureml\")\n",
    "    else:\n",
    "        ROOT_DIR = Path(\"/content/resume-ner-azureml\")  # Default to Colab path\n",
    "\n",
    "SRC_DIR = ROOT_DIR / \"src\"\n",
    "CONFIG_DIR = ROOT_DIR / \"config\"\n",
    "NOTEBOOK_DIR = ROOT_DIR / \"notebooks\"\n",
    "\n",
    "# Add to Python path\n",
    "sys.path.insert(0, str(ROOT_DIR))\n",
    "sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "print(\"Notebook directory:\", NOTEBOOK_DIR)\n",
    "print(\"Project root:\", ROOT_DIR)\n",
    "print(\"Source directory:\", SRC_DIR)\n",
    "print(\"Config directory:\", CONFIG_DIR)\n",
    "print(\"Platform:\", PLATFORM if 'PLATFORM' in globals() else \"unknown\")\n",
    "print(\"In Colab:\", IN_COLAB if 'IN_COLAB' in globals() else False)\n",
    "print(\"In Kaggle:\", IN_KAGGLE if 'IN_KAGGLE' in globals() else False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Mount Google Drive\n",
    "\n",
    "Mount Google Drive to enable checkpoint persistence across Colab sessions. Checkpoints will be automatically saved to Drive after training completes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Backup/restore helper functions defined\n"
     ]
    }
   ],
   "source": [
    "# Helper functions for checkpoint backup/restore (platform-aware)\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "def backup_to_drive(source_path: Path, backup_name: str, is_directory: bool = False) -> bool:\n",
    "    \"\"\"\n",
    "    Backup a file or directory to Google Drive if available.\n",
    "    \n",
    "    Args:\n",
    "        source_path: Path to the file or directory to backup\n",
    "        backup_name: Name for the backup (will be placed in DRIVE_BACKUP_DIR)\n",
    "        is_directory: True if backing up a directory, False for a file\n",
    "    \n",
    "    Returns:\n",
    "        True if backup was successful, False if backup is not available or failed\n",
    "    \"\"\"\n",
    "    if not BACKUP_ENABLED or not DRIVE_BACKUP_DIR:\n",
    "        return False\n",
    "    \n",
    "    if not source_path.exists():\n",
    "        print(f\"⚠ Warning: Source path does not exist: {source_path}\")\n",
    "        return False\n",
    "    \n",
    "    backup_path = DRIVE_BACKUP_DIR / backup_name\n",
    "    \n",
    "    try:\n",
    "        if is_directory:\n",
    "            # Remove existing backup if it exists\n",
    "            if backup_path.exists():\n",
    "                shutil.rmtree(backup_path)\n",
    "            shutil.copytree(source_path, backup_path)\n",
    "        else:\n",
    "            shutil.copy2(source_path, backup_path)\n",
    "        \n",
    "        print(f\"✓ Backed up to Google Drive: {backup_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Warning: Backup failed: {e}\")\n",
    "        return False\n",
    "\n",
    "def restore_from_drive(backup_name: str, target_path: Path, is_directory: bool = False) -> bool:\n",
    "    \"\"\"\n",
    "    Restore a file or directory from Google Drive if available.\n",
    "    \n",
    "    Args:\n",
    "        backup_name: Name of the backup in DRIVE_BACKUP_DIR\n",
    "        target_path: Path where the restored file/directory should be placed\n",
    "        is_directory: True if restoring a directory, False for a file\n",
    "    \n",
    "    Returns:\n",
    "        True if restore was successful, False if backup is not available or failed\n",
    "    \"\"\"\n",
    "    if not BACKUP_ENABLED or not DRIVE_BACKUP_DIR:\n",
    "        return False\n",
    "    \n",
    "    backup_path = DRIVE_BACKUP_DIR / backup_name\n",
    "    \n",
    "    if not backup_path.exists():\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        if is_directory:\n",
    "            # Create parent directory if needed\n",
    "            target_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            shutil.copytree(backup_path, target_path)\n",
    "        else:\n",
    "            target_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            shutil.copy2(backup_path, target_path)\n",
    "        \n",
    "        print(f\"✓ Restored from Google Drive: {target_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Warning: Restore failed: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"✓ Backup/restore helper functions defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Warning: Unknown environment. Backup to Google Drive will be disabled.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Mount Google Drive (Colab only - Kaggle doesn't need this)\n",
    "DRIVE_BACKUP_DIR = None\n",
    "\n",
    "if IN_COLAB:\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        DRIVE_BACKUP_DIR = Path(\"/content/drive/MyDrive/resume-ner-checkpoints\")\n",
    "        DRIVE_BACKUP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"✓ Google Drive mounted\")\n",
    "        print(f\"✓ Checkpoint backup directory: {DRIVE_BACKUP_DIR}\")\n",
    "        print(f\"\\nNote: Checkpoints will be automatically saved to this directory after training completes.\")\n",
    "    except ImportError:\n",
    "        print(\"⚠ Warning: google.colab.drive not available. Backup to Google Drive will be disabled.\")\n",
    "        BACKUP_ENABLED = False\n",
    "elif IN_KAGGLE:\n",
    "    print(\"✓ Kaggle environment detected - outputs are automatically persisted (no Drive mount needed)\")\n",
    "else:\n",
    "    print(\"⚠ Warning: Unknown environment. Backup to Google Drive will be disabled.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.1: Load Centralized Configs\n",
    "\n",
    "Load and validate all configuration files. Configs are immutable and will be logged with each job for reproducibility.\n",
    "\n",
    "**Note**: \n",
    "- **Local**: Config files should already exist in the repository\n",
    "- **Colab/Kaggle**: Config files will be auto-created if missing (useful for fresh environments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Update repository from git (only for Colab/Kaggle if needed)\n",
    "# Uncomment and run if you need to pull latest changes\n",
    "# if not IS_LOCAL:\n",
    "#     !cd {ROOT_DIR} && git fetch origin feature/google-colab-compute\n",
    "#     !cd {ROOT_DIR} && git reset --hard origin/feature/google-colab-compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Local environment - assuming config files already exist in repository\n"
     ]
    }
   ],
   "source": [
    "# Write config files only if they don't exist (useful for Colab/Kaggle fresh environments)\n",
    "# Local environments should have configs already in the repo\n",
    "if IS_LOCAL:\n",
    "    print(\"✓ Local environment - assuming config files already exist in repository\")\n",
    "else:\n",
    "    # Create the experiment config directory if it doesn't exist\n",
    "    experiment_config_dir = CONFIG_DIR / \"experiment\"\n",
    "    experiment_config_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    config_path = experiment_config_dir / \"resume_ner_baseline.yaml\"\n",
    "    \n",
    "    # Only write if file doesn't exist\n",
    "    if not config_path.exists():\n",
    "        config_content = \"\"\"\n",
    "experiment_name: \"resume_ner_baseline\"\n",
    "\n",
    "# Relative to the top-level config directory\n",
    "data_config: \"data/resume_v1.yaml\"\n",
    "model_config: \"model/distilbert.yaml\"\n",
    "train_config: \"train.yaml\"\n",
    "hpo_config: \"hpo/prod.yaml\"      # default HPO config; stages can override if needed\n",
    "env_config: \"env/azure.yaml\"\n",
    "benchmark_config: \"benchmark.yaml\"\n",
    "\n",
    "# High-level orchestration design:\n",
    "# - Stages: smoke → hpo → training\n",
    "# - Smoke and HPO stage backbones are controlled by the HPO config file (search_space.backbone.values)\n",
    "# - Training stage can target specific backbones via stage config\n",
    "# - AML experiment names are per-stage, optionally per-backbone\n",
    "\n",
    "stages:\n",
    "  smoke:\n",
    "    # AML experiment base name for smoke tests\n",
    "    aml_experiment: \"resume-ner-smoke\"\n",
    "    # HPO config for smoke/dry run tests (uses smoke.yaml with reduced trials)\n",
    "    hpo_config: \"hpo/smoke.yaml\"\n",
    "    # Backbones are controlled by the HPO config file (hpo_config) via search_space.backbone.values\n",
    "\n",
    "  hpo:\n",
    "    # AML experiment base name for HPO sweeps\n",
    "    aml_experiment: \"resume-ner-hpo\"\n",
    "    # HPO config override for production HPO sweep (uses prod.yaml instead of default smoke.yaml)\n",
    "    hpo_config: \"hpo/prod.yaml\"\n",
    "    # Backbones are controlled by the HPO config file (hpo_config) via search_space.backbone.values\n",
    "\n",
    "  training:\n",
    "    # AML experiment base name for final single-run training\n",
    "    aml_experiment: \"resume-ner-train\"\n",
    "    # Final production backbone(s); typically one chosen after HPO\n",
    "    backbones:\n",
    "      - \"distilbert\"\n",
    "\n",
    "# Optional naming policy for how AML experiments are derived per backbone.\n",
    "# If true, the orchestrator should build experiment_name as:\n",
    "#   \"<aml_experiment>-<backbone>\"\n",
    "# otherwise it should use \"<aml_experiment>\" directly and rely on tags\n",
    "# (stage/backbone) for grouping in AML.\n",
    "naming:\n",
    "  include_backbone_in_experiment: true\n",
    "\"\"\"\n",
    "        config_path.write_text(config_content)\n",
    "        print(f\"✓ Config file written to: {config_path}\")\n",
    "    else:\n",
    "        print(f\"✓ Config file already exists: {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Local environment - assuming HPO config files already exist in repository\n"
     ]
    }
   ],
   "source": [
    "# Write HPO config file only if it doesn't exist (useful for Colab/Kaggle fresh environments)\n",
    "# Local environments should have configs already in the repo\n",
    "if IS_LOCAL:\n",
    "    print(\"✓ Local environment - assuming HPO config files already exist in repository\")\n",
    "else:\n",
    "    # Create the HPO config directory if it doesn't exist\n",
    "    hpo_config_dir = CONFIG_DIR / \"hpo\"\n",
    "    hpo_config_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    config_path = hpo_config_dir / \"prod.yaml\"\n",
    "    \n",
    "    # Only write if file doesn't exist\n",
    "    if not config_path.exists():\n",
    "        config_content = \"\"\"\n",
    "search_space:\n",
    "  backbone:\n",
    "    type: \"choice\"\n",
    "    values: [\"distilroberta\"]\n",
    "  \n",
    "  learning_rate:\n",
    "    type: \"loguniform\"\n",
    "    min: 1e-5\n",
    "    max: 5e-5\n",
    "  \n",
    "  batch_size:\n",
    "    type: \"choice\"\n",
    "    values: [8, 16]\n",
    "  \n",
    "  dropout:\n",
    "    type: \"uniform\"\n",
    "    min: 0.1\n",
    "    max: 0.3\n",
    "  \n",
    "  weight_decay:\n",
    "    type: \"loguniform\"\n",
    "    min: 0.001\n",
    "    max: 0.1\n",
    "\n",
    "sampling:\n",
    "  algorithm: \"random\"\n",
    "  max_trials: 20\n",
    "  timeout_minutes: 960\n",
    "\n",
    "early_termination:\n",
    "  policy: \"bandit\"\n",
    "  evaluation_interval: 1\n",
    "  slack_factor: 0.2\n",
    "  delay_evaluation: 2\n",
    "\n",
    "objective:\n",
    "  metric: \"macro-f1\"\n",
    "  goal: \"maximize\"\n",
    "\n",
    "# Selection strategy configuration for accuracy-speed tradeoff\n",
    "selection:\n",
    "  # Accuracy threshold for speed tradeoff (0.015 = 1.5% relative)\n",
    "  # If two models are within this accuracy difference, prefer faster model\n",
    "  # Set to null for accuracy-only selection (default behavior)\n",
    "  accuracy_threshold: 0.015\n",
    "  \n",
    "  # Use relative threshold (percentage of best accuracy) vs absolute difference\n",
    "  # Relative thresholds are more robust across different accuracy ranges\n",
    "  # Default: true (recommended)\n",
    "  use_relative_threshold: true\n",
    "  \n",
    "  # Minimum relative accuracy gain to justify slower model (optional)\n",
    "  # If DeBERTa is < 2% better than DistilBERT, prefer DistilBERT\n",
    "  # Set to null to disable this check\n",
    "  min_accuracy_gain: 0.02\n",
    "\n",
    "k_fold:\n",
    "  enabled: true\n",
    "  n_splits: 5\n",
    "  random_seed: 42\n",
    "  shuffle: true\n",
    "  stratified: true\n",
    "\n",
    "# Checkpoint configuration for HPO resume support\n",
    "# Enables saving study state to SQLite database for resuming interrupted runs\n",
    "checkpoint:\n",
    "  enabled: true  # Set to true to enable checkpointing (useful for Colab/Kaggle)\n",
    "  storage_path: \"{backbone}/study.db\"  # Relative to output_dir, {backbone} placeholder\n",
    "  auto_resume: true  # Automatically resume if checkpoint exists (only if enabled=true)\n",
    "\"\"\"\n",
    "        config_path.write_text(config_content)\n",
    "        print(f\"✓ HPO config written to: {config_path}\")\n",
    "    else:\n",
    "        print(f\"✓ HPO config file already exists: {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Local environment - assuming training config file already exists in repository\n"
     ]
    }
   ],
   "source": [
    "# Write training config file only if it doesn't exist (useful for Colab/Kaggle fresh environments)\n",
    "# Local environments should have configs already in the repo\n",
    "if IS_LOCAL:\n",
    "    print(\"✓ Local environment - assuming training config file already exists in repository\")\n",
    "else:\n",
    "    # Ensure config directory exists\n",
    "    CONFIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    config_path = CONFIG_DIR / \"train.yaml\"\n",
    "    \n",
    "    # Only write if file doesn't exist\n",
    "    if not config_path.exists():\n",
    "        config_content = \"\"\"\n",
    "# Global Training Defaults\n",
    "# Applied to all training runs\n",
    "\n",
    "training:\n",
    "  epochs: 5\n",
    "  batch_size: 12 \n",
    "  gradient_accumulation_steps: 2\n",
    "  learning_rate: 2e-5\n",
    "  weight_decay: 0.01\n",
    "  warmup_steps: 500\n",
    "  max_grad_norm: 1.0\n",
    "  # Data splitting and model-specific settings\n",
    "  val_split_divisor: 10  # Divide train set by this to create validation split if none exists\n",
    "  deberta_max_batch_size: 16  # Maximum batch size for DeBERTa models (memory constraints)\n",
    "  warmup_steps_divisor: 10  # Divide total steps by this to cap warmup steps\n",
    "  \n",
    "  # EDA-based metric selection\n",
    "  metric: \"macro-f1\"  # Class imbalance requires macro-f1\n",
    "  metric_mode: \"max\"  # Maximize macro-f1\n",
    "  \n",
    "  early_stopping:\n",
    "    enabled: true\n",
    "    patience: 3\n",
    "    min_delta: 0.001\n",
    "\n",
    "logging:\n",
    "  log_interval: 100\n",
    "  eval_interval: 500\n",
    "  save_interval: 1000\n",
    "\n",
    "checkpointing:\n",
    "  save_strategy: \"steps\"\n",
    "  save_total_limit: 3\n",
    "  load_best_model_at_end: true\n",
    "\n",
    "# NOTE: Multi-GPU / DDP is optional and currently experimental. When enabled,\n",
    "# the training code will use this section together with hardware detection to\n",
    "# decide whether to run single-GPU vs multi-GPU. If no multiple GPUs or DDP\n",
    "# backend are available, it will safely fall back to single-GPU.\n",
    "distributed:\n",
    "  enabled: true         # Set true to enable multi-GPU / DDP\n",
    "  backend: \"nccl\"        # Typically 'nccl' for GPUs\n",
    "  world_size: \"auto\"     # 'auto' = use all visible GPUs; or set an int\n",
    "  init_method: \"env://\"  # Default init method; can be overridden if needed\n",
    "  timeout_seconds: 1800  # Process group init timeout (in seconds)\n",
    "\"\"\"\n",
    "        config_path.write_text(config_content)\n",
    "        print(f\"✓ Training config written to: {config_path}\")\n",
    "    else:\n",
    "        print(f\"✓ Training config file already exists: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Constants\n",
    "\n",
    "Define constants for file and directory names used throughout the notebook. Benchmark settings come from centralized config, not hard-coded here. These constants work across all environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import constants from centralized module\n",
    "from orchestration import (\n",
    "    METRICS_FILENAME,\n",
    "    BENCHMARK_FILENAME,\n",
    "    CHECKPOINT_DIRNAME,\n",
    "    OUTPUTS_DIRNAME,\n",
    "    MLRUNS_DIRNAME,\n",
    "    DEFAULT_RANDOM_SEED,\n",
    "    DEFAULT_K_FOLDS,\n",
    ")\n",
    "\n",
    "# Note: Benchmark settings (batch_sizes, iterations, etc.) come from configs[\"benchmark\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Helper Functions\n",
    "\n",
    "Reusable helper functions following DRY principle for common operations. These functions work across all environments (local, Colab, Kaggle).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import helper functions from consolidated modules (DRY principle)\n",
    "from typing import List, Optional\n",
    "from orchestration import (\n",
    "    build_mlflow_experiment_name,\n",
    "    setup_mlflow_for_stage,\n",
    "    run_benchmarking,\n",
    ")\n",
    "from shared import verify_output_file\n",
    "\n",
    "# Wrapper function for run_benchmarking that uses notebook-specific paths\n",
    "def run_benchmarking_local(\n",
    "    checkpoint_dir: Path,\n",
    "    test_data_path: Path,\n",
    "    output_path: Path,\n",
    "    batch_sizes: List[int],\n",
    "    iterations: int,\n",
    "    warmup_iterations: int,\n",
    "    max_length: int = 512,\n",
    "    device: Optional[str] = None,\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Run benchmarking on a model checkpoint (local notebook wrapper).\n",
    "    \n",
    "    This is a thin wrapper around orchestration.benchmark_utils.run_benchmarking\n",
    "    that automatically uses the notebook's SRC_DIR and ROOT_DIR.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_dir: Path to checkpoint directory.\n",
    "        test_data_path: Path to test data JSON file.\n",
    "        output_path: Path to output benchmark.json file.\n",
    "        batch_sizes: List of batch sizes to test.\n",
    "        iterations: Number of iterations per batch size.\n",
    "        warmup_iterations: Number of warmup iterations.\n",
    "        max_length: Maximum sequence length.\n",
    "        device: Device to use (None = auto-detect).\n",
    "    \n",
    "    Returns:\n",
    "        True if successful, False otherwise.\n",
    "    \"\"\"\n",
    "    return run_benchmarking(\n",
    "        checkpoint_dir=checkpoint_dir,\n",
    "        test_data_path=test_data_path,\n",
    "        output_path=output_path,\n",
    "        batch_sizes=batch_sizes,\n",
    "        iterations=iterations,\n",
    "        warmup_iterations=warmup_iterations,\n",
    "        max_length=max_length,\n",
    "        device=device,\n",
    "        project_root=ROOT_DIR,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded experiment: resume_ner_baseline\n",
      "Loaded config domains: ['benchmark', 'data', 'env', 'hpo', 'model', 'train']\n",
      "Config hashes: {'data': 'e87b126b961fa20d', 'model': '5f90a66353401b44', 'train': '781de5190c9f6bcc', 'hpo': 'a55c5ddfff162498', 'env': '3e54b931c7640cf2', 'benchmark': '33da3b0fc59ff812'}\n",
      "Config metadata: {'data_config_hash': 'e87b126b961fa20d', 'model_config_hash': '5f90a66353401b44', 'train_config_hash': '781de5190c9f6bcc', 'hpo_config_hash': 'a55c5ddfff162498', 'env_config_hash': '3e54b931c7640cf2', 'data_version': 'v3', 'model_backbone': 'distilbert-base-uncased'}\n",
      "Dataset path (from data config): C:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\dataset_tiny\\seed0\n",
      "Using seed: 0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import Any, Dict\n",
    "\n",
    "from orchestration import EXPERIMENT_NAME\n",
    "from orchestration.config_loader import (\n",
    "    ExperimentConfig,\n",
    "    compute_config_hashes,\n",
    "    create_config_metadata,\n",
    "    load_all_configs,\n",
    "    load_experiment_config,\n",
    "    snapshot_configs,\n",
    "    validate_config_immutability,\n",
    ")\n",
    "\n",
    "# P1-3.1: Load Centralized Configs (local-only)\n",
    "# Mirrors the Azure orchestration notebook, but does not create an Azure ML client.\n",
    "\n",
    "if not CONFIG_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Config directory not found: {CONFIG_DIR}\")\n",
    "\n",
    "experiment_config: ExperimentConfig = load_experiment_config(CONFIG_DIR, EXPERIMENT_NAME)\n",
    "configs: Dict[str, Any] = load_all_configs(experiment_config)\n",
    "config_hashes = compute_config_hashes(configs)\n",
    "config_metadata = create_config_metadata(configs, config_hashes)\n",
    "\n",
    "# Immutable snapshots for runtime mutation checks\n",
    "original_configs = snapshot_configs(configs)\n",
    "validate_config_immutability(configs, original_configs)\n",
    "\n",
    "print(f\"Loaded experiment: {experiment_config.name}\")\n",
    "print(\"Loaded config domains:\", sorted(configs.keys()))\n",
    "print(\"Config hashes:\", config_hashes)\n",
    "print(\"Config metadata:\", config_metadata)\n",
    "\n",
    "# Get dataset path from data config (centralized configuration)\n",
    "# The local_path in the data config is relative to the config directory\n",
    "data_config = configs[\"data\"]\n",
    "local_path_str = data_config.get(\"local_path\", \"../dataset\")\n",
    "DATASET_LOCAL_PATH = (CONFIG_DIR / local_path_str).resolve()\n",
    "\n",
    "# Check if seed-based dataset structure (for dataset_tiny with seed subdirectories)\n",
    "seed = data_config.get(\"seed\")\n",
    "if seed is not None and \"dataset_tiny\" in str(DATASET_LOCAL_PATH):\n",
    "    DATASET_LOCAL_PATH = DATASET_LOCAL_PATH / f\"seed{seed}\"\n",
    "\n",
    "print(f\"Dataset path (from data config): {DATASET_LOCAL_PATH}\")\n",
    "if seed is not None:\n",
    "    print(f\"Using seed: {seed}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.2: Verify Local Dataset\n",
    "\n",
    "Verify that the dataset directory (specified by `local_path` in the data config) exists and contains the required files. The dataset path is loaded from the centralized data configuration in Step P1-3.1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset directory found: C:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\dataset_tiny\\seed0\n",
      "  (from data config: resume-ner-data-tiny-short vv3)\n",
      "  ✓ train.json (28,721 bytes)\n",
      "  ⚠ validation.json not found (optional - training will proceed without validation set)\n"
     ]
    }
   ],
   "source": [
    "# P1-3.2: Verify Local Dataset\n",
    "# The dataset path comes from the data config's local_path field (loaded in Step P1-3.1).\n",
    "# This ensures the dataset location is controlled by centralized configuration.\n",
    "# Note: train.json is required, but validation.json is optional (matches training script behavior).\n",
    "\n",
    "REQUIRED_FILE = \"train.json\"\n",
    "OPTIONAL_FILE = \"validation.json\"\n",
    "\n",
    "if not DATASET_LOCAL_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Dataset directory not found: {DATASET_LOCAL_PATH}\\n\"\n",
    "        f\"This path comes from the data config's 'local_path' field.\\n\"\n",
    "        f\"If you need to create the dataset, run the notebook: notebooks/00_make_tiny_dataset.ipynb\"\n",
    "    )\n",
    "\n",
    "# Check required file\n",
    "train_file = DATASET_LOCAL_PATH / REQUIRED_FILE\n",
    "if not train_file.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Required dataset file not found: {train_file}\\n\"\n",
    "        f\"This path comes from the data config's 'local_path' field.\\n\"\n",
    "        f\"If you need to create it, run the notebook: notebooks/00_make_tiny_dataset.ipynb\"\n",
    "    )\n",
    "\n",
    "# Check optional file\n",
    "val_file = DATASET_LOCAL_PATH / OPTIONAL_FILE\n",
    "has_validation = val_file.exists()\n",
    "\n",
    "print(f\"✓ Dataset directory found: {DATASET_LOCAL_PATH}\")\n",
    "print(f\"  (from data config: {data_config.get('name', 'unknown')} v{data_config.get('version', 'unknown')})\")\n",
    "\n",
    "train_size = train_file.stat().st_size\n",
    "print(f\"  ✓ {REQUIRED_FILE} ({train_size:,} bytes)\")\n",
    "\n",
    "if has_validation:\n",
    "    val_size = val_file.stat().st_size\n",
    "    print(f\"  ✓ {OPTIONAL_FILE} ({val_size:,} bytes)\")\n",
    "else:\n",
    "    print(f\"  ⚠ {OPTIONAL_FILE} not found (optional - training will proceed without validation set)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.2.1: Optional Train/Test Split\n",
    "\n",
    "**Optional step**: Create a train/test split if `test.json` is missing. This is useful when you only have `train.json` and `validation.json` and want to create a separate test set.\n",
    "\n",
    "**⚠ WARNING**: This will overwrite `train.json` with the split version. Only enable if you want to create a permanent train/test split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Found existing test.json at C:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\dataset_tiny\\seed0\\test.json\n"
     ]
    }
   ],
   "source": [
    "# Optional: create train/test split if test.json is missing\n",
    "# WARNING: This will overwrite train.json with the split version\n",
    "# Only enable if you want to create a permanent train/test split\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "from training.data import split_train_test, save_split_files\n",
    "\n",
    "CREATE_TEST_SPLIT = False  # Set True to create test.json when absent (WARNING: overwrites train.json)\n",
    "\n",
    "train_file = DATASET_LOCAL_PATH / \"train.json\"\n",
    "val_file = DATASET_LOCAL_PATH / \"validation.json\"\n",
    "test_file = DATASET_LOCAL_PATH / \"test.json\"\n",
    "\n",
    "if CREATE_TEST_SPLIT and not test_file.exists():\n",
    "    # Backup original train.json before overwriting\n",
    "    backup_file = DATASET_LOCAL_PATH / \"train.json.backup\"\n",
    "    if train_file.exists() and not backup_file.exists():\n",
    "        import shutil\n",
    "        shutil.copy2(train_file, backup_file)\n",
    "        print(f\"⚠ Backed up original train.json to {backup_file}\")\n",
    "    \n",
    "    full_dataset = []\n",
    "    # Start with train data; optionally include validation to maximize coverage\n",
    "    with open(train_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        full_dataset.extend(json.load(f))\n",
    "    if val_file.exists():\n",
    "        with open(val_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            full_dataset.extend(json.load(f))\n",
    "\n",
    "    split_cfg = configs.get(\"data\", {}).get(\"splitting\", {})\n",
    "    train_ratio = split_cfg.get(\"train_test_ratio\", 0.8)\n",
    "    stratified = split_cfg.get(\"stratified\", False)\n",
    "    random_seed = split_cfg.get(\"random_seed\", 42)\n",
    "    entity_types = configs.get(\"data\", {}).get(\"schema\", {}).get(\"entity_types\", [])\n",
    "\n",
    "    print(f\"Creating train/test split (train_ratio={train_ratio}, stratified={stratified})...\")\n",
    "    print(f\"⚠ WARNING: This will overwrite train.json with {int(len(full_dataset) * train_ratio)} samples\")\n",
    "    \n",
    "    new_train, new_test = split_train_test(\n",
    "        dataset=full_dataset,\n",
    "        train_ratio=train_ratio,\n",
    "        stratified=stratified,\n",
    "        random_seed=random_seed,\n",
    "        entity_types=entity_types,\n",
    "    )\n",
    "\n",
    "    save_split_files(DATASET_LOCAL_PATH, new_train, new_test)\n",
    "    print(f\"✓ Wrote train.json ({len(new_train)}) and test.json ({len(new_test)})\")\n",
    "elif test_file.exists():\n",
    "    print(f\"✓ Found existing test.json at {test_file}\")\n",
    "else:\n",
    "    print(\"⚠ test.json not found. Set CREATE_TEST_SPLIT=True to generate a split.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.3: Setup Local Environment\n",
    "\n",
    "Verify GPU availability, set up MLflow tracking (local file store), and check that key dependencies are installed. This step ensures the local environment is ready for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "DEFAULT_DEVICE = \"cuda\"\n",
    "\n",
    "env_config = configs[\"env\"]\n",
    "device_type = env_config.get(\"compute\", {}).get(\"device\", DEFAULT_DEVICE)\n",
    "\n",
    "if device_type == \"cuda\" and not torch.cuda.is_available():\n",
    "    raise RuntimeError(\n",
    "        \"CUDA device requested but not available. \"\n",
    "        \"In Colab, ensure you've selected a GPU runtime: Runtime > Change runtime type > GPU\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-27 23:54:14,403 - shared.mlflow_setup - INFO - Azure ML enabled in config, attempting to connect...\n",
      "2025-12-27 23:54:17,137 - shared.mlflow_setup - INFO - Environment variables not set, loading from c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\config.env\n",
      "2025-12-27 23:54:17,138 - shared.mlflow_setup - INFO - Loaded credentials from config.env\n",
      "Class DeploymentTemplateOperations: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "2025-12-27 23:54:27,273 - shared.mlflow_setup - INFO - Using Azure ML workspace tracking\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'azureml://japanwest.api.azureml.ms/mlflow/v2.0/subscriptions/a23fa87c-802c-4fdf-9e59-e3d7969bcf31/resourceGroups/resume_ner_2025-12-14-13-17-35/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from shared.mlflow_setup import setup_mlflow_from_config\n",
    "\n",
    "# Setup MLflow from config (automatically uses Azure ML if enabled in config/mlflow.yaml)\n",
    "# To enable Azure ML Workspace tracking:\n",
    "# 1. Edit config/mlflow.yaml and set azure_ml.enabled: true\n",
    "# 2. Set environment variables: AZURE_SUBSCRIPTION_ID and AZURE_RESOURCE_GROUP\n",
    "setup_mlflow_from_config(\n",
    "    experiment_name=\"placeholder\",  # Will be set per HPO run\n",
    "    config_dir=CONFIG_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Kaggle-specific package installation (not running on Kaggle)\n"
     ]
    }
   ],
   "source": [
    "# For Kaggle only - install specific package versions required for Optuna checkpointing\n",
    "if IN_KAGGLE:\n",
    "    %pip install \"SQLAlchemy<2.0.0\" \"alembic<1.13.0\" \"optuna<4.0.0\" --quiet\n",
    "else:\n",
    "    print(\"Skipping Kaggle-specific package installation (not running on Kaggle)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import mlflow\n",
    "    import transformers\n",
    "    import optuna\n",
    "except ImportError as e:\n",
    "    raise ImportError(f\"Required package not installed: {e}\")\n",
    "\n",
    "REQUIRED_PACKAGES = {\n",
    "    \"torch\": torch,\n",
    "    \"transformers\": transformers,\n",
    "    \"mlflow\": mlflow,\n",
    "    \"optuna\": optuna,\n",
    "}\n",
    "\n",
    "for name, module in REQUIRED_PACKAGES.items():\n",
    "    if not hasattr(module, \"__version__\"):\n",
    "        raise ImportError(\n",
    "            f\"Required package '{name}' is not properly installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.4: The Sweep (HPO) - Local with Optuna\n",
    "\n",
    "Run the full hyperparameter optimization sweep using Optuna to systematically search for the best model configuration. Uses the production HPO configuration with more trials than the dry run.\n",
    "\n",
    "**Note on K-Fold Cross-Validation:**\n",
    "- When k-fold CV is enabled (`k_fold.enabled: true`), each trial trains **k models** (one per fold) and returns the **average metric** across folds\n",
    "- The number of **trials** is controlled by `sampling.max_trials` (e.g., 2 trials in smoke.yaml)\n",
    "- With k=5 folds and 2 trials: **2 trials × 5 folds = 10 model trainings total**\n",
    "- K-fold CV provides more robust hyperparameter evaluation but increases compute time (k× per trial)\n",
    "\n",
    "**Note on Checkpoint and Resume:**\n",
    "- When `checkpoint.enabled: true` is set in the HPO config, the system automatically saves the Optuna study state to a SQLite database\n",
    "- This allows interrupted HPO runs to be resumed from the last checkpoint\n",
    "- The checkpoint is automatically detected and loaded on the next run if `auto_resume: true` (default)\n",
    "- Platform-specific paths are handled automatically (local, Colab, Kaggle)\n",
    "- See `docs/HPO_CHECKPOINT_RESUME.md` for detailed documentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from orchestration import STAGE_HPO\n",
    "from orchestration.jobs import run_local_hpo_sweep\n",
    "\n",
    "# Constants are imported from orchestration module\n",
    "HPO_OUTPUT_DIR = ROOT_DIR / \"outputs\" / \"hpo\"\n",
    "HPO_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using stage-specific HPO config for hpo: hpo/smoke.yaml\n"
     ]
    }
   ],
   "source": [
    "# Use HPO config already loaded in configs (from Step P1-3.1)\n",
    "# Following DRY principle - don't reload configs that are already available\n",
    "# Check for stage-specific hpo_config override\n",
    "from orchestration.naming import get_stage_config\n",
    "from shared.yaml_utils import load_yaml\n",
    "\n",
    "hpo_stage_config = get_stage_config(experiment_config, STAGE_HPO)\n",
    "hpo_config_override = hpo_stage_config.get(\"hpo_config\")\n",
    "\n",
    "if hpo_config_override:\n",
    "    # Load stage-specific HPO config override\n",
    "    hpo_config_path = CONFIG_DIR / hpo_config_override\n",
    "    hpo_config = load_yaml(hpo_config_path)\n",
    "    print(f\"✓ Using stage-specific HPO config for hpo: {hpo_config_override}\")\n",
    "else:\n",
    "    # Use default HPO config from top-level experiment config\n",
    "    hpo_config = configs[\"hpo\"]\n",
    "    print(f\"✓ Using default HPO config: {experiment_config.hpo_config.name}\")\n",
    "train_config = configs[\"train\"]\n",
    "backbone_values = hpo_config[\"search_space\"][\"backbone\"][\"values\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup K-Fold Splits and Google Drive Backup for HPO Trials\n",
    "\n",
    "**K-Fold Cross-Validation Setup**: If k-fold CV is enabled in the HPO config, create and save fold splits before starting the sweep.\n",
    "\n",
    "**Colab-specific feature**: Configure automatic backup of each HPO trial to Google Drive immediately after completion. This prevents data loss if the Colab session disconnects during long-running hyperparameter optimization sweeps.\n",
    "\n",
    "Each trial's results (including `metrics.json` and checkpoint) are automatically backed up to Google Drive as soon as the trial completes, ensuring no progress is lost even if the session times out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up 2-fold cross-validation splits...\n",
      "[CV] Fold 0: {'SKILL': 154} | Missing: ['EDUCATION', 'DESIGNATION', 'EXPERIENCE', 'NAME', 'EMAIL', 'PHONE', 'LOCATION']\n",
      "[CV] Fold 1: {'SKILL': 107, 'LOCATION': 4, 'DESIGNATION': 1, 'EXPERIENCE': 1, 'EDUCATION': 1} | Missing: ['NAME', 'EMAIL', 'PHONE']\n",
      "✓ K-fold splits saved to: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\hpo\\fold_splits.json\n"
     ]
    }
   ],
   "source": [
    "from training.cv_utils import create_kfold_splits, save_fold_splits, validate_splits\n",
    "from training.data import load_dataset\n",
    "\n",
    "# Setup k-fold splits if enabled\n",
    "k_fold_config = hpo_config.get(\"k_fold\", {})\n",
    "k_folds_enabled = k_fold_config.get(\"enabled\", False)\n",
    "fold_splits_file = None\n",
    "\n",
    "if k_folds_enabled:\n",
    "    n_splits = k_fold_config.get(\"n_splits\", DEFAULT_K_FOLDS)\n",
    "    random_seed = k_fold_config.get(\"random_seed\", DEFAULT_RANDOM_SEED)\n",
    "    shuffle = k_fold_config.get(\"shuffle\", True)\n",
    "    stratified = k_fold_config.get(\"stratified\", False)\n",
    "    entity_types = configs.get(\"data\", {}).get(\"schema\", {}).get(\"entity_types\", [])\n",
    "    \n",
    "    print(f\"Setting up {n_splits}-fold cross-validation splits...\")\n",
    "    full_dataset = load_dataset(str(DATASET_LOCAL_PATH))\n",
    "    train_data = full_dataset.get(\"train\", [])\n",
    "    \n",
    "    fold_splits = create_kfold_splits(\n",
    "        dataset=train_data,\n",
    "        k=n_splits,\n",
    "        random_seed=random_seed,\n",
    "        shuffle=shuffle,\n",
    "        stratified=stratified,\n",
    "        entity_types=entity_types,\n",
    "    )\n",
    "    \n",
    "    # Optional validation to ensure rare entities appear across folds\n",
    "    validate_splits(train_data, fold_splits, entity_types=entity_types)\n",
    "    \n",
    "    HPO_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    fold_splits_file = HPO_OUTPUT_DIR / \"fold_splits.json\"\n",
    "    save_fold_splits(\n",
    "        fold_splits,\n",
    "        fold_splits_file,\n",
    "        metadata={\n",
    "            \"k\": n_splits,\n",
    "            \"random_seed\": random_seed,\n",
    "            \"shuffle\": shuffle,\n",
    "            \"stratified\": stratified,\n",
    "            \"dataset_path\": str(DATASET_LOCAL_PATH),\n",
    "        }\n",
    "    )\n",
    "    print(f\"✓ K-fold splits saved to: {fold_splits_file}\")\n",
    "else:\n",
    "    print(\"K-fold CV disabled - using single train/validation split\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint functionality is now handled automatically by run_local_hpo_sweep\n",
    "# when checkpoint.enabled: true is set in the HPO config.\n",
    "# No manual backup callbacks are needed - SQLite persistence is built-in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint functionality is now handled automatically by run_local_hpo_sweep\n",
    "# when checkpoint.enabled: true is set in the HPO config.\n",
    "# No wrapper functions are needed - SQLite persistence is built-in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In a Kaggle notebook cell\n",
    "# !cd /kaggle/working/resume-ner-azureml && git fetch origin feature/google-colab-compute && git checkout origin/feature/google-colab-compute -- src/train.py src/training/trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-27 23:56:48,609 - orchestration.jobs.hpo.local_sweeps - INFO - [HPO] Starting optimization for distilbert with checkpointing...\n",
      "2025-12-27 23:56:49,165 - orchestration.jobs.hpo.local_sweeps - WARNING - Could not set MLflow experiment: local variable 'mlflow' referenced before assignment\n",
      "2025-12-27 23:56:49,166 - orchestration.jobs.hpo.local_sweeps - WARNING - Continuing without MLflow tracking...\n",
      "2025-12-27 23:56:51,029 - orchestration.jobs.tracking.mlflow_tracker - INFO - Using Azure ML Workspace for MLflow tracking\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run trial_0 at: https://japanwest.api.azureml.ms/mlflow/v2.0/subscriptions/a23fa87c-802c-4fdf-9e59-e3d7969bcf31/resourceGroups/resume_ner_2025-12-14-13-17-35/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/4e8ebac4-d78a-46fc-9993-724858d8a72d/runs/e129db92-3add-4027-b389-975ac70b2bf8\n",
      "🧪 View experiment at: https://japanwest.api.azureml.ms/mlflow/v2.0/subscriptions/a23fa87c-802c-4fdf-9e59-e3d7969bcf31/resourceGroups/resume_ner_2025-12-14-13-17-35/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/4e8ebac4-d78a-46fc-9993-724858d8a72d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-27 23:59:31,938 - orchestration.jobs.hpo.local_sweeps - INFO - \n",
      "2025-12-27 23:59:31,938 - orchestration.jobs.hpo.local_sweeps - INFO - [BEST]: trial_0\n",
      "2025-12-27 23:59:31,939 - orchestration.jobs.hpo.local_sweeps - INFO -   Metrics: macro-f1=0.210576 | span=0.181818 | loss=2.074378 | entity_f1=0.060388 (8 entities)\n",
      "2025-12-27 23:59:31,939 - orchestration.jobs.hpo.local_sweeps - INFO -   Params: learning_rate=1.85e-05 | batch_size=4 | dropout=0.184636 | weight_decay=0.036972 (Run ID: e129db92-3ad...)\n",
      "Best trial: 0. Best value: 0.210576:  50%|█████     | 1/2 [02:13<02:13, 133.15s/it, 133.13/1200 seconds]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run trial_1 at: https://japanwest.api.azureml.ms/mlflow/v2.0/subscriptions/a23fa87c-802c-4fdf-9e59-e3d7969bcf31/resourceGroups/resume_ner_2025-12-14-13-17-35/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/4e8ebac4-d78a-46fc-9993-724858d8a72d/runs/3ab1246b-3369-41c3-8241-56f5f19a809b\n",
      "🧪 View experiment at: https://japanwest.api.azureml.ms/mlflow/v2.0/subscriptions/a23fa87c-802c-4fdf-9e59-e3d7969bcf31/resourceGroups/resume_ner_2025-12-14-13-17-35/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/4e8ebac4-d78a-46fc-9993-724858d8a72d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-28 00:01:43,949 - orchestration.jobs.hpo.local_sweeps - INFO - \n",
      "2025-12-28 00:01:43,951 - orchestration.jobs.hpo.local_sweeps - INFO - [Trial 1]: trial_1\n",
      "2025-12-28 00:01:43,951 - orchestration.jobs.hpo.local_sweeps - INFO -   Metrics: macro-f1=0.141890 | span=0.021818 | loss=2.220258 | entity_f1=0.011132 (7 entities)\n",
      "2025-12-28 00:01:43,952 - orchestration.jobs.hpo.local_sweeps - INFO -   Params: learning_rate=1.02e-05 | batch_size=4 | dropout=0.209284 | weight_decay=0.028925 (Run ID: 3ab1246b-336...)\n",
      "Best trial: 0. Best value: 0.210576: 100%|██████████| 2/2 [04:25<00:00, 132.58s/it, 265.14/1200 seconds]\n",
      "2025-12-28 00:01:57,926 - orchestration.jobs.tracking.mlflow_tracker - INFO - Found 2 total child runs for best trial search\n",
      "2025-12-28 00:02:03,015 - orchestration.jobs.tracking.mlflow_tracker - INFO - Best trial: 0 (run ID: e129db92-3ad...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run hpo_distilbert_smoke_test_1.4 at: https://japanwest.api.azureml.ms/mlflow/v2.0/subscriptions/a23fa87c-802c-4fdf-9e59-e3d7969bcf31/resourceGroups/resume_ner_2025-12-14-13-17-35/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/4e8ebac4-d78a-46fc-9993-724858d8a72d/runs/38904fb6-e696-40e9-b5f4-1e62551593d2\n",
      "🧪 View experiment at: https://japanwest.api.azureml.ms/mlflow/v2.0/subscriptions/a23fa87c-802c-4fdf-9e59-e3d7969bcf31/resourceGroups/resume_ner_2025-12-14-13-17-35/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/4e8ebac4-d78a-46fc-9993-724858d8a72d\n"
     ]
    }
   ],
   "source": [
    "# Extract checkpoint configuration from HPO config\n",
    "checkpoint_config = hpo_config.get(\"checkpoint\", {})\n",
    "\n",
    "hpo_studies = {}\n",
    "k_folds_param = k_fold_config.get(\"n_splits\", DEFAULT_K_FOLDS) if k_folds_enabled else None\n",
    "\n",
    "for backbone in backbone_values:\n",
    "    mlflow_experiment_name = build_mlflow_experiment_name(\n",
    "        experiment_config.name, STAGE_HPO, backbone\n",
    "    )\n",
    "    backbone_output_dir = HPO_OUTPUT_DIR / backbone\n",
    "    \n",
    "    # Use standard run_local_hpo_sweep with checkpoint_config\n",
    "    # Checkpoint.enabled handles persistence via SQLite (better than manual Drive backup)\n",
    "    study = run_local_hpo_sweep(\n",
    "        dataset_path=str(DATASET_LOCAL_PATH),\n",
    "        config_dir=CONFIG_DIR,\n",
    "        backbone=backbone,\n",
    "        hpo_config=hpo_config,\n",
    "        train_config=train_config,\n",
    "        output_dir=backbone_output_dir,\n",
    "        mlflow_experiment_name=mlflow_experiment_name,\n",
    "        k_folds=k_folds_param,\n",
    "        fold_splits_file=fold_splits_file,\n",
    "        checkpoint_config=checkpoint_config,\n",
    "    )\n",
    "    \n",
    "    hpo_studies[backbone] = study\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert: 2 trials completed\n",
      "  Best macro-f1: 0.2106\n",
      "  Best params: {'learning_rate': 1.850331567345489e-05, 'batch_size': 4, 'dropout': 0.1846357866460817, 'weight_decay': 0.03697191631420018}\n",
      "  CV Statistics: Mean: 0.2106 ± 0.0441\n"
     ]
    }
   ],
   "source": [
    "def extract_cv_statistics(best_trial):\n",
    "    if not hasattr(best_trial, \"user_attrs\"):\n",
    "        return None\n",
    "    cv_mean = best_trial.user_attrs.get(\"cv_mean\")\n",
    "    cv_std = best_trial.user_attrs.get(\"cv_std\")\n",
    "    return (cv_mean, cv_std) if cv_mean is not None else None\n",
    "\n",
    "objective_metric = hpo_config['objective']['metric']\n",
    "\n",
    "for backbone, study in hpo_studies.items():\n",
    "    if not study.trials:\n",
    "        continue\n",
    "    \n",
    "    best_trial = study.best_trial\n",
    "    cv_stats = extract_cv_statistics(best_trial)\n",
    "    \n",
    "    print(f\"{backbone}: {len(study.trials)} trials completed\")\n",
    "    print(f\"  Best {objective_metric}: {best_trial.value:.4f}\")\n",
    "    print(f\"  Best params: {best_trial.params}\")\n",
    "    \n",
    "    if cv_stats:\n",
    "        cv_mean, cv_std = cv_stats\n",
    "        print(f\"  CV Statistics: Mean: {cv_mean:.4f} ± {cv_std:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.5: Benchmarking Best Trials\n",
    "\n",
    "Benchmark the best trial from each backbone to measure actual inference performance. This provides real latency data that replaces parameter-count proxies in model selection, enabling more accurate speed comparisons.\n",
    "\n",
    "**Workflow:**\n",
    "1. Identify best trial per backbone (from HPO results)\n",
    "2. Run benchmarking on each best trial checkpoint\n",
    "3. Save benchmark results as `benchmark.json` in trial directories\n",
    "4. Model selection will automatically use this data when available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert: Best trial is trial_0_20251227_175111 (macro-f1=0.3840)\n"
     ]
    }
   ],
   "source": [
    "from orchestration.jobs.local_selection import load_best_trial_from_disk\n",
    "import json\n",
    "\n",
    "# Load benchmark config (if available)\n",
    "benchmark_config = configs.get(\"benchmark\", {})\n",
    "benchmark_settings = benchmark_config.get(\"benchmarking\", {})\n",
    "\n",
    "# Get benchmark parameters from config or use defaults\n",
    "benchmark_batch_sizes = benchmark_settings.get(\"batch_sizes\", [1, 8, 16])\n",
    "benchmark_iterations = benchmark_settings.get(\"iterations\", 100)\n",
    "benchmark_warmup = benchmark_settings.get(\"warmup_iterations\", 10)\n",
    "benchmark_max_length = benchmark_settings.get(\"max_length\", 512)\n",
    "benchmark_device = benchmark_settings.get(\"device\")\n",
    "\n",
    "# Get test data path from benchmark config or data config\n",
    "test_data_path_str = benchmark_settings.get(\"test_data\")\n",
    "if test_data_path_str:\n",
    "    test_data_path = (CONFIG_DIR / test_data_path_str).resolve()\n",
    "else:\n",
    "    # Fallback to dataset directory\n",
    "    test_data_path = DATASET_LOCAL_PATH / \"test.json\"\n",
    "\n",
    "if not test_data_path.exists():\n",
    "    print(f\"Warning: Test data not found at {test_data_path}\")\n",
    "    print(\"Benchmarking will be skipped. Model selection will use parameter proxy.\")\n",
    "    test_data_path = None\n",
    "\n",
    "# Identify best trials per backbone\n",
    "objective_metric = hpo_config[\"objective\"][\"metric\"]\n",
    "best_trials = {}\n",
    "\n",
    "for backbone in backbone_values:\n",
    "    best_trial_info = load_best_trial_from_disk(\n",
    "        HPO_OUTPUT_DIR,\n",
    "        backbone,\n",
    "        objective_metric\n",
    "    )\n",
    "    if best_trial_info:\n",
    "        best_trials[backbone] = best_trial_info\n",
    "        print(f\"{backbone}: Best trial is {best_trial_info['trial_name']} \"\n",
    "              f\"({objective_metric}={best_trial_info['accuracy']:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Benchmarking distilbert (trial_0_20251227_175111)...\n",
      "✓ Benchmark completed: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\hpo\\distilbert\\trial_0_20251227_175111_fold0\\benchmark.json\n",
      "\n",
      "✓ Benchmarking complete. 1/1 trials benchmarked.\n"
     ]
    }
   ],
   "source": [
    "# Run benchmarking on best trials\n",
    "if test_data_path and test_data_path.exists():\n",
    "    benchmark_results = {}\n",
    "    \n",
    "    for backbone, trial_info in best_trials.items():\n",
    "        trial_dir = Path(trial_info[\"trial_dir\"])\n",
    "        checkpoint_dir = trial_dir / CHECKPOINT_DIRNAME\n",
    "        benchmark_output = trial_dir / BENCHMARK_FILENAME\n",
    "        \n",
    "        if not checkpoint_dir.exists():\n",
    "            print(f\"Warning: Checkpoint not found for {backbone} {trial_info['trial_name']}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nBenchmarking {backbone} ({trial_info['trial_name']})...\")\n",
    "        \n",
    "        success = run_benchmarking_local(\n",
    "            checkpoint_dir=checkpoint_dir,\n",
    "            test_data_path=test_data_path,\n",
    "            output_path=benchmark_output,\n",
    "            batch_sizes=benchmark_batch_sizes,\n",
    "            iterations=benchmark_iterations,\n",
    "            warmup_iterations=benchmark_warmup,\n",
    "            max_length=benchmark_max_length,\n",
    "            device=benchmark_device,\n",
    "        )\n",
    "        \n",
    "        if success:\n",
    "            benchmark_results[backbone] = benchmark_output\n",
    "            print(f\"✓ Benchmark completed: {benchmark_output}\")\n",
    "        else:\n",
    "            print(f\"✗ Benchmark failed for {backbone}\")\n",
    "    \n",
    "    print(f\"\\n✓ Benchmarking complete. {len(benchmark_results)}/{len(best_trials)} trials benchmarked.\")\n",
    "else:\n",
    "    print(\"Skipping benchmarking (test data not available)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert: benchmark.json exists (batch_1 latency: 5.777592999365879 ms)\n"
     ]
    }
   ],
   "source": [
    "# Verify benchmark files were created\n",
    "if test_data_path and test_data_path.exists():\n",
    "    for backbone, trial_info in best_trials.items():\n",
    "        trial_dir = Path(trial_info[\"trial_dir\"])\n",
    "        benchmark_file = trial_dir / BENCHMARK_FILENAME\n",
    "        \n",
    "        if benchmark_file.exists():\n",
    "            with open(benchmark_file, \"r\") as f:\n",
    "                benchmark_data = json.load(f)\n",
    "            batch_1_latency = benchmark_data.get(\"batch_1\", {}).get(\"mean_ms\", \"N/A\")\n",
    "            print(f\"{backbone}: benchmark.json exists (batch_1 latency: {batch_1_latency} ms)\")\n",
    "        else:\n",
    "            print(f\"{backbone}: benchmark.json not found (will use parameter proxy)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.6: Best Configuration Selection (Automated)\n",
    "\n",
    "Programmatically select the best configuration from all HPO sweep runs across all backbone models. The best configuration is determined by the objective metric specified in the HPO config.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import importlib.util\n",
    "from shared.json_cache import save_json\n",
    "\n",
    "# Import local_selection directly to avoid triggering Azure ML imports in __init__.py\n",
    "local_selection_spec = importlib.util.spec_from_file_location(\n",
    "    \"local_selection\", SRC_DIR / \"orchestration\" / \"jobs\" / \"local_selection.py\"\n",
    ")\n",
    "local_selection = importlib.util.module_from_spec(local_selection_spec)\n",
    "local_selection_spec.loader.exec_module(local_selection)\n",
    "select_best_configuration_across_studies = local_selection.select_best_configuration_across_studies\n",
    "\n",
    "BEST_CONFIG_CACHE_FILE = ROOT_DIR / \"notebooks\" / \"best_configuration_cache.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_version = data_config.get(\"version\", \"unknown\")\n",
    "\n",
    "# Select best configuration with accuracy-speed tradeoff\n",
    "# Supports both in-memory studies and disk-based selection\n",
    "# Uses threshold from hpo_config[\"selection\"] if configured\n",
    "\n",
    "# Option 1: Use in-memory studies (if notebook still running)\n",
    "if 'hpo_studies' in locals() and hpo_studies:\n",
    "    best_configuration = select_best_configuration_across_studies(\n",
    "        studies=hpo_studies,\n",
    "        hpo_config=hpo_config,\n",
    "        dataset_version=dataset_version,\n",
    "        # Uses accuracy_threshold from hpo_config[\"selection\"] if set\n",
    "    )\n",
    "else:\n",
    "    # Option 2: Load from disk (works after notebook restart)\n",
    "    HPO_OUTPUT_DIR = ROOT_DIR / \"outputs\" / \"hpo\"\n",
    "    best_configuration = select_best_configuration_across_studies(\n",
    "        studies=None,  # No in-memory studies\n",
    "        hpo_config=hpo_config,\n",
    "        dataset_version=dataset_version,\n",
    "        hpo_output_dir=HPO_OUTPUT_DIR,  # Load from saved metrics.json files\n",
    "        # Uses accuracy_threshold from hpo_config[\"selection\"] if set\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best configuration selected:\n",
      "  Backbone: distilbert\n",
      "  Trial: trial_0\n",
      "  Best macro-f1: 0.2106\n",
      "  Selection reason: Best accuracy (0.2106)\n",
      "\n",
      "✓ Saved timestamped cache: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\cache\\best_configurations\\best_config_distilbert_trial_0_20251228_000511.json\n",
      "✓ Updated latest cache: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\cache\\best_configurations\\latest_best_configuration.json\n",
      "✓ Updated index: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\cache\\best_configurations\\index.json\n",
      "✓ Saved legacy cache (backward compatibility): c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\notebooks\\best_configuration_cache.json\n",
      "\n",
      "  Cache directory: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\cache\\best_configurations\n"
     ]
    }
   ],
   "source": [
    "from orchestration.paths import (\n",
    "    resolve_output_path,\n",
    "    save_cache_with_dual_strategy,\n",
    ")\n",
    "from datetime import datetime\n",
    "\n",
    "# Use centralized path resolution\n",
    "BEST_CONFIG_CACHE_DIR = resolve_output_path(\n",
    "    ROOT_DIR,\n",
    "    CONFIG_DIR,\n",
    "    \"cache\",\n",
    "    subcategory=\"best_configurations\"\n",
    ")\n",
    "\n",
    "# Generate timestamp and identifiers\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "backbone = best_configuration.get('backbone', 'unknown')\n",
    "trial_name = best_configuration.get('trial_name', 'unknown')\n",
    "\n",
    "# Save using dual file strategy\n",
    "timestamped_file, latest_file, index_file = save_cache_with_dual_strategy(\n",
    "    root_dir=ROOT_DIR,\n",
    "    config_dir=CONFIG_DIR,\n",
    "    cache_type=\"best_configurations\",\n",
    "    data=best_configuration,\n",
    "    backbone=backbone,\n",
    "    identifier=trial_name,\n",
    "    timestamp=timestamp,\n",
    "    additional_metadata={\n",
    "        \"experiment_name\": experiment_config.name if 'experiment_config' in locals() else \"unknown\",\n",
    "        \"hpo_study_name\": hpo_config.get('study_name', 'unknown') if 'hpo_config' in locals() else \"unknown\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Also save to legacy location for backward compatibility\n",
    "LEGACY_CACHE_FILE = ROOT_DIR / \"notebooks\" / \"best_configuration_cache.json\"\n",
    "save_json(LEGACY_CACHE_FILE, best_configuration)\n",
    "\n",
    "print(f\"Best configuration selected:\")\n",
    "print(f\"  Backbone: {backbone}\")\n",
    "print(f\"  Trial: {trial_name}\")\n",
    "print(f\"  Best {hpo_config['objective']['metric']}: {best_configuration.get('selection_criteria', {}).get('best_value'):.4f}\")\n",
    "\n",
    "# Show selection reasoning (if available)\n",
    "selection_criteria = best_configuration.get('selection_criteria', {})\n",
    "if 'reason' in selection_criteria:\n",
    "    print(f\"  Selection reason: {selection_criteria['reason']}\")\n",
    "if 'accuracy_diff_from_best' in selection_criteria:\n",
    "    print(f\"  Accuracy difference from best: {selection_criteria['accuracy_diff_from_best']:.4f}\")\n",
    "\n",
    "# Show all candidates (if available)\n",
    "if 'all_candidates' in selection_criteria:\n",
    "    print(f\"\\nAll candidates considered:\")\n",
    "    for c in selection_criteria['all_candidates']:\n",
    "        marker = \"✓\" if c['backbone'] == backbone else \" \"\n",
    "        print(f\"  {marker} {c['backbone']}: acc={c['accuracy']:.4f}, speed={c['speed_score']:.2f}x\")\n",
    "\n",
    "print(f\"\\n✓ Saved timestamped cache: {timestamped_file}\")\n",
    "print(f\"✓ Updated latest cache: {latest_file}\")\n",
    "print(f\"✓ Updated index: {index_file}\")\n",
    "print(f\"✓ Saved legacy cache (backward compatibility): {LEGACY_CACHE_FILE}\")\n",
    "print(f\"\\n  Cache directory: {BEST_CONFIG_CACHE_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.7: Final Training (Post-HPO, Single Run)\n",
    "\n",
    "Train the final production model using the best configuration from HPO with stable, controlled conditions. This uses the full training epochs (no early stopping) and the best hyperparameters found during HPO.\n",
    "\n",
    "**Note**: After training completes, the checkpoint will be automatically backed up to Google Drive for persistence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import mlflow\n",
    "from shared.json_cache import load_json, save_json\n",
    "from orchestration import STAGE_TRAINING\n",
    "\n",
    "# Define build_final_training_config locally to avoid importing Azure ML dependencies\n",
    "# This function doesn't use Azure ML, so we can define it here\n",
    "def build_final_training_config(\n",
    "    best_config: dict,\n",
    "    train_config: dict,\n",
    "    random_seed: int = 42,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Build final training configuration by merging best HPO config with train.yaml defaults.\n",
    "    \"\"\"\n",
    "    hyperparameters = best_config.get(\"hyperparameters\", {})\n",
    "    training_defaults = train_config.get(\"training\", {})\n",
    "    \n",
    "    return {\n",
    "        \"backbone\": best_config[\"backbone\"],\n",
    "        \"learning_rate\": hyperparameters.get(\"learning_rate\", training_defaults.get(\"learning_rate\", 2e-5)),\n",
    "        \"dropout\": hyperparameters.get(\"dropout\", training_defaults.get(\"dropout\", 0.1)),\n",
    "        \"weight_decay\": hyperparameters.get(\"weight_decay\", training_defaults.get(\"weight_decay\", 0.01)),\n",
    "        \"batch_size\": training_defaults.get(\"batch_size\", 16),\n",
    "        \"epochs\": training_defaults.get(\"epochs\", 5),\n",
    "        \"random_seed\": random_seed,\n",
    "        \"early_stopping_enabled\": False,\n",
    "        \"use_combined_data\": True,\n",
    "        \"use_all_data\": True,\n",
    "    }\n",
    "\n",
    "DEFAULT_RANDOM_SEED = 42\n",
    "BEST_CONFIG_CACHE_FILE = ROOT_DIR / \"notebooks\" / \"best_configuration_cache.json\"\n",
    "FINAL_TRAINING_OUTPUT_DIR = ROOT_DIR / \"outputs\" / \"final_training\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orchestration.paths import load_cache_file\n",
    "\n",
    "# Try loading from centralized cache first\n",
    "best_configuration = load_cache_file(\n",
    "    ROOT_DIR, CONFIG_DIR, \"best_configurations\", use_latest=True\n",
    ")\n",
    "\n",
    "# Fallback to legacy location\n",
    "if best_configuration is None:\n",
    "    LEGACY_CACHE_FILE = ROOT_DIR / \"notebooks\" / \"best_configuration_cache.json\"\n",
    "    best_configuration = load_json(LEGACY_CACHE_FILE, default=None)\n",
    "\n",
    "if best_configuration is None:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Best configuration cache not found.\\n\"\n",
    "        f\"Please run Step P1-3.6: Best Configuration Selection first.\\n\"\n",
    "        f\"Cache directory: {resolve_output_path(ROOT_DIR, CONFIG_DIR, 'cache', subcategory='best_configurations')}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training configuration:\n",
      "  Backbone: distilbert\n",
      "  Learning rate: 1.850331567345489e-05\n",
      "  Batch size: 3\n",
      "  Dropout: 0.1846357866460817\n",
      "  Weight decay: 0.03697191631420018\n",
      "  Epochs: 1\n",
      "  Random seed: 42\n",
      "  Early stopping: False\n"
     ]
    }
   ],
   "source": [
    "# Build final training configuration from best HPO configuration\n",
    "# Use train_config from configs if available, otherwise load it\n",
    "if 'train_config' not in locals():\n",
    "    train_config = configs.get(\"train\", {})\n",
    "\n",
    "final_training_config = build_final_training_config(\n",
    "    best_config=best_configuration,\n",
    "    train_config=train_config,\n",
    "    random_seed=DEFAULT_RANDOM_SEED,\n",
    ")\n",
    "\n",
    "print(f\"Final training configuration:\")\n",
    "print(f\"  Backbone: {final_training_config['backbone']}\")\n",
    "print(f\"  Learning rate: {final_training_config['learning_rate']}\")\n",
    "print(f\"  Batch size: {final_training_config['batch_size']}\")\n",
    "print(f\"  Dropout: {final_training_config['dropout']}\")\n",
    "print(f\"  Weight decay: {final_training_config['weight_decay']}\")\n",
    "print(f\"  Epochs: {final_training_config['epochs']}\")\n",
    "print(f\"  Random seed: {final_training_config['random_seed']}\")\n",
    "print(f\"  Early stopping: {final_training_config['early_stopping_enabled']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training Run ID: 20251228_000723 (prevents overwriting on reruns)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='', creation_time=1766263257813, experiment_id='a5897b88-fd66-448c-ae65-1ef21bfc11dd', last_update_time=None, lifecycle_stage='active', name='resume_ner_baseline-training-distilbert', tags={}>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow_experiment_name = f\"{experiment_config.name}-{STAGE_TRAINING}-{final_training_config['backbone']}\"\n",
    "from datetime import datetime\n",
    "\n",
    "# Generate unique run ID to prevent overwriting on reruns\n",
    "final_training_run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "print(f\"Final training Run ID: {final_training_run_id} (prevents overwriting on reruns)\")\n",
    "\n",
    "mlflow_experiment_name = f\"{experiment_config.name}-{STAGE_TRAINING}-{final_training_config['backbone']}\"\n",
    "final_output_dir = FINAL_TRAINING_OUTPUT_DIR / f\"{final_training_config['backbone']}_{final_training_run_id}\"\n",
    "final_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "mlflow.set_experiment(mlflow_experiment_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training as a module (python -m training.train) to allow relative imports to work\n",
    "# This requires src/ to be in PYTHONPATH (set in env below)\n",
    "training_args = [\n",
    "    sys.executable,\n",
    "    \"-m\",\n",
    "    \"training.train\",\n",
    "    \"--data-asset\",\n",
    "    str(DATASET_LOCAL_PATH),\n",
    "    \"--config-dir\",\n",
    "    str(CONFIG_DIR),\n",
    "    \"--backbone\",\n",
    "    final_training_config[\"backbone\"],\n",
    "    \"--learning-rate\",\n",
    "    str(final_training_config[\"learning_rate\"]),\n",
    "    \"--batch-size\",\n",
    "    str(final_training_config[\"batch_size\"]),\n",
    "    \"--dropout\",\n",
    "    str(final_training_config[\"dropout\"]),\n",
    "    \"--weight-decay\",\n",
    "    str(final_training_config[\"weight_decay\"]),\n",
    "    \"--epochs\",\n",
    "    str(final_training_config[\"epochs\"]),\n",
    "    \"--random-seed\",\n",
    "    str(final_training_config[\"random_seed\"]),\n",
    "    \"--early-stopping-enabled\",\n",
    "    str(final_training_config[\"early_stopping_enabled\"]).lower(),\n",
    "    \"--use-combined-data\",\n",
    "    str(final_training_config[\"use_combined_data\"]).lower(),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow run name: distilbert_20251228_000723\n"
     ]
    }
   ],
   "source": [
    "training_env = os.environ.copy()\n",
    "training_env[\"AZURE_ML_OUTPUT_checkpoint\"] = str(final_output_dir)\n",
    "\n",
    "# Add src directory to PYTHONPATH to allow relative imports in training.train\n",
    "pythonpath = training_env.get(\"PYTHONPATH\", \"\")\n",
    "if pythonpath:\n",
    "    training_env[\"PYTHONPATH\"] = f\"{str(SRC_DIR)}{os.pathsep}{pythonpath}\"\n",
    "else:\n",
    "    training_env[\"PYTHONPATH\"] = str(SRC_DIR)\n",
    "\n",
    "mlflow_tracking_uri = mlflow.get_tracking_uri()\n",
    "if mlflow_tracking_uri:\n",
    "    training_env[\"MLFLOW_TRACKING_URI\"] = mlflow_tracking_uri\n",
    "training_env[\"MLFLOW_EXPERIMENT_NAME\"] = mlflow_experiment_name\n",
    "\n",
    "# Set custom run name for easier searching: {backbone}_{run_id}\n",
    "# Extract backbone name (e.g., \"distilbert\" from \"distilbert-base-uncased\" or use as-is if already short)\n",
    "backbone_value = final_training_config[\"backbone\"]\n",
    "# If backbone contains hyphens, extract the first part (e.g., \"distilbert\" from \"distilbert-base-uncased\")\n",
    "# Otherwise use as-is (e.g., \"distilbert\" stays \"distilbert\")\n",
    "backbone_name = backbone_value.split(\"-\")[0] if \"-\" in backbone_value else backbone_value\n",
    "training_env[\"MLFLOW_RUN_NAME\"] = f\"{backbone_name}_{final_training_run_id}\"\n",
    "print(f\"MLflow run name: {training_env['MLFLOW_RUN_NAME']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric param_learning_rate:\n",
      "1.850331567345489e-05\n",
      "Attempted to log scalar metric param_batch_size:\n",
      "3\n",
      "Attempted to log scalar metric param_dropout:\n",
      "0.1846357866460817\n",
      "Attempted to log scalar metric param_weight_decay:\n",
      "0.03697191631420018\n",
      "Attempted to log scalar metric param_epochs:\n",
      "1\n",
      "Attempted to log scalar metric param_backbone:\n",
      "distilbert-base-uncased\n",
      "Attempted to log scalar metric macro-f1:\n",
      "0.2900154400411734\n",
      "Attempted to log scalar metric macro-f1-span:\n",
      "0.060606060606060615\n",
      "Attempted to log scalar metric loss:\n",
      "2.014418840408325\n",
      "🏃 View run plucky_calypso_v3n8cx1y at: https://japanwest.api.azureml.ms/mlflow/v2.0/subscriptions/a23fa87c-802c-4fdf-9e59-e3d7969bcf31/resourceGroups/resume_ner_2025-12-14-13-17-35/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/a5897b88-fd66-448c-ae65-1ef21bfc11dd/runs/c8e83b94-82a3-4152-b649-152ee581d5f4\n",
      "🧪 View experiment at: https://japanwest.api.azureml.ms/mlflow/v2.0/subscriptions/a23fa87c-802c-4fdf-9e59-e3d7969bcf31/resourceGroups/resume_ner_2025-12-14-13-17-35/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/a5897b88-fd66-448c-ae65-1ef21bfc11dd\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = subprocess.run(\n",
    "    training_args,\n",
    "    cwd=ROOT_DIR,\n",
    "    env=training_env,\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    ")\n",
    "\n",
    "if result.returncode != 0:\n",
    "    raise RuntimeError(f\"Final training failed with return code {result.returncode}\")\n",
    "else:\n",
    "    # Print output for successful runs too (helpful for debugging)\n",
    "    if result.stdout:\n",
    "        print(result.stdout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking training completion...\n",
      "  Expected checkpoint: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\final_training\\distilbert_20251228_000723\\checkpoint (exists: True)\n",
      "  Actual checkpoint: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\checkpoint (exists: False)\n",
      "  Expected metrics: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\final_training\\distilbert_20251228_000723\\metrics.json (exists: True)\n",
      "  Actual metrics: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\metrics.json (exists: False)\n",
      "✓ Using expected checkpoint location: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\final_training\\distilbert_20251228_000723\\checkpoint\n",
      "✓ Metrics loaded from: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\final_training\\distilbert_20251228_000723\\metrics.json\n",
      "  Metrics: {'macro-f1': 0.2900154400411734, 'macro-f1-span': 0.060606060606060615, 'loss': 2.014418840408325, 'per_entity': {'AME': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 0}, 'ESIGNATION': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 0}, 'KILL': {'precision': 0.375, 'recall': 0.14285714285714285, 'f1': 0.20689655172413796, 'support': 21}, 'MAIL': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 0}, 'XPERIENCE': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'support': 0}}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Check actual checkpoint location\n",
    "# The training script may save to outputs/checkpoint instead of final_output_dir/checkpoint\n",
    "actual_checkpoint = ROOT_DIR / \"outputs\" / \"checkpoint\"\n",
    "actual_metrics = ROOT_DIR / \"outputs\" / METRICS_FILENAME\n",
    "expected_checkpoint = final_output_dir / \"checkpoint\"\n",
    "expected_metrics = final_output_dir / METRICS_FILENAME\n",
    "\n",
    "print(\"Checking training completion...\")\n",
    "print(f\"  Expected checkpoint: {expected_checkpoint} (exists: {expected_checkpoint.exists()})\")\n",
    "print(f\"  Actual checkpoint: {actual_checkpoint} (exists: {actual_checkpoint.exists()})\")\n",
    "print(f\"  Expected metrics: {expected_metrics} (exists: {expected_metrics.exists()})\")\n",
    "print(f\"  Actual metrics: {actual_metrics} (exists: {actual_metrics.exists()})\")\n",
    "\n",
    "# Determine which checkpoint and metrics to use\n",
    "checkpoint_source = None\n",
    "metrics_file = None\n",
    "\n",
    "if expected_checkpoint.exists() and any(expected_checkpoint.iterdir()):\n",
    "    checkpoint_source = expected_checkpoint\n",
    "    print(f\"✓ Using expected checkpoint location: {checkpoint_source}\")\n",
    "elif actual_checkpoint.exists() and any(actual_checkpoint.iterdir()):\n",
    "    checkpoint_source = actual_checkpoint\n",
    "    print(f\"✓ Using actual checkpoint location: {checkpoint_source}\")\n",
    "    # Update final_output_dir to match actual location\n",
    "    final_output_dir = actual_checkpoint.parent\n",
    "\n",
    "if expected_metrics.exists():\n",
    "    metrics_file = expected_metrics\n",
    "elif actual_metrics.exists():\n",
    "    metrics_file = actual_metrics\n",
    "\n",
    "# Load metrics if available\n",
    "metrics = None\n",
    "if metrics_file and metrics_file.exists():\n",
    "    with open(metrics_file, \"r\") as f:\n",
    "        metrics = json.load(f)\n",
    "    print(f\"✓ Metrics loaded from: {metrics_file}\")\n",
    "    print(f\"  Metrics: {metrics}\")\n",
    "elif checkpoint_source:\n",
    "    print(f\"⚠ Warning: Metrics file not found, but checkpoint exists.\")\n",
    "    metrics = {\"status\": \"completed\", \"checkpoint_found\": True}\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Training completed but no checkpoint found.\\n\"\n",
    "        f\"  Expected: {expected_checkpoint}\\n\"\n",
    "        f\"  Actual: {actual_checkpoint}\\n\"\n",
    "        f\"  Please check training logs for errors.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved timestamped final training cache: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\cache\\final_training\\final_training_distilbert_20251228_000723_20251228_001051.json\n",
      "✓ Updated latest cache: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\cache\\final_training\\latest_final_training_cache.json\n",
      "✓ Updated index: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\cache\\final_training\\final_training_index.json\n",
      "✓ Saved legacy cache: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\notebooks\\final_training_cache.json\n"
     ]
    }
   ],
   "source": [
    "from orchestration.paths import (\n",
    "    resolve_output_path,\n",
    "    save_cache_with_dual_strategy,\n",
    ")\n",
    "from datetime import datetime\n",
    "\n",
    "# Prepare cache data\n",
    "final_training_cache_data = {\n",
    "    \"output_dir\": str(final_output_dir),\n",
    "    \"backbone\": final_training_config[\"backbone\"],\n",
    "    \"run_id\": final_training_run_id,\n",
    "    \"config\": final_training_config,\n",
    "    \"metrics\": metrics,  # Include metrics if available\n",
    "}\n",
    "\n",
    "# Save using dual file strategy\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "backbone = final_training_config[\"backbone\"].replace('-', '_').replace('/', '_')\n",
    "run_id = final_training_run_id.replace('-', '_')\n",
    "\n",
    "timestamped_file, latest_file, index_file = save_cache_with_dual_strategy(\n",
    "    root_dir=ROOT_DIR,\n",
    "    config_dir=CONFIG_DIR,\n",
    "    cache_type=\"final_training\",\n",
    "    data=final_training_cache_data,\n",
    "    backbone=backbone,\n",
    "    identifier=run_id,\n",
    "    timestamp=timestamp,\n",
    "    additional_metadata={\n",
    "        \"checkpoint_path\": str(checkpoint_source) if checkpoint_source else None,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Also save to legacy location for backward compatibility\n",
    "LEGACY_CACHE_FILE = ROOT_DIR / \"notebooks\" / \"final_training_cache.json\"\n",
    "save_json(LEGACY_CACHE_FILE, final_training_cache_data)\n",
    "\n",
    "print(f\"✓ Saved timestamped final training cache: {timestamped_file}\")\n",
    "print(f\"✓ Updated latest cache: {latest_file}\")\n",
    "print(f\"✓ Updated index: {index_file}\")\n",
    "print(f\"✓ Saved legacy cache: {LEGACY_CACHE_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.8: Continued Training (Optional)\n",
    "\n",
    "Continue training with new data by loading a checkpoint from a previous training run. This enables fine-tuning and domain adaptation while preserving the model's learned knowledge.\n",
    "\n",
    "**Use Cases:**\n",
    "- Fine-tuning on new domain-specific data\n",
    "- Incremental learning with additional training samples\n",
    "- Domain adaptation to new data distributions\n",
    "\n",
    "**Configuration:**\n",
    "- Uses `config/train_continued.yaml` for continued training settings\n",
    "- Supports multiple data combination strategies (new_only, combined, append)\n",
    "- Automatically resolves checkpoint path from previous training cache or config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve checkpoint path and prepare dataset for continued training\n",
    "if CONTINUED_EXPERIMENT_ENABLED:\n",
    "    # Get previous training cache\n",
    "    from orchestration.paths import load_cache_file\n",
    "    \n",
    "    # Try loading from centralized cache first\n",
    "    previous_training = load_cache_file(\n",
    "        ROOT_DIR, CONFIG_DIR, \"final_training\", use_latest=True\n",
    "    )\n",
    "    \n",
    "    # Fallback to legacy location\n",
    "    if previous_training is None:\n",
    "        previous_cache_path = ROOT_DIR / continued_training_config.get(\n",
    "            \"previous_training_cache\", \n",
    "            \"notebooks/final_training_cache.json\"\n",
    "        )\n",
    "        previous_training = load_json(previous_cache_path, default=None)\n",
    "    \n",
    "    if previous_training:\n",
    "        # Get checkpoint directory from previous training\n",
    "        previous_output_dir = Path(previous_training.get(\"output_dir\", \"\"))\n",
    "        if previous_output_dir.exists():\n",
    "            previous_checkpoint_dir = previous_output_dir / \"checkpoint\"\n",
    "        else:\n",
    "            # Try to get from final_training_cache.json\n",
    "            final_training_cache = load_json(\n",
    "                ROOT_DIR / \"notebooks\" / \"final_training_cache.json\",\n",
    "                default=None\n",
    "            )\n",
    "            if final_training_cache:\n",
    "                previous_output_dir = Path(final_training_cache.get(\"output_dir\", \"\"))\n",
    "                previous_checkpoint_dir = previous_output_dir / \"checkpoint\"\n",
    "            else:\n",
    "                previous_checkpoint_dir = None\n",
    "    else:\n",
    "        previous_checkpoint_dir = None\n",
    "    \n",
    "    # Resolve checkpoint path using checkpoint loader\n",
    "    backbone = continued_configs[\"model\"][\"backbone\"].split(\"-\")[0] if \"-\" in continued_configs[\"model\"][\"backbone\"] else continued_configs[\"model\"][\"backbone\"]\n",
    "    run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Add checkpoint config to training config for resolution\n",
    "    continued_configs[\"training\"][\"checkpoint\"] = checkpoint_config\n",
    "    continued_configs[\"training\"][\"run_id\"] = run_id\n",
    "    continued_configs[\"_config_dir\"] = CONFIG_DIR\n",
    "    \n",
    "    checkpoint_path = resolve_checkpoint_path(\n",
    "        config=continued_configs,\n",
    "        previous_cache_path=previous_cache_path if previous_training else None,\n",
    "        backbone=backbone,\n",
    "        run_id=run_id,\n",
    "    )\n",
    "    \n",
    "    if checkpoint_path:\n",
    "        print(f\"✓ Resolved checkpoint: {checkpoint_path}\")\n",
    "    else:\n",
    "        print(\"⚠ No checkpoint found. Will create new model from backbone.\")\n",
    "        checkpoint_path = None\n",
    "    \n",
    "    # Prepare dataset based on strategy\n",
    "    data_strategy = data_config_continued.get(\"strategy\", \"combined\")\n",
    "    new_dataset_path_str = data_config_continued.get(\"new_dataset_path\")\n",
    "    \n",
    "    if not new_dataset_path_str:\n",
    "        # Try to get from experiment config\n",
    "        new_data_config = continued_training_config.get(\"new_data\", {})\n",
    "        new_dataset_path_str = new_data_config.get(\"local_path\")\n",
    "        if new_dataset_path_str:\n",
    "            new_dataset_path = (CONFIG_DIR / new_dataset_path_str).resolve()\n",
    "        else:\n",
    "            # Fallback to data config\n",
    "            new_dataset_path = DATASET_LOCAL_PATH\n",
    "    else:\n",
    "        new_dataset_path = (CONFIG_DIR / new_dataset_path_str).resolve() if not Path(new_dataset_path_str).is_absolute() else Path(new_dataset_path_str)\n",
    "    \n",
    "    print(f\"New dataset path: {new_dataset_path}\")\n",
    "    \n",
    "    # Combine datasets based on strategy\n",
    "    if data_strategy == \"new_only\":\n",
    "        combined_dataset = load_dataset(str(new_dataset_path))\n",
    "        print(f\"✓ Using new dataset only ({len(combined_dataset.get('train', []))} samples)\")\n",
    "    else:\n",
    "        old_dataset_path_str = data_config_continued.get(\"old_dataset_path\")\n",
    "        if old_dataset_path_str:\n",
    "            old_dataset_path = (CONFIG_DIR / old_dataset_path_str).resolve() if not Path(old_dataset_path_str).is_absolute() else Path(old_dataset_path_str)\n",
    "        else:\n",
    "            old_dataset_path = DATASET_LOCAL_PATH\n",
    "        \n",
    "        validation_ratio = data_config_continued.get(\"validation_ratio\", 0.1)\n",
    "        random_seed = data_config_continued.get(\"random_seed\", 42)\n",
    "        \n",
    "        combined_dataset = combine_datasets(\n",
    "            old_dataset_path=old_dataset_path,\n",
    "            new_dataset_path=new_dataset_path,\n",
    "            strategy=data_strategy,\n",
    "            validation_ratio=validation_ratio,\n",
    "            random_seed=random_seed,\n",
    "        )\n",
    "        print(f\"✓ Combined datasets using '{data_strategy}' strategy\")\n",
    "        print(f\"  Total training samples: {len(combined_dataset.get('train', []))}\")\n",
    "        print(f\"  Validation samples: {len(combined_dataset.get('validation', []))}\")\n",
    "    \n",
    "    # Update data config with combined dataset path (temporary location)\n",
    "    # We'll save the combined dataset to a temp location\n",
    "    combined_dataset_dir = ROOT_DIR / \"outputs\" / \"continued_training\" / \"combined_dataset\"\n",
    "    combined_dataset_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    import json\n",
    "    with open(combined_dataset_dir / \"train.json\", \"w\") as f:\n",
    "        json.dump(combined_dataset.get(\"train\", []), f, indent=2)\n",
    "    if combined_dataset.get(\"validation\"):\n",
    "        with open(combined_dataset_dir / \"validation.json\", \"w\") as f:\n",
    "            json.dump(combined_dataset[\"validation\"], f, indent=2)\n",
    "    \n",
    "    CONTINUED_DATASET_PATH = combined_dataset_dir\n",
    "    print(f\"✓ Combined dataset saved to: {CONTINUED_DATASET_PATH}\")\n",
    "else:\n",
    "    print(\"Skipping continued training setup (disabled)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve checkpoint path and prepare dataset for continued training\n",
    "if CONTINUED_EXPERIMENT_ENABLED:\n",
    "    # Get previous training cache\n",
    "    from orchestration.paths import load_cache_file\n",
    "\n",
    "    # Try loading from centralized cache first\n",
    "    previous_training = load_cache_file(\n",
    "        ROOT_DIR, CONFIG_DIR, \"final_training\", use_latest=True\n",
    "    )\n",
    "\n",
    "    # Fallback to legacy location\n",
    "    if previous_training is None:\n",
    "        previous_cache_path = ROOT_DIR / continued_training_config.get(\n",
    "            \"previous_training_cache\",\n",
    "            \"notebooks/final_training_cache.json\",\n",
    "        )\n",
    "        previous_training = load_json(previous_cache_path, default=None)\n",
    "\n",
    "    if previous_training:\n",
    "        # Get checkpoint directory from previous training\n",
    "        previous_output_dir = Path(previous_training.get(\"output_dir\", \"\"))\n",
    "        if previous_output_dir.exists():\n",
    "            previous_checkpoint_dir = previous_output_dir / \"checkpoint\"\n",
    "        else:\n",
    "            # Try to get from final_training_cache.json\n",
    "            final_training_cache = load_json(\n",
    "                ROOT_DIR / \"notebooks\" / \"final_training_cache.json\",\n",
    "                default=None,\n",
    "            )\n",
    "            if final_training_cache:\n",
    "                previous_output_dir = Path(\n",
    "                    final_training_cache.get(\"output_dir\", \"\"))\n",
    "                previous_checkpoint_dir = previous_output_dir / \"checkpoint\"\n",
    "            else:\n",
    "                previous_checkpoint_dir = None\n",
    "    else:\n",
    "        previous_checkpoint_dir = None\n",
    "\n",
    "    # Resolve checkpoint path using checkpoint loader\n",
    "    backbone = (\n",
    "        continued_configs[\"model\"][\"backbone\"].split(\"-\")[0]\n",
    "        if \"-\" in continued_configs[\"model\"][\"backbone\"]\n",
    "        else continued_configs[\"model\"][\"backbone\"]\n",
    "    )\n",
    "    run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # Add checkpoint config to training config for resolution\n",
    "    continued_configs[\"training\"][\"checkpoint\"] = checkpoint_config\n",
    "    continued_configs[\"training\"][\"run_id\"] = run_id\n",
    "    continued_configs[\"_config_dir\"] = CONFIG_DIR\n",
    "\n",
    "    checkpoint_path = resolve_checkpoint_path(\n",
    "        config=continued_configs,\n",
    "        previous_cache_path=previous_cache_path if previous_training else None,\n",
    "        backbone=backbone,\n",
    "        run_id=run_id,\n",
    "    )\n",
    "\n",
    "    if checkpoint_path:\n",
    "        print(f\"✓ Resolved checkpoint: {checkpoint_path}\")\n",
    "    else:\n",
    "        print(\"⚠ No checkpoint found. Will create new model from backbone.\")\n",
    "        checkpoint_path = None\n",
    "\n",
    "    # Prepare dataset based on strategy\n",
    "    data_strategy = data_config_continued.get(\"strategy\", \"combined\")\n",
    "    new_dataset_path_str = data_config_continued.get(\"new_dataset_path\")\n",
    "\n",
    "    if not new_dataset_path_str:\n",
    "        # Try to get from experiment config\n",
    "        new_data_config = continued_training_config.get(\"new_data\", {})\n",
    "        new_dataset_path_str = new_data_config.get(\"local_path\")\n",
    "        if new_dataset_path_str:\n",
    "            new_dataset_path = (CONFIG_DIR / new_dataset_path_str).resolve()\n",
    "        else:\n",
    "            # Fallback to data config\n",
    "            new_dataset_path = DATASET_LOCAL_PATH\n",
    "    else:\n",
    "        new_dataset_path = (\n",
    "            (CONFIG_DIR / new_dataset_path_str).resolve()\n",
    "            if not Path(new_dataset_path_str).is_absolute()\n",
    "            else Path(new_dataset_path_str)\n",
    "        )\n",
    "\n",
    "    print(f\"New dataset path: {new_dataset_path}\")\n",
    "\n",
    "    # Combine datasets based on strategy\n",
    "    if data_strategy == \"new_only\":\n",
    "        combined_dataset = load_dataset(str(new_dataset_path))\n",
    "        print(\n",
    "            f\"✓ Using new dataset only ({len(combined_dataset.get('train', []))} samples)\"\n",
    "        )\n",
    "    else:\n",
    "        old_dataset_path_str = data_config_continued.get(\"old_dataset_path\")\n",
    "        if old_dataset_path_str:\n",
    "            old_dataset_path = (\n",
    "                (CONFIG_DIR / old_dataset_path_str).resolve()\n",
    "                if not Path(old_dataset_path_str).is_absolute()\n",
    "                else Path(old_dataset_path_str)\n",
    "            )\n",
    "        else:\n",
    "            old_dataset_path = DATASET_LOCAL_PATH\n",
    "\n",
    "        validation_ratio = data_config_continued.get(\"validation_ratio\", 0.1)\n",
    "        random_seed = data_config_continued.get(\"random_seed\", 42)\n",
    "\n",
    "        combined_dataset = combine_datasets(\n",
    "            old_dataset_path=old_dataset_path,\n",
    "            new_dataset_path=new_dataset_path,\n",
    "            strategy=data_strategy,\n",
    "            validation_ratio=validation_ratio,\n",
    "            random_seed=random_seed,\n",
    "        )\n",
    "        print(f\"✓ Combined datasets using '{data_strategy}' strategy\")\n",
    "        print(\n",
    "            f\"  Total training samples: {len(combined_dataset.get('train', []))}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  Validation samples: {len(combined_dataset.get('validation', []))}\"\n",
    "        )\n",
    "\n",
    "    # Save combined dataset\n",
    "    combined_dataset_dir = (\n",
    "        ROOT_DIR / \"outputs\" / \"continued_training\" / \"combined_dataset\"\n",
    "    )\n",
    "    combined_dataset_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    import json\n",
    "\n",
    "    with open(combined_dataset_dir / \"train.json\", \"w\") as f:\n",
    "        json.dump(combined_dataset.get(\"train\", []), f, indent=2)\n",
    "\n",
    "    if combined_dataset.get(\"validation\"):\n",
    "        with open(combined_dataset_dir / \"validation.json\", \"w\") as f:\n",
    "            json.dump(combined_dataset[\"validation\"], f, indent=2)\n",
    "\n",
    "    CONTINUED_DATASET_PATH = combined_dataset_dir\n",
    "    print(f\"✓ Combined dataset saved to: {CONTINUED_DATASET_PATH}\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping continued training setup (disabled)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-4: Model Conversion & Optimization\n",
    "\n",
    "Convert the final training checkpoint to an optimized ONNX model (int8 quantized) for production inference.\n",
    "\n",
    "**Platform Adapter Note**: The conversion script (`src/model_conversion/convert_to_onnx.py`) uses the platform adapter to automatically handle output paths and logging appropriately for local execution.\n",
    "\n",
    "**Checkpoint Restoration**: \n",
    "- **Google Colab**: If the checkpoint is not found locally (e.g., after a session disconnect), it will be automatically restored from Google Drive.\n",
    "- **Kaggle**: Checkpoints are automatically persisted in `/kaggle/working/` - no restoration needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import mlflow\n",
    "import shutil\n",
    "from shared.json_cache import load_json\n",
    "\n",
    "CONVERSION_SCRIPT_PATH = SRC_DIR / \"model_conversion\" / \"convert_to_onnx.py\"\n",
    "FINAL_TRAINING_CACHE_FILE = ROOT_DIR / \"notebooks\" / \"final_training_cache.json\"\n",
    "CONVERSION_OUTPUT_DIR = ROOT_DIR / \"outputs\" / \"conversion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orchestration.paths import load_cache_file\n",
    "\n",
    "# Try loading from centralized cache first\n",
    "training_cache = load_cache_file(\n",
    "    ROOT_DIR, CONFIG_DIR, \"final_training\", use_latest=True\n",
    ")\n",
    "\n",
    "# Fallback to legacy location\n",
    "if training_cache is None:\n",
    "    LEGACY_CACHE_FILE = ROOT_DIR / \"notebooks\" / \"final_training_cache.json\"\n",
    "    training_cache = load_json(LEGACY_CACHE_FILE, default=None)\n",
    "\n",
    "# Try to restore from Google Drive if still not found\n",
    "if training_cache is None:\n",
    "    if restore_from_drive(\"final_training_cache.json\", LEGACY_CACHE_FILE, is_directory=False):\n",
    "        training_cache = load_json(LEGACY_CACHE_FILE, default=None)\n",
    "\n",
    "if training_cache is None:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Final training cache not found locally or in backup.\\n\"\n",
    "        f\"Please run Step P1-3.7: Final Training first.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using checkpoint: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\final_training\\distilbert_20251228_000723\\checkpoint\n",
      "Conversion output directory: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\conversion\\distilbert_20251228_001226\n",
      "Backbone: distilbert\n",
      "Conversion Run ID: 20251228_001226\n"
     ]
    }
   ],
   "source": [
    "# Extract checkpoint directory, backbone, and create conversion output directory\n",
    "from datetime import datetime\n",
    "\n",
    "# Get checkpoint directory from training cache\n",
    "checkpoint_source = Path(training_cache.get(\"output_dir\", \"\")) / \"checkpoint\"\n",
    "if not checkpoint_source.exists():\n",
    "    # Try alternative location\n",
    "    checkpoint_source = Path(training_cache.get(\"output_dir\", \"\")) / CHECKPOINT_DIRNAME\n",
    "    if not checkpoint_source.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Checkpoint not found in training cache output_dir: {training_cache.get('output_dir', '')}\"\n",
    "        )\n",
    "\n",
    "checkpoint_dir = checkpoint_source\n",
    "print(f\"Using checkpoint: {checkpoint_dir}\")\n",
    "\n",
    "# Extract backbone from training cache\n",
    "backbone = training_cache.get(\"backbone\", \"unknown\")\n",
    "if backbone == \"unknown\":\n",
    "    # Try to get from config\n",
    "    backbone = training_cache.get(\"config\", {}).get(\"backbone\", \"unknown\")\n",
    "    if backbone == \"unknown\":\n",
    "        raise ValueError(\"Could not determine backbone from training cache\")\n",
    "\n",
    "# Extract backbone name (e.g., \"distilbert\" from \"distilbert-base-uncased\")\n",
    "backbone_name = backbone.split(\"-\")[0] if \"-\" in backbone else backbone\n",
    "\n",
    "# Generate conversion run ID\n",
    "conversion_run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Create conversion output directory\n",
    "conversion_output_dir = CONVERSION_OUTPUT_DIR / f\"{backbone_name}_{conversion_run_id}\"\n",
    "conversion_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Conversion output directory: {conversion_output_dir}\")\n",
    "print(f\"Backbone: {backbone}\")\n",
    "print(f\"Conversion Run ID: {conversion_run_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run conversion as a module (python -m model_conversion.convert_to_onnx) to allow relative imports to work\n",
    "# This requires src/ to be in PYTHONPATH (set in env below)\n",
    "conversion_args = [\n",
    "    sys.executable,\n",
    "    \"-m\",\n",
    "    \"model_conversion.convert_to_onnx\",\n",
    "    \"--checkpoint-path\",\n",
    "    str(checkpoint_dir),\n",
    "    \"--config-dir\",\n",
    "    str(CONFIG_DIR),\n",
    "    \"--backbone\",\n",
    "    backbone,\n",
    "    \"--output-dir\",\n",
    "    str(conversion_output_dir),\n",
    "    \"--quantize-int8\",\n",
    "    \"--run-smoke-test\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_env = os.environ.copy()\n",
    "conversion_env[\"AZURE_ML_OUTPUT_onnx_model\"] = str(conversion_output_dir)\n",
    "\n",
    "# Add src directory to PYTHONPATH to allow relative imports in model_conversion.convert_to_onnx\n",
    "pythonpath = conversion_env.get(\"PYTHONPATH\", \"\")\n",
    "if pythonpath:\n",
    "    conversion_env[\"PYTHONPATH\"] = f\"{str(SRC_DIR)}{os.pathsep}{pythonpath}\"\n",
    "else:\n",
    "    conversion_env[\"PYTHONPATH\"] = str(SRC_DIR)\n",
    "\n",
    "mlflow_tracking_uri = mlflow.get_tracking_uri()\n",
    "if mlflow_tracking_uri:\n",
    "    conversion_env[\"MLFLOW_TRACKING_URI\"] = mlflow_tracking_uri\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = subprocess.run(\n",
    "    conversion_args,\n",
    "    cwd=ROOT_DIR,\n",
    "    env=conversion_env,\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    ")\n",
    "\n",
    "if result.returncode != 0:\n",
    "    print(\"Model conversion failed with the following output:\")\n",
    "    print(\"=\" * 80)\n",
    "    if result.stdout:\n",
    "        print(\"STDOUT:\")\n",
    "        print(result.stdout)\n",
    "    if result.stderr:\n",
    "        print(\"STDERR:\")\n",
    "        print(result.stderr)\n",
    "    print(\"=\" * 80)\n",
    "    raise RuntimeError(f\"Model conversion failed with return code {result.returncode}\")\n",
    "else:\n",
    "    # Print output for successful runs too (helpful for debugging)\n",
    "    if result.stdout:\n",
    "        print(result.stdout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Conversion completed. ONNX model: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\conversion\\distilbert_20251228_001226\\model_int8.onnx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from shared.json_cache import save_json\n",
    "import shutil\n",
    "\n",
    "ONNX_MODEL_FILENAME = \"model_int8.onnx\"\n",
    "FALLBACK_ONNX_MODEL_FILENAME = \"model.onnx\"\n",
    "CONVERSION_CACHE_FILE = ROOT_DIR / \"notebooks\" / \"conversion_cache.json\"\n",
    "\n",
    "onnx_model_path = conversion_output_dir / ONNX_MODEL_FILENAME\n",
    "if not onnx_model_path.exists():\n",
    "    onnx_model_path = conversion_output_dir / FALLBACK_ONNX_MODEL_FILENAME\n",
    "\n",
    "if not onnx_model_path.exists():\n",
    "    raise FileNotFoundError(f\"ONNX model not found in {conversion_output_dir}\")\n",
    "\n",
    "print(f\"✓ Conversion completed. ONNX model: {onnx_model_path}\")\n",
    "\n",
    "save_json(CONVERSION_CACHE_FILE, {\n",
    "    \"onnx_model_path\": str(onnx_model_path),\n",
    "    \"backbone\": backbone,\n",
    "    \"checkpoint_dir\": str(checkpoint_dir),\n",
    "})\n",
    "\n",
    "# Backup ONNX model to Google Drive (if available)\n",
    "if onnx_model_path.exists():\n",
    "    backup_to_drive(onnx_model_path, f\"{backbone}_model.onnx\", is_directory=False)\n",
    "else:\n",
    "    print(f\"⚠ Warning: ONNX model not found for backup: {onnx_model_path}\")\n",
    "\n",
    "# Backup conversion cache file to Drive\n",
    "backup_to_drive(CONVERSION_CACHE_FILE, \"conversion_cache.json\", is_directory=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
