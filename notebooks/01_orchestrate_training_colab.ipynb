{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Training Orchestration (Google Colab & Kaggle)\n",
    "\n",
    "This notebook orchestrates all training activities for **Google Colab or Kaggle execution** with GPU compute support.\n",
    "\n",
    "## Important\n",
    "\n",
    "- This notebook **executes training in Google Colab or Kaggle** (not on Azure ML)\n",
    "- All computation happens on the platform's GPU\n",
    "- **Storage & Persistence**:\n",
    "  - **Google Colab**: Checkpoints are automatically saved to Google Drive for persistence across sessions\n",
    "  - **Kaggle**: Outputs in `/kaggle/working/` are automatically persisted - no manual backup needed\n",
    "- The notebook must be **re-runnable end-to-end**\n",
    "- Uses the dataset path specified in the data config (from `config/data/*.yaml`), typically pointing to a local folder included in the repository\n",
    "- **Session Management**:\n",
    "  - **Colab**: Sessions timeout after 12-24 hours (depending on Colab plan). Checkpoints are saved to Drive automatically.\n",
    "  - **Kaggle**: Sessions have time limits based on your plan. All outputs are automatically saved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Detection\n",
    "\n",
    "The notebook automatically detects the execution environment (local, Google Colab, or Kaggle) and adapts its behavior accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Detected environment: LOCAL\n",
      "Platform: local\n",
      "Base directory: Current working directory\n",
      "Backup enabled: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "# Detect execution environment\n",
    "IN_COLAB = \"COLAB_GPU\" in os.environ or \"COLAB_TPU\" in os.environ\n",
    "IN_KAGGLE = \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
    "IS_LOCAL = not IN_COLAB and not IN_KAGGLE\n",
    "# Set platform-specific constants\n",
    "if IN_COLAB:\n",
    "    PLATFORM = \"colab\"\n",
    "    BASE_DIR = Path(\"/content\")\n",
    "    BACKUP_ENABLED = True\n",
    "elif IN_KAGGLE:\n",
    "    PLATFORM = \"kaggle\"\n",
    "    BASE_DIR = Path(\"/kaggle/working\")\n",
    "    BACKUP_ENABLED = False\n",
    "else:\n",
    "    PLATFORM = \"local\"\n",
    "    BASE_DIR = None\n",
    "    BACKUP_ENABLED = False\n",
    "print(f\"✓ Detected environment: {PLATFORM.upper()}\")\n",
    "print(f\"Platform: {PLATFORM}\")\n",
    "print(\n",
    "    f\"Base directory: {BASE_DIR if BASE_DIR else 'Current working directory'}\")\n",
    "print(f\"Backup enabled: {BACKUP_ENABLED}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Repository Setup\n",
    "\n",
    "**Note**: Repository setup is only needed for Colab/Kaggle environments. Local environments should already have the repository cloned.\n",
    "\n",
    "### For Colab/Kaggle: Clone from Git or Upload Files\n",
    "\n",
    "Choose one of the following options:\n",
    "\n",
    "**Option A: Clone from Git (Recommended)**\n",
    "\n",
    "If your repository is on GitHub/GitLab, clone it:\n",
    "\n",
    "**For Google Colab:**\n",
    "```python\n",
    "!git clone -b gg_final_training_2 https://github.com/longdang193/resume-ner-azureml.git /content/resume-ner-azureml\n",
    "```\n",
    "\n",
    "**For Kaggle:**\n",
    "```python\n",
    "!git clone -b gg_final_training_2 https://github.com/longdang193/resume-ner-azureml.git /kaggle/working/resume-ner-azureml\n",
    "```\n",
    "\n",
    "**Option B: Upload Files**\n",
    "\n",
    "**For Google Colab:**\n",
    "1. Use the Colab file browser (folder icon on left sidebar)\n",
    "2. Upload your project files to `/content/resume-ner-azureml/`\n",
    "3. Ensure the directory structure matches: `src/`, `config/`, `notebooks/`, etc.\n",
    "\n",
    "**For Kaggle:**\n",
    "1. Use the Kaggle file browser (Data tab)\n",
    "2. Upload your project files to `/kaggle/working/resume-ner-azureml/`\n",
    "3. Ensure the directory structure matches: `src/`, `config/`, `notebooks/`, etc.\n",
    "\n",
    "### For Local: Repository Already Exists\n",
    "\n",
    "Local environments should have the repository already cloned. The notebook will automatically detect the repository location.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Local environment detected - assuming repository already exists\n"
     ]
    }
   ],
   "source": [
    "# Repository setup - only needed for Colab/Kaggle\n",
    "if not IS_LOCAL:\n",
    "    if IN_KAGGLE:\n",
    "        # For Kaggle\n",
    "        !git clone -b gg_final_training_2 https://github.com/longdang193/resume-ner-azureml.git /kaggle/working/resume-ner-azureml\n",
    "    elif IN_COLAB:\n",
    "        # For Google Colab\n",
    "        !git clone -b gg_final_training_2 https://github.com/longdang193/resume-ner-azureml.git /content/resume-ner-azureml\n",
    "else:\n",
    "    print(\"✓ Local environment detected - assuming repository already exists\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Repository Setup\n",
    "\n",
    "Verify the repository structure exists:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Repository found at: /workspaces/resume-ner-azureml\n",
      "✓ Required directories found: ['src', 'config', 'notebooks']\n",
      "Notebook directory: /workspaces/resume-ner-azureml/notebooks\n",
      "Project root: /workspaces/resume-ner-azureml\n",
      "Source directory: /workspaces/resume-ner-azureml/src\n",
      "Config directory: /workspaces/resume-ner-azureml/config\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Unified path setup for all environments\n",
    "if IS_LOCAL:\n",
    "    # Local: assume notebook is in notebooks/ directory\n",
    "    NOTEBOOK_DIR = Path.cwd()\n",
    "    ROOT_DIR = NOTEBOOK_DIR.parent\n",
    "else:\n",
    "    # Colab/Kaggle: use fixed paths\n",
    "    ROOT_DIR = BASE_DIR / \"resume-ner-azureml\"\n",
    "\n",
    "SRC_DIR = ROOT_DIR / \"src\"\n",
    "CONFIG_DIR = ROOT_DIR / \"config\"\n",
    "NOTEBOOK_DIR = ROOT_DIR / \"notebooks\"\n",
    "\n",
    "# Verify repository structure\n",
    "if not ROOT_DIR.exists():\n",
    "    if IS_LOCAL:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Repository not found at {ROOT_DIR}\\n\"\n",
    "            f\"Please ensure you're running this notebook from the notebooks/ directory of the repository.\"\n",
    "        )\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Repository not found at {ROOT_DIR}\\n\"\n",
    "            f\"Please run Step 2 to clone or upload the repository.\"\n",
    "        )\n",
    "\n",
    "required_dirs = [\"src\", \"config\", \"notebooks\"]\n",
    "missing_dirs = [d for d in required_dirs if not (ROOT_DIR / d).exists()]\n",
    "\n",
    "if missing_dirs:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Missing required directories: {missing_dirs}\\n\"\n",
    "        f\"Please ensure the repository structure is correct.\"\n",
    "    )\n",
    "\n",
    "# Add to Python path\n",
    "sys.path.insert(0, str(ROOT_DIR))\n",
    "sys.path.insert(0, str(SRC_DIR))\n",
    "import importlib.util\n",
    "# Import directly from module file to avoid triggering orchestration.__init__.py\n",
    "# which imports mlflow (not yet installed at this point)\n",
    "paths_validation_spec = importlib.util.spec_from_file_location(\n",
    "    \"paths_validation\",\n",
    "    SRC_DIR / \"infrastructure\" / \"paths\" / \"validation.py\"\n",
    ")\n",
    "paths_validation = importlib.util.module_from_spec(paths_validation_spec)\n",
    "paths_validation_spec.loader.exec_module(paths_validation)\n",
    "validate_path_before_mkdir = paths_validation.validate_path_before_mkdir\n",
    "\n",
    "print(f\"✓ Repository found at: {ROOT_DIR}\")\n",
    "print(f\"✓ Required directories found: {required_dirs}\")\n",
    "print(f\"Notebook directory: {NOTEBOOK_DIR}\")\n",
    "print(f\"Project root: {ROOT_DIR}\")\n",
    "print(f\"Source directory: {SRC_DIR}\")\n",
    "print(f\"Config directory: {CONFIG_DIR}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Install Dependencies\n",
    "\n",
    "**For Local**: Use conda environment (instructions below).  \n",
    "**For Colab/Kaggle**: Install packages via pip (automated below).\n",
    "\n",
    "### Local Environment Setup\n",
    "\n",
    "For local execution, create and activate a conda environment:\n",
    "\n",
    "1. Open a terminal in the project root\n",
    "2. Create the conda environment: `conda env create -f config/environment/conda.yaml`\n",
    "3. Activate: `conda activate resume-ner-training`\n",
    "4. Restart the kernel after activation\n",
    "\n",
    "### Colab/Kaggle: Automated Installation\n",
    "\n",
    "PyTorch is usually pre-installed in Colab/Kaggle, but we'll verify and install other required packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1\n",
      "CUDA available: False\n",
      "✓ PyTorch version meets requirements\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check PyTorch version and GPU availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    device_count = torch.cuda.device_count()\n",
    "    print(f\"Visible GPUs: {device_count}\")\n",
    "    for i in range(device_count):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "# Verify PyTorch version meets requirements (>=2.6.0)\n",
    "torch_version = tuple(map(int, torch.__version__.split('.')[:2]))\n",
    "if torch_version < (2, 6):\n",
    "    print(f\"⚠ Warning: PyTorch {torch.__version__} may not meet requirements (>=2.6.0)\")\n",
    "    if not IS_LOCAL:\n",
    "        print(\"Consider upgrading: !pip install torch>=2.6.0 --upgrade\")\n",
    "else:\n",
    "    print(\"✓ PyTorch version meets requirements\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For local environment, please:\n",
      "1. Create conda environment: conda env create -f config/environment/conda.yaml\n",
      "2. Activate: conda activate resume-ner-training\n",
      "3. Restart kernel after activation\n",
      "\n",
      "If you've already done this, you can continue to the next cell.\n",
      "\n",
      "Installing Azure ML SDK (required for imports)...\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "if IS_LOCAL:\n",
    "    print(\"For local environment, please:\")\n",
    "    print(\"1. Create conda environment: conda env create -f config/environment/conda.yaml\")\n",
    "    print(\"2. Activate: conda activate resume-ner-training\")\n",
    "    print(\"3. Restart kernel after activation\")\n",
    "    print(\"\\nIf you've already done this, you can continue to the next cell.\")\n",
    "    print(\"\\nInstalling Azure ML SDK (required for imports)...\")\n",
    "    # Install Azure ML packages even for local (in case conda env not activated)\n",
    "    %pip install \"azure-ai-ml>=1.0.0\" --quiet\n",
    "    %pip install \"azure-identity>=1.12.0\" --quiet\n",
    "    %pip install azureml-defaults --quiet\n",
    "    %pip install azureml-mlflow --quiet\n",
    "else:\n",
    "    # Core ML libraries\n",
    "    %pip install \"transformers>=4.35.0,<5.0.0\" --quiet\n",
    "    %pip install \"safetensors>=0.4.0\" --quiet\n",
    "    %pip install \"datasets>=2.12.0\" --quiet\n",
    "\n",
    "    # ML utilities\n",
    "    %pip install \"numpy>=1.24.0,<2.0.0\" --quiet\n",
    "    %pip install \"pandas>=2.0.0\" --quiet\n",
    "    %pip install \"scikit-learn>=1.3.0\" --quiet\n",
    "\n",
    "    # Utilities\n",
    "    %pip install \"pyyaml>=6.0\" --quiet\n",
    "    %pip install \"tqdm>=4.65.0\" --quiet\n",
    "    %pip install \"seqeval>=1.2.2\" --quiet\n",
    "    %pip install \"sentencepiece>=0.1.99\" --quiet\n",
    "\n",
    "    # Experiment tracking\n",
    "    %pip install mlflow --quiet\n",
    "    %pip install optuna --quiet\n",
    "\n",
    "    # Azure ML SDK (required for orchestration imports)\n",
    "    %pip install \"azure-ai-ml>=1.0.0\" --quiet\n",
    "    %pip install \"azure-identity>=1.12.0\" --quiet\n",
    "    %pip install azureml-defaults --quiet\n",
    "    %pip install azureml-mlflow --quiet\n",
    "\n",
    "    # ONNX support\n",
    "    %pip install onnxruntime --quiet\n",
    "    %pip install \"onnx>=1.16.0\" --quiet\n",
    "    %pip install \"onnxscript>=0.1.0\" --quiet\n",
    "\n",
    "    print(\"✓ All dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Setup Paths and Import Paths\n",
    "\n",
    "Python paths are already configured in Step 2. This section verifies the setup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform: local\n",
      "Base directory: Will use current working directory\n",
      "Backup enabled: False\n"
     ]
    }
   ],
   "source": [
    "# Environment detection and platform configuration\n",
    "# Note: This cell is a duplicate of Cell 2. If Cell 2 was already executed, these variables are already set.\n",
    "# This cell ensures they're set even if Cell 2 was skipped.\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect execution environment\n",
    "IN_COLAB = \"COLAB_GPU\" in os.environ or \"COLAB_TPU\" in os.environ\n",
    "IN_KAGGLE = \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
    "IS_LOCAL = not IN_COLAB and not IN_KAGGLE\n",
    "\n",
    "# Set platform-specific constants (only if not already set)\n",
    "if 'PLATFORM' not in globals():\n",
    "    if IN_COLAB:\n",
    "        PLATFORM = \"colab\"\n",
    "        BASE_DIR = Path(\"/content\")\n",
    "        BACKUP_ENABLED = True\n",
    "        print(\"✓ Detected: Google Colab environment\")\n",
    "    elif IN_KAGGLE:\n",
    "        PLATFORM = \"kaggle\"\n",
    "        BASE_DIR = Path(\"/kaggle/working\")\n",
    "        BACKUP_ENABLED = False  # Kaggle outputs are automatically persisted\n",
    "        print(\"✓ Detected: Kaggle environment\")\n",
    "    else:\n",
    "        PLATFORM = \"local\"\n",
    "        BASE_DIR = None  # Will use Path.cwd() instead\n",
    "        BACKUP_ENABLED = False\n",
    "        print(\"✓ Detected: Local environment\")\n",
    "\n",
    "if 'PLATFORM' in globals():\n",
    "    print(f\"Platform: {PLATFORM}\")\n",
    "    if BASE_DIR:\n",
    "        print(f\"Base directory: {BASE_DIR}\")\n",
    "    else:\n",
    "        print(f\"Base directory: Will use current working directory\")\n",
    "    print(f\"Backup enabled: {BACKUP_ENABLED}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook directory: /workspaces/resume-ner-azureml/notebooks\n",
      "Project root: /workspaces/resume-ner-azureml\n",
      "Source directory: /workspaces/resume-ner-azureml/src\n",
      "Config directory: /workspaces/resume-ner-azureml/config\n",
      "Platform: local\n",
      "In Colab: False\n",
      "In Kaggle: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup paths (ROOT_DIR should be set in Cell 2)\n",
    "# If not, set it here\n",
    "if 'ROOT_DIR' not in globals():\n",
    "    if IN_COLAB:\n",
    "        ROOT_DIR = Path(\"/content/resume-ner-azureml\")\n",
    "    elif IN_KAGGLE:\n",
    "        ROOT_DIR = Path(\"/kaggle/working/resume-ner-azureml\")\n",
    "    else:\n",
    "        ROOT_DIR = Path(\"/content/resume-ner-azureml\")  # Default to Colab path\n",
    "\n",
    "SRC_DIR = ROOT_DIR / \"src\"\n",
    "CONFIG_DIR = ROOT_DIR / \"config\"\n",
    "NOTEBOOK_DIR = ROOT_DIR / \"notebooks\"\n",
    "\n",
    "# Add to Python path\n",
    "sys.path.insert(0, str(ROOT_DIR))\n",
    "sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "print(\"Notebook directory:\", NOTEBOOK_DIR)\n",
    "print(\"Project root:\", ROOT_DIR)\n",
    "print(\"Source directory:\", SRC_DIR)\n",
    "print(\"Config directory:\", CONFIG_DIR)\n",
    "print(\"Platform:\", PLATFORM if 'PLATFORM' in globals() else \"unknown\")\n",
    "print(\"In Colab:\", IN_COLAB if 'IN_COLAB' in globals() else False)\n",
    "print(\"In Kaggle:\", IN_KAGGLE if 'IN_KAGGLE' in globals() else False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Mount Google Drive\n",
    "\n",
    "Mount Google Drive to enable checkpoint persistence across Colab sessions. Checkpoints will be automatically saved to Drive after training completes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Backup/restore wrapper functions defined (using DriveBackupStore)\n"
     ]
    }
   ],
   "source": [
    "# Google Drive backup/restore functionality\n",
    "# Uses the DriveBackupStore from orchestration.drive_backup module\n",
    "# The drive_store is created in Cell 15 (after mounting)\n",
    "\n",
    "# Backward-compatible wrapper functions (delegate to drive_store)\n",
    "# These maintain the old API for gradual migration\n",
    "from pathlib import Path\n",
    "\n",
    "# Note: drive_store is created in Cell 15 (Mount Google Drive)\n",
    "# If drive_store is None, backup/restore operations are disabled\n",
    "\n",
    "def backup_to_drive(source_path: Path, is_directory: bool = False) -> bool:\n",
    "    \"\"\"\n",
    "    Backward-compatible wrapper for drive_store.backup().\n",
    "    \n",
    "    Note: Prefer using drive_store.backup() directly for better error handling.\n",
    "    \"\"\"\n",
    "    if not BACKUP_ENABLED or drive_store is None:\n",
    "        return False\n",
    "    \n",
    "    if not source_path.exists():\n",
    "        print(f\"⚠ Warning: Source path does not exist: {source_path}\")\n",
    "        return False\n",
    "    \n",
    "    # Map is_directory to expect parameter\n",
    "    expect = \"dir\" if is_directory else \"file\"\n",
    "    result = drive_store.backup(source_path, expect=expect)\n",
    "    \n",
    "    if result.ok:\n",
    "        print(result)\n",
    "    else:\n",
    "        print(f\"⚠ Warning: Backup failed: {result.reason}\")\n",
    "    \n",
    "    return result.ok\n",
    "\n",
    "def restore_from_drive(local_path: Path, is_directory: bool = False) -> bool:\n",
    "    \"\"\"\n",
    "    Backward-compatible wrapper for drive_store.restore().\n",
    "    \n",
    "    Note: Prefer using drive_store.restore() directly for better error handling.\n",
    "    \"\"\"\n",
    "    if not BACKUP_ENABLED or drive_store is None:\n",
    "        return False\n",
    "    \n",
    "    # Map is_directory to expect parameter\n",
    "    expect = \"dir\" if is_directory else \"file\"\n",
    "    result = drive_store.restore(local_path, expect=expect)\n",
    "    \n",
    "    if result.ok:\n",
    "        print(result)\n",
    "    else:\n",
    "        print(f\"⚠ Warning: Restore failed: {result.reason}\")\n",
    "    \n",
    "    return result.ok\n",
    "\n",
    "def ensure_restored_from_drive(local_path: Path, is_directory: bool = False) -> bool:\n",
    "    \"\"\"\n",
    "    Ensure file/directory exists locally, restoring from Drive if missing.\n",
    "    \n",
    "    This is the primary entry point for most use cases.\n",
    "    \"\"\"\n",
    "    if not BACKUP_ENABLED or drive_store is None:\n",
    "        return False\n",
    "    \n",
    "    # Map is_directory to expect parameter\n",
    "    expect = \"dir\" if is_directory else \"file\"\n",
    "    result = drive_store.ensure_local(local_path)\n",
    "    \n",
    "    if result.ok and result.action.value == \"copied\":\n",
    "        print(result)\n",
    "    \n",
    "    return result.ok\n",
    "\n",
    "print(\"✓ Backup/restore wrapper functions defined (using DriveBackupStore)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Warning: Unknown environment. Backup to Google Drive will be disabled.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "# Fix numpy/pandas compatibility before importing orchestration modules\n",
    "try:\n",
    "    from infrastructure.storage.drive import create_colab_store\n",
    "except (ValueError, ImportError) as e:\n",
    "    if \"numpy.dtype size changed\" in str(e) or \"numpy\" in str(e).lower():\n",
    "        print(\"⚠ Numpy/pandas compatibility issue detected. Fixing...\")\n",
    "        import subprocess\n",
    "        import sys\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"--force-reinstall\", \"--no-cache-dir\", \"numpy>=1.24.0,<2.0.0\", \"pandas>=2.0.0\", \"--quiet\"])\n",
    "        print(\"✓ Numpy/pandas reinstalled. Please restart the kernel and re-run this cell.\")\n",
    "        raise RuntimeError(\"Please restart kernel after numpy/pandas fix\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# Mount Google Drive and create backup store (Colab only - Kaggle doesn't need this)\n",
    "# Uses centralized config from config/paths.yaml\n",
    "DRIVE_BACKUP_DIR = None\n",
    "drive_store = None\n",
    "\n",
    "if IN_COLAB:\n",
    "    drive_store = create_colab_store(ROOT_DIR, CONFIG_DIR)\n",
    "    if drive_store:\n",
    "        BACKUP_ENABLED = True\n",
    "        DRIVE_BACKUP_DIR = drive_store.backup_root\n",
    "        print(f\"✓ Google Drive mounted\")\n",
    "        print(f\"✓ Backup base directory: {DRIVE_BACKUP_DIR}\")\n",
    "        print(f\"\\nNote: All outputs/ will be mirrored to: {DRIVE_BACKUP_DIR / 'outputs'}\")\n",
    "    else:\n",
    "        BACKUP_ENABLED = False\n",
    "        print(\"⚠ Warning: Could not mount Google Drive. Backup to Google Drive will be disabled.\")\n",
    "elif IN_KAGGLE:\n",
    "    print(\"✓ Kaggle environment detected - outputs are automatically persisted (no Drive mount needed)\")\n",
    "    BACKUP_ENABLED = False\n",
    "else:\n",
    "    print(\"⚠ Warning: Unknown environment. Backup to Google Drive will be disabled.\")\n",
    "    BACKUP_ENABLED = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.1: Load Centralized Configs\n",
    "\n",
    "Load and validate all configuration files. Configs are immutable and will be logged with each job for reproducibility.\n",
    "\n",
    "**Note**: \n",
    "- **Local**: Config files should already exist in the repository\n",
    "- **Colab/Kaggle**: Config files will be auto-created if missing (useful for fresh environments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Update repository from git (only for Colab/Kaggle if needed)\n",
    "# Uncomment and run if you need to pull latest changes\n",
    "# if not IS_LOCAL:\n",
    "#     !cd {ROOT_DIR} && git fetch origin gg_final_training_2\n",
    "#     !cd {ROOT_DIR} && git reset --hard origin/gg_final_training_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Local environment - assuming config files already exist in repository\n"
     ]
    }
   ],
   "source": [
    "# Write/override config files (useful for Colab/Kaggle where file editing is limited)\n",
    "# Local environments should have configs already in the repo\n",
    "if IS_LOCAL:\n",
    "    print(\"✓ Local environment - assuming config files already exist in repository\")\n",
    "else:\n",
    "    # Create the experiment config directory if it doesn't exist\n",
    "    experiment_config_dir = CONFIG_DIR / \"experiment\"\n",
    "    experiment_config_dir = validate_path_before_mkdir(experiment_config_dir, context=\"directory\")\n",
    "    experiment_config_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    config_path = experiment_config_dir / \"resume_ner_baseline.yaml\"\n",
    "\n",
    "    # Always write/override the config file (useful for Kaggle where editing is difficult)\n",
    "    config_content = \"\"\"\n",
    "experiment_name: \"resume_ner_baseline\"\n",
    "\n",
    "# Relative to the top-level config directory\n",
    "data_config: \"data/resume_tiny.yaml\"\n",
    "model_config: \"model/distilbert.yaml\"\n",
    "train_config: \"train.yaml\"\n",
    "hpo_config: \"hpo/prod.yaml\"      # default HPO config; stages can override if needed\n",
    "env_config: \"env/azure.yaml\"\n",
    "benchmark_config: \"benchmark.yaml\"\n",
    "\n",
    "# High-level orchestration design:\n",
    "# - Stages: smoke → hpo → training\n",
    "# - Smoke and HPO stage backbones are controlled by the HPO config file (search_space.backbone.values)\n",
    "# - Training stage can target specific backbones via stage config\n",
    "# - AML experiment names are per-stage, optionally per-backbone\n",
    "\n",
    "stages:\n",
    "  smoke:\n",
    "    # AML experiment base name for smoke tests\n",
    "    aml_experiment: \"resume-ner-smoke\"\n",
    "    # HPO config for smoke/dry run tests (uses smoke.yaml with reduced trials)\n",
    "    hpo_config: \"hpo/smoke.yaml\"\n",
    "    # Backbones are controlled by the HPO config file (hpo_config) via search_space.backbone.values\n",
    "\n",
    "  hpo:\n",
    "    # AML experiment base name for HPO sweeps\n",
    "    aml_experiment: \"resume-ner-hpo\"\n",
    "    # HPO config override for production HPO sweep (uses prod.yaml instead of default smoke.yaml)\n",
    "    hpo_config: \"hpo/smoke.yaml\"\n",
    "    # Backbones are controlled by the HPO config file (hpo_config) via search_space.backbone.values\n",
    "\n",
    "  training:\n",
    "    # AML experiment base name for final single-run training\n",
    "    aml_experiment: \"resume-ner-train\"\n",
    "    # Final production backbone(s); typically one chosen after HPO\n",
    "    backbones:\n",
    "      - \"distilbert\"\n",
    "\n",
    "# Optional naming policy for how AML experiments are derived per backbone.\n",
    "# If true, the orchestrator should build experiment_name as:\n",
    "#   \"<aml_experiment>-<backbone>\"\n",
    "# otherwise it should use \"<aml_experiment>\" directly and rely on tags\n",
    "# (stage/backbone) for grouping in AML.\n",
    "naming:\n",
    "  include_backbone_in_experiment: true\n",
    "\"\"\"\n",
    "\n",
    "    config_path.write_text(config_content)\n",
    "\n",
    "    if config_path.exists():\n",
    "        print(f\"✓ Config overridden at: {config_path}\")\n",
    "    else:\n",
    "        print(f\"✓ Config written to: {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Local environment - assuming HPO config files already exist in repository\n"
     ]
    }
   ],
   "source": [
    "# Write/override HPO config file (useful for Colab/Kaggle where file editing is limited)\n",
    "# Local environments should have configs already in the repo\n",
    "if IS_LOCAL:\n",
    "    print(\"✓ Local environment - assuming HPO config files already exist in repository\")\n",
    "else:\n",
    "    # Create the HPO config directory if it doesn't exist\n",
    "    hpo_config_dir = CONFIG_DIR / \"hpo\"\n",
    "    hpo_config_dir = validate_path_before_mkdir(hpo_config_dir, context=\"directory\")\n",
    "    hpo_config_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    config_path = hpo_config_dir / \"smoke.yaml\"\n",
    "\n",
    "    # Always write/override the config file (useful for Kaggle where editing is difficult)\n",
    "    config_content = \"\"\"\n",
    "search_space:\n",
    "  backbone:\n",
    "    type: \"choice\"\n",
    "    values: [\"distilbert\"]  # [\"distilbert\", \"distilroberta\"]\n",
    "    # Note: \"deberta\" excluded from smoke tests due to CUDA/NVRTC issues on Windows\n",
    "    # DeBERTa requires nvrtc-builtins64_129.dll which may not be available in all environments\n",
    "  \n",
    "  learning_rate:\n",
    "    type: \"loguniform\"\n",
    "    min: 1e-5\n",
    "    max: 5e-5\n",
    "  \n",
    "  batch_size:\n",
    "    type: \"choice\"\n",
    "    values: [4]\n",
    "  \n",
    "  dropout:\n",
    "    type: \"uniform\"\n",
    "    min: 0.1\n",
    "    max: 0.3\n",
    "  \n",
    "  weight_decay:\n",
    "    type: \"loguniform\"\n",
    "    min: 0.001\n",
    "    max: 0.1\n",
    "\n",
    "sampling:\n",
    "  algorithm: \"random\"\n",
    "  max_trials: 1\n",
    "  timeout_minutes: 20\n",
    "\n",
    "# Checkpoint configuration for HPO resume support\n",
    "# Enables saving study state to SQLite database for resuming interrupted runs\n",
    "checkpoint:\n",
    "  enabled: true\n",
    "  study_name: \"hpo_{backbone}_smoke_test_3.67\"\n",
    "  storage_path: \"{study_name}/study.db\"\n",
    "  auto_resume: true\n",
    "  # Only save checkpoints for best trials locally (reduces storage from ~30 GB to ~300 MB)\n",
    "  save_only_best: true\n",
    "\n",
    "mlflow:\n",
    "  # Log best trial checkpoint to MLflow after HPO completes\n",
    "  # Set to false to disable MLflow checkpoint logging entirely\n",
    "  log_best_checkpoint: true\n",
    "\n",
    "early_termination:\n",
    "  policy: \"bandit\"\n",
    "  evaluation_interval: 1\n",
    "  slack_factor: 0.2\n",
    "  delay_evaluation: 2\n",
    "\n",
    "objective:\n",
    "  metric: \"macro-f1\"\n",
    "  goal: \"maximize\"\n",
    "\n",
    "# Selection strategy configuration for accuracy-speed tradeoff\n",
    "selection:\n",
    "  # Accuracy threshold for speed tradeoff (0.015 = 1.5% relative)\n",
    "  # If two models are within this accuracy difference, prefer faster model\n",
    "  # Set to null for accuracy-only selection (default behavior)\n",
    "  accuracy_threshold: 0.015\n",
    "  \n",
    "  # Use relative threshold (percentage of best accuracy) vs absolute difference\n",
    "  # Relative thresholds are more robust across different accuracy ranges\n",
    "  # Default: true (recommended)\n",
    "  use_relative_threshold: true\n",
    "  \n",
    "  # Minimum relative accuracy gain to justify slower model (optional)\n",
    "  # If DeBERTa is < 2% better than DistilBERT, prefer DistilBERT\n",
    "  # Set to null to disable this check\n",
    "  min_accuracy_gain: 0.02\n",
    "\n",
    "k_fold:\n",
    "  enabled: true\n",
    "  n_splits: 2\n",
    "  random_seed: 42\n",
    "  shuffle: true\n",
    "  stratified: true\n",
    "\n",
    "# Refit training configuration\n",
    "# After HPO completes, train the best trial on the full training dataset\n",
    "# This creates a canonical checkpoint for production use (instead of using arbitrary fold checkpoints)\n",
    "refit:\n",
    "  enabled: true  # Default: enabled. Set to false to skip refit training\n",
    "  # Optional: Add timeout, max_epochs overrides if needed in the future\n",
    "\n",
    "# Cleanup configuration for interrupted runs\n",
    "# Controls automatic cleanup/marking of interrupted runs from previous sessions\n",
    "cleanup:\n",
    "  # Disable automatic MLflow cleanup (tagging interrupted runs with code.interrupted=true)\n",
    "  # Default: true (disabled for speed). Set to false to enable automatic cleanup\n",
    "  disable_auto_cleanup: true\n",
    "  \n",
    "  # Disable automatic Optuna marking (marking RUNNING trials as FAILED)\n",
    "  # Default: false (enabled). Set to true to disable automatic Optuna state cleanup\n",
    "  disable_auto_optuna_mark: false\n",
    "\"\"\"\n",
    "\n",
    "    config_path.write_text(config_content)\n",
    "\n",
    "    if config_path.exists():\n",
    "        print(f\"✓ HPO config overridden at: {config_path}\")\n",
    "    else:\n",
    "        print(f\"✓ HPO config written to: {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Local environment - assuming training config file already exists in repository\n"
     ]
    }
   ],
   "source": [
    "# Write/override training config file (useful for Colab/Kaggle where file editing is limited)\n",
    "# Local environments should have configs already in the repo\n",
    "if IS_LOCAL:\n",
    "    print(\"✓ Local environment - assuming training config file already exists in repository\")\n",
    "else:\n",
    "    # Ensure config directory exists\n",
    "    CONFIG_DIR = validate_path_before_mkdir(CONFIG_DIR, context=\"directory\")\n",
    "    CONFIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    config_path = CONFIG_DIR / \"train.yaml\"\n",
    "\n",
    "    # Always write/override the config file (useful for Kaggle where editing is difficult)\n",
    "    config_content = \"\"\"\n",
    "# Global Training Defaults\n",
    "# Applied to all training runs\n",
    "\n",
    "training:\n",
    "  epochs: 1  # 5\n",
    "  batch_size: 2  # 12 \n",
    "  gradient_accumulation_steps: 2\n",
    "  learning_rate: 2e-5\n",
    "  weight_decay: 0.01\n",
    "  warmup_steps: 500\n",
    "  max_grad_norm: 1.0\n",
    "  # Data splitting and model-specific settings\n",
    "  val_split_divisor: 10  # Divide train set by this to create validation split if none exists\n",
    "  deberta_max_batch_size: 8  # 16  # Maximum batch size for DeBERTa models (memory constraints)\n",
    "  warmup_steps_divisor: 10  # Divide total steps by this to cap warmup steps\n",
    "  \n",
    "  # EDA-based metric selection\n",
    "  metric: \"macro-f1\"  # Class imbalance requires macro-f1\n",
    "  metric_mode: \"max\"  # Maximize macro-f1\n",
    "  \n",
    "  early_stopping:\n",
    "    enabled: true\n",
    "    patience: 3\n",
    "    min_delta: 0.001\n",
    "\n",
    "logging:\n",
    "  log_interval: 100\n",
    "  eval_interval: 500\n",
    "  save_interval: 1000\n",
    "\n",
    "# NOTE: Multi-GPU / DDP is optional and currently experimental. When enabled,\n",
    "# the training code will use this section together with hardware detection to\n",
    "# decide whether to run single-GPU vs multi-GPU. If no multiple GPUs or DDP\n",
    "# backend are available, it will safely fall back to single-GPU.\n",
    "distributed:\n",
    "  enabled: false         # Set true to enable multi-GPU / DDP\n",
    "  backend: \"nccl\"        # Typically 'nccl' for GPUs\n",
    "  world_size: \"auto\"     # 'auto' = use all visible GPUs; or set an int\n",
    "  init_method: \"env://\"  # Default init method; can be overridden if needed\n",
    "  timeout_seconds: 1800  # Process group init timeout (in seconds)\n",
    "\"\"\"\n",
    "\n",
    "    config_path.write_text(config_content)\n",
    "\n",
    "    if config_path.exists():\n",
    "        print(f\"✓ Training config overridden at: {config_path}\")\n",
    "    else:\n",
    "        print(f\"✓ Training config written to: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Constants\n",
    "\n",
    "Define constants for file and directory names used throughout the notebook. Benchmark settings come from centralized config, not hard-coded here. These constants work across all environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_88428/143452681.py:2: DeprecationWarning: orchestration module is deprecated. Use 'infrastructure', 'common', or 'data' modules instead. This will be removed in 2 releases.\n",
      "  from orchestration import (\n",
      "2026-01-12 17:23:57,953 - infrastructure.tracking.mlflow.compatibility - INFO - Applied Azure ML artifact compatibility patch\n",
      "/tmp/ipykernel_88428/143452681.py:2: DeprecationWarning: Importing 'constants' from 'orchestration' is deprecated. Please import from 'constants' instead.\n",
      "  from orchestration import (\n",
      "/tmp/ipykernel_88428/143452681.py:2: DeprecationWarning: Importing 'fingerprints' from 'orchestration' is deprecated. Please import from 'fingerprints' instead.\n",
      "  from orchestration import (\n",
      "/tmp/ipykernel_88428/143452681.py:2: DeprecationWarning: Importing 'metadata/index_manager' from 'orchestration' is deprecated. Please import from 'metadata' instead.\n",
      "  from orchestration import (\n",
      "/tmp/ipykernel_88428/143452681.py:2: DeprecationWarning: Importing 'metadata/metadata_manager' from 'orchestration' is deprecated. Please import from 'metadata' instead.\n",
      "  from orchestration import (\n",
      "/tmp/ipykernel_88428/143452681.py:2: DeprecationWarning: Importing 'drive_backup' from 'orchestration' is deprecated. Please import from 'storage' instead.\n",
      "  from orchestration import (\n",
      "/tmp/ipykernel_88428/143452681.py:2: DeprecationWarning: Importing 'benchmark_utils' from 'orchestration' is deprecated. Please import from 'benchmarking.utils' instead.\n",
      "  from orchestration import (\n",
      "/tmp/ipykernel_88428/143452681.py:2: DeprecationWarning: Importing 'config_loader' from 'orchestration' is deprecated. Please import from 'config.loader' instead.\n",
      "  from orchestration import (\n",
      "/tmp/ipykernel_88428/143452681.py:2: DeprecationWarning: Importing 'conversion_config' from 'orchestration' is deprecated. Please import from 'config.conversion' instead.\n",
      "  from orchestration import (\n",
      "/tmp/ipykernel_88428/143452681.py:2: DeprecationWarning: Importing 'final_training_config' from 'orchestration' is deprecated. Please import from 'config.training' instead.\n",
      "  from orchestration import (\n",
      "/tmp/ipykernel_88428/143452681.py:2: DeprecationWarning: Importing 'environment' from 'orchestration' is deprecated. Please import from 'config.environment' instead.\n",
      "  from orchestration import (\n",
      "/tmp/ipykernel_88428/143452681.py:2: DeprecationWarning: Importing 'config_compat' from 'orchestration' is deprecated. Please import from 'config.validation' instead.\n",
      "  from orchestration import (\n",
      "/tmp/ipykernel_88428/143452681.py:2: DeprecationWarning: Importing 'data_assets' from 'orchestration' is deprecated. Please import from 'azureml.data_assets' instead.\n",
      "  from orchestration import (\n",
      "/tmp/ipykernel_88428/143452681.py:2: DeprecationWarning: Importing 'normalize' from 'orchestration' is deprecated. Please import from 'core.normalize' instead.\n",
      "  from orchestration import (\n",
      "/tmp/ipykernel_88428/143452681.py:2: DeprecationWarning: Importing 'tokens' from 'orchestration' is deprecated. Please import from 'core.tokens' instead.\n",
      "  from orchestration import (\n"
     ]
    }
   ],
   "source": [
    "# Import constants from centralized module\n",
    "from orchestration import (\n",
    "    STAGE_HPO,\n",
    "    STAGE_TRAINING,\n",
    "    METRICS_FILENAME,\n",
    "    BENCHMARK_FILENAME,\n",
    "    CHECKPOINT_DIRNAME,\n",
    "    DEFAULT_RANDOM_SEED,\n",
    "    DEFAULT_K_FOLDS,\n",
    ")\n",
    "\n",
    "    # Note: These imports still work via orchestration facade for backward compatibility\n",
    "from orchestration.jobs.tracking.mlflow_tracker import (\n",
    "    MLflowSweepTracker,\n",
    "    MLflowBenchmarkTracker,\n",
    "    MLflowTrainingTracker,\n",
    "    MLflowConversionTracker,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Helper Functions\n",
    "\n",
    "Reusable helper functions following DRY principle for common operations. These functions work across all environments (local, Colab, Kaggle).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import helper functions from consolidated modules (DRY principle)\n",
    "from typing import List, Optional, Any\n",
    "from orchestration import (\n",
    "    build_mlflow_experiment_name,\n",
    "    setup_mlflow_for_stage,\n",
    "    run_benchmarking,\n",
    ")\n",
    "from common.shared import verify_output_file\n",
    "\n",
    "# Wrapper function for run_benchmarking that uses notebook-specific paths\n",
    "def run_benchmarking_local(\n",
    "    checkpoint_dir: Path,\n",
    "    test_data_path: Path,\n",
    "    output_path: Path,\n",
    "    batch_sizes: List[int],\n",
    "    iterations: int,\n",
    "    warmup_iterations: int,\n",
    "    max_length: int = 512,\n",
    "    device: Optional[str] = None,\n",
    "    tracker: Optional[Any] = None,\n",
    "    backbone: Optional[str] = None,\n",
    "    benchmark_source: str = \"final_training\",\n",
    "    study_key_hash: Optional[str] = None,\n",
    "    trial_key_hash: Optional[str] = None,\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Run benchmarking on a model checkpoint (local notebook wrapper).\n",
    "    \n",
    "    This is a thin wrapper around orchestration.benchmark_utils.run_benchmarking\n",
    "    that automatically uses the notebook's SRC_DIR and ROOT_DIR.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_dir: Path to checkpoint directory.\n",
    "        test_data_path: Path to test data JSON file.\n",
    "        output_path: Path to output benchmark.json file.\n",
    "        batch_sizes: List of batch sizes to test.\n",
    "        iterations: Number of iterations per batch size.\n",
    "        warmup_iterations: Number of warmup iterations.\n",
    "        max_length: Maximum sequence length.\n",
    "        device: Device to use (None = auto-detect).\n",
    "        tracker: Optional MLflowBenchmarkTracker instance.\n",
    "        backbone: Optional model backbone name.\n",
    "        benchmark_source: Source of benchmark (\"hpo_trial\" or \"final_training\").\n",
    "        study_key_hash: Optional study key hash for grouping tags.\n",
    "        trial_key_hash: Optional trial key hash for grouping tags.\n",
    "    \n",
    "    Returns:\n",
    "        True if successful, False otherwise.\n",
    "    \"\"\"\n",
    "    return run_benchmarking(\n",
    "        checkpoint_dir=checkpoint_dir,\n",
    "        test_data_path=test_data_path,\n",
    "        output_path=output_path,\n",
    "        batch_sizes=batch_sizes,\n",
    "        iterations=iterations,\n",
    "        warmup_iterations=warmup_iterations,\n",
    "        max_length=max_length,\n",
    "        device=device,\n",
    "        tracker=tracker,\n",
    "        backbone=backbone,\n",
    "        benchmark_source=benchmark_source,\n",
    "        project_root=ROOT_DIR,\n",
    "        study_key_hash=study_key_hash,\n",
    "        trial_key_hash=trial_key_hash,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded experiment: resume_ner_baseline\n",
      "Loaded config domains: ['benchmark', 'data', 'env', 'hpo', 'model', 'train']\n",
      "Config hashes: {'data': 'e87b126b961fa20d', 'model': '5f90a66353401b44', 'train': '129186d04c5b57c6', 'hpo': 'da13ba360954bf71', 'env': '3e54b931c7640cf2', 'benchmark': '9c427b69c6e9db79'}\n",
      "Config metadata: {'data_config_hash': 'e87b126b961fa20d', 'model_config_hash': '5f90a66353401b44', 'train_config_hash': '129186d04c5b57c6', 'hpo_config_hash': 'da13ba360954bf71', 'env_config_hash': '3e54b931c7640cf2', 'data_version': 'v3', 'model_backbone': 'distilbert-base-uncased'}\n",
      "Dataset path (from data config): /workspaces/resume-ner-azureml/dataset_tiny/seed0\n",
      "Using seed: 0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import Any, Dict\n",
    "\n",
    "from common.constants import EXPERIMENT_NAME\n",
    "from infrastructure.config.loader import (\n",
    "    ExperimentConfig,\n",
    "    compute_config_hashes,\n",
    "    create_config_metadata,\n",
    "    load_all_configs,\n",
    "    load_experiment_config,\n",
    "    snapshot_configs,\n",
    "    validate_config_immutability,\n",
    ")\n",
    "\n",
    "# P1-3.1: Load Centralized Configs (local-only)\n",
    "# Mirrors the Azure orchestration notebook, but does not create an Azure ML client.\n",
    "\n",
    "if not CONFIG_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Config directory not found: {CONFIG_DIR}\")\n",
    "\n",
    "experiment_config: ExperimentConfig = load_experiment_config(CONFIG_DIR, EXPERIMENT_NAME)\n",
    "configs: Dict[str, Any] = load_all_configs(experiment_config)\n",
    "config_hashes = compute_config_hashes(configs)\n",
    "config_metadata = create_config_metadata(configs, config_hashes)\n",
    "\n",
    "# Immutable snapshots for runtime mutation checks\n",
    "original_configs = snapshot_configs(configs)\n",
    "validate_config_immutability(configs, original_configs)\n",
    "\n",
    "print(f\"Loaded experiment: {experiment_config.name}\")\n",
    "print(\"Loaded config domains:\", sorted(configs.keys()))\n",
    "print(\"Config hashes:\", config_hashes)\n",
    "print(\"Config metadata:\", config_metadata)\n",
    "\n",
    "# Get dataset path from data config (centralized configuration)\n",
    "# The local_path in the data config is relative to the config directory\n",
    "data_config = configs[\"data\"]\n",
    "local_path_str = data_config.get(\"local_path\", \"../dataset\")\n",
    "DATASET_LOCAL_PATH = (CONFIG_DIR / local_path_str).resolve()\n",
    "\n",
    "# Check if seed-based dataset structure (for dataset_tiny with seed subdirectories)\n",
    "seed = data_config.get(\"seed\")\n",
    "if seed is not None and \"dataset_tiny\" in str(DATASET_LOCAL_PATH):\n",
    "    DATASET_LOCAL_PATH = DATASET_LOCAL_PATH / f\"seed{seed}\"\n",
    "\n",
    "print(f\"Dataset path (from data config): {DATASET_LOCAL_PATH}\")\n",
    "if seed is not None:\n",
    "    print(f\"Using seed: {seed}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.2: Verify Local Dataset\n",
    "\n",
    "Verify that the dataset directory (specified by `local_path` in the data config) exists and contains the required files. The dataset path is loaded from the centralized data configuration in Step P1-3.1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset directory found: /workspaces/resume-ner-azureml/dataset_tiny/seed0\n",
      "  (from data config: resume-ner-data-tiny-short vv3)\n",
      "  ✓ train.json (27,315 bytes)\n",
      "  ⚠ validation.json not found (optional - training will proceed without validation set)\n"
     ]
    }
   ],
   "source": [
    "# P1-3.2: Verify Local Dataset\n",
    "# The dataset path comes from the data config's local_path field (loaded in Step P1-3.1).\n",
    "# This ensures the dataset location is controlled by centralized configuration.\n",
    "# Note: train.json is required, but validation.json is optional (matches training script behavior).\n",
    "\n",
    "REQUIRED_FILE = \"train.json\"\n",
    "OPTIONAL_FILE = \"validation.json\"\n",
    "\n",
    "if not DATASET_LOCAL_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Dataset directory not found: {DATASET_LOCAL_PATH}\\n\"\n",
    "        f\"This path comes from the data config's 'local_path' field.\\n\"\n",
    "        f\"If you need to create the dataset, run the notebook: notebooks/00_make_tiny_dataset.ipynb\"\n",
    "    )\n",
    "\n",
    "# Check required file\n",
    "train_file = DATASET_LOCAL_PATH / REQUIRED_FILE\n",
    "if not train_file.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Required dataset file not found: {train_file}\\n\"\n",
    "        f\"This path comes from the data config's 'local_path' field.\\n\"\n",
    "        f\"If you need to create it, run the notebook: notebooks/00_make_tiny_dataset.ipynb\"\n",
    "    )\n",
    "\n",
    "# Check optional file\n",
    "val_file = DATASET_LOCAL_PATH / OPTIONAL_FILE\n",
    "has_validation = val_file.exists()\n",
    "\n",
    "print(f\"✓ Dataset directory found: {DATASET_LOCAL_PATH}\")\n",
    "print(f\"  (from data config: {data_config.get('name', 'unknown')} v{data_config.get('version', 'unknown')})\")\n",
    "\n",
    "train_size = train_file.stat().st_size\n",
    "print(f\"  ✓ {REQUIRED_FILE} ({train_size:,} bytes)\")\n",
    "\n",
    "if has_validation:\n",
    "    val_size = val_file.stat().st_size\n",
    "    print(f\"  ✓ {OPTIONAL_FILE} ({val_size:,} bytes)\")\n",
    "else:\n",
    "    print(f\"  ⚠ {OPTIONAL_FILE} not found (optional - training will proceed without validation set)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.2.1: Optional Train/Test Split\n",
    "\n",
    "**Optional step**: Create a train/test split if `test.json` is missing. This is useful when you only have `train.json` and `validation.json` and want to create a separate test set.\n",
    "\n",
    "**⚠ WARNING**: This will overwrite `train.json` with the split version. Only enable if you want to create a permanent train/test split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Found existing test.json at /workspaces/resume-ner-azureml/dataset_tiny/seed0/test.json\n"
     ]
    }
   ],
   "source": [
    "# Optional: create train/test split if test.json is missing\n",
    "# WARNING: This will overwrite train.json with the split version\n",
    "# Only enable if you want to create a permanent train/test split\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "from data.loaders.dataset_loader import split_train_test, save_split_files\n",
    "\n",
    "CREATE_TEST_SPLIT = False  # Set True to create test.json when absent (WARNING: overwrites train.json)\n",
    "\n",
    "train_file = DATASET_LOCAL_PATH / \"train.json\"\n",
    "val_file = DATASET_LOCAL_PATH / \"validation.json\"\n",
    "test_file = DATASET_LOCAL_PATH / \"test.json\"\n",
    "\n",
    "if CREATE_TEST_SPLIT and not test_file.exists():\n",
    "    # Backup original train.json before overwriting\n",
    "    backup_file = DATASET_LOCAL_PATH / \"train.json.backup\"\n",
    "    if train_file.exists() and not backup_file.exists():\n",
    "        import shutil\n",
    "        shutil.copy2(train_file, backup_file)\n",
    "        print(f\"⚠ Backed up original train.json to {backup_file}\")\n",
    "    \n",
    "    full_dataset = []\n",
    "    # Start with train data; optionally include validation to maximize coverage\n",
    "    with open(train_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        full_dataset.extend(json.load(f))\n",
    "    if val_file.exists():\n",
    "        with open(val_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            full_dataset.extend(json.load(f))\n",
    "\n",
    "    split_cfg = configs.get(\"data\", {}).get(\"splitting\", {})\n",
    "    train_ratio = split_cfg.get(\"train_test_ratio\", 0.8)\n",
    "    stratified = split_cfg.get(\"stratified\", False)\n",
    "    random_seed = split_cfg.get(\"random_seed\", 42)\n",
    "    entity_types = configs.get(\"data\", {}).get(\"schema\", {}).get(\"entity_types\", [])\n",
    "\n",
    "    print(f\"Creating train/test split (train_ratio={train_ratio}, stratified={stratified})...\")\n",
    "    print(f\"⚠ WARNING: This will overwrite train.json with {int(len(full_dataset) * train_ratio)} samples\")\n",
    "    \n",
    "    new_train, new_test = split_train_test(\n",
    "        dataset=full_dataset,\n",
    "        train_ratio=train_ratio,\n",
    "        stratified=stratified,\n",
    "        random_seed=random_seed,\n",
    "        entity_types=entity_types,\n",
    "    )\n",
    "\n",
    "    save_split_files(DATASET_LOCAL_PATH, new_train, new_test)\n",
    "    print(f\"✓ Wrote train.json ({len(new_train)}) and test.json ({len(new_test)})\")\n",
    "elif test_file.exists():\n",
    "    print(f\"✓ Found existing test.json at {test_file}\")\n",
    "else:\n",
    "    print(\"⚠ test.json not found. Set CREATE_TEST_SPLIT=True to generate a split.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.3: Setup Local Environment\n",
    "\n",
    "Verify GPU availability, set up MLflow tracking (local file store), and check that key dependencies are installed. This step ensures the local environment is ready for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ Warning: CUDA device requested but not available. Falling back to CPU.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "DEFAULT_DEVICE = \"cuda\"\n",
    "\n",
    "env_config = configs[\"env\"]\n",
    "device_type = env_config.get(\"compute\", {}).get(\"device\", DEFAULT_DEVICE)\n",
    "\n",
    "# Fallback to CPU if CUDA is requested but not available\n",
    "if device_type == \"cuda\" and not torch.cuda.is_available():\n",
    "    print(\"⚠ Warning: CUDA device requested but not available. Falling back to CPU.\")\n",
    "    if not IS_LOCAL:\n",
    "        print(\"  In Colab, ensure you've selected a GPU runtime: Runtime > Change runtime type > GPU\")\n",
    "    device_type = \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-12 17:24:00,119 - common.shared.mlflow_setup - INFO - Azure ML enabled in config, attempting to connect...\n",
      "2026-01-12 17:24:00,120 - common.shared.mlflow_setup - WARNING - [DEBUG] Initial env check - subscription_id: False, resource_group: False, client_id: False, client_secret: False, tenant_id: False\n",
      "2026-01-12 17:24:00,120 - common.shared.mlflow_setup - INFO - Attempting to load credentials from config.env at: /workspaces/resume-ner-azureml/config.env\n",
      "2026-01-12 17:24:00,121 - common.shared.mlflow_setup - INFO - Loading credentials from /workspaces/resume-ner-azureml/config.env\n",
      "2026-01-12 17:24:00,122 - common.shared.mlflow_setup - INFO - Loaded subscription/resource group from config.env\n",
      "2026-01-12 17:24:00,122 - common.shared.mlflow_setup - INFO - Loaded service principal credentials from config.env\n",
      "2026-01-12 17:24:00,123 - common.shared.mlflow_setup - WARNING - [DEBUG] Platform detected: local\n",
      "2026-01-12 17:24:00,123 - common.shared.mlflow_setup - WARNING - [DEBUG] Service Principal check - client_id present: True, client_secret present: True, tenant_id present: True, has_service_principal: True\n",
      "2026-01-12 17:24:00,124 - common.shared.mlflow_setup - INFO - Using Service Principal authentication (from config.env)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow tracking URI: sqlite:///mlflow.db...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class DeploymentTemplateOperations: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "2026-01-12 17:24:01,359 - common.shared.mlflow_setup - INFO - Successfully connected to Azure ML workspace: resume-ner-ws\n",
      "2026-01-12 17:24:02,543 - common.shared.mlflow_setup - INFO - Using Azure ML workspace tracking\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'azureml://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import mlflow\n",
    "from common.shared.mlflow_setup import setup_mlflow_from_config\n",
    "\n",
    "# Get MLflow tracking URI for later use\n",
    "mlflow_tracking_uri = mlflow.get_tracking_uri()\n",
    "if mlflow_tracking_uri:\n",
    "    print(f\"MLflow tracking URI: {mlflow_tracking_uri[:80]}...\")\n",
    "else:\n",
    "    print(\"Warning: MLflow tracking URI not set\")\n",
    "\n",
    "# Setup MLflow from config (automatically uses Azure ML if enabled in config/mlflow.yaml)\n",
    "# To enable Azure ML Workspace tracking:\n",
    "# 1. Edit config/mlflow.yaml and set azure_ml.enabled: true\n",
    "# 2. Set environment variables: AZURE_SUBSCRIPTION_ID and AZURE_RESOURCE_GROUP\n",
    "setup_mlflow_from_config(\n",
    "    experiment_name=\"placeholder\",  # Will be set per HPO run\n",
    "    config_dir=CONFIG_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping Kaggle-specific package installation (not running on Kaggle)\n"
     ]
    }
   ],
   "source": [
    "# For Kaggle only - install specific package versions required for Optuna checkpointing\n",
    "if IN_KAGGLE:\n",
    "    %pip install \"\"SQLAlchemy<2.0.0\" \"alembic<1.13.0\" \"optuna<4.0.0\"\" --quiet\n",
    "else:\n",
    "    print(\"Skipping Kaggle-specific package installation (not running on Kaggle)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import mlflow\n",
    "    import transformers\n",
    "    import optuna\n",
    "except ImportError as e:\n",
    "    raise ImportError(f\"Required package not installed: {e}\")\n",
    "\n",
    "REQUIRED_PACKAGES = {\n",
    "    \"torch\": torch,\n",
    "    \"transformers\": transformers,\n",
    "    \"mlflow\": mlflow,\n",
    "    \"optuna\": optuna,\n",
    "}\n",
    "\n",
    "for name, module in REQUIRED_PACKAGES.items():\n",
    "    if not hasattr(module, \"__version__\"):\n",
    "        raise ImportError(\n",
    "            f\"Required package '{name}' is not properly installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.4: The Sweep (HPO) - Local with Optuna\n",
    "\n",
    "Run the full hyperparameter optimization sweep using Optuna to systematically search for the best model configuration. Uses the production HPO configuration with more trials than the dry run.\n",
    "\n",
    "**Note on K-Fold Cross-Validation:**\n",
    "- When k-fold CV is enabled (`k_fold.enabled: true`), each trial trains **k models** (one per fold) and returns the **average metric** across folds\n",
    "- The number of **trials** is controlled by `sampling.max_trials` (e.g., 2 trials in smoke.yaml)\n",
    "- With k=5 folds and 2 trials: **2 trials × 5 folds = 10 model trainings total**\n",
    "- K-fold CV provides more robust hyperparameter evaluation but increases compute time (k× per trial)\n",
    "\n",
    "**Note on Checkpoint and Resume:**\n",
    "- When `checkpoint.enabled: true` is set in the HPO config, the system automatically saves the Optuna study state to a SQLite database\n",
    "- This allows interrupted HPO runs to be resumed from the last checkpoint\n",
    "- The checkpoint is automatically detected and loaded on the next run if `auto_resume: true` (default)\n",
    "- Platform-specific paths are handled automatically (local, Colab, Kaggle)\n",
    "- **Selective Checkpoint Saving**: When `checkpoint.save_only_best: true` is set, only best trial checkpoints are saved locally (reduces storage from ~30 GB to ~300 MB for 100 trials)\n",
    "- **MLflow Checkpoint Logging**: When `mlflow.log_best_checkpoint: true` is set, the best trial checkpoint is automatically logged to MLflow after HPO completes (artifact path: `best_trial_checkpoint`)\n",
    "- **Refit Training**: When `refit.enabled: true` is set (default), after HPO completes, the best trial is automatically retrained on the full training dataset. This produces a canonical checkpoint in `trial_<n>_<ts>/refit/checkpoint/` that is preferred over fold checkpoints for benchmarking and production use.\n",
    "- See `docs/HPO_CHECKPOINT_RESUME.md` for detailed documentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from orchestration import STAGE_HPO\n",
    "from training.hpo import run_local_hpo_sweep\n",
    "\n",
    "# Use new paths module (orchestration.paths is deprecated)\n",
    "from infrastructure.paths import resolve_output_path\n",
    "\n",
    "# Use centralized HPO root from paths.yaml (respects env_overrides / storage_env)\n",
    "HPO_ROOT = resolve_output_path(ROOT_DIR, CONFIG_DIR, \"hpo\")\n",
    "\n",
    "# Keep fold_splits as a study-level meta artifact, not mixed with trials\n",
    "HPO_META_DIR = validate_path_before_mkdir(HPO_ROOT / \"_meta\", context=\"directory\")\n",
    "HPO_META_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using stage-specific HPO config for hpo: hpo/smoke.yaml\n"
     ]
    }
   ],
   "source": [
    "# Use HPO config already loaded in configs (from Step P1-3.1)\n",
    "# Following DRY principle - don't reload configs that are already available\n",
    "# Check for stage-specific hpo_config override\n",
    "from infrastructure.naming.experiments import get_stage_config\n",
    "from common.shared.yaml_utils import load_yaml\n",
    "\n",
    "hpo_stage_config = get_stage_config(experiment_config, STAGE_HPO)\n",
    "hpo_config_override = hpo_stage_config.get(\"hpo_config\")\n",
    "\n",
    "if hpo_config_override:\n",
    "    # Load stage-specific HPO config override\n",
    "    hpo_config_path = CONFIG_DIR / hpo_config_override\n",
    "    hpo_config = load_yaml(hpo_config_path)\n",
    "    print(f\"✓ Using stage-specific HPO config for hpo: {hpo_config_override}\")\n",
    "else:\n",
    "    # Use default HPO config from top-level experiment config\n",
    "    # Always reload default HPO config from file (don't use cached configs[\"hpo\"])\n",
    "    # This ensures changes to the YAML file are picked up even if configs dict wasn't reloaded\n",
    "    # This is especially important in Colab where configs might be cached in memory\n",
    "    # after editing YAML files without restarting the kernel\n",
    "    hpo_config_path = experiment_config.hpo_config\n",
    "    hpo_config = load_yaml(hpo_config_path)\n",
    "    print(f\"✓ Using default HPO config (reloaded from file): {experiment_config.hpo_config.name}\")\n",
    "train_config = configs[\"train\"]\n",
    "backbone_values = hpo_config[\"search_space\"][\"backbone\"][\"values\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup K-Fold Splits and Google Drive Backup for HPO Trials\n",
    "\n",
    "**K-Fold Cross-Validation Setup**: If k-fold CV is enabled in the HPO config, create and save fold splits before starting the sweep.\n",
    "\n",
    "**Colab-specific feature**: Configure automatic backup of each HPO trial to Google Drive immediately after completion. This prevents data loss if the Colab session disconnects during long-running hyperparameter optimization sweeps.\n",
    "\n",
    "**Note on Checkpoint Backup:**\n",
    "- If `checkpoint.save_only_best: true` is enabled, only best trial checkpoints are saved locally and backed up to Drive\n",
    "- Each trial's `metrics.json` is always saved and backed up\n",
    "- The best trial checkpoint is also automatically logged to MLflow (if `mlflow.log_best_checkpoint: true`)\n",
    "- This reduces storage usage while ensuring the best model is always available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up 2-fold cross-validation splits...\n",
      "[CV] Fold 0: {'SKILL': 154} | Missing: ['EDUCATION', 'DESIGNATION', 'EXPERIENCE', 'NAME', 'EMAIL', 'PHONE', 'LOCATION']\n",
      "[CV] Fold 1: {'SKILL': 107, 'LOCATION': 4, 'DESIGNATION': 1, 'EXPERIENCE': 1, 'EDUCATION': 1} | Missing: ['NAME', 'EMAIL', 'PHONE']\n",
      "✓ K-fold splits saved to: /workspaces/resume-ner-azureml/outputs/hpo/_meta/fold_splits.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_88428/908799909.py:1: DeprecationWarning: Importing from 'training.cv_utils' is deprecated. Please use 'training.core.cv_utils' instead. This shim will be removed in a future release.\n",
      "  from training.cv_utils import (\n"
     ]
    }
   ],
   "source": [
    "from training.cv_utils import (\n",
    "    create_kfold_splits,\n",
    "    save_fold_splits,\n",
    "    validate_splits,\n",
    ")\n",
    "from data.loaders import load_dataset\n",
    "from infrastructure.paths import resolve_output_path\n",
    "\n",
    "# Setup k-fold splits if enabled\n",
    "k_fold_config = hpo_config.get(\"k_fold\", {})\n",
    "k_folds_enabled = k_fold_config.get(\"enabled\", False)\n",
    "fold_splits_file = None\n",
    "\n",
    "if k_folds_enabled:\n",
    "    n_splits = k_fold_config.get(\"n_splits\", DEFAULT_K_FOLDS)\n",
    "    random_seed = k_fold_config.get(\"random_seed\", DEFAULT_RANDOM_SEED)\n",
    "    shuffle = k_fold_config.get(\"shuffle\", True)\n",
    "    stratified = k_fold_config.get(\"stratified\", False)\n",
    "    entity_types = (\n",
    "        configs.get(\"data\", {})\n",
    "        .get(\"schema\", {})\n",
    "        .get(\"entity_types\", [])\n",
    "    )\n",
    "\n",
    "    print(f\"Setting up {n_splits}-fold cross-validation splits...\")\n",
    "\n",
    "    full_dataset = load_dataset(str(DATASET_LOCAL_PATH))\n",
    "    train_data = full_dataset.get(\"train\", [])\n",
    "\n",
    "    fold_splits = create_kfold_splits(\n",
    "        dataset=train_data,\n",
    "        k=n_splits,\n",
    "        random_seed=random_seed,\n",
    "        shuffle=shuffle,\n",
    "        stratified=stratified,\n",
    "        entity_types=entity_types,\n",
    "    )\n",
    "\n",
    "    # Optional validation to ensure rare entities appear across folds\n",
    "    validate_splits(train_data, fold_splits, entity_types=entity_types)\n",
    "\n",
    "    # Use centralized HPO root from paths.yaml (respects env_overrides / storage_env)\n",
    "    HPO_ROOT = resolve_output_path(ROOT_DIR, CONFIG_DIR, \"hpo\")\n",
    "\n",
    "    # Keep fold_splits as a study-level meta artifact\n",
    "    HPO_META_DIR = validate_path_before_mkdir(\n",
    "        HPO_ROOT / \"_meta\", context=\"directory\"\n",
    "    )\n",
    "    HPO_META_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    fold_splits_file = HPO_META_DIR / \"fold_splits.json\"\n",
    "\n",
    "    save_fold_splits(\n",
    "        fold_splits,\n",
    "        fold_splits_file,\n",
    "        metadata={\n",
    "            \"k\": n_splits,\n",
    "            \"random_seed\": random_seed,\n",
    "            \"shuffle\": shuffle,\n",
    "            \"stratified\": stratified,\n",
    "            \"dataset_path\": str(DATASET_LOCAL_PATH),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(f\"✓ K-fold splits saved to: {fold_splits_file}\")\n",
    "\n",
    "else:\n",
    "    print(\"K-fold CV disabled - using single train/validation split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint functionality is now handled automatically by run_local_hpo_sweep\n",
    "# when checkpoint.enabled: true is set in the HPO config.\n",
    "# No manual backup callbacks are needed - SQLite persistence is built-in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint functionality is now handled automatically by run_local_hpo_sweep\n",
    "# when checkpoint.enabled: true is set in the HPO config.\n",
    "# No wrapper functions are needed - SQLite persistence is built-in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In a Kaggle notebook cell\n",
    "# !cd /kaggle/working/resume-ner-azureml && git fetch origin gg_final_training_2 && git checkout origin/gg_final_training_2 -- src/train.py src/training/trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-12 17:24:02,730 - training.hpo.core.study - INFO - [HPO] Starting optimization for distilbert with checkpointing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected environment: local\n",
      "✓ HPO output directory: /workspaces/resume-ner-azureml/outputs/hpo/local/distilbert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-12 17:24:03,403 - training.hpo.core.study - WARNING - Study 'hpo_distilbert' already exists in database. Incrementing variant for force_new mode...\n",
      "2026-01-12 17:24:03,404 - training.hpo.core.study - INFO - Retrying with incremented study name: 'hpo_distilbert_v2'\n",
      "2026-01-12 17:24:03,452 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}\n",
      "2026-01-12 17:24:03,453 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo\n",
      "2026-01-12 17:24:03,454 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:7580f7593872b0c166ace1a5f2e713aed06a37f8bd0dd..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-12 17:24:03,454 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Loaded 16 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-12 17:24:03,455 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Found 1 allocations for counter_key (after deduplication): committed=[1], reserved=[], expired=[], max_committed_version=1\n",
      "2026-01-12 17:24:03,456 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Allocation details: [(1, 'committed', '45f077cc-1f1')]\n",
      "2026-01-12 17:24:03,458 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Reserving next version: 2 (incremented from max_committed=1, skipped 0 reserved/expired versions)\n",
      "2026-01-12 17:24:03,459 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] ✓ Successfully reserved version 2 for counter_key resume-ner:hpo:7580f7593872b0c166ace1a5f2e713aed06... (run_id: pending_2026...)\n",
      "2026-01-12 17:24:03,460 - training.hpo.execution.local.sweep - INFO - [HPO Setup] k_folds=2, fold_splits_file=/workspaces/resume-ner-azureml/outputs/hpo/_meta/fold_splits.json, k_fold config: {'enabled': True, 'n_splits': 2, 'random_seed': 42, 'shuffle': True, 'stratified': True}\n",
      "2026-01-12 17:24:03,534 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - Using Azure ML Workspace for MLflow tracking\n",
      "2026-01-12 17:24:04,208 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=584922cebb01e555..., study_family_hash=6715315ed8693848...\n",
      "2026-01-12 17:24:05,284 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Yielding RunHandle. run_id=b0b5118c-883...\n",
      "2026-01-12 17:24:05,402 - training.hpo.execution.local.sweep - INFO - [HPO] Parent run b0b5118c-883... is now visible in MLflow (status: RUNNING)\n",
      "2026-01-12 17:24:05,403 - training.hpo.tracking.setup - INFO - [HPO Commit] Found version 2 in run name 'local_distilbert_hpo_study-584922ce_2'\n",
      "2026-01-12 17:24:05,404 - orchestration.jobs.tracking.config.loader - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}\n",
      "2026-01-12 17:24:05,405 - orchestration.jobs.tracking.config.loader - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo\n",
      "2026-01-12 17:24:05,405 - training.hpo.tracking.setup - INFO - [HPO Commit] Committing version 2 for run b0b5118c-883..., counter_key=resume-ner:hpo:7580f7593872b0c166ace1a5f2e713aed06...\n",
      "2026-01-12 17:24:05,406 - orchestration.jobs.tracking.index.version_counter - INFO - [Commit Version] Starting commit: counter_key=resume-ner:hpo:7580f7593872b0c166ace1a5f2e713aed06a37f8bd0dd..., version=2, run_id=b0b5118c-883..., counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-12 17:24:05,407 - orchestration.jobs.tracking.index.version_counter - INFO - [Commit Version] Loaded 17 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-12 17:24:05,407 - orchestration.jobs.tracking.index.version_counter - INFO - [Commit Version] ✓ Found and committed reservation: version=2, status changed from 'reserved' to 'committed', run_id=b0b5118c-883..., counter_key=resume-ner:hpo:7580f7593872b0c166ace1a5f2e713aed06...\n",
      "2026-01-12 17:24:05,409 - orchestration.jobs.tracking.index.version_counter - INFO - [Commit Version] ✓ Successfully saved committed version 2 to /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-12 17:24:05,409 - training.hpo.tracking.setup - INFO - [HPO Commit] ✓ Successfully committed version 2 for HPO parent run b0b5118c-883...\n",
      "2026-01-12 17:24:05,523 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Starting cleanup check: parent_run_id=b0b5118c-883...\n",
      "2026-01-12 17:24:05,616 - training.hpo.tracking.cleanup - INFO - [CLEANUP] MLflow imported successfully. Current env: local, run_key_hash: 7580f7593872...\n",
      "2026-01-12 17:24:05,671 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Retrieved experiment: 2ff6dc5c-9384-4db6-ae7e-53869501b3db\n",
      "2026-01-12 17:24:05,672 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Fetching all runs in experiment (may paginate for large experiments)...\n",
      "2026-01-12 17:24:05,974 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Fetched 13 total runs from experiment\n",
      "2026-01-12 17:24:05,975 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Built parent→children map: 5 parents have children\n",
      "2026-01-12 17:24:05,975 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Status breakdown: {'FINISHED': 13}\n",
      "2026-01-12 17:24:05,976 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Found 0 tag-based matches, 0 name-fallback matches (legacy), 0 total eligible for tagging\n",
      "2026-01-12 17:24:05,976 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Found 0 orphaned child runs (RUNNING children with terminal parents)\n",
      "2026-01-12 17:24:05,977 - training.hpo.tracking.cleanup - INFO - [CLEANUP] No interrupted parent runs found to tag\n",
      "2026-01-12 17:24:05,977 - training.hpo.tracking.cleanup - INFO - [CLEANUP] No orphaned child runs found to tag\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]2026-01-12 17:24:06,192 - training.hpo.execution.local.sweep - INFO - [Trial 0] fold_splits=present, using CV path\n",
      "2026-01-12 17:24:06,193 - training.hpo.execution.local.sweep - INFO - [Trial 0] Running 2-fold CV\n",
      "2026-01-12 17:24:06,194 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}\n",
      "2026-01-12 17:24:06,195 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo\n",
      "2026-01-12 17:24:06,196 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:85522ae3cd38ce0b767b4731f34065b36286e7e2a7f80..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-12 17:24:06,197 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Loaded 17 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-12 17:24:06,197 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0\n",
      "2026-01-12 17:24:06,198 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)\n",
      "2026-01-12 17:24:06,199 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] ✓ Successfully reserved version 1 for counter_key resume-ner:hpo:85522ae3cd38ce0b767b4731f34065b3628... (run_id: pending_2026...)\n",
      "2026-01-12 17:24:06,334 - training.hpo.execution.local.cv - INFO - [CV] Created trial folder: /workspaces/resume-ner-azureml/outputs/hpo/local/distilbert/study-584922ce/trial-f302370a (trial 0)\n",
      "2026-01-12 17:24:23,795 - training.hpo.execution.local.trial - INFO - 🏃 View run local_distilbert_hpo_trial_study-584922ce_t00_fold0 at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/2ff6dc5c-9384-4db6-ae7e-53869501b3db/runs/d7022c48-9235-465d-ba6d-a360050b04ef\n",
      "🧪 View experiment at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/2ff6dc5c-9384-4db6-ae7e-53869501b3db\n",
      "\n",
      "2026-01-12 17:24:23,796 - training.hpo.execution.local.trial - WARNING - 2026-01-12 17:24:09,269 - infrastructure.tracking.mlflow.compatibility - INFO - Applied Azure ML artifact compatibility patch\n",
      "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/azureml/core/__init__.py:11: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "  [Training] Set MLflow tracking URI: azureml://germanywestcentral.api.azureml.ms/mlflow...\n",
      "  [Training] Set MLflow experiment: resume_ner_baseline-hpo-distilbert\n",
      "2026-01-12 17:24:13,147 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}\n",
      "2026-01-12 17:24:13,147 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo_trial_fold\n",
      "  [Training] Creating child run with parent: a9b6d430-4ba... (trial 0, fold 0)\n",
      "  [Training] ✓ Created child run: d7022c48-923...\n",
      "  [Training] ✓ Started child run\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SKILL seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NAME seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: EDUCATION seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: EXPERIENCE seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: DESIGNATION seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PHONE seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LOCATION seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "  [Training] Skipping artifact logging (MLFLOW_SKIP_ARTIFACT_LOGGING=true) - HPO trial artifacts will be logged only for best trial refit\n",
      "  [Training] Ended child run\n",
      "\n",
      "2026-01-12 17:24:23,797 - training.hpo.execution.local.trial - INFO - [TRIAL] Training completed. Objective metric 'macro-f1': 0.3496677740863787\n",
      "2026-01-12 17:24:40,028 - training.hpo.execution.local.trial - INFO - 🏃 View run local_distilbert_hpo_trial_study-584922ce_t00_fold1 at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/2ff6dc5c-9384-4db6-ae7e-53869501b3db/runs/2701d124-a578-45f8-b9cc-739fadaee01d\n",
      "🧪 View experiment at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/2ff6dc5c-9384-4db6-ae7e-53869501b3db\n",
      "\n",
      "2026-01-12 17:24:40,029 - training.hpo.execution.local.trial - WARNING - 2026-01-12 17:24:27,180 - infrastructure.tracking.mlflow.compatibility - INFO - Applied Azure ML artifact compatibility patch\n",
      "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/azureml/core/__init__.py:11: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "  [Training] Set MLflow tracking URI: azureml://germanywestcentral.api.azureml.ms/mlflow...\n",
      "  [Training] Set MLflow experiment: resume_ner_baseline-hpo-distilbert\n",
      "2026-01-12 17:24:31,208 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}\n",
      "2026-01-12 17:24:31,208 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo_trial_fold\n",
      "  [Training] Creating child run with parent: a9b6d430-4ba... (trial 0, fold 1)\n",
      "  [Training] ✓ Created child run: 2701d124-a57...\n",
      "  [Training] ✓ Started child run\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SKILL seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: EXPERIENCE seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LOCATION seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PHONE seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NAME seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: EDUCATION seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "  [Training] Skipping artifact logging (MLFLOW_SKIP_ARTIFACT_LOGGING=true) - HPO trial artifacts will be logged only for best trial refit\n",
      "  [Training] Ended child run\n",
      "\n",
      "2026-01-12 17:24:40,030 - training.hpo.execution.local.trial - INFO - [TRIAL] Training completed. Objective metric 'macro-f1': 0.3865546218487395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run local_distilbert_hpo_trial_study-584922ce_t00_1 at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/2ff6dc5c-9384-4db6-ae7e-53869501b3db/runs/a9b6d430-4ba3-4103-b9a1-f471a780e1c1\n",
      "🧪 View experiment at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/2ff6dc5c-9384-4db6-ae7e-53869501b3db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-12 17:24:43,544 - infrastructure.tracking.mlflow.lifecycle - INFO - Successfully terminated run a9b6d430-4ba... with status FINISHED\n",
      "2026-01-12 17:24:44,555 - training.hpo.trial.callback - INFO - \n",
      "2026-01-12 17:24:44,556 - training.hpo.trial.callback - INFO - [BEST]: trial_0\n",
      "2026-01-12 17:24:44,556 - training.hpo.trial.callback - INFO -   Metrics: macro-f1=0.368111\n",
      "2026-01-12 17:24:44,557 - training.hpo.trial.callback - INFO -   Params: learning_rate=2.29e-05 | batch_size=4 | dropout=0.119936 | weight_decay=0.009404 (Run ID: a9b6d430-4ba...)\n",
      "Best trial: 0. Best value: 0.368111: 100%|██████████| 1/1 [00:38<00:00, 38.53s/it, 38.52/1200 seconds]\n",
      "2026-01-12 17:24:44,568 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=b0b5118c-883...\n",
      "2026-01-12 17:24:44,576 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Found 1 completed trials out of 1 total\n",
      "2026-01-12 17:24:44,578 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging metrics: n_trials=1, n_completed_trials=1\n",
      "2026-01-12 17:24:45,132 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=0.3681111979675591\n",
      "2026-01-12 17:24:45,401 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging best hyperparameters: {'learning_rate': 2.292541417390017e-05, 'batch_size': 4, 'dropout': 0.11993635851671354, 'weight_decay': 0.00940445086478489}\n",
      "2026-01-12 17:24:45,632 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Finding and logging best trial run ID...\n",
      "2026-01-12 17:24:45,635 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - Using 0 cached child runs for trial search\n",
      "2026-01-12 17:24:45,636 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - Searching for trial 0 in 0 child runs. Parent run ID: b0b5118c-883...\n",
      "2026-01-12 17:24:45,636 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - No child runs found for parent b0b5118c-883... This may indicate: (1) runs haven't been created yet, (2) runs are not direct children of parent, or (3) search filter is incorrect. Experiment ID: 2ff6dc5c-9384-4db6-ae7e-53869501b3db\n",
      "2026-01-12 17:24:45,637 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Could not find MLflow run ID for best trial 0. No child runs found for parent b0b5118c-883... This may be a timing issue - trial runs may not be created/committed yet.\n",
      "2026-01-12 17:24:45,637 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Completed best trial ID logging\n",
      "2026-01-12 17:24:45,638 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Checkpoint logging enabled: False (upload_checkpoint=False)\n",
      "2026-01-12 17:24:45,639 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)\n",
      "2026-01-12 17:24:45,748 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Completed log_final_metrics successfully\n",
      "2026-01-12 17:24:45,752 - training.hpo.execution.local.sweep - INFO - [REFIT] Starting refit training for best trial 0\n",
      "2026-01-12 17:24:45,755 - training.hpo.execution.local.sweep - INFO - [REFIT] Computed trial_key_hash=f302370a4cc43f41... from study_key_hash and best trial hyperparameters\n",
      "2026-01-12 17:24:45,757 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 2.292541417390017e-05, 'batch_size': 4, 'dropout': 0.11993635851671354, 'weight_decay': 0.00940445086478489}\n",
      "2026-01-12 17:24:45,757 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id='trial_0_20260112_172402', run_id='20260112_172402', trial_number=0\n",
      "2026-01-12 17:24:45,804 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}\n",
      "2026-01-12 17:24:45,805 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo\n",
      "2026-01-12 17:24:45,806 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:7b2c680ce4e0e8ad317f0cb00c61fd4987bfa6853ccc2..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-12 17:24:45,836 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Loaded 18 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-12 17:24:45,836 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0\n",
      "2026-01-12 17:24:45,837 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)\n",
      "2026-01-12 17:24:45,838 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] ✓ Successfully reserved version 1 for counter_key resume-ner:hpo:7b2c680ce4e0e8ad317f0cb00c61fd4987b... (run_id: pending_2026...)\n",
      "2026-01-12 17:24:46,053 - training.execution.mlflow_setup - INFO - 🏃 View run local_distilbert_hpo_refit_study-584922ce_trial-f302370a_t00_1 at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/2ff6dc5c-9384-4db6-ae7e-53869501b3db/runs/1e0bd7d4-de53-4c23-aab9-56c41dd2af14\n",
      "2026-01-12 17:24:46,053 - training.execution.mlflow_setup - INFO - Created MLflow run: local_distilbert_hpo_refit_study-584922ce_trial-f302370a_t00_1 (1e0bd7d4-de5...)\n",
      "2026-01-12 17:24:59,362 - training.hpo.execution.local.refit - WARNING - 2026-01-12 17:24:49,055 - infrastructure.tracking.mlflow.compatibility - INFO - Applied Azure ML artifact compatibility patch\n",
      "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/azureml/core/__init__.py:11: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "  [Training] Set MLflow tracking URI: azureml://germanywestcentral.api.azureml.ms/mlflow...\n",
      "  [Training] Set MLflow experiment: resume_ner_baseline-hpo-distilbert\n",
      "  [Training] Using existing run: 1e0bd7d4-de5... (refit mode)\n",
      "  [Training] ✓ Will log to existing run via client API (run stays RUNNING)\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  [Training] Skipping artifact logging (MLFLOW_SKIP_ARTIFACT_LOGGING=true) - HPO trial artifacts will be logged only for best trial refit\n",
      "  [Training] Refit run remains RUNNING (will be marked FINISHED after artifacts)\n",
      "\n",
      "2026-01-12 17:24:59,879 - training.hpo.execution.local.refit - INFO - [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)\n",
      "2026-01-12 17:24:59,880 - training.hpo.execution.local.refit - INFO - [REFIT] Refit training completed. Metrics: N/A, Checkpoint: /workspaces/resume-ner-azureml/outputs/hpo/local/distilbert/study-584922ce/trial-f302370a/refit/checkpoint\n",
      "2026-01-12 17:24:59,881 - training.hpo.execution.local.sweep - INFO - [REFIT] Refit training completed. Metrics: {'note': 'No validation set - training on all data'}, Checkpoint: /workspaces/resume-ner-azureml/outputs/hpo/local/distilbert/study-584922ce/trial-f302370a/refit/checkpoint, Run ID: 1e0bd7d4-de5...\n",
      "2026-01-12 17:25:00,332 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_BEST_CHECKPOINT] Using preferred checkpoint directory: /workspaces/resume-ner-azureml/outputs/hpo/local/distilbert/study-584922ce/trial-f302370a/refit/checkpoint\n",
      "2026-01-12 17:25:00,333 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - Uploading checkpoint archive to MLflow...\n",
      "2026-01-12 17:25:00,334 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - Uploading checkpoint to refit run 1e0bd7d4-de5 (child of parent b0b5118c-883)\n",
      "2026-01-12 17:25:00,334 - infrastructure.tracking.mlflow.artifacts.uploader - INFO - Creating checkpoint archive from /workspaces/resume-ner-azureml/outputs/hpo/local/distilbert/study-584922ce/trial-f302370a/refit/checkpoint...\n",
      "2026-01-12 17:25:12,363 - infrastructure.tracking.mlflow.artifacts.manager - INFO - Created checkpoint archive: /tmp/checkpoint_okaq_dl_.tar.gz (6 files, 254.1MB)\n",
      "2026-01-12 17:25:12,365 - infrastructure.tracking.mlflow._artifacts_file - INFO - Uploading checkpoint archive (233.8MB)...\n",
      "2026-01-12 17:25:19,572 - infrastructure.tracking.mlflow._artifacts_file - INFO - Successfully uploaded checkpoint archive: checkpoint_okaq_dl_.tar.gz\n",
      "2026-01-12 17:25:19,573 - infrastructure.tracking.mlflow.artifacts.uploader - INFO - Successfully uploaded checkpoint archive: 6 files (254.1MB) for trial 0\n",
      "2026-01-12 17:25:19,603 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - Checkpoint upload completed successfully for trial 0\n",
      "2026-01-12 17:25:19,627 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - Marked study as complete with checkpoint uploaded (best trial: 0)\n",
      "2026-01-12 17:25:19,627 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Checkpoint upload returned False (may have been skipped)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run local_distilbert_hpo_refit_study-584922ce_trial-f302370a_t00_1 at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/2ff6dc5c-9384-4db6-ae7e-53869501b3db/runs/1e0bd7d4-de53-4c23-aab9-56c41dd2af14\n",
      "🧪 View experiment at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/2ff6dc5c-9384-4db6-ae7e-53869501b3db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-12 17:25:21,193 - infrastructure.tracking.mlflow.lifecycle - INFO - Successfully terminated run 1e0bd7d4-de5... with status FINISHED\n",
      "2026-01-12 17:25:21,194 - training.hpo.execution.local.sweep - INFO - [REFIT] ✓ Artifacts uploaded and run marked as FINISHED: 1e0bd7d4-de5...\n",
      "2026-01-12 17:25:21,195 - training.hpo.checkpoint.cleanup - INFO - Final cleanup: kept checkpoints for best trial 0 (metric=0.368111, CV=no, refit=no), deleted 0 non-best checkpoints\n",
      "2026-01-12 17:25:21,195 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Context manager exiting normally. run_id=b0b5118c-883...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run local_distilbert_hpo_study-584922ce_2 at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/2ff6dc5c-9384-4db6-ae7e-53869501b3db/runs/b0b5118c-8839-4a53-a4b4-be7f1485c72f\n",
      "🧪 View experiment at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/2ff6dc5c-9384-4db6-ae7e-53869501b3db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_88428/2457126461.py:82: DeprecationWarning: Importing from 'orchestration.jobs.hpo' is deprecated. Please use 'hpo' module directly instead. Example: 'from hpo import run_local_hpo_sweep'\n",
      "  from orchestration.jobs.hpo import backup_hpo_study_to_drive\n",
      "2026-01-12 17:25:22,254 - training.hpo.core.study - INFO - [HPO] Starting optimization for distilroberta with checkpointing...\n",
      "2026-01-12 17:25:22,289 - training.hpo.core.study - WARNING - Study 'hpo_distilroberta' already exists in database. Incrementing variant for force_new mode...\n",
      "2026-01-12 17:25:22,290 - training.hpo.core.study - INFO - Retrying with incremented study name: 'hpo_distilroberta_v2'\n",
      "2026-01-12 17:25:22,321 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}\n",
      "2026-01-12 17:25:22,321 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo\n",
      "2026-01-12 17:25:22,322 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:55c3188417763b2de9636b3e55c79744062efeafe7209..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-12 17:25:22,323 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Loaded 19 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-12 17:25:22,324 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Found 1 allocations for counter_key (after deduplication): committed=[1], reserved=[], expired=[], max_committed_version=1\n",
      "2026-01-12 17:25:22,324 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Allocation details: [(1, 'committed', 'ef33aa8a-f17')]\n",
      "2026-01-12 17:25:22,325 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Reserving next version: 2 (incremented from max_committed=1, skipped 0 reserved/expired versions)\n",
      "2026-01-12 17:25:22,326 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] ✓ Successfully reserved version 2 for counter_key resume-ner:hpo:55c3188417763b2de9636b3e55c79744062... (run_id: pending_2026...)\n",
      "2026-01-12 17:25:22,328 - training.hpo.execution.local.sweep - INFO - [HPO Setup] k_folds=2, fold_splits_file=/workspaces/resume-ner-azureml/outputs/hpo/_meta/fold_splits.json, k_fold config: {'enabled': True, 'n_splits': 2, 'random_seed': 42, 'shuffle': True, 'stratified': True}\n",
      "2026-01-12 17:25:22,383 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - Using Azure ML Workspace for MLflow tracking\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ HPO output directory: /workspaces/resume-ner-azureml/outputs/hpo/local/distilroberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-12 17:25:22,481 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=25cad2a25d5a991d..., study_family_hash=6715315ed8693848...\n",
      "2026-01-12 17:25:23,526 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Yielding RunHandle. run_id=3e4a2c15-3c9...\n",
      "2026-01-12 17:25:23,657 - training.hpo.execution.local.sweep - INFO - [HPO] Parent run 3e4a2c15-3c9... is now visible in MLflow (status: RUNNING)\n",
      "2026-01-12 17:25:23,658 - training.hpo.tracking.setup - INFO - [HPO Commit] Found version 2 in run name 'local_distilroberta_hpo_study-25cad2a2_2'\n",
      "2026-01-12 17:25:23,659 - orchestration.jobs.tracking.config.loader - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}\n",
      "2026-01-12 17:25:23,659 - orchestration.jobs.tracking.config.loader - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo\n",
      "2026-01-12 17:25:23,660 - training.hpo.tracking.setup - INFO - [HPO Commit] Committing version 2 for run 3e4a2c15-3c9..., counter_key=resume-ner:hpo:55c3188417763b2de9636b3e55c79744062...\n",
      "2026-01-12 17:25:23,660 - orchestration.jobs.tracking.index.version_counter - INFO - [Commit Version] Starting commit: counter_key=resume-ner:hpo:55c3188417763b2de9636b3e55c79744062efeafe7209..., version=2, run_id=3e4a2c15-3c9..., counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-12 17:25:23,662 - orchestration.jobs.tracking.index.version_counter - INFO - [Commit Version] Loaded 20 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-12 17:25:23,662 - orchestration.jobs.tracking.index.version_counter - INFO - [Commit Version] ✓ Found and committed reservation: version=2, status changed from 'reserved' to 'committed', run_id=3e4a2c15-3c9..., counter_key=resume-ner:hpo:55c3188417763b2de9636b3e55c79744062...\n",
      "2026-01-12 17:25:23,664 - orchestration.jobs.tracking.index.version_counter - INFO - [Commit Version] ✓ Successfully saved committed version 2 to /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-12 17:25:23,664 - training.hpo.tracking.setup - INFO - [HPO Commit] ✓ Successfully committed version 2 for HPO parent run 3e4a2c15-3c9...\n",
      "2026-01-12 17:25:23,766 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Starting cleanup check: parent_run_id=3e4a2c15-3c9...\n",
      "2026-01-12 17:25:23,850 - training.hpo.tracking.cleanup - INFO - [CLEANUP] MLflow imported successfully. Current env: local, run_key_hash: 55c318841776...\n",
      "2026-01-12 17:25:23,895 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Retrieved experiment: b5a68c09-5298-4f47-ba70-8562f26f7015\n",
      "2026-01-12 17:25:23,895 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Fetching all runs in experiment (may paginate for large experiments)...\n",
      "2026-01-12 17:25:23,987 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Fetched 13 total runs from experiment\n",
      "2026-01-12 17:25:23,988 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Built parent→children map: 5 parents have children\n",
      "2026-01-12 17:25:23,989 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Status breakdown: {'FINISHED': 13}\n",
      "2026-01-12 17:25:23,989 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Found 0 tag-based matches, 0 name-fallback matches (legacy), 0 total eligible for tagging\n",
      "2026-01-12 17:25:23,990 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Found 0 orphaned child runs (RUNNING children with terminal parents)\n",
      "2026-01-12 17:25:23,990 - training.hpo.tracking.cleanup - INFO - [CLEANUP] No interrupted parent runs found to tag\n",
      "2026-01-12 17:25:23,991 - training.hpo.tracking.cleanup - INFO - [CLEANUP] No orphaned child runs found to tag\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]2026-01-12 17:25:24,146 - training.hpo.execution.local.sweep - INFO - [Trial 0] fold_splits=present, using CV path\n",
      "2026-01-12 17:25:24,148 - training.hpo.execution.local.sweep - INFO - [Trial 0] Running 2-fold CV\n",
      "2026-01-12 17:25:24,149 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}\n",
      "2026-01-12 17:25:24,150 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo\n",
      "2026-01-12 17:25:24,150 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:9a9359ec811a873da22c3c6df8a53db3f8414547eea36..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-12 17:25:24,151 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Loaded 20 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-12 17:25:24,152 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0\n",
      "2026-01-12 17:25:24,152 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)\n",
      "2026-01-12 17:25:24,153 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] ✓ Successfully reserved version 1 for counter_key resume-ner:hpo:9a9359ec811a873da22c3c6df8a53db3f84... (run_id: pending_2026...)\n",
      "2026-01-12 17:25:24,246 - training.hpo.execution.local.cv - INFO - [CV] Created trial folder: /workspaces/resume-ner-azureml/outputs/hpo/local/distilroberta/study-25cad2a2/trial-5d87285d (trial 0)\n",
      "2026-01-12 17:25:41,457 - training.hpo.execution.local.trial - INFO - 🏃 View run local_distilroberta_hpo_trial_study-25cad2a2_t00_fold0 at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/b5a68c09-5298-4f47-ba70-8562f26f7015/runs/07f456c1-ad2f-4d65-9d5d-7d897a5e5162\n",
      "🧪 View experiment at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/b5a68c09-5298-4f47-ba70-8562f26f7015\n",
      "\n",
      "2026-01-12 17:25:41,457 - training.hpo.execution.local.trial - WARNING - 2026-01-12 17:25:27,237 - infrastructure.tracking.mlflow.compatibility - INFO - Applied Azure ML artifact compatibility patch\n",
      "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/azureml/core/__init__.py:11: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "  [Training] Set MLflow tracking URI: azureml://germanywestcentral.api.azureml.ms/mlflow...\n",
      "  [Training] Set MLflow experiment: resume_ner_baseline-hpo-distilroberta\n",
      "2026-01-12 17:25:31,119 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}\n",
      "2026-01-12 17:25:31,119 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo_trial_fold\n",
      "  [Training] Creating child run with parent: 00e8b616-f36... (trial 0, fold 0)\n",
      "  [Training] ✓ Created child run: 07f456c1-ad2...\n",
      "  [Training] ✓ Started child run\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SKILL seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NAME seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: LOCATION seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: PHONE seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "  [Training] Skipping artifact logging (MLFLOW_SKIP_ARTIFACT_LOGGING=true) - HPO trial artifacts will be logged only for best trial refit\n",
      "  [Training] Ended child run\n",
      "\n",
      "2026-01-12 17:25:41,459 - training.hpo.execution.local.trial - INFO - [TRIAL] Training completed. Objective metric 'macro-f1': 0.009523809523809525\n",
      "2026-01-12 17:25:58,708 - training.hpo.execution.local.trial - INFO - 🏃 View run local_distilroberta_hpo_trial_study-25cad2a2_t00_fold1 at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/b5a68c09-5298-4f47-ba70-8562f26f7015/runs/54bd1b37-3ad7-4835-b7c1-7c1e1b8942b1\n",
      "🧪 View experiment at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/b5a68c09-5298-4f47-ba70-8562f26f7015\n",
      "\n",
      "2026-01-12 17:25:58,709 - training.hpo.execution.local.trial - WARNING - 2026-01-12 17:25:44,524 - infrastructure.tracking.mlflow.compatibility - INFO - Applied Azure ML artifact compatibility patch\n",
      "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/azureml/core/__init__.py:11: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "  [Training] Set MLflow tracking URI: azureml://germanywestcentral.api.azureml.ms/mlflow...\n",
      "  [Training] Set MLflow experiment: resume_ner_baseline-hpo-distilroberta\n",
      "2026-01-12 17:25:49,422 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}\n",
      "2026-01-12 17:25:49,422 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo_trial_fold\n",
      "  [Training] Creating child run with parent: 00e8b616-f36... (trial 0, fold 1)\n",
      "  [Training] ✓ Created child run: 54bd1b37-3ad...\n",
      "  [Training] ✓ Started child run\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SKILL seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: EXPERIENCE seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "  [Training] Skipping artifact logging (MLFLOW_SKIP_ARTIFACT_LOGGING=true) - HPO trial artifacts will be logged only for best trial refit\n",
      "  [Training] Ended child run\n",
      "\n",
      "2026-01-12 17:25:58,710 - training.hpo.execution.local.trial - INFO - [TRIAL] Training completed. Objective metric 'macro-f1': 0.40566037735849053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run local_distilroberta_hpo_trial_study-25cad2a2_t00_1 at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/b5a68c09-5298-4f47-ba70-8562f26f7015/runs/00e8b616-f360-4778-9565-2beb290f93a7\n",
      "🧪 View experiment at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/b5a68c09-5298-4f47-ba70-8562f26f7015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-12 17:26:02,408 - infrastructure.tracking.mlflow.lifecycle - INFO - Successfully terminated run 00e8b616-f36... with status FINISHED\n",
      "2026-01-12 17:26:02,525 - training.hpo.trial.callback - INFO - \n",
      "2026-01-12 17:26:02,526 - training.hpo.trial.callback - INFO - [BEST]: trial_0\n",
      "2026-01-12 17:26:02,527 - training.hpo.trial.callback - INFO -   Metrics: macro-f1=0.207592\n",
      "2026-01-12 17:26:02,527 - training.hpo.trial.callback - INFO -   Params: learning_rate=2.66e-05 | batch_size=4 | dropout=0.230819 | weight_decay=0.051453 (Run ID: 00e8b616-f36...)\n",
      "Best trial: 0. Best value: 0.207592: 100%|██████████| 1/1 [00:38<00:00, 38.50s/it, 38.49/1200 seconds]\n",
      "2026-01-12 17:26:02,537 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=3e4a2c15-3c9...\n",
      "2026-01-12 17:26:02,545 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Found 1 completed trials out of 1 total\n",
      "2026-01-12 17:26:02,547 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging metrics: n_trials=1, n_completed_trials=1\n",
      "2026-01-12 17:26:03,210 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=0.20759209344115004\n",
      "2026-01-12 17:26:03,512 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging best hyperparameters: {'learning_rate': 2.6645191042883973e-05, 'batch_size': 4, 'dropout': 0.23081913753578615, 'weight_decay': 0.05145285299047054}\n",
      "2026-01-12 17:26:03,741 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Finding and logging best trial run ID...\n",
      "2026-01-12 17:26:03,744 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - Using 0 cached child runs for trial search\n",
      "2026-01-12 17:26:03,744 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - Searching for trial 0 in 0 child runs. Parent run ID: 3e4a2c15-3c9...\n",
      "2026-01-12 17:26:03,745 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - No child runs found for parent 3e4a2c15-3c9... This may indicate: (1) runs haven't been created yet, (2) runs are not direct children of parent, or (3) search filter is incorrect. Experiment ID: b5a68c09-5298-4f47-ba70-8562f26f7015\n",
      "2026-01-12 17:26:03,746 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Could not find MLflow run ID for best trial 0. No child runs found for parent 3e4a2c15-3c9... This may be a timing issue - trial runs may not be created/committed yet.\n",
      "2026-01-12 17:26:03,746 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Completed best trial ID logging\n",
      "2026-01-12 17:26:03,747 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Checkpoint logging enabled: False (upload_checkpoint=False)\n",
      "2026-01-12 17:26:03,747 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)\n",
      "2026-01-12 17:26:03,851 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Completed log_final_metrics successfully\n",
      "2026-01-12 17:26:03,855 - training.hpo.execution.local.sweep - INFO - [REFIT] Starting refit training for best trial 0\n",
      "2026-01-12 17:26:03,858 - training.hpo.execution.local.sweep - INFO - [REFIT] Computed trial_key_hash=5d87285dc5129245... from study_key_hash and best trial hyperparameters\n",
      "2026-01-12 17:26:03,860 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 2.6645191042883973e-05, 'batch_size': 4, 'dropout': 0.23081913753578615, 'weight_decay': 0.05145285299047054}\n",
      "2026-01-12 17:26:03,860 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id='trial_0_20260112_172522', run_id='20260112_172522', trial_number=0\n",
      "2026-01-12 17:26:03,920 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}\n",
      "2026-01-12 17:26:03,921 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo\n",
      "2026-01-12 17:26:03,922 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:a2f7fffee3c908e0550996be5d7a67fc45f1283f5064d..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-12 17:26:03,923 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Loaded 21 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-12 17:26:03,924 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0\n",
      "2026-01-12 17:26:03,924 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)\n",
      "2026-01-12 17:26:03,925 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] ✓ Successfully reserved version 1 for counter_key resume-ner:hpo:a2f7fffee3c908e0550996be5d7a67fc45f... (run_id: pending_2026...)\n",
      "2026-01-12 17:26:04,171 - training.execution.mlflow_setup - INFO - 🏃 View run local_distilroberta_hpo_refit_study-25cad2a2_trial-5d87285d_t00_1 at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/b5a68c09-5298-4f47-ba70-8562f26f7015/runs/27a25fe6-e5ed-4682-8d5a-07d38ea76083\n",
      "2026-01-12 17:26:04,171 - training.execution.mlflow_setup - INFO - Created MLflow run: local_distilroberta_hpo_refit_study-25cad2a2_trial-5d87285d_t00_1 (27a25fe6-e5e...)\n",
      "2026-01-12 17:26:17,421 - training.hpo.execution.local.refit - WARNING - 2026-01-12 17:26:07,103 - infrastructure.tracking.mlflow.compatibility - INFO - Applied Azure ML artifact compatibility patch\n",
      "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/azureml/core/__init__.py:11: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "  [Training] Set MLflow tracking URI: azureml://germanywestcentral.api.azureml.ms/mlflow...\n",
      "  [Training] Set MLflow experiment: resume_ner_baseline-hpo-distilroberta\n",
      "  [Training] Using existing run: 27a25fe6-e5e... (refit mode)\n",
      "  [Training] ✓ Will log to existing run via client API (run stays RUNNING)\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  [Training] Skipping artifact logging (MLFLOW_SKIP_ARTIFACT_LOGGING=true) - HPO trial artifacts will be logged only for best trial refit\n",
      "  [Training] Refit run remains RUNNING (will be marked FINISHED after artifacts)\n",
      "\n",
      "2026-01-12 17:26:17,991 - training.hpo.execution.local.refit - INFO - [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)\n",
      "2026-01-12 17:26:17,992 - training.hpo.execution.local.refit - INFO - [REFIT] Refit training completed. Metrics: N/A, Checkpoint: /workspaces/resume-ner-azureml/outputs/hpo/local/distilroberta/study-25cad2a2/trial-5d87285d/refit/checkpoint\n",
      "2026-01-12 17:26:17,993 - training.hpo.execution.local.sweep - INFO - [REFIT] Refit training completed. Metrics: {'note': 'No validation set - training on all data'}, Checkpoint: /workspaces/resume-ner-azureml/outputs/hpo/local/distilroberta/study-25cad2a2/trial-5d87285d/refit/checkpoint, Run ID: 27a25fe6-e5e...\n",
      "2026-01-12 17:26:18,412 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_BEST_CHECKPOINT] Using preferred checkpoint directory: /workspaces/resume-ner-azureml/outputs/hpo/local/distilroberta/study-25cad2a2/trial-5d87285d/refit/checkpoint\n",
      "2026-01-12 17:26:18,412 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - Uploading checkpoint archive to MLflow...\n",
      "2026-01-12 17:26:18,413 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - Uploading checkpoint to refit run 27a25fe6-e5e (child of parent 3e4a2c15-3c9)\n",
      "2026-01-12 17:26:18,414 - infrastructure.tracking.mlflow.artifacts.uploader - INFO - Creating checkpoint archive from /workspaces/resume-ner-azureml/outputs/hpo/local/distilroberta/study-25cad2a2/trial-5d87285d/refit/checkpoint...\n",
      "2026-01-12 17:26:33,429 - infrastructure.tracking.mlflow.artifacts.manager - INFO - Created checkpoint archive: /tmp/checkpoint_t99l1zsa.tar.gz (7 files, 315.6MB)\n",
      "2026-01-12 17:26:33,430 - infrastructure.tracking.mlflow._artifacts_file - INFO - Uploading checkpoint archive (289.8MB)...\n",
      "2026-01-12 17:26:41,015 - infrastructure.tracking.mlflow._artifacts_file - INFO - Successfully uploaded checkpoint archive: checkpoint_t99l1zsa.tar.gz\n",
      "2026-01-12 17:26:41,016 - infrastructure.tracking.mlflow.artifacts.uploader - INFO - Successfully uploaded checkpoint archive: 7 files (315.6MB) for trial 0\n",
      "2026-01-12 17:26:41,054 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - Checkpoint upload completed successfully for trial 0\n",
      "2026-01-12 17:26:41,077 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - Marked study as complete with checkpoint uploaded (best trial: 0)\n",
      "2026-01-12 17:26:41,078 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Checkpoint upload returned False (may have been skipped)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run local_distilroberta_hpo_refit_study-25cad2a2_trial-5d87285d_t00_1 at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/b5a68c09-5298-4f47-ba70-8562f26f7015/runs/27a25fe6-e5ed-4682-8d5a-07d38ea76083\n",
      "🧪 View experiment at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/b5a68c09-5298-4f47-ba70-8562f26f7015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-12 17:26:43,052 - infrastructure.tracking.mlflow.lifecycle - INFO - Successfully terminated run 27a25fe6-e5e... with status FINISHED\n",
      "2026-01-12 17:26:43,052 - training.hpo.execution.local.sweep - INFO - [REFIT] ✓ Artifacts uploaded and run marked as FINISHED: 27a25fe6-e5e...\n",
      "2026-01-12 17:26:43,053 - training.hpo.checkpoint.cleanup - INFO - Final cleanup: kept checkpoints for best trial 0 (metric=0.207592, CV=no, refit=no), deleted 0 non-best checkpoints\n",
      "2026-01-12 17:26:43,054 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Context manager exiting normally. run_id=3e4a2c15-3c9...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run local_distilroberta_hpo_study-25cad2a2_2 at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/b5a68c09-5298-4f47-ba70-8562f26f7015/runs/3e4a2c15-3c9b-44d4-a507-ea0b0cbcc9d8\n",
      "🧪 View experiment at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/b5a68c09-5298-4f47-ba70-8562f26f7015\n"
     ]
    }
   ],
   "source": [
    "# Extract checkpoint configuration from HPO config\n",
    "checkpoint_config = hpo_config.get(\"checkpoint\", {})\n",
    "\n",
    "hpo_studies = {}\n",
    "k_folds_param = k_fold_config.get(\"n_splits\", DEFAULT_K_FOLDS) if k_folds_enabled else None\n",
    "\n",
    "# Use new centralized naming system for HPO\n",
    "# Build base output directory: outputs/hpo/<env>/<model>/\n",
    "# Trial-specific paths will be created by run_local_hpo_sweep as subdirectories\n",
    "\n",
    "# Import required functions\n",
    "from pathlib import Path\n",
    "from orchestration import STAGE_HPO\n",
    "from training.hpo import run_local_hpo_sweep\n",
    "from infrastructure.naming.experiments import build_mlflow_experiment_name\n",
    "from infrastructure.paths.validation import validate_path_before_mkdir\n",
    "from common.shared.platform_detection import detect_platform\n",
    "\n",
    "# Ensure environment is defined\n",
    "environment = detect_platform()\n",
    "print(f\"Detected environment: {environment}\")\n",
    "\n",
    "for backbone in backbone_values:\n",
    "    mlflow_experiment_name = build_mlflow_experiment_name(\n",
    "        experiment_config.name, STAGE_HPO, backbone\n",
    "    )\n",
    "    \n",
    "    backbone_name = backbone.split(\"-\")[0] if \"-\" in backbone else backbone\n",
    "    \n",
    "    # Build base HPO directory using new structure: outputs/hpo/<env>/<model>/\n",
    "    backbone_output_dir = ROOT_DIR / \"outputs\" / \"hpo\" / environment / backbone_name\n",
    "    backbone_output_dir = validate_path_before_mkdir(backbone_output_dir, context=\"directory\")\n",
    "    backbone_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"✓ HPO output directory: {backbone_output_dir}\")\n",
    "    \n",
    "    # Create restore function for HPO checkpoint if checkpointing enabled and BACKUP_ENABLED\n",
    "    restore_fn = None\n",
    "    if checkpoint_config.get(\"enabled\", False) and BACKUP_ENABLED:\n",
    "        # Resolve study_name from checkpoint_config (same logic as create_study_name)\n",
    "        study_name_template = checkpoint_config.get(\"study_name\") or hpo_config.get(\"study_name\")\n",
    "        study_name = None\n",
    "        if study_name_template:\n",
    "            study_name = study_name_template.replace(\"{backbone}\", backbone)\n",
    "        \n",
    "        # Resolve storage_path with both {backbone} and {study_name} placeholders\n",
    "        storage_path_template = checkpoint_config.get(\"storage_path\", \"{backbone}/study.db\")\n",
    "        storage_path_str = storage_path_template.replace(\"{backbone}\", backbone)\n",
    "        if study_name:\n",
    "            storage_path_str = storage_path_str.replace(\"{study_name}\", study_name)\n",
    "        expected_checkpoint = backbone_output_dir / storage_path_str\n",
    "        \n",
    "        def make_restore_fn(checkpoint_path):\n",
    "            def restore_fn_inner(path: Path) -> bool:\n",
    "                # Only restore if path matches expected checkpoint\n",
    "                if path == checkpoint_path:\n",
    "                    return ensure_restored_from_drive(checkpoint_path, is_directory=False)\n",
    "                return False\n",
    "            return restore_fn_inner\n",
    "        \n",
    "        restore_fn = make_restore_fn(expected_checkpoint)\n",
    "    \n",
    "    # Use standard run_local_hpo_sweep with checkpoint_config\n",
    "    # Checkpoint.enabled handles persistence via SQLite (better than manual Drive backup)\n",
    "    study = run_local_hpo_sweep(\n",
    "        dataset_path=str(DATASET_LOCAL_PATH),\n",
    "        config_dir=CONFIG_DIR,\n",
    "        backbone=backbone,\n",
    "        hpo_config=hpo_config,\n",
    "        train_config=train_config,\n",
    "        output_dir=backbone_output_dir,\n",
    "        mlflow_experiment_name=mlflow_experiment_name,\n",
    "        k_folds=k_folds_param,\n",
    "        fold_splits_file=fold_splits_file,\n",
    "        checkpoint_config=checkpoint_config,\n",
    "        restore_from_drive=restore_fn,\n",
    "        data_config=configs.get(\"data\"),\n",
    "        benchmark_config=configs.get(\"benchmark\"),\n",
    "    )\n",
    "    # Backup HPO study.db and study folder to Drive\n",
    "    # Note: HPO backup function may still be in orchestration.jobs.hpo\n",
    "    from orchestration.jobs.hpo import backup_hpo_study_to_drive\n",
    "    \n",
    "    backup_hpo_study_to_drive(\n",
    "        backbone=backbone,\n",
    "        backbone_output_dir=backbone_output_dir,\n",
    "        checkpoint_config=checkpoint_config,\n",
    "        hpo_config=hpo_config,\n",
    "        backup_to_drive=backup_to_drive,\n",
    "        backup_enabled=BACKUP_ENABLED,\n",
    "    )\n",
    "\n",
    "    # Store study in hpo_studies dict (must be inside loop!)\n",
    "    hpo_studies[backbone] = study\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Skipping trial_meta.json generation (BACKUP_ENABLED=False or hpo_studies not available)\n"
     ]
    }
   ],
   "source": [
    "# Generate missing trial_meta.json files for existing trials\n",
    "from training.hpo.trial.meta import generate_missing_trial_meta_for_all_studies\n",
    "\n",
    "if BACKUP_ENABLED and \"hpo_studies\" in locals():\n",
    "    total_created = generate_missing_trial_meta_for_all_studies(\n",
    "        hpo_studies=hpo_studies if \"hpo_studies\" in locals() else {},\n",
    "        backbone_values=backbone_values,\n",
    "        root_dir=ROOT_DIR,\n",
    "        environment=environment,\n",
    "        hpo_config=hpo_config,\n",
    "        data_config=data_config if \"data_config\" in locals() else None,\n",
    "        backup_enabled=BACKUP_ENABLED,\n",
    "    )\n",
    "    print(f\"\\n[OK] Total: Created {total_created} trial_meta.json files\")\n",
    "else:\n",
    "    print(\"[INFO] Skipping trial_meta.json generation (BACKUP_ENABLED=False or hpo_studies not available)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 distilbert: 1 trials, best macro-f1=0.3681 [study-584922ce, trial-f302370a, t0]\n",
      "   CV: 0.3681 ± 0.0184\n",
      "📊 distilroberta: 1 trials, best macro-f1=0.2076 [study-25cad2a2, trial-5d87285d, t0]\n",
      "   CV: 0.2076 ± 0.1981\n"
     ]
    }
   ],
   "source": [
    "from evaluation.selection.study_summary import print_study_summaries\n",
    "from common.shared.platform_detection import detect_platform\n",
    "\n",
    "# Get environment if not already set\n",
    "if 'environment' not in locals():\n",
    "    environment = detect_platform()\n",
    "\n",
    "# Print study summaries using the module\n",
    "print_study_summaries(\n",
    "    hpo_studies=hpo_studies if \"hpo_studies\" in locals() else None,\n",
    "    backbone_values=backbone_values if \"backbone_values\" in locals() else [],\n",
    "    hpo_config=hpo_config,\n",
    "    root_dir=ROOT_DIR,\n",
    "    environment=environment,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step P1-3.5: Benchmarking Best Trials\n",
    "\n",
    "Benchmark the best trial from each backbone to measure actual inference performance. This provides real latency data that replaces parameter-count proxies in model selection, enabling more accurate speed comparisons.\n",
    "\n",
    "**Workflow:**\n",
    "1. Identify best trial per backbone (from HPO results)\n",
    "2. Select checkpoint: prefers `refit/checkpoint/` (if refit training completed), otherwise uses best fold from `cv/foldN/checkpoint/`\n",
    "3. Run benchmarking on each best trial checkpoint\n",
    "4. Save benchmark results as `benchmark.json` in trial directories\n",
    "5. Model selection will automatically use this data when available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_88428/1658426337.py:1: DeprecationWarning: selection is deprecated, use evaluation.selection instead. This shim will be removed in 2 releases.\n",
      "  from selection.trial_finder import find_best_trials_for_backbones\n",
      "2026-01-12 17:30:19,042 - evaluation.selection.trial_finder - INFO - Looking for best trial for distilbert (distilbert)...\n",
      "2026-01-12 17:30:19,050 - evaluation.selection.trial_finder - INFO - distilbert: Best trial is study-584922ce, trial-f302370a, t0 (macro-f1=0.3681)\n",
      "2026-01-12 17:30:19,051 - evaluation.selection.trial_finder - INFO - Looking for best trial for distilroberta (distilroberta)...\n",
      "2026-01-12 17:30:19,059 - evaluation.selection.trial_finder - INFO - distilroberta: Best trial is study-25cad2a2, trial-5d87285d, t0 (macro-f1=0.2076)\n",
      "2026-01-12 17:30:19,060 - evaluation.selection.trial_finder - INFO - Summary: Found 2 / 2 best trials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Found 2 best trial(s) from 2 backbone(s)\n",
      "\n",
      "  ✓ distilbert: study-584922ce, trial-f302370a, t0 (study-584922ce)\n",
      "    Best macro-f1: 0.3681\n",
      "  ✓ distilroberta: study-25cad2a2, trial-5d87285d, t0 (study-25cad2a2)\n",
      "    Best macro-f1: 0.2076\n"
     ]
    }
   ],
   "source": [
    "from selection.trial_finder import find_best_trials_for_backbones\n",
    "from common.shared.platform_detection import detect_platform\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def get_trial_hash_info(trial_dir):\n",
    "    \"\"\"Extract study_key_hash, trial_key_hash, and trial_number from trial_meta.json if available.\"\"\"\n",
    "    trial_meta_path = Path(trial_dir) / \"trial_meta.json\"\n",
    "    if not trial_meta_path.exists():\n",
    "        return None, None, None\n",
    "    try:\n",
    "        with open(trial_meta_path, \"r\") as f:\n",
    "            meta = json.load(f)\n",
    "        return meta.get(\"study_key_hash\"), meta.get(\"trial_key_hash\"), meta.get(\"trial_number\")\n",
    "    except Exception:\n",
    "        return None, None, None\n",
    "\n",
    "# Find best trials for all backbones\n",
    "environment = detect_platform()\n",
    "best_trials = find_best_trials_for_backbones(\n",
    "    backbone_values=backbone_values,\n",
    "    hpo_studies=hpo_studies if \"hpo_studies\" in locals() else None,\n",
    "    hpo_config=hpo_config,\n",
    "    data_config=data_config,\n",
    "    root_dir=ROOT_DIR,\n",
    "    environment=environment,\n",
    ")\n",
    "\n",
    "# Print summary with hash-based identifiers\n",
    "print(f\"\\n📊 Found {len(best_trials)} best trial(s) from {len(backbone_values)} backbone(s)\\n\")\n",
    "for backbone, trial_info in best_trials.items():\n",
    "    study_name = trial_info.get('study_name', 'unknown')\n",
    "    \n",
    "    # Get hash info and trial number from trial directory\n",
    "    trial_dir = trial_info.get('trial_dir')\n",
    "    study_key_hash, trial_key_hash, trial_number = None, None, None\n",
    "    if trial_dir:\n",
    "        study_key_hash, trial_key_hash, trial_number = get_trial_hash_info(trial_dir)\n",
    "    \n",
    "    # Extract trial number from trial_name if not in metadata (fallback)\n",
    "    if trial_number is None:\n",
    "        trial_name = trial_info.get('trial_name', '')\n",
    "        if trial_name:\n",
    "            import re\n",
    "            match = re.match(r\"trial_(\\d+)_\", trial_name)\n",
    "            if match:\n",
    "                trial_number = int(match.group(1))\n",
    "    \n",
    "    # Format output with hash-based identifiers (matching MLflow UI naming)\n",
    "    if study_key_hash and trial_key_hash and trial_number is not None:\n",
    "        identifier = f\"study-{study_key_hash[:8]}, trial-{trial_key_hash[:8]}, t{trial_number}\"\n",
    "    elif study_key_hash and trial_key_hash:\n",
    "        identifier = f\"study-{study_key_hash[:8]}, trial-{trial_key_hash[:8]}\"\n",
    "    elif trial_number is not None:\n",
    "        identifier = f\"t{trial_number}\"\n",
    "    else:\n",
    "        identifier = \"unknown\"\n",
    "    \n",
    "    accuracy = trial_info.get('accuracy', 0)\n",
    "    print(f\"  ✓ {backbone}: {identifier} ({study_name})\")\n",
    "    print(f\"    Best {hpo_config['objective']['metric']}: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_88428/2590804500.py:26: DeprecationWarning: benchmarking is deprecated, use evaluation.benchmarking instead. This shim will be removed in 2 releases.\n",
      "  from benchmarking import benchmark_best_trials\n",
      "2026-01-12 17:30:19,958 - evaluation.benchmarking.orchestrator - INFO - Found refit checkpoint: /workspaces/resume-ner-azureml/outputs/hpo/local/distilbert/study-584922ce/trial-f302370a/refit/checkpoint\n",
      "2026-01-12 17:30:19,963 - evaluation.benchmarking.orchestrator - INFO - Benchmarking distilbert (trial-f302370a)...\n",
      "2026-01-12 17:30:20,143 - evaluation.benchmarking.orchestrator - INFO - [BENCHMARK] Found trial run ID from MLflow: a9b6d430-4ba... (via trial_key_hash=f302370a4cc43f41...)\n",
      "2026-01-12 17:30:20,246 - evaluation.benchmarking.orchestrator - INFO - [BENCHMARK] Found refit run ID from MLflow: 1e0bd7d4-de5...\n",
      "2026-01-12 17:30:20,247 - evaluation.benchmarking.orchestrator - INFO - [BENCHMARK] Final run IDs: trial=a9b6d430-4ba..., refit=1e0bd7d4-de5..., sweep=None...\n",
      "2026-01-12 17:30:20,247 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /workspaces/resume-ner-azureml/src/evaluation/benchmarking/cli.py --checkpoint /workspaces/resume-ner-azureml/outputs/hpo/local/distilbert/study-584922ce/trial-f302370a/refit/checkpoint --test-data /workspaces/resume-ner-azureml/dataset_tiny/seed0/test.json --batch-sizes 1 --iterations 10 --warmup 10 --max-length 512 --output /workspaces/resume-ner-azureml/outputs/benchmarking/local/distilbert/study-584922ce/trial-f302370a/bench-7d5a2dd2/benchmark.json\n",
      "2026-01-12 17:30:22,861 - infrastructure.tracking.mlflow.compatibility - INFO - Applied Azure ML artifact compatibility patch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 test texts\n",
      "Starting benchmark for checkpoint: /workspaces/resume-ner-azureml/outputs/hpo/local/distilbert/study-584922ce/trial-f302370a/refit/checkpoint\n",
      "Loading tokenizer from /workspaces/resume-ner-azureml/outputs/hpo/local/distilbert/study-584922ce/trial-f302370a/refit/checkpoint...\n",
      "Tokenizer loaded.\n",
      "Loading model from /workspaces/resume-ner-azureml/outputs/hpo/local/distilbert/study-584922ce/trial-f302370a/refit/checkpoint...\n",
      "Moving model to cpu...\n",
      "Model loaded and set to eval mode.\n",
      "Model ready on device: cpu\n",
      "\n",
      "Benchmarking batch size 1...\n",
      "  Running 10 warmup iterations, then 10 measurement iterations...\n",
      "    Warmup: 10 iterations... 10/10 done.\n",
      "    Measurement: 10 iterations... 10/10 done.\n",
      "  Mean latency: 163.73 ms\n",
      "  P95 latency: 204.06 ms\n",
      "  Throughput: 6.11 docs/sec\n",
      "\n",
      "Saving results to /workspaces/resume-ner-azureml/outputs/benchmarking/local/distilbert/study-584922ce/trial-f302370a/bench-7d5a2dd2/benchmark.json...\n",
      "Benchmark results saved to /workspaces/resume-ner-azureml/outputs/benchmarking/local/distilbert/study-584922ce/trial-f302370a/bench-7d5a2dd2/benchmark.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-12 17:30:31,787 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Building run name: trial_id=trial-f302370a, root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config\n",
      "2026-01-12 17:30:31,788 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}\n",
      "2026-01-12 17:30:31,789 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=benchmarking\n",
      "2026-01-12 17:30:31,789 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Starting reservation: counter_key=resume-ner:benchmarking:683ba9302d7bef091548337b36e6c66897d1..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-12 17:30:31,793 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Loaded 22 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-12 17:30:31,793 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0\n",
      "2026-01-12 17:30:31,794 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)\n",
      "2026-01-12 17:30:31,795 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] ✓ Successfully reserved version 1 for counter_key resume-ner:benchmarking:683ba9302d7bef091548337b36... (run_id: pending_2026...)\n",
      "2026-01-12 17:30:31,796 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Generated run name: local_distilbert_benchmark_study-584922ce_trial-f302370a_bench-7d5a2dd2_1\n",
      "2026-01-12 17:30:32,136 - orchestration.jobs.tracking.trackers.benchmark_tracker - INFO - [START_BENCHMARK_RUN] Building tags: context=NamingContext(process_type='benchmarking', model='distilbert', environment='local', stage=None, storage_env='local', study_name=None, spec_fp=None, exec_fp=None, variant=1, trial_id='trial-f302370a', trial_number=None, fold_idx=None, parent_training_id=None, conv_fp=None, study_key_hash='584922cebb01e55511966582f8a69b0845bfa471f5b712af52689ec1d80cb7f2', trial_key_hash='f302370a4cc43f419a302a1d0d0b4654dd4f8957112e52cc7ce6a27ee41746c2', benchmark_config_hash='7d5a2dd29cc01f4aedde1d486688ba09d475f7a27cc7889d1ad9ca0ba61db39a'), study_key_hash=584922cebb01e555..., trial_key_hash=f302370a4cc43f41..., context.model=distilbert, context.process_type=benchmarking\n",
      "2026-01-12 17:30:32,142 - orchestration.jobs.tracking.trackers.benchmark_tracker - INFO - [START_BENCHMARK_RUN] Built tags, grouping tags present: study_key_hash=True, trial_key_hash=True, code.model=distilbert, code.stage=benchmarking\n",
      "2026-01-12 17:30:32,144 - orchestration.jobs.tracking.trackers.benchmark_tracker - INFO - [START_BENCHMARK_RUN] Set lineage tags: parent_run_id=a9b6d430-4ba..., parent_kind=trial\n",
      "2026-01-12 17:30:32,312 - orchestration.jobs.tracking.trackers.benchmark_tracker - INFO - [Benchmark Commit] Found version 1 in run name 'local_distilbert_benchmark_study-584922ce_trial-f302370a_bench-7d5a2dd2_1'\n",
      "2026-01-12 17:30:32,313 - orchestration.jobs.tracking.config.loader - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}\n",
      "2026-01-12 17:30:32,313 - orchestration.jobs.tracking.config.loader - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=benchmarking\n",
      "2026-01-12 17:30:32,314 - orchestration.jobs.tracking.trackers.benchmark_tracker - INFO - [Benchmark Commit] Committing version 1 for run 064931d2-935..., counter_key=resume-ner:benchmarking:683ba9302d7bef091548337b36...\n",
      "2026-01-12 17:30:32,315 - orchestration.jobs.tracking.index.version_counter - INFO - [Commit Version] Starting commit: counter_key=resume-ner:benchmarking:683ba9302d7bef091548337b36e6c66897d1..., version=1, run_id=064931d2-935..., counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-12 17:30:32,316 - orchestration.jobs.tracking.index.version_counter - INFO - [Commit Version] Loaded 23 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-12 17:30:32,316 - orchestration.jobs.tracking.index.version_counter - INFO - [Commit Version] ✓ Found and committed reservation: version=1, status changed from 'reserved' to 'committed', run_id=064931d2-935..., counter_key=resume-ner:benchmarking:683ba9302d7bef091548337b36...\n",
      "2026-01-12 17:30:32,318 - orchestration.jobs.tracking.index.version_counter - INFO - [Commit Version] ✓ Successfully saved committed version 1 to /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-12 17:30:32,318 - orchestration.jobs.tracking.trackers.benchmark_tracker - INFO - [Benchmark Commit] ✓ Successfully committed version 1 for benchmark run 064931d2-935...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run local_distilbert_benchmark_study-584922ce_trial-f302370a_bench-7d5a2dd2_1 at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/29716cbc-2f1e-485a-87be-3ef5c2f931dd/runs/064931d2-935e-4f30-aca4-d26d8e120e86\n",
      "🧪 View experiment at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/29716cbc-2f1e-485a-87be-3ef5c2f931dd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-12 17:30:36,544 - evaluation.benchmarking.orchestrator - INFO - Benchmark completed: /workspaces/resume-ner-azureml/outputs/benchmarking/local/distilbert/study-584922ce/trial-f302370a/bench-7d5a2dd2/benchmark.json\n",
      "2026-01-12 17:30:36,545 - evaluation.benchmarking.orchestrator - INFO - Found refit checkpoint: /workspaces/resume-ner-azureml/outputs/hpo/local/distilroberta/study-25cad2a2/trial-5d87285d/refit/checkpoint\n",
      "2026-01-12 17:30:36,554 - evaluation.benchmarking.orchestrator - INFO - Benchmarking distilroberta (trial-5d87285d)...\n",
      "2026-01-12 17:30:36,630 - evaluation.benchmarking.orchestrator - INFO - [BENCHMARK] Found trial run ID from MLflow: 00e8b616-f36... (via trial_key_hash=5d87285dc5129245...)\n",
      "2026-01-12 17:30:36,696 - evaluation.benchmarking.orchestrator - INFO - [BENCHMARK] Found refit run ID from MLflow: 27a25fe6-e5e...\n",
      "2026-01-12 17:30:36,696 - evaluation.benchmarking.orchestrator - INFO - [BENCHMARK] Final run IDs: trial=00e8b616-f36..., refit=27a25fe6-e5e..., sweep=None...\n",
      "2026-01-12 17:30:36,697 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /workspaces/resume-ner-azureml/src/evaluation/benchmarking/cli.py --checkpoint /workspaces/resume-ner-azureml/outputs/hpo/local/distilroberta/study-25cad2a2/trial-5d87285d/refit/checkpoint --test-data /workspaces/resume-ner-azureml/dataset_tiny/seed0/test.json --batch-sizes 1 --iterations 10 --warmup 10 --max-length 512 --output /workspaces/resume-ner-azureml/outputs/benchmarking/local/distilroberta/study-25cad2a2/trial-5d87285d/bench-7d5a2dd2/benchmark.json\n",
      "2026-01-12 17:30:39,150 - infrastructure.tracking.mlflow.compatibility - INFO - Applied Azure ML artifact compatibility patch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 test texts\n",
      "Starting benchmark for checkpoint: /workspaces/resume-ner-azureml/outputs/hpo/local/distilroberta/study-25cad2a2/trial-5d87285d/refit/checkpoint\n",
      "Loading tokenizer from /workspaces/resume-ner-azureml/outputs/hpo/local/distilroberta/study-25cad2a2/trial-5d87285d/refit/checkpoint...\n",
      "Tokenizer loaded.\n",
      "Loading model from /workspaces/resume-ner-azureml/outputs/hpo/local/distilroberta/study-25cad2a2/trial-5d87285d/refit/checkpoint...\n",
      "Moving model to cpu...\n",
      "Model loaded and set to eval mode.\n",
      "Model ready on device: cpu\n",
      "\n",
      "Benchmarking batch size 1...\n",
      "  Running 10 warmup iterations, then 10 measurement iterations...\n",
      "    Warmup: 10 iterations... 10/10 done.\n",
      "    Measurement: 10 iterations... 10/10 done.\n",
      "  Mean latency: 198.67 ms\n",
      "  P95 latency: 211.21 ms\n",
      "  Throughput: 5.03 docs/sec\n",
      "\n",
      "Saving results to /workspaces/resume-ner-azureml/outputs/benchmarking/local/distilroberta/study-25cad2a2/trial-5d87285d/bench-7d5a2dd2/benchmark.json...\n",
      "Benchmark results saved to /workspaces/resume-ner-azureml/outputs/benchmarking/local/distilroberta/study-25cad2a2/trial-5d87285d/bench-7d5a2dd2/benchmark.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-12 17:30:47,707 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Building run name: trial_id=trial-5d87285d, root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config\n",
      "2026-01-12 17:30:47,708 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}\n",
      "2026-01-12 17:30:47,709 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=benchmarking\n",
      "2026-01-12 17:30:47,710 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Starting reservation: counter_key=resume-ner:benchmarking:2643c8f2ab9ddd9013a79a2b5ae6aa98a270..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-12 17:30:47,711 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Loaded 23 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-12 17:30:47,711 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0\n",
      "2026-01-12 17:30:47,712 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)\n",
      "2026-01-12 17:30:47,713 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] ✓ Successfully reserved version 1 for counter_key resume-ner:benchmarking:2643c8f2ab9ddd9013a79a2b5a... (run_id: pending_2026...)\n",
      "2026-01-12 17:30:47,715 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Generated run name: local_distilroberta_benchmark_study-25cad2a2_trial-5d87285d_bench-7d5a2dd2_1\n",
      "2026-01-12 17:30:47,830 - orchestration.jobs.tracking.trackers.benchmark_tracker - INFO - [START_BENCHMARK_RUN] Building tags: context=NamingContext(process_type='benchmarking', model='distilroberta', environment='local', stage=None, storage_env='local', study_name=None, spec_fp=None, exec_fp=None, variant=1, trial_id='trial-5d87285d', trial_number=None, fold_idx=None, parent_training_id=None, conv_fp=None, study_key_hash='25cad2a25d5a991d7836e75c8e76a8c809835ce732d861607ca47fbd3ded1c69', trial_key_hash='5d87285dc5129245d9ad7102f835c861bcac0540476a1a9374be5a53b675969b', benchmark_config_hash='7d5a2dd29cc01f4aedde1d486688ba09d475f7a27cc7889d1ad9ca0ba61db39a'), study_key_hash=25cad2a25d5a991d..., trial_key_hash=5d87285dc5129245..., context.model=distilroberta, context.process_type=benchmarking\n",
      "2026-01-12 17:30:47,831 - orchestration.jobs.tracking.trackers.benchmark_tracker - INFO - [START_BENCHMARK_RUN] Built tags, grouping tags present: study_key_hash=True, trial_key_hash=True, code.model=distilroberta, code.stage=benchmarking\n",
      "2026-01-12 17:30:47,831 - orchestration.jobs.tracking.trackers.benchmark_tracker - INFO - [START_BENCHMARK_RUN] Set lineage tags: parent_run_id=00e8b616-f36..., parent_kind=trial\n",
      "2026-01-12 17:30:47,951 - orchestration.jobs.tracking.trackers.benchmark_tracker - INFO - [Benchmark Commit] Found version 1 in run name 'local_distilroberta_benchmark_study-25cad2a2_trial-5d87285d_bench-7d5a2dd2_1'\n",
      "2026-01-12 17:30:47,951 - orchestration.jobs.tracking.config.loader - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}\n",
      "2026-01-12 17:30:47,952 - orchestration.jobs.tracking.config.loader - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=benchmarking\n",
      "2026-01-12 17:30:47,953 - orchestration.jobs.tracking.trackers.benchmark_tracker - INFO - [Benchmark Commit] Committing version 1 for run de93d0d5-77b..., counter_key=resume-ner:benchmarking:2643c8f2ab9ddd9013a79a2b5a...\n",
      "2026-01-12 17:30:47,953 - orchestration.jobs.tracking.index.version_counter - INFO - [Commit Version] Starting commit: counter_key=resume-ner:benchmarking:2643c8f2ab9ddd9013a79a2b5ae6aa98a270..., version=1, run_id=de93d0d5-77b..., counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-12 17:30:47,954 - orchestration.jobs.tracking.index.version_counter - INFO - [Commit Version] Loaded 24 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-12 17:30:47,955 - orchestration.jobs.tracking.index.version_counter - INFO - [Commit Version] ✓ Found and committed reservation: version=1, status changed from 'reserved' to 'committed', run_id=de93d0d5-77b..., counter_key=resume-ner:benchmarking:2643c8f2ab9ddd9013a79a2b5a...\n",
      "2026-01-12 17:30:47,956 - orchestration.jobs.tracking.index.version_counter - INFO - [Commit Version] ✓ Successfully saved committed version 1 to /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-12 17:30:47,957 - orchestration.jobs.tracking.trackers.benchmark_tracker - INFO - [Benchmark Commit] ✓ Successfully committed version 1 for benchmark run de93d0d5-77b...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run local_distilroberta_benchmark_study-25cad2a2_trial-5d87285d_bench-7d5a2dd2_1 at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/29716cbc-2f1e-485a-87be-3ef5c2f931dd/runs/de93d0d5-77bb-40b4-9f4c-26d8ed5d9f73\n",
      "🧪 View experiment at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/29716cbc-2f1e-485a-87be-3ef5c2f931dd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-12 17:30:51,665 - evaluation.benchmarking.orchestrator - INFO - Benchmark completed: /workspaces/resume-ner-azureml/outputs/benchmarking/local/distilroberta/study-25cad2a2/trial-5d87285d/bench-7d5a2dd2/benchmark.json\n",
      "2026-01-12 17:30:51,665 - evaluation.benchmarking.orchestrator - INFO - Benchmarking complete. 2/2 trials benchmarked.\n"
     ]
    }
   ],
   "source": [
    "# Run benchmarking on best trials\n",
    "# Define test_data_path directly\n",
    "test_data_path = DATASET_LOCAL_PATH / \"test.json\"  # Use test.json as test_data_path\n",
    "\n",
    "# Load benchmark config (if available)\n",
    "benchmark_config = configs.get(\"benchmark\", {})\n",
    "benchmark_settings = benchmark_config.get(\"benchmarking\", {})\n",
    "\n",
    "# Get benchmark parameters from config or use defaults\n",
    "benchmark_batch_sizes = benchmark_settings.get(\"batch_sizes\", [1, 8, 16])\n",
    "benchmark_iterations = benchmark_settings.get(\"iterations\", 100)\n",
    "benchmark_warmup = benchmark_settings.get(\"warmup_iterations\", 10)\n",
    "benchmark_max_length = benchmark_settings.get(\"max_length\", 512)\n",
    "benchmark_device = benchmark_settings.get(\"device\")\n",
    "\n",
    "# Create MLflow tracker for benchmarking\n",
    "    # Note: These imports still work via orchestration facade for backward compatibility\n",
    "from orchestration.jobs.tracking.mlflow_tracker import MLflowBenchmarkTracker\n",
    "# Use benchmark experiment name (typically same as HPO experiment with -benchmark suffix)\n",
    "benchmark_experiment_name = f\"{experiment_config.name}-benchmark\" if 'experiment_config' in locals() else \"resume_ner_baseline-benchmark\"\n",
    "benchmark_tracker = MLflowBenchmarkTracker(benchmark_experiment_name)\n",
    "\n",
    "\n",
    "if test_data_path and test_data_path.exists():\n",
    "    benchmark_results = {}\n",
    "from benchmarking import benchmark_best_trials\n",
    "from common.shared.platform_detection import detect_platform\n",
    "\n",
    "# Get environment if not already set\n",
    "if 'environment' not in locals():\n",
    "    environment = detect_platform()\n",
    "\n",
    "# Run benchmarking using orchestrator\n",
    "benchmark_results = benchmark_best_trials(\n",
    "    best_trials=best_trials,\n",
    "    test_data_path=test_data_path,\n",
    "    root_dir=ROOT_DIR,\n",
    "    environment=environment,\n",
    "    data_config=data_config,\n",
    "    hpo_config=hpo_config,\n",
    "    benchmark_config=benchmark_config,\n",
    "    benchmark_batch_sizes=benchmark_batch_sizes,\n",
    "    benchmark_iterations=benchmark_iterations,\n",
    "    benchmark_warmup=benchmark_warmup,\n",
    "    benchmark_max_length=benchmark_max_length,\n",
    "    benchmark_device=benchmark_device,\n",
    "    benchmark_tracker=benchmark_tracker,\n",
    "    backup_enabled=BACKUP_ENABLED,\n",
    "    backup_to_drive=backup_to_drive if \"backup_to_drive\" in locals() else None,\n",
    "    ensure_restored_from_drive=ensure_restored_from_drive if \"ensure_restored_from_drive\" in locals() else None,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resume-ner-training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
