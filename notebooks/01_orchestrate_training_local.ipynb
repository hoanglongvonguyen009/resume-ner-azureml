{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 1: Local Training Orchestration (Google Colab Compatible)\n",
        "\n",
        "This notebook orchestrates all training activities for **local execution** with Google Colab GPU compute support.\n",
        "\n",
        "## Overview\n",
        "\n",
        "- **Step 1**: Load Centralized Configs\n",
        "- **Step 2**: Verify Local Dataset (from data config)\n",
        "- **Step 3**: Setup Local Environment\n",
        "- **Step 4**: The Dry Run\n",
        "- **Step 5**: The Sweep (HPO) - Local with Optuna\n",
        "- **Step 6**: Best Configuration Selection (Automated)\n",
        "- **Step 7**: Final Training (Post-HPO, Single Run)\n",
        "- **Step 8**: Model Conversion & Optimization\n",
        "\n",
        "## Important\n",
        "\n",
        "- This notebook **executes training locally** (not on Azure ML)\n",
        "- All computation happens on the local machine or Google Colab GPU\n",
        "- The notebook must be **re-runnable end-to-end**\n",
        "- Uses the dataset path specified in the data config (from `config/data/*.yaml`), typically pointing to a local folder included in the repository (no external downloads needed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a local Conda environment\n",
        "\n",
        "\n",
        "### 1. Open a terminal in the project root\n",
        "\n",
        "In PowerShell:\n",
        "\n",
        "```powershell\n",
        "cd \"C:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\"\n",
        "```\n",
        "\n",
        "### 2. Create the Conda environment from the project’s `conda.yaml`\n",
        "\n",
        "```powershell\n",
        "conda env create -f config\\environment\\conda.yaml\n",
        "```\n",
        "\n",
        "- This will create an environment named `resume-ner-training` (from the `name:` field in the YAML).\n",
        "- It installs Python 3.10, PyTorch, transformers, Azure ML SDK, etc.\n",
        "\n",
        "If Conda says the env already exists, use:\n",
        "\n",
        "```powershell\n",
        "conda env update -f config\\environment\\conda.yaml\n",
        "```\n",
        "\n",
        "### 3. Activate the environment\n",
        "\n",
        "```powershell\n",
        "conda activate resume-ner-training\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step P1-3.1: Load Centralized Configs\n",
        "\n",
        "Load and validate all configuration files. Configs are immutable and will be logged with each job for reproducibility.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Install required packages for local execution\n",
        "# %pip install kagglehub optuna mlflow --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Notebook directory: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\notebooks\n",
            "Project root: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\n",
            "Source directory: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\src\n",
            "Config directory: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\config\n",
            "In Colab: False\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "IN_COLAB = \"COLAB_GPU\" in os.environ or \"COLAB_TPU\" in os.environ\n",
        "\n",
        "# Assume this notebook lives in `notebooks/` under the project root\n",
        "NOTEBOOK_DIR = Path.cwd()\n",
        "ROOT_DIR = NOTEBOOK_DIR.parent\n",
        "SRC_DIR = ROOT_DIR / \"src\"\n",
        "CONFIG_DIR = ROOT_DIR / \"config\"\n",
        "\n",
        "sys.path.append(str(ROOT_DIR))\n",
        "sys.path.append(str(SRC_DIR))\n",
        "\n",
        "print(\"Notebook directory:\", NOTEBOOK_DIR)\n",
        "print(\"Project root:\", ROOT_DIR)\n",
        "print(\"Source directory:\", SRC_DIR)\n",
        "print(\"Config directory:\", CONFIG_DIR)\n",
        "print(\"In Colab:\", IN_COLAB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded experiment: resume_ner_baseline\n",
            "Loaded config domains: ['data', 'env', 'hpo', 'model', 'train']\n",
            "Config hashes: {'data': '99115e7d01de0510', 'model': '5f90a66353401b44', 'train': 'f0c2caf728759868', 'hpo': 'b28114c649d43a67', 'env': '3e54b931c7640cf2'}\n",
            "Config metadata: {'data_config_hash': '99115e7d01de0510', 'model_config_hash': '5f90a66353401b44', 'train_config_hash': 'f0c2caf728759868', 'hpo_config_hash': 'b28114c649d43a67', 'env_config_hash': '3e54b931c7640cf2', 'data_version': 'v2.2', 'model_backbone': 'distilbert-base-uncased'}\n",
            "Dataset path (from data config): C:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\dataset_tiny\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "from typing import Any, Dict\n",
        "\n",
        "from orchestration import EXPERIMENT_NAME\n",
        "from orchestration.config_loader import (\n",
        "    ExperimentConfig,\n",
        "    compute_config_hashes,\n",
        "    create_config_metadata,\n",
        "    load_all_configs,\n",
        "    load_experiment_config,\n",
        "    snapshot_configs,\n",
        "    validate_config_immutability,\n",
        ")\n",
        "\n",
        "# P1-3.1: Load Centralized Configs (local-only)\n",
        "# Mirrors the Azure orchestration notebook, but does not create an Azure ML client.\n",
        "\n",
        "if not CONFIG_DIR.exists():\n",
        "    raise FileNotFoundError(f\"Config directory not found: {CONFIG_DIR}\")\n",
        "\n",
        "experiment_config: ExperimentConfig = load_experiment_config(CONFIG_DIR, EXPERIMENT_NAME)\n",
        "configs: Dict[str, Any] = load_all_configs(experiment_config)\n",
        "config_hashes = compute_config_hashes(configs)\n",
        "config_metadata = create_config_metadata(configs, config_hashes)\n",
        "\n",
        "# Immutable snapshots for runtime mutation checks\n",
        "original_configs = snapshot_configs(configs)\n",
        "validate_config_immutability(configs, original_configs)\n",
        "\n",
        "print(f\"Loaded experiment: {experiment_config.name}\")\n",
        "print(\"Loaded config domains:\", sorted(configs.keys()))\n",
        "print(\"Config hashes:\", config_hashes)\n",
        "print(\"Config metadata:\", config_metadata)\n",
        "\n",
        "# Get dataset path from data config (centralized configuration)\n",
        "# The local_path in the data config is relative to the config directory\n",
        "data_config = configs[\"data\"]\n",
        "local_path_str = data_config.get(\"local_path\", \"../dataset\")\n",
        "DATASET_LOCAL_PATH = (CONFIG_DIR / local_path_str).resolve()\n",
        "\n",
        "print(f\"Dataset path (from data config): {DATASET_LOCAL_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step P1-3.2: Verify Local Dataset\n",
        "\n",
        "Verify that the dataset directory (specified by `local_path` in the data config) exists and contains the required files. The dataset path is loaded from the centralized data configuration in Step P1-3.1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Dataset directory found: C:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\dataset_tiny\n",
            "  (from data config: resume-ner-data-tiny-short vv2.2)\n",
            "  ✓ train.json (13,499 bytes)\n",
            "  ✓ validation.json (2,665 bytes)\n"
          ]
        }
      ],
      "source": [
        "# P1-3.2: Verify Local Dataset\n",
        "# The dataset path comes from the data config's local_path field (loaded in Step P1-3.1).\n",
        "# This ensures the dataset location is controlled by centralized configuration.\n",
        "# Note: train.json is required, but validation.json is optional (matches training script behavior).\n",
        "\n",
        "REQUIRED_FILE = \"train.json\"\n",
        "OPTIONAL_FILE = \"validation.json\"\n",
        "\n",
        "if not DATASET_LOCAL_PATH.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Dataset directory not found: {DATASET_LOCAL_PATH}\\n\"\n",
        "        f\"This path comes from the data config's 'local_path' field.\\n\"\n",
        "        f\"If you need to create the dataset, run the notebook: notebooks/00_make_tiny_dataset.ipynb\"\n",
        "    )\n",
        "\n",
        "# Check required file\n",
        "train_file = DATASET_LOCAL_PATH / REQUIRED_FILE\n",
        "if not train_file.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Required dataset file not found: {train_file}\\n\"\n",
        "        f\"This path comes from the data config's 'local_path' field.\\n\"\n",
        "        f\"If you need to create it, run the notebook: notebooks/00_make_tiny_dataset.ipynb\"\n",
        "    )\n",
        "\n",
        "# Check optional file\n",
        "val_file = DATASET_LOCAL_PATH / OPTIONAL_FILE\n",
        "has_validation = val_file.exists()\n",
        "\n",
        "print(f\"✓ Dataset directory found: {DATASET_LOCAL_PATH}\")\n",
        "print(f\"  (from data config: {data_config.get('name', 'unknown')} v{data_config.get('version', 'unknown')})\")\n",
        "\n",
        "train_size = train_file.stat().st_size\n",
        "print(f\"  ✓ {REQUIRED_FILE} ({train_size:,} bytes)\")\n",
        "\n",
        "if has_validation:\n",
        "    val_size = val_file.stat().st_size\n",
        "    print(f\"  ✓ {OPTIONAL_FILE} ({val_size:,} bytes)\")\n",
        "else:\n",
        "    print(f\"  ⚠ {OPTIONAL_FILE} not found (optional - training will proceed without validation set)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step P1-3.3: Setup Local Environment\n",
        "\n",
        "Verify GPU availability, set up MLflow tracking (local file store), and check that key dependencies are installed. This step ensures the local environment is ready for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import torch\n",
        "\n",
        "DEFAULT_DEVICE = \"cuda\"\n",
        "\n",
        "env_config = configs[\"env\"]\n",
        "device_type = env_config.get(\"compute\", {}).get(\"device\", DEFAULT_DEVICE)\n",
        "\n",
        "if device_type == \"cuda\" and not torch.cuda.is_available():\n",
        "    raise RuntimeError(\n",
        "        \"CUDA device requested but not available. \"\n",
        "        \"Set device to 'cpu' in env config or ensure CUDA is properly installed.\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mlflow\n",
        "\n",
        "MLFLOW_DIR = \"mlruns\"\n",
        "mlflow_tracking_path = ROOT_DIR / MLFLOW_DIR\n",
        "mlflow_tracking_path.mkdir(exist_ok=True)\n",
        "\n",
        "# Convert Windows path to file:// URI format for MLflow\n",
        "mlflow_tracking_uri = mlflow_tracking_path.as_uri()\n",
        "mlflow.set_tracking_uri(mlflow_tracking_uri)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import transformers\n",
        "    import optuna\n",
        "except ImportError as e:\n",
        "    raise ImportError(f\"Required package not installed: {e}\")\n",
        "\n",
        "REQUIRED_PACKAGES = {\n",
        "    \"torch\": torch,\n",
        "    \"transformers\": transformers,\n",
        "    \"mlflow\": mlflow,\n",
        "    \"optuna\": optuna,\n",
        "}\n",
        "\n",
        "for name, module in REQUIRED_PACKAGES.items():\n",
        "    if not hasattr(module, \"__version__\"):\n",
        "        raise ImportError(f\"Required package '{name}' is not properly installed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step P1-3.4: The Dry Run\n",
        "\n",
        "Run a minimal HPO sweep to validate the training pipeline works correctly before launching the full HPO sweep. Uses the smoke HPO configuration with reduced trials.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from orchestration import STAGE_SMOKE\n",
        "from orchestration.jobs.local_sweeps import run_local_hpo_sweep\n",
        "\n",
        "TRAINING_SCRIPT_PATH = SRC_DIR / \"train.py\"\n",
        "DRY_RUN_OUTPUT_DIR = ROOT_DIR / \"outputs\" / \"dry_run\"\n",
        "\n",
        "if not TRAINING_SCRIPT_PATH.exists():\n",
        "    raise FileNotFoundError(f\"Training script not found: {TRAINING_SCRIPT_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-12-19 00:15:21,033] A new study created in memory with name: hpo_deberta\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]c:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\mlflow\\tracking\\_tracking_service\\utils.py:177: FutureWarning: The filesystem tracking backend (e.g., './mlruns') will be deprecated in February 2026. Consider transitioning to a database backend (e.g., 'sqlite:///mlflow.db') to take advantage of the latest MLflow features. See https://github.com/mlflow/mlflow/issues/18534 for more details and migration guidance.\n",
            "  return FileStore(store_uri, store_uri)\n",
            "2025/12/19 00:15:21 INFO mlflow.tracking.fluent: Experiment with name 'resume_ner_baseline-smoke-deberta' does not exist. Creating a new experiment.\n",
            "Best trial: 0. Best value: 0.068687:  50%|█████     | 1/2 [03:23<03:23, 203.35s/it, 203.35/1200 seconds]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[I 2025-12-19 00:18:44,387] Trial 0 finished with value: 0.06868697262811278 and parameters: {'backbone': 'deberta', 'learning_rate': 1.2780420377992051e-05, 'batch_size': 4, 'dropout': 0.18252475295510473, 'weight_decay': 0.051208753785545}. Best is trial 0 with value: 0.06868697262811278.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Best trial: 1. Best value: 0.153773: 100%|██████████| 2/2 [03:43<00:00, 111.83s/it, 223.67/1200 seconds]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[I 2025-12-19 00:19:04,702] Trial 1 finished with value: 0.15377348779855068 and parameters: {'backbone': 'deberta', 'learning_rate': 1.9319872207119493e-05, 'batch_size': 4, 'dropout': 0.10385127864214172, 'weight_decay': 0.001801170452800572}. Best is trial 1 with value: 0.15377348779855068.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "hpo_config = configs[\"hpo\"]\n",
        "train_config = configs[\"train\"]\n",
        "backbone_values = hpo_config[\"search_space\"][\"backbone\"][\"values\"]\n",
        "\n",
        "dry_run_studies = {}\n",
        "\n",
        "for backbone in backbone_values:\n",
        "    mlflow_experiment_name = f\"{experiment_config.name}-{STAGE_SMOKE}-{backbone}\"\n",
        "    backbone_output_dir = DRY_RUN_OUTPUT_DIR / backbone\n",
        "    \n",
        "    study = run_local_hpo_sweep(\n",
        "        dataset_path=str(DATASET_LOCAL_PATH),\n",
        "        config_dir=CONFIG_DIR,\n",
        "        backbone=backbone,\n",
        "        hpo_config=hpo_config,\n",
        "        train_config=train_config,\n",
        "        output_dir=backbone_output_dir,\n",
        "        mlflow_experiment_name=mlflow_experiment_name,\n",
        "    )\n",
        "    \n",
        "    dry_run_studies[backbone] = study\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for backbone, study in dry_run_studies.items():\n",
        "    if study.trials:\n",
        "        best_trial = study.best_trial\n",
        "        print(f\"{backbone}: {len(study.trials)} trials completed\")\n",
        "        print(\n",
        "            f\"  Best {hpo_config['objective']['metric']}: {best_trial.value:.4f}\")\n",
        "        print(f\"  Best params: {best_trial.params}\")\n",
        "    else:\n",
        "        print(f\"{backbone}: No trials completed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step P1-3.5: The Sweep (HPO) - Local with Optuna\n",
        "\n",
        "Run the full hyperparameter optimization sweep using Optuna to systematically search for the best model configuration. Uses the production HPO configuration with more trials than the dry run.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from orchestration import STAGE_HPO\n",
        "from shared.yaml_utils import load_yaml\n",
        "from orchestration.jobs.local_sweeps import run_local_hpo_sweep\n",
        "\n",
        "HPO_OUTPUT_DIR = ROOT_DIR / \"outputs\" / \"hpo\"\n",
        "\n",
        "hpo_stage_config = experiment_config.stages.get(STAGE_HPO, {})\n",
        "hpo_config_override = hpo_stage_config.get(\"hpo_config\")\n",
        "\n",
        "if hpo_config_override:\n",
        "    hpo_config_path = CONFIG_DIR / hpo_config_override\n",
        "else:\n",
        "    hpo_config_path = experiment_config.hpo_config\n",
        "\n",
        "if not hpo_config_path.exists():\n",
        "    raise FileNotFoundError(f\"HPO config not found: {hpo_config_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "hpo_config = load_yaml(hpo_config_path)\n",
        "train_config = configs[\"train\"]\n",
        "backbone_values = hpo_config[\"search_space\"][\"backbone\"][\"values\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'search_space': {'backbone': {'type': 'choice', 'values': ['deberta']},\n",
              "  'learning_rate': {'type': 'loguniform', 'min': '1e-5', 'max': '5e-5'},\n",
              "  'batch_size': {'type': 'choice', 'values': [4]},\n",
              "  'dropout': {'type': 'uniform', 'min': 0.1, 'max': 0.3},\n",
              "  'weight_decay': {'type': 'loguniform', 'min': 0.001, 'max': 0.1}},\n",
              " 'sampling': {'algorithm': 'random', 'max_trials': 2, 'timeout_minutes': 20},\n",
              " 'early_termination': {'policy': 'bandit',\n",
              "  'evaluation_interval': 1,\n",
              "  'slack_factor': 0.2,\n",
              "  'delay_evaluation': 2},\n",
              " 'objective': {'metric': 'macro-f1', 'goal': 'maximize'}}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hpo_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-12-19 00:42:54,169] A new study created in memory with name: hpo_deberta\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]c:\\Users\\HOANG PHI LONG DANG\\Miniconda3\\envs\\resume-ner-training\\lib\\site-packages\\mlflow\\tracking\\_tracking_service\\utils.py:177: FutureWarning: The filesystem tracking backend (e.g., './mlruns') will be deprecated in February 2026. Consider transitioning to a database backend (e.g., 'sqlite:///mlflow.db') to take advantage of the latest MLflow features. See https://github.com/mlflow/mlflow/issues/18534 for more details and migration guidance.\n",
            "  return FileStore(store_uri, store_uri)\n",
            "2025/12/19 00:42:54 INFO mlflow.tracking.fluent: Experiment with name 'resume_ner_baseline-hpo-deberta' does not exist. Creating a new experiment.\n",
            "Best trial: 0. Best value: 0.0804882:  50%|█████     | 1/2 [00:22<00:22, 22.21s/it, 22.21/1200 seconds]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[I 2025-12-19 00:43:16,376] Trial 0 finished with value: 0.0804881924580967 and parameters: {'backbone': 'deberta', 'learning_rate': 1.2288688273017396e-05, 'batch_size': 4, 'dropout': 0.2838947702094308, 'weight_decay': 0.020483659291694205}. Best is trial 0 with value: 0.0804881924580967.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Best trial: 1. Best value: 0.216243: 100%|██████████| 2/2 [00:43<00:00, 21.70s/it, 43.40/1200 seconds] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[I 2025-12-19 00:43:37,574] Trial 1 finished with value: 0.21624301624301623 and parameters: {'backbone': 'deberta', 'learning_rate': 2.1861220321511983e-05, 'batch_size': 4, 'dropout': 0.2770894954670945, 'weight_decay': 0.0327476003023506}. Best is trial 1 with value: 0.21624301624301623.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "hpo_studies = {}\n",
        "\n",
        "for backbone in backbone_values:\n",
        "    mlflow_experiment_name = f\"{experiment_config.name}-{STAGE_HPO}-{backbone}\"\n",
        "    backbone_output_dir = HPO_OUTPUT_DIR / backbone\n",
        "    \n",
        "    study = run_local_hpo_sweep(\n",
        "        dataset_path=str(DATASET_LOCAL_PATH),\n",
        "        config_dir=CONFIG_DIR,\n",
        "        backbone=backbone,\n",
        "        hpo_config=hpo_config,\n",
        "        train_config=train_config,\n",
        "        output_dir=backbone_output_dir,\n",
        "        mlflow_experiment_name=mlflow_experiment_name,\n",
        "    )\n",
        "    \n",
        "    hpo_studies[backbone] = study\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "deberta: 2 trials completed\n",
            "  Best macro-f1: 0.2162\n",
            "  Best params: {'backbone': 'deberta', 'learning_rate': 2.1861220321511983e-05, 'batch_size': 4, 'dropout': 0.2770894954670945, 'weight_decay': 0.0327476003023506}\n"
          ]
        }
      ],
      "source": [
        "for backbone, study in hpo_studies.items():\n",
        "    if study.trials:\n",
        "        best_trial = study.best_trial\n",
        "        print(f\"{backbone}: {len(study.trials)} trials completed\")\n",
        "        print(f\"  Best {hpo_config['objective']['metric']}: {best_trial.value:.4f}\")\n",
        "        print(f\"  Best params: {best_trial.params}\")\n",
        "    else:\n",
        "        print(f\"{backbone}: No trials completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step P1-3.6: Best Configuration Selection (Automated)\n",
        "\n",
        "Programmatically select the best configuration from all HPO sweep runs across all backbone models. The best configuration is determined by the objective metric specified in the HPO config.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from shared.json_cache import save_json\n",
        "from orchestration.jobs.local_selection import select_best_configuration_across_studies\n",
        "\n",
        "BEST_CONFIG_CACHE_FILE = ROOT_DIR / \"notebooks\" / \"best_configuration_cache.json\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_version = data_config.get(\"version\", \"unknown\")\n",
        "\n",
        "best_configuration = select_best_configuration_across_studies(\n",
        "    studies=hpo_studies,\n",
        "    hpo_config=hpo_config,\n",
        "    dataset_version=dataset_version,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best configuration selected:\n",
            "  Backbone: deberta\n",
            "  Trial: trial_1\n",
            "  Best macro-f1: 0.2162\n",
            "  Hyperparameters: {'learning_rate': 2.1861220321511983e-05, 'batch_size': 4, 'dropout': 0.2770894954670945, 'weight_decay': 0.0327476003023506}\n",
            "\n",
            "Saved to: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\notebooks\\best_configuration_cache.json\n"
          ]
        }
      ],
      "source": [
        "save_json(BEST_CONFIG_CACHE_FILE, best_configuration)\n",
        "\n",
        "print(f\"Best configuration selected:\")\n",
        "print(f\"  Backbone: {best_configuration.get('backbone')}\")\n",
        "print(f\"  Trial: {best_configuration.get('trial_name')}\")\n",
        "print(f\"  Best {hpo_config['objective']['metric']}: {best_configuration.get('selection_criteria', {}).get('best_value'):.4f}\")\n",
        "print(f\"  Hyperparameters: {best_configuration.get('hyperparameters')}\")\n",
        "print(f\"\\nSaved to: {BEST_CONFIG_CACHE_FILE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step P1-3.7: Final Training (Post-HPO, Single Run)\n",
        "\n",
        "Train the final production model using the best configuration from HPO with stable, controlled conditions. This uses the full training epochs (no early stopping) and the best hyperparameters found during HPO.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import mlflow\n",
        "from shared.json_cache import load_json, save_json\n",
        "from orchestration import STAGE_TRAINING\n",
        "from orchestration.jobs.training import build_final_training_config\n",
        "\n",
        "DEFAULT_RANDOM_SEED = 42\n",
        "BEST_CONFIG_CACHE_FILE = ROOT_DIR / \"notebooks\" / \"best_configuration_cache.json\"\n",
        "FINAL_TRAINING_OUTPUT_DIR = ROOT_DIR / \"outputs\" / \"final_training\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_configuration = load_json(BEST_CONFIG_CACHE_FILE, default=None)\n",
        "\n",
        "if best_configuration is None:\n",
        "    raise FileNotFoundError(\n",
        "        f\"Best configuration cache not found: {BEST_CONFIG_CACHE_FILE}\\n\"\n",
        "        f\"Please run Step P1-3.6: Best Configuration Selection first.\"\n",
        "    )\n",
        "\n",
        "final_training_config = build_final_training_config(\n",
        "    best_config=best_configuration,\n",
        "    train_config=configs[\"train\"],\n",
        "    random_seed=DEFAULT_RANDOM_SEED,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/12/19 01:09:01 INFO mlflow.tracking.fluent: Experiment with name 'resume_ner_baseline-training-deberta' does not exist. Creating a new experiment.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Experiment: artifact_location='file:///c:/Users/HOANG%20PHI%20LONG%20DANG/repos/resume-ner-azureml/mlruns/248982361299698356', creation_time=1766102941733, experiment_id='248982361299698356', last_update_time=1766102941733, lifecycle_stage='active', name='resume_ner_baseline-training-deberta', tags={}>"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mlflow_experiment_name = f\"{experiment_config.name}-{STAGE_TRAINING}-{final_training_config['backbone']}\"\n",
        "final_output_dir = FINAL_TRAINING_OUTPUT_DIR / final_training_config['backbone']\n",
        "final_output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "mlflow.set_experiment(mlflow_experiment_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_script_path = SRC_DIR / \"train.py\"\n",
        "training_args = [\n",
        "    sys.executable,\n",
        "    str(training_script_path),\n",
        "    \"--data-asset\",\n",
        "    str(DATASET_LOCAL_PATH),\n",
        "    \"--config-dir\",\n",
        "    str(CONFIG_DIR),\n",
        "    \"--backbone\",\n",
        "    final_training_config[\"backbone\"],\n",
        "    \"--learning-rate\",\n",
        "    str(final_training_config[\"learning_rate\"]),\n",
        "    \"--batch-size\",\n",
        "    str(final_training_config[\"batch_size\"]),\n",
        "    \"--dropout\",\n",
        "    str(final_training_config[\"dropout\"]),\n",
        "    \"--weight-decay\",\n",
        "    str(final_training_config[\"weight_decay\"]),\n",
        "    \"--epochs\",\n",
        "    str(final_training_config[\"epochs\"]),\n",
        "    \"--random-seed\",\n",
        "    str(final_training_config[\"random_seed\"]),\n",
        "    \"--early-stopping-enabled\",\n",
        "    str(final_training_config[\"early_stopping_enabled\"]).lower(),\n",
        "    \"--use-combined-data\",\n",
        "    str(final_training_config[\"use_combined_data\"]).lower(),\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_env = os.environ.copy()\n",
        "training_env[\"AZURE_ML_OUTPUT_checkpoint\"] = str(final_output_dir)\n",
        "\n",
        "mlflow_tracking_uri = mlflow.get_tracking_uri()\n",
        "if mlflow_tracking_uri:\n",
        "    training_env[\"MLFLOW_TRACKING_URI\"] = mlflow_tracking_uri\n",
        "training_env[\"MLFLOW_EXPERIMENT_NAME\"] = mlflow_experiment_name\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = subprocess.run(\n",
        "    training_args,\n",
        "    cwd=ROOT_DIR,\n",
        "    env=training_env,\n",
        "    capture_output=False,\n",
        "    text=True,\n",
        ")\n",
        "\n",
        "if result.returncode != 0:\n",
        "    raise RuntimeError(f\"Final training failed with return code {result.returncode}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Training completed. Checkpoint: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\final_training\\deberta\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "METRICS_FILENAME = \"metrics.json\"\n",
        "FINAL_TRAINING_CACHE_FILE = ROOT_DIR / \"notebooks\" / \"final_training_cache.json\"\n",
        "\n",
        "metrics_file = final_output_dir / METRICS_FILENAME\n",
        "if metrics_file.exists():\n",
        "    with open(metrics_file, \"r\") as f:\n",
        "        metrics = json.load(f)\n",
        "    print(f\"✓ Training completed. Checkpoint: {final_output_dir}\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Training metrics not found: {metrics_file}\")\n",
        "\n",
        "save_json(FINAL_TRAINING_CACHE_FILE, {\n",
        "    \"output_dir\": str(final_output_dir),\n",
        "    \"backbone\": final_training_config[\"backbone\"],\n",
        "    \"config\": final_training_config,\n",
        "})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step P1-4: Model Conversion & Optimization\n",
        "\n",
        "Convert the final training checkpoint to an optimized ONNX model (int8 quantized) for production inference.\n",
        "\n",
        "**Platform Adapter Note**: The conversion script (`src/convert_to_onnx.py`) uses the platform adapter to automatically handle output paths and logging appropriately for local execution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import mlflow\n",
        "from shared.json_cache import load_json\n",
        "\n",
        "CONVERSION_SCRIPT_PATH = SRC_DIR / \"convert_to_onnx.py\"\n",
        "FINAL_TRAINING_CACHE_FILE = ROOT_DIR / \"notebooks\" / \"final_training_cache.json\"\n",
        "CONVERSION_OUTPUT_DIR = ROOT_DIR / \"outputs\" / \"conversion\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "training_cache = load_json(FINAL_TRAINING_CACHE_FILE, default=None)\n",
        "\n",
        "if training_cache is None:\n",
        "    raise FileNotFoundError(\n",
        "        f\"Final training cache not found: {FINAL_TRAINING_CACHE_FILE}\\n\"\n",
        "        f\"Please run Step P1-3.7: Final Training first.\"\n",
        "    )\n",
        "\n",
        "checkpoint_dir = Path(training_cache[\"output_dir\"]) / \"checkpoint\"\n",
        "if not checkpoint_dir.exists():\n",
        "    raise FileNotFoundError(f\"Checkpoint directory not found: {checkpoint_dir}\")\n",
        "\n",
        "backbone = training_cache[\"backbone\"]\n",
        "conversion_output_dir = CONVERSION_OUTPUT_DIR / backbone\n",
        "conversion_output_dir.mkdir(parents=True, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "conversion_args = [\n",
        "    sys.executable,\n",
        "    str(CONVERSION_SCRIPT_PATH),\n",
        "    \"--checkpoint-path\",\n",
        "    str(checkpoint_dir),\n",
        "    \"--config-dir\",\n",
        "    str(CONFIG_DIR),\n",
        "    \"--backbone\",\n",
        "    backbone,\n",
        "    \"--output-dir\",\n",
        "    str(conversion_output_dir),\n",
        "    \"--quantize-int8\",\n",
        "    \"--run-smoke-test\",\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "conversion_env = os.environ.copy()\n",
        "conversion_env[\"AZURE_ML_OUTPUT_onnx_model\"] = str(conversion_output_dir)\n",
        "\n",
        "mlflow_tracking_uri = mlflow.get_tracking_uri()\n",
        "if mlflow_tracking_uri:\n",
        "    conversion_env[\"MLFLOW_TRACKING_URI\"] = mlflow_tracking_uri\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "result = subprocess.run(\n",
        "    conversion_args,\n",
        "    cwd=ROOT_DIR,\n",
        "    env=conversion_env,\n",
        "    capture_output=False,\n",
        "    text=True,\n",
        ")\n",
        "\n",
        "if result.returncode != 0:\n",
        "    raise RuntimeError(f\"Model conversion failed with return code {result.returncode}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Conversion completed. ONNX model: c:\\Users\\HOANG PHI LONG DANG\\repos\\resume-ner-azureml\\outputs\\conversion\\deberta\\model_int8.onnx\n"
          ]
        }
      ],
      "source": [
        "from shared.json_cache import save_json\n",
        "\n",
        "ONNX_MODEL_FILENAME = \"model_int8.onnx\"\n",
        "FALLBACK_ONNX_MODEL_FILENAME = \"model.onnx\"\n",
        "CONVERSION_CACHE_FILE = ROOT_DIR / \"notebooks\" / \"conversion_cache.json\"\n",
        "\n",
        "onnx_model_path = conversion_output_dir / ONNX_MODEL_FILENAME\n",
        "if not onnx_model_path.exists():\n",
        "    onnx_model_path = conversion_output_dir / FALLBACK_ONNX_MODEL_FILENAME\n",
        "\n",
        "if not onnx_model_path.exists():\n",
        "    raise FileNotFoundError(f\"ONNX model not found in {conversion_output_dir}\")\n",
        "\n",
        "print(f\"✓ Conversion completed. ONNX model: {onnx_model_path}\")\n",
        "\n",
        "save_json(CONVERSION_CACHE_FILE, {\n",
        "    \"onnx_model_path\": str(onnx_model_path),\n",
        "    \"backbone\": backbone,\n",
        "    \"checkpoint_dir\": str(checkpoint_dir),\n",
        "})\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
