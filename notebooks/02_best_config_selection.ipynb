{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Best Configuration Selection (Local, Google Colab & Kaggle)\n",
    "\n",
    "This notebook automates the selection of the best model configuration from MLflow\n",
    "based on metrics and benchmarking results, then performs final training and model conversion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n",
    "**Prerequisites**: Run `01_orchestrate_training_colab.ipynb` first to:\n",
    "- Train models via HPO\n",
    "- Run benchmarking on best trials (using `evaluation.benchmarking.benchmark_best_trials`)\n",
    "\n",
    "Then this notebook:\n",
    "\n",
    "1. **Best Model Selection**: Query MLflow benchmark runs, join to training runs via grouping tags (`code.study_key_hash`, `code.trial_key_hash`), select best using normalized composite scoring\n",
    "2. **Artifact Acquisition**: Download the best model's checkpoint using fallback strategy (local disk \u2192 drive restore \u2192 MLflow download)\n",
    "3. **Final Training**: Optionally retrain with best config on full dataset (if not already final training)\n",
    "4. **Model Conversion**: Convert the final model to ONNX format using canonical path structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important\n",
    "\n",
    "- This notebook **executes on Local, Google Colab, or Kaggle** (not on Azure ML compute)\n",
    "- Requires MLflow tracking to be set up (Azure ML workspace or local SQLite)\n",
    "- All computation happens on the platform's GPU (if available) or CPU\n",
    "- **Storage & Persistence**:\n",
    "  - **Local**: Outputs saved to `outputs/` directory in repository root\n",
    "  - **Google Colab**: Checkpoints are automatically saved to Google Drive for persistence across sessions\n",
    "  - **Kaggle**: Outputs in `/kaggle/working/` are automatically persisted - no manual backup needed\n",
    "- The notebook must be **re-runnable end-to-end**\n",
    "- Uses the dataset path specified in the data config (from `config/data/*.yaml`), typically pointing to a local folder included in the repository\n",
    "- **Session Management**:\n",
    "  - **Local**: No session limits, outputs persist in repository\n",
    "  - **Colab**: Sessions timeout after 12-24 hours (depending on Colab plan). Checkpoints are saved to Drive automatically.\n",
    "  - **Kaggle**: Sessions have time limits based on your plan. All outputs are automatically saved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Detection\n",
    "\n",
    "The notebook automatically detects the execution environment (local, Google Colab, or Kaggle) and adapts its behavior accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Detected environment: LOCAL\n",
      "Platform: local\n",
      "Base directory: Current working directory\n",
      "Backup enabled: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "# Detect execution environment\n",
    "IN_COLAB = \"COLAB_GPU\" in os.environ or \"COLAB_TPU\" in os.environ\n",
    "IN_KAGGLE = \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
    "IS_LOCAL = not IN_COLAB and not IN_KAGGLE\n",
    "# Set platform-specific constants\n",
    "if IN_COLAB:\n",
    "    PLATFORM = \"colab\"\n",
    "    BASE_DIR = Path(\"/content\")\n",
    "    BACKUP_ENABLED = True\n",
    "elif IN_KAGGLE:\n",
    "    PLATFORM = \"kaggle\"\n",
    "    BASE_DIR = Path(\"/kaggle/working\")\n",
    "    BACKUP_ENABLED = False\n",
    "else:\n",
    "    PLATFORM = \"local\"\n",
    "    BASE_DIR = None\n",
    "    BACKUP_ENABLED = False\n",
    "print(f\"\u2713 Detected environment: {PLATFORM.upper()}\")\n",
    "print(f\"Platform: {PLATFORM}\")\n",
    "print(\n",
    "    f\"Base directory: {BASE_DIR if BASE_DIR else 'Current working directory'}\")\n",
    "print(f\"Backup enabled: {BACKUP_ENABLED}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Required Packages\n",
    "\n",
    "Install required packages based on the execution environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For local environment, please:\n",
      "1. Create conda environment: conda env create -f config/environment/conda.yaml\n",
      "2. Activate: conda activate resume-ner-training\n",
      "3. Restart kernel after activation\n",
      "\n",
      "If you've already done this, you can continue to the next cell.\n",
      "\n",
      "Installing Azure ML SDK (required for imports)...\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "if IS_LOCAL:\n",
    "    print(\"For local environment, please:\")\n",
    "    print(\"1. Create conda environment: conda env create -f config/environment/conda.yaml\")\n",
    "    print(\"2. Activate: conda activate resume-ner-training\")\n",
    "    print(\"3. Restart kernel after activation\")\n",
    "    print(\"\\nIf you've already done this, you can continue to the next cell.\")\n",
    "    print(\"\\nInstalling Azure ML SDK (required for imports)...\")\n",
    "    # Install Azure ML packages even for local (in case conda env not activated)\n",
    "    %pip install \"azure-ai-ml>=1.0.0\" --quiet\n",
    "    %pip install \"azure-identity>=1.12.0\" --quiet\n",
    "    %pip install azureml-defaults --quiet\n",
    "    %pip install azureml-mlflow --quiet\n",
    "else:\n",
    "    # Core ML libraries\n",
    "    %pip install \"transformers>=4.35.0,<5.0.0\" --quiet\n",
    "    %pip install \"safetensors>=0.4.0\" --quiet\n",
    "    %pip install \"datasets>=2.12.0\" --quiet\n",
    "\n",
    "    # ML utilities\n",
    "    %pip install \"numpy>=1.24.0,<2.0.0\" --quiet\n",
    "    %pip install \"pandas>=2.0.0\" --quiet\n",
    "    %pip install \"scikit-learn>=1.3.0\" --quiet\n",
    "\n",
    "    # Utilities\n",
    "    %pip install \"pyyaml>=6.0\" --quiet\n",
    "    %pip install \"tqdm>=4.65.0\" --quiet\n",
    "    %pip install \"seqeval>=1.2.2\" --quiet\n",
    "    %pip install \"sentencepiece>=0.1.99\" --quiet\n",
    "\n",
    "    # Experiment tracking\n",
    "    %pip install mlflow --quiet\n",
    "    %pip install optuna --quiet\n",
    "\n",
    "    # Azure ML SDK (required for orchestration imports)\n",
    "    %pip install \"azure-ai-ml>=1.0.0\" --quiet\n",
    "    %pip install \"azure-identity>=1.12.0\" --quiet\n",
    "    %pip install azureml-defaults --quiet\n",
    "    %pip install azureml-mlflow --quiet\n",
    "\n",
    "    # ONNX support\n",
    "    %pip install onnxruntime --quiet\n",
    "    %pip install \"onnx>=1.16.0\" --quiet\n",
    "    %pip install \"onnxscript>=0.1.0\" --quiet\n",
    "\n",
    "    print(\"\u2713 All dependencies installed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Repository Setup\n",
    "\n",
    "**Note**: Repository setup is only needed for Colab/Kaggle environments. Local environments should already have the repository cloned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Local environment detected - detecting repository root...\n",
      "\u2713 Repository: /workspaces/resume-ner-azureml (config=config, src=src)\n",
      "\u2713 Repository structure verified\n"
     ]
    }
   ],
   "source": [
    "# Repository setup - only needed for Colab/Kaggle\n",
    "if not IS_LOCAL:\n",
    "    if IN_KAGGLE:\n",
    "        !git clone -b gg_final_training_2 https://github.com/longdang193/resume-ner-azureml.git /kaggle/working/resume-ner-azureml\n",
    "    elif IN_COLAB:\n",
    "        !git clone -b gg_final_training_2 https://github.com/longdang193/resume-ner-azureml.git /content/resume-ner-azureml\n",
    "else:\n",
    "    print(\"\u2713 Local environment detected - detecting repository root...\")\n",
    "\n",
    "# Set up paths\n",
    "if not IS_LOCAL:\n",
    "    ROOT_DIR = BASE_DIR / \"resume-ner-azureml\"\n",
    "else:\n",
    "    # For local, detect repo root by searching for config/ and src/ directories\n",
    "    # Start from current working directory and search up\n",
    "    current_dir = Path.cwd()\n",
    "    ROOT_DIR = None\n",
    "    \n",
    "    # Check current directory first\n",
    "    if (current_dir / \"config\").exists() and (current_dir / \"src\").exists():\n",
    "        ROOT_DIR = current_dir\n",
    "    else:\n",
    "        # Search up the directory tree\n",
    "        for parent in current_dir.parents:\n",
    "            if (parent / \"config\").exists() and (parent / \"src\").exists():\n",
    "                ROOT_DIR = parent\n",
    "                break\n",
    "    \n",
    "    if ROOT_DIR is None:\n",
    "        raise ValueError(\n",
    "            f\"Could not find repository root. Searched from: {current_dir}\\n\"\n",
    "            \"Please ensure you're running from within the repository or a subdirectory.\"\n",
    "        )\n",
    "\n",
    "CONFIG_DIR = ROOT_DIR / \"config\"\n",
    "SRC_DIR = ROOT_DIR / \"src\"\n",
    "\n",
    "# Add src to path\n",
    "import sys\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "print(f\"\u2713 Repository: {ROOT_DIR} (config={CONFIG_DIR.name}, src={SRC_DIR.name})\")\n",
    "\n",
    "# Verify repository structure\n",
    "required_dirs = [CONFIG_DIR, SRC_DIR]\n",
    "for dir_path in required_dirs:\n",
    "    if not dir_path.exists():\n",
    "        raise ValueError(f\"Required directory not found: {dir_path}\")\n",
    "print(\"\u2713 Repository structure verified\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Configuration\n",
    "\n",
    "Load experiment configuration and define experiment naming convention.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_86299/2783883798.py:5: DeprecationWarning: orchestration module is deprecated. Use 'infrastructure', 'common', or 'data' modules instead. This will be removed in 2 releases.\n",
      "  from orchestration.jobs.tracking.naming.tags_registry import load_tags_registry\n",
      "2026-01-13 00:12:27,854 - infrastructure.tracking.mlflow.compatibility - INFO - Applied Azure ML artifact compatibility patch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Loaded configs: experiment=resume_ner_baseline, tags, selection, conversion, acquisition\n",
      "\u2713 Experiment names: benchmark=resume_ner_baseline-benchmark, training=resume_ner_baseline-training, conversion=resume_ner_baseline-conversion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_86299/2783883798.py:5: DeprecationWarning: Importing 'constants' from 'orchestration' is deprecated. Please import from 'constants' instead.\n",
      "  from orchestration.jobs.tracking.naming.tags_registry import load_tags_registry\n",
      "/tmp/ipykernel_86299/2783883798.py:5: DeprecationWarning: Importing 'fingerprints' from 'orchestration' is deprecated. Please import from 'fingerprints' instead.\n",
      "  from orchestration.jobs.tracking.naming.tags_registry import load_tags_registry\n",
      "/tmp/ipykernel_86299/2783883798.py:5: DeprecationWarning: Importing 'metadata/index_manager' from 'orchestration' is deprecated. Please import from 'metadata' instead.\n",
      "  from orchestration.jobs.tracking.naming.tags_registry import load_tags_registry\n",
      "/tmp/ipykernel_86299/2783883798.py:5: DeprecationWarning: Importing 'metadata/metadata_manager' from 'orchestration' is deprecated. Please import from 'metadata' instead.\n",
      "  from orchestration.jobs.tracking.naming.tags_registry import load_tags_registry\n",
      "/tmp/ipykernel_86299/2783883798.py:5: DeprecationWarning: Importing 'drive_backup' from 'orchestration' is deprecated. Please import from 'storage' instead.\n",
      "  from orchestration.jobs.tracking.naming.tags_registry import load_tags_registry\n",
      "/tmp/ipykernel_86299/2783883798.py:5: DeprecationWarning: Importing 'benchmark_utils' from 'orchestration' is deprecated. Please import from 'benchmarking.utils' instead.\n",
      "  from orchestration.jobs.tracking.naming.tags_registry import load_tags_registry\n",
      "/tmp/ipykernel_86299/2783883798.py:5: DeprecationWarning: Importing 'config_loader' from 'orchestration' is deprecated. Please import from 'config.loader' instead.\n",
      "  from orchestration.jobs.tracking.naming.tags_registry import load_tags_registry\n",
      "/tmp/ipykernel_86299/2783883798.py:5: DeprecationWarning: Importing 'conversion_config' from 'orchestration' is deprecated. Please import from 'config.conversion' instead.\n",
      "  from orchestration.jobs.tracking.naming.tags_registry import load_tags_registry\n",
      "/tmp/ipykernel_86299/2783883798.py:5: DeprecationWarning: Importing 'final_training_config' from 'orchestration' is deprecated. Please import from 'config.training' instead.\n",
      "  from orchestration.jobs.tracking.naming.tags_registry import load_tags_registry\n",
      "/tmp/ipykernel_86299/2783883798.py:5: DeprecationWarning: Importing 'environment' from 'orchestration' is deprecated. Please import from 'config.environment' instead.\n",
      "  from orchestration.jobs.tracking.naming.tags_registry import load_tags_registry\n",
      "/tmp/ipykernel_86299/2783883798.py:5: DeprecationWarning: Importing 'config_compat' from 'orchestration' is deprecated. Please import from 'config.validation' instead.\n",
      "  from orchestration.jobs.tracking.naming.tags_registry import load_tags_registry\n",
      "/tmp/ipykernel_86299/2783883798.py:5: DeprecationWarning: Importing 'data_assets' from 'orchestration' is deprecated. Please import from 'azureml.data_assets' instead.\n",
      "  from orchestration.jobs.tracking.naming.tags_registry import load_tags_registry\n",
      "/tmp/ipykernel_86299/2783883798.py:5: DeprecationWarning: Importing 'normalize' from 'orchestration' is deprecated. Please import from 'core.normalize' instead.\n",
      "  from orchestration.jobs.tracking.naming.tags_registry import load_tags_registry\n",
      "/tmp/ipykernel_86299/2783883798.py:5: DeprecationWarning: Importing 'tokens' from 'orchestration' is deprecated. Please import from 'core.tokens' instead.\n",
      "  from orchestration.jobs.tracking.naming.tags_registry import load_tags_registry\n"
     ]
    }
   ],
   "source": [
    "from infrastructure.config.loader import load_experiment_config\n",
    "from common.constants import EXPERIMENT_NAME\n",
    "from common.shared.yaml_utils import load_yaml\n",
    "    # Note: Still in orchestration.jobs.tracking for now\n",
    "from orchestration.jobs.tracking.naming.tags_registry import load_tags_registry\n",
    "\n",
    "# Load experiment config\n",
    "experiment_config = load_experiment_config(CONFIG_DIR, EXPERIMENT_NAME)\n",
    "\n",
    "# Load best model selection configs\n",
    "tags_config = load_tags_registry(CONFIG_DIR)\n",
    "selection_config = load_yaml(CONFIG_DIR / \"best_model_selection.yaml\")\n",
    "conversion_config = load_yaml(CONFIG_DIR / \"conversion.yaml\")\n",
    "acquisition_config = load_yaml(CONFIG_DIR / \"artifact_acquisition.yaml\")\n",
    "\n",
    "print(f\"\u2713 Loaded configs: experiment={experiment_config.name}, tags, selection, conversion, acquisition\")\n",
    "\n",
    "# Define experiment names (discovery happens after MLflow setup in Cell 4)\n",
    "experiment_name = experiment_config.name\n",
    "benchmark_experiment_name = f\"{experiment_name}-benchmark\"\n",
    "training_experiment_name = f\"{experiment_name}-training\"  # For final training runs\n",
    "conversion_experiment_name = f\"{experiment_name}-conversion\"\n",
    "\n",
    "print(f\"\u2713 Experiment names: benchmark={benchmark_experiment_name}, training={training_experiment_name}, conversion={conversion_experiment_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Setup MLflow\n",
    "\n",
    "Setup MLflow tracking with fallback to local if Azure ML is unavailable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 00:12:29,887 - common.shared.mlflow_setup - INFO - Azure ML enabled in config, attempting to connect...\n",
      "2026-01-13 00:12:29,889 - common.shared.mlflow_setup - WARNING - [DEBUG] Initial env check - subscription_id: False, resource_group: False, client_id: False, client_secret: False, tenant_id: False\n",
      "2026-01-13 00:12:29,890 - common.shared.mlflow_setup - INFO - Attempting to load credentials from config.env at: /workspaces/resume-ner-azureml/config.env\n",
      "2026-01-13 00:12:29,891 - common.shared.mlflow_setup - INFO - Loading credentials from /workspaces/resume-ner-azureml/config.env\n",
      "2026-01-13 00:12:29,891 - common.shared.mlflow_setup - INFO - Loaded subscription/resource group from config.env\n",
      "2026-01-13 00:12:29,892 - common.shared.mlflow_setup - INFO - Loaded service principal credentials from config.env\n",
      "2026-01-13 00:12:29,892 - common.shared.mlflow_setup - WARNING - [DEBUG] Platform detected: local\n",
      "2026-01-13 00:12:29,893 - common.shared.mlflow_setup - WARNING - [DEBUG] Service Principal check - client_id present: True, client_secret present: True, tenant_id present: True, has_service_principal: True\n",
      "2026-01-13 00:12:29,894 - common.shared.mlflow_setup - INFO - Using Service Principal authentication (from config.env)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 azureml.mlflow is available - Azure ML tracking will be used if configured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class DeploymentTemplateOperations: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "2026-01-13 00:12:31,096 - common.shared.mlflow_setup - INFO - Successfully connected to Azure ML workspace: resume-ner-ws\n",
      "2026-01-13 00:12:34,349 - common.shared.mlflow_setup - INFO - Using Azure ML workspace tracking\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 MLflow tracking URI: azureml://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws\n",
      "\u2713 MLflow experiment: resume_ner_baseline-training\n",
      "\u2713 Experiments: 2 HPO (distilbert, distilroberta), benchmark=found, training=resume_ner_baseline-training, conversion=resume_ner_baseline-conversion\n"
     ]
    }
   ],
   "source": [
    "# Check if azureml.mlflow is available\n",
    "try:\n",
    "    import azureml.mlflow  # noqa: F401\n",
    "    print(\"\u2713 azureml.mlflow is available - Azure ML tracking will be used if configured\")\n",
    "except ImportError:\n",
    "    print(\"\u26a0 azureml.mlflow is not available - will fallback to local SQLite tracking\")\n",
    "    print(\"  To use Azure ML tracking, install: pip install azureml-mlflow\")\n",
    "    print(\"  Then restart the kernel and re-run this cell\")\n",
    "\n",
    "from common.shared.mlflow_setup import setup_mlflow_from_config\n",
    "import mlflow\n",
    "\n",
    "# Setup MLflow tracking (use training experiment for setup - actual queries use discovered experiments)\n",
    "tracking_uri = setup_mlflow_from_config(\n",
    "    experiment_name=training_experiment_name,\n",
    "    config_dir=CONFIG_DIR,\n",
    "    fallback_to_local=True,\n",
    ")\n",
    "\n",
    "print(f\"\u2713 MLflow tracking URI: {tracking_uri}\")\n",
    "print(f\"\u2713 MLflow experiment: {training_experiment_name}\")\n",
    "\n",
    "# Discover HPO and benchmark experiments from MLflow (after setup)\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "all_experiments = client.search_experiments()\n",
    "\n",
    "# Find HPO experiments (format: {experiment_name}-hpo-{backbone})\n",
    "hpo_experiments = {}\n",
    "for exp in all_experiments:\n",
    "    if exp.name.startswith(f\"{experiment_name}-hpo-\"):\n",
    "        backbone = exp.name.replace(f\"{experiment_name}-hpo-\", \"\")\n",
    "        hpo_experiments[backbone] = {\n",
    "            \"name\": exp.name,\n",
    "            \"id\": exp.experiment_id\n",
    "        }\n",
    "\n",
    "# Find benchmark experiment\n",
    "benchmark_experiment = None\n",
    "for exp in all_experiments:\n",
    "    if exp.name == benchmark_experiment_name:\n",
    "        benchmark_experiment = {\n",
    "            \"name\": exp.name,\n",
    "            \"id\": exp.experiment_id\n",
    "        }\n",
    "        break\n",
    "\n",
    "hpo_backbones = \", \".join(hpo_experiments.keys())\n",
    "print(f\"\u2713 Experiments: {len(hpo_experiments)} HPO ({hpo_backbones}), benchmark={'found' if benchmark_experiment else 'not found'}, training={training_experiment_name}, conversion={conversion_experiment_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Drive Backup Setup (Colab Only)\n",
    "\n",
    "Setup Google Drive backup/restore for Colab environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Local environment detected - outputs will be saved to repository (no Drive backup needed)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Fix numpy/pandas compatibility before importing orchestration modules\n",
    "try:\n",
    "    from infrastructure.storage.drive import create_colab_store\n",
    "except (ValueError, ImportError) as e:\n",
    "    if \"numpy.dtype size changed\" in str(e) or \"numpy\" in str(e).lower():\n",
    "        print(\"\u26a0 Numpy/pandas compatibility issue detected. Fixing...\")\n",
    "        import subprocess\n",
    "        import sys\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"--force-reinstall\", \"--no-cache-dir\", \"numpy>=1.24.0,<2.0.0\", \"pandas>=2.0.0\", \"--quiet\"])\n",
    "        print(\"\u2713 Numpy/pandas reinstalled. Please restart the kernel and re-run this cell.\")\n",
    "        raise RuntimeError(\"Please restart kernel after numpy/pandas fix\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# Mount Google Drive and create backup store (Colab only - Kaggle doesn't need this)\n",
    "DRIVE_BACKUP_DIR = None\n",
    "drive_store = None\n",
    "restore_from_drive = None\n",
    "\n",
    "if IN_COLAB:\n",
    "    drive_store = create_colab_store(ROOT_DIR, CONFIG_DIR)\n",
    "    if drive_store:\n",
    "        BACKUP_ENABLED = True\n",
    "        DRIVE_BACKUP_DIR = drive_store.backup_root\n",
    "        # Create restore function wrapper\n",
    "        def restore_from_drive(local_path: Path, is_directory: bool = False) -> bool:\n",
    "            \"\"\"Restore file/directory from Drive backup.\"\"\"\n",
    "            try:\n",
    "                expect = \"dir\" if is_directory else \"file\"\n",
    "                result = drive_store.restore(local_path, expect=expect)\n",
    "                return result.ok\n",
    "            except Exception as e:\n",
    "                print(f\"\u26a0 Drive restore failed: {e}\")\n",
    "                return False\n",
    "        print(f\"\u2713 Google Drive mounted\")\n",
    "        print(f\"\u2713 Backup base directory: {DRIVE_BACKUP_DIR}\")\n",
    "        print(f\"\\nNote: All outputs/ will be mirrored to: {DRIVE_BACKUP_DIR / 'outputs'}\")\n",
    "    else:\n",
    "        BACKUP_ENABLED = False\n",
    "        print(\"\u26a0 Warning: Could not mount Google Drive. Backup to Google Drive will be disabled.\")\n",
    "elif IN_KAGGLE:\n",
    "    print(\"\u2713 Kaggle environment detected - outputs are automatically persisted (no Drive mount needed)\")\n",
    "    BACKUP_ENABLED = False\n",
    "else:\n",
    "    # Local environment\n",
    "    print(\"\u2713 Local environment detected - outputs will be saved to repository (no Drive backup needed)\")\n",
    "    BACKUP_ENABLED = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Optional - Run Benchmarking on Champions\n",
    "\n",
    "**Optional Step**: If you haven't run benchmarking in `01_orchestrate_training_colab.ipynb`, you can run it here before selecting the best model. This step will:\n",
    "1. Select champions (best trials) from HPO runs using Phase 2 selection logic\n",
    "2. Run benchmarking on each champion to measure inference performance\n",
    "3. Save benchmark results to MLflow for use in Step 7\n",
    "\n",
    "**Note**: If benchmark runs already exist in MLflow, you can skip this step and proceed directly to Step 7.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udd04 Running benchmarking on champions...\n",
      "\u2713 Found 2 HPO experiment(s)\n",
      "\n",
      "\ud83c\udfc6 Selecting champions per backbone...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 00:12:34,719 - evaluation.selection.trial_finder - INFO - No runs found with stage='hpo_trial' for distilbert, trying legacy stage='hpo'\n",
      "2026-01-13 00:12:34,922 - evaluation.selection.trial_finder - INFO - Found 2 runs with stage tag for distilbert (backbone=distilbert)\n",
      "2026-01-13 00:12:34,923 - evaluation.selection.trial_finder - INFO - Filtered out 1 parent run(s) (only child/trial runs have metrics). 1 child runs remaining.\n",
      "2026-01-13 00:12:35,148 - evaluation.selection.trial_finder - INFO - Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)\n",
      "2026-01-13 00:12:35,149 - evaluation.selection.trial_finder - INFO - Found 1 eligible group(s) for distilbert (0 skipped due to min_trials requirement)\n",
      "2026-01-13 00:12:35,487 - evaluation.selection.trial_finder - INFO - No runs found with stage='hpo_trial' for distilroberta, trying legacy stage='hpo'\n",
      "2026-01-13 00:12:35,777 - evaluation.selection.trial_finder - INFO - Found 2 runs with stage tag for distilroberta (backbone=distilroberta)\n",
      "2026-01-13 00:12:35,778 - evaluation.selection.trial_finder - INFO - Filtered out 1 parent run(s) (only child/trial runs have metrics). 1 child runs remaining.\n",
      "2026-01-13 00:12:35,977 - evaluation.selection.trial_finder - INFO - Grouped runs for distilroberta: 1 v1 group(s), 0 v2 group(s)\n",
      "2026-01-13 00:12:35,978 - evaluation.selection.trial_finder - INFO - Found 1 eligible group(s) for distilroberta (0 skipped due to min_trials requirement)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Acquiring checkpoint for distilbert from MLflow (run 1227a495-7bf...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 00:12:36,398 - evaluation.selection.artifact_acquisition - WARNING - [ACQUISITION] Found 1 refit run(s) but none match exact hashes. Trying all refit runs as fallback...\n",
      "Downloading artifacts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:05<00:00,  5.45s/it]\n",
      "2026-01-13 00:12:42,415 - evaluation.selection.artifact_acquisition - INFO - [ACQUISITION] Successfully downloaded checkpoint from run d896bab0-d2b...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    \u2713 Acquired checkpoint from MLflow\n",
      "  Acquiring checkpoint for distilroberta from MLflow (run c3014f98-39c...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 00:12:46,450 - evaluation.selection.artifact_acquisition - WARNING - [ACQUISITION] Found 1 refit run(s) but none match exact hashes. Trying all refit runs as fallback...\n",
      "Downloading artifacts: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:07<00:00,  7.05s/it]\n",
      "2026-01-13 00:12:54,047 - evaluation.selection.artifact_acquisition - INFO - [ACQUISITION] Successfully downloaded checkpoint from run 1e77f7ef-b39...\n",
      "2026-01-13 00:12:58,744 - evaluation.benchmarking.orchestrator - INFO - Benchmarking distilbert (ba1768feed3e6d36c556b1c6ccdb69dbb3a37e447c4fb7a62559055534df3fc7)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    \u2713 Acquired checkpoint from MLflow\n",
      "\n",
      "\u2713 Found 2 champion(s) to benchmark\n",
      "\n",
      "\ud83d\udcca Running benchmarking on 2 champion(s)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 00:12:58,882 - evaluation.benchmarking.orchestrator - INFO - [BENCHMARK] Found trial run ID from MLflow: 1227a495-7bf... (via trial_key_hash=ba1768feed3e6d36...)\n",
      "2026-01-13 00:12:58,883 - evaluation.benchmarking.orchestrator - INFO - [BENCHMARK] Final run IDs: trial=1227a495-7bf..., refit=1227a495-7bf..., sweep=None...\n",
      "2026-01-13 00:12:58,884 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /workspaces/resume-ner-azureml/src/evaluation/benchmarking/cli.py --checkpoint /workspaces/resume-ner-azureml/outputs/best_model_selection/local/distilbert/sel_712fbee8_ba1768fe/best_trial_checkpoint.tar.gz/best_trial_checkpoint --test-data /workspaces/resume-ner-azureml/dataset_tiny/seed0/test.json --batch-sizes 1 --iterations 10 --warmup 10 --max-length 512 --output /workspaces/resume-ner-azureml/outputs/benchmarking/local/distilbert/study-712fbee8/trial-ba1768fe/bench-7d5a2dd2/benchmark.json\n",
      "2026-01-13 00:13:02,301 - infrastructure.tracking.mlflow.compatibility - INFO - Applied Azure ML artifact compatibility patch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 test texts\n",
      "Starting benchmark for checkpoint: /workspaces/resume-ner-azureml/outputs/best_model_selection/local/distilbert/sel_712fbee8_ba1768fe/best_trial_checkpoint.tar.gz/best_trial_checkpoint\n",
      "Loading tokenizer from /workspaces/resume-ner-azureml/outputs/best_model_selection/local/distilbert/sel_712fbee8_ba1768fe/best_trial_checkpoint.tar.gz/best_trial_checkpoint...\n",
      "Tokenizer loaded.\n",
      "Loading model from /workspaces/resume-ner-azureml/outputs/best_model_selection/local/distilbert/sel_712fbee8_ba1768fe/best_trial_checkpoint.tar.gz/best_trial_checkpoint...\n",
      "Moving model to cpu...\n",
      "Model loaded and set to eval mode.\n",
      "Model ready on device: cpu\n",
      "\n",
      "Benchmarking batch size 1...\n",
      "  Running 10 warmup iterations, then 10 measurement iterations...\n",
      "    Warmup: 10 iterations... 10/10 done.\n",
      "    Measurement: 10 iterations... 10/10 done.\n",
      "  Mean latency: 161.16 ms\n",
      "  P95 latency: 165.18 ms\n",
      "  Throughput: 6.21 docs/sec\n",
      "\n",
      "Saving results to /workspaces/resume-ner-azureml/outputs/benchmarking/local/distilbert/study-712fbee8/trial-ba1768fe/bench-7d5a2dd2/benchmark.json...\n",
      "Benchmark results saved to /workspaces/resume-ner-azureml/outputs/benchmarking/local/distilbert/study-712fbee8/trial-ba1768fe/bench-7d5a2dd2/benchmark.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 00:13:13,234 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Building run name: trial_id=ba1768feed3e6d36c556b1c6ccdb69dbb3a37e447c4fb7a62559055534df3fc7, root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config\n",
      "2026-01-13 00:13:13,250 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}\n",
      "2026-01-13 00:13:13,251 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=benchmarking\n",
      "2026-01-13 00:13:13,252 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Starting reservation: counter_key=resume-ner:benchmarking:7c2b4c550470dc1d07edd33c1b048ac1fa6b..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-13 00:13:13,254 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Loaded 10 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-13 00:13:13,255 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Found 2 allocations for counter_key (after deduplication): committed=[1, 2], reserved=[], expired=[], max_committed_version=2\n",
      "2026-01-13 00:13:13,256 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Allocation details: [(1, 'committed', '85e69ccc-833'), (2, 'committed', 'aeb15085-a3b')]\n",
      "2026-01-13 00:13:13,256 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Reserving next version: 3 (incremented from max_committed=2, skipped 0 reserved/expired versions)\n",
      "2026-01-13 00:13:13,257 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] \u2713 Successfully reserved version 3 for counter_key resume-ner:benchmarking:7c2b4c550470dc1d07edd33c1b... (run_id: pending_2026...)\n",
      "2026-01-13 00:13:13,258 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Generated run name: local_distilbert_benchmark_study-712fbee8_trial-ba1768fe_bench-7d5a2dd2_3\n",
      "2026-01-13 00:13:13,520 - orchestration.jobs.tracking.trackers.benchmark_tracker - INFO - [START_BENCHMARK_RUN] Building tags: context=NamingContext(process_type='benchmarking', model='distilbert', environment='local', stage=None, storage_env='local', study_name=None, spec_fp=None, exec_fp=None, variant=1, trial_id='ba1768feed3e6d36c556b1c6ccdb69dbb3a37e447c4fb7a62559055534df3fc7', trial_number=None, fold_idx=None, parent_training_id=None, conv_fp=None, study_key_hash='712fbee85e89992eb7c7612788ec0851212ac9410f6a35c474dfbfb1cadef818', trial_key_hash='ba1768feed3e6d36c556b1c6ccdb69dbb3a37e447c4fb7a62559055534df3fc7', benchmark_config_hash='7d5a2dd29cc01f4aedde1d486688ba09d475f7a27cc7889d1ad9ca0ba61db39a'), study_key_hash=712fbee85e89992e..., trial_key_hash=ba1768feed3e6d36..., context.model=distilbert, context.process_type=benchmarking\n",
      "2026-01-13 00:13:13,521 - orchestration.jobs.tracking.trackers.benchmark_tracker - INFO - [START_BENCHMARK_RUN] Built tags, grouping tags present: study_key_hash=True, trial_key_hash=True, code.model=distilbert, code.stage=benchmarking\n",
      "2026-01-13 00:13:13,524 - orchestration.jobs.tracking.trackers.benchmark_tracker - INFO - [START_BENCHMARK_RUN] Set lineage tags: parent_run_id=1227a495-7bf..., parent_kind=trial\n",
      "2026-01-13 00:13:13,660 - orchestration.jobs.tracking.trackers.benchmark_tracker - INFO - [Benchmark Commit] Found version 3 in run name 'local_distilbert_benchmark_study-712fbee8_trial-ba1768fe_bench-7d5a2dd2_3'\n",
      "2026-01-13 00:13:13,661 - orchestration.jobs.tracking.config.loader - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}\n",
      "2026-01-13 00:13:13,662 - orchestration.jobs.tracking.config.loader - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=benchmarking\n",
      "2026-01-13 00:13:13,662 - orchestration.jobs.tracking.trackers.benchmark_tracker - INFO - [Benchmark Commit] Committing version 3 for run 5ca0b6d5-f8f..., counter_key=resume-ner:benchmarking:7c2b4c550470dc1d07edd33c1b...\n",
      "2026-01-13 00:13:13,663 - orchestration.jobs.tracking.index.version_counter - INFO - [Commit Version] Starting commit: counter_key=resume-ner:benchmarking:7c2b4c550470dc1d07edd33c1b048ac1fa6b..., version=3, run_id=5ca0b6d5-f8f..., counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-13 00:13:13,664 - orchestration.jobs.tracking.index.version_counter - INFO - [Commit Version] Loaded 11 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-13 00:13:13,665 - orchestration.jobs.tracking.index.version_counter - INFO - [Commit Version] \u2713 Found and committed reservation: version=3, status changed from 'reserved' to 'committed', run_id=5ca0b6d5-f8f..., counter_key=resume-ner:benchmarking:7c2b4c550470dc1d07edd33c1b...\n",
      "2026-01-13 00:13:13,666 - orchestration.jobs.tracking.index.version_counter - INFO - [Commit Version] \u2713 Successfully saved committed version 3 to /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-13 00:13:13,666 - orchestration.jobs.tracking.trackers.benchmark_tracker - INFO - [Benchmark Commit] \u2713 Successfully committed version 3 for benchmark run 5ca0b6d5-f8f...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83c\udfc3 View run local_distilbert_benchmark_study-712fbee8_trial-ba1768fe_bench-7d5a2dd2_3 at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/29716cbc-2f1e-485a-87be-3ef5c2f931dd/runs/5ca0b6d5-f8fd-444a-bdde-2f06337272bd\n",
      "\ud83e\uddea View experiment at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/29716cbc-2f1e-485a-87be-3ef5c2f931dd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 00:13:18,008 - evaluation.benchmarking.orchestrator - INFO - Benchmark completed: /workspaces/resume-ner-azureml/outputs/benchmarking/local/distilbert/study-712fbee8/trial-ba1768fe/bench-7d5a2dd2/benchmark.json\n",
      "2026-01-13 00:13:18,010 - evaluation.benchmarking.orchestrator - INFO - Benchmarking distilroberta (66030b67f3a57be34f001d1eb1e5583ddae4467f1670dc4b3429faaa54673818)...\n",
      "2026-01-13 00:13:18,078 - evaluation.benchmarking.orchestrator - INFO - [BENCHMARK] Found trial run ID from MLflow: c3014f98-39c... (via trial_key_hash=66030b67f3a57be3...)\n",
      "2026-01-13 00:13:18,078 - evaluation.benchmarking.orchestrator - INFO - [BENCHMARK] Final run IDs: trial=c3014f98-39c..., refit=c3014f98-39c..., sweep=None...\n",
      "2026-01-13 00:13:18,079 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /workspaces/resume-ner-azureml/src/evaluation/benchmarking/cli.py --checkpoint /workspaces/resume-ner-azureml/outputs/best_model_selection/local/distilroberta/sel_87caa65a_66030b67/best_trial_checkpoint.tar.gz/best_trial_checkpoint --test-data /workspaces/resume-ner-azureml/dataset_tiny/seed0/test.json --batch-sizes 1 --iterations 10 --warmup 10 --max-length 512 --output /workspaces/resume-ner-azureml/outputs/benchmarking/local/distilroberta/study-87caa65a/trial-66030b67/bench-7d5a2dd2/benchmark.json\n",
      "2026-01-13 00:13:20,629 - infrastructure.tracking.mlflow.compatibility - INFO - Applied Azure ML artifact compatibility patch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 test texts\n",
      "Starting benchmark for checkpoint: /workspaces/resume-ner-azureml/outputs/best_model_selection/local/distilroberta/sel_87caa65a_66030b67/best_trial_checkpoint.tar.gz/best_trial_checkpoint\n",
      "Loading tokenizer from /workspaces/resume-ner-azureml/outputs/best_model_selection/local/distilroberta/sel_87caa65a_66030b67/best_trial_checkpoint.tar.gz/best_trial_checkpoint...\n",
      "Tokenizer loaded.\n",
      "Loading model from /workspaces/resume-ner-azureml/outputs/best_model_selection/local/distilroberta/sel_87caa65a_66030b67/best_trial_checkpoint.tar.gz/best_trial_checkpoint...\n",
      "Moving model to cpu...\n",
      "Model loaded and set to eval mode.\n",
      "Model ready on device: cpu\n",
      "\n",
      "Benchmarking batch size 1...\n",
      "  Running 10 warmup iterations, then 10 measurement iterations...\n",
      "    Warmup: 10 iterations... 10/10 done.\n",
      "    Measurement: 10 iterations... 10/10 done.\n",
      "  Mean latency: 216.06 ms\n",
      "  P95 latency: 280.35 ms\n",
      "  Throughput: 4.63 docs/sec\n",
      "\n",
      "Saving results to /workspaces/resume-ner-azureml/outputs/benchmarking/local/distilroberta/study-87caa65a/trial-66030b67/bench-7d5a2dd2/benchmark.json...\n",
      "Benchmark results saved to /workspaces/resume-ner-azureml/outputs/benchmarking/local/distilroberta/study-87caa65a/trial-66030b67/bench-7d5a2dd2/benchmark.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 00:13:30,185 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Building run name: trial_id=66030b67f3a57be34f001d1eb1e5583ddae4467f1670dc4b3429faaa54673818, root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config\n",
      "2026-01-13 00:13:30,186 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}\n",
      "2026-01-13 00:13:30,186 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=benchmarking\n",
      "2026-01-13 00:13:30,187 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Starting reservation: counter_key=resume-ner:benchmarking:963160ebc14c86117b779a0da8f67c311d8e..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-13 00:13:30,189 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Loaded 11 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-13 00:13:30,189 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Found 2 allocations for counter_key (after deduplication): committed=[1, 2], reserved=[], expired=[], max_committed_version=2\n",
      "2026-01-13 00:13:30,190 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Allocation details: [(1, 'committed', '3826b872-ff6'), (2, 'committed', '61b0a757-84d')]\n",
      "2026-01-13 00:13:30,191 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Reserving next version: 3 (incremented from max_committed=2, skipped 0 reserved/expired versions)\n",
      "2026-01-13 00:13:30,192 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] \u2713 Successfully reserved version 3 for counter_key resume-ner:benchmarking:963160ebc14c86117b779a0da8... (run_id: pending_2026...)\n",
      "2026-01-13 00:13:30,192 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Generated run name: local_distilroberta_benchmark_study-87caa65a_trial-66030b67_bench-7d5a2dd2_3\n",
      "2026-01-13 00:13:30,295 - orchestration.jobs.tracking.trackers.benchmark_tracker - INFO - [START_BENCHMARK_RUN] Building tags: context=NamingContext(process_type='benchmarking', model='distilroberta', environment='local', stage=None, storage_env='local', study_name=None, spec_fp=None, exec_fp=None, variant=1, trial_id='66030b67f3a57be34f001d1eb1e5583ddae4467f1670dc4b3429faaa54673818', trial_number=None, fold_idx=None, parent_training_id=None, conv_fp=None, study_key_hash='87caa65aa5d147ba308691b999c3561eff2ef4a8a592e8c9b1bf8e9f149670ac', trial_key_hash='66030b67f3a57be34f001d1eb1e5583ddae4467f1670dc4b3429faaa54673818', benchmark_config_hash='7d5a2dd29cc01f4aedde1d486688ba09d475f7a27cc7889d1ad9ca0ba61db39a'), study_key_hash=87caa65aa5d147ba..., trial_key_hash=66030b67f3a57be3..., context.model=distilroberta, context.process_type=benchmarking\n",
      "2026-01-13 00:13:30,296 - orchestration.jobs.tracking.trackers.benchmark_tracker - INFO - [START_BENCHMARK_RUN] Built tags, grouping tags present: study_key_hash=True, trial_key_hash=True, code.model=distilroberta, code.stage=benchmarking\n",
      "2026-01-13 00:13:30,297 - orchestration.jobs.tracking.trackers.benchmark_tracker - INFO - [START_BENCHMARK_RUN] Set lineage tags: parent_run_id=c3014f98-39c..., parent_kind=trial\n",
      "2026-01-13 00:13:30,417 - orchestration.jobs.tracking.trackers.benchmark_tracker - INFO - [Benchmark Commit] Found version 3 in run name 'local_distilroberta_benchmark_study-87caa65a_trial-66030b67_bench-7d5a2dd2_3'\n",
      "2026-01-13 00:13:30,418 - orchestration.jobs.tracking.config.loader - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}\n",
      "2026-01-13 00:13:30,419 - orchestration.jobs.tracking.config.loader - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=benchmarking\n",
      "2026-01-13 00:13:30,420 - orchestration.jobs.tracking.trackers.benchmark_tracker - INFO - [Benchmark Commit] Committing version 3 for run b6393868-24b..., counter_key=resume-ner:benchmarking:963160ebc14c86117b779a0da8...\n",
      "2026-01-13 00:13:30,421 - orchestration.jobs.tracking.index.version_counter - INFO - [Commit Version] Starting commit: counter_key=resume-ner:benchmarking:963160ebc14c86117b779a0da8f67c311d8e..., version=3, run_id=b6393868-24b..., counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-13 00:13:30,422 - orchestration.jobs.tracking.index.version_counter - INFO - [Commit Version] Loaded 12 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-13 00:13:30,422 - orchestration.jobs.tracking.index.version_counter - INFO - [Commit Version] \u2713 Found and committed reservation: version=3, status changed from 'reserved' to 'committed', run_id=b6393868-24b..., counter_key=resume-ner:benchmarking:963160ebc14c86117b779a0da8...\n",
      "2026-01-13 00:13:30,423 - orchestration.jobs.tracking.index.version_counter - INFO - [Commit Version] \u2713 Successfully saved committed version 3 to /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json\n",
      "2026-01-13 00:13:30,424 - orchestration.jobs.tracking.trackers.benchmark_tracker - INFO - [Benchmark Commit] \u2713 Successfully committed version 3 for benchmark run b6393868-24b...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83c\udfc3 View run local_distilroberta_benchmark_study-87caa65a_trial-66030b67_bench-7d5a2dd2_3 at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/29716cbc-2f1e-485a-87be-3ef5c2f931dd/runs/b6393868-24b8-4039-83bb-e019f6c2a63d\n",
      "\ud83e\uddea View experiment at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/29716cbc-2f1e-485a-87be-3ef5c2f931dd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 00:13:34,201 - evaluation.benchmarking.orchestrator - INFO - Benchmark completed: /workspaces/resume-ner-azureml/outputs/benchmarking/local/distilroberta/study-87caa65a/trial-66030b67/bench-7d5a2dd2/benchmark.json\n",
      "2026-01-13 00:13:34,201 - evaluation.benchmarking.orchestrator - INFO - Benchmarking complete. 2/2 trials benchmarked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u2713 Benchmarking complete. Results saved to MLflow experiment: resume_ner_baseline-benchmark\n"
     ]
    }
   ],
   "source": [
    "# Optional: Run benchmarking on champions if not already done\n",
    "# Skip this cell if benchmark runs already exist in MLflow\n",
    "\n",
    "RUN_BENCHMARKING = True  # Set to True to run benchmarking\n",
    "\n",
    "if RUN_BENCHMARKING:\n",
    "    from evaluation.selection.trial_finder import select_champions_for_backbones\n",
    "    from evaluation.benchmarking import benchmark_best_trials\n",
    "    from common.shared.platform_detection import detect_platform\n",
    "    from common.shared.yaml_utils import load_yaml\n",
    "    from mlflow.tracking import MlflowClient\n",
    "    from infrastructure.naming.experiments import build_mlflow_experiment_name\n",
    "    from orchestration import STAGE_HPO\n",
    "    from orchestration.jobs.tracking.mlflow_tracker import MLflowBenchmarkTracker\n",
    "    \n",
    "    print(\"\ud83d\udd04 Running benchmarking on champions...\")\n",
    "    \n",
    "    # Step 1: Load selection config and setup MLflow client\n",
    "    selection_config = load_yaml(CONFIG_DIR / \"best_model_selection.yaml\")\n",
    "    mlflow_client = MlflowClient()\n",
    "    \n",
    "    # Step 2: Build HPO experiments dict (backbone -> {name, id})\n",
    "    hpo_experiments = {}\n",
    "    for exp in mlflow_client.search_experiments():\n",
    "        if exp.name.startswith(f\"{experiment_name}-hpo-\"):\n",
    "            backbone = exp.name.replace(f\"{experiment_name}-hpo-\", \"\")\n",
    "            hpo_experiments[backbone] = {\n",
    "                \"name\": exp.name,\n",
    "                \"id\": exp.experiment_id\n",
    "            }\n",
    "    \n",
    "    if not hpo_experiments:\n",
    "        print(\"\u26a0 No HPO experiments found. Skipping benchmarking.\")\n",
    "    else:\n",
    "        # Step 3: Select champions per backbone\n",
    "        backbone_values = list(hpo_experiments.keys())\n",
    "        print(f\"\u2713 Found {len(hpo_experiments)} HPO experiment(s)\")\n",
    "        print(\"\\n\ud83c\udfc6 Selecting champions per backbone...\")\n",
    "        \n",
    "        champions = select_champions_for_backbones(\n",
    "            backbone_values=backbone_values,\n",
    "            hpo_experiments=hpo_experiments,\n",
    "            selection_config=selection_config,\n",
    "            mlflow_client=mlflow_client,\n",
    "        )\n",
    "        \n",
    "        if not champions:\n",
    "            print(\"\u26a0 No champions found. Skipping benchmarking.\")\n",
    "            \n",
    "            # Add diagnostics to help debug\n",
    "            print(\"\\n\ud83d\udd0d Diagnostics:\")\n",
    "            from infrastructure.naming.mlflow.tags_registry import load_tags_registry\n",
    "            tags_registry = load_tags_registry(CONFIG_DIR)\n",
    "            \n",
    "            for backbone, exp_info in hpo_experiments.items():\n",
    "                backbone_name = backbone.split(\"-\")[0] if \"-\" in backbone else backbone\n",
    "                runs = mlflow_client.search_runs(\n",
    "                    experiment_ids=[exp_info[\"id\"]],\n",
    "                    filter_string=\"\",\n",
    "                    max_results=100,\n",
    "                )\n",
    "                finished_runs = [r for r in runs if r.info.status == \"FINISHED\"]\n",
    "                print(f\"\\n  {backbone}: {len(finished_runs)} finished run(s)\")\n",
    "                \n",
    "                # Check for required tags for champion selection\n",
    "                if finished_runs:\n",
    "                    # Separate parent and child runs\n",
    "                    # Child runs: have mlflow.parentRunId tag\n",
    "                    # Parent runs: don't have mlflow.parentRunId tag\n",
    "                    child_runs = [r for r in finished_runs if r.data.tags.get(\"mlflow.parentRunId\")]\n",
    "                    parent_run_ids = {r.data.tags.get(\"mlflow.parentRunId\") for r in child_runs if r.data.tags.get(\"mlflow.parentRunId\")}\n",
    "                    parent_runs = [r for r in finished_runs if r.info.run_id in parent_run_ids or not r.data.tags.get(\"mlflow.parentRunId\")]\n",
    "                    \n",
    "                    print(f\"    Parent runs: {len(parent_runs)}, Child runs: {len(child_runs)}\")\n",
    "                    \n",
    "                    # Check child runs (what select_champion_per_backbone queries)\n",
    "                    if child_runs:\n",
    "                        sample_child = child_runs[0]\n",
    "                        tags = sample_child.data.tags\n",
    "                        stage_tag = tags_registry.key(\"process\", \"stage\")\n",
    "                        study_key_tag = tags_registry.key(\"grouping\", \"study_key_hash\")\n",
    "                        trial_key_tag = tags_registry.key(\"grouping\", \"trial_key_hash\")\n",
    "                        schema_tag = tags_registry.key(\"study\", \"key_schema_version\")\n",
    "                        \n",
    "                        print(f\"    Sample child run:\")\n",
    "                        print(f\"      - stage: {tags.get(stage_tag, 'missing')}\")\n",
    "                        print(f\"      - study_key_hash: {'present' if tags.get(study_key_tag) else 'missing'}\")\n",
    "                        print(f\"      - trial_key_hash: {'present' if tags.get(trial_key_tag) else 'missing'}\")\n",
    "                        print(f\"      - schema_version: {tags.get(schema_tag, 'missing')}\")\n",
    "                    \n",
    "                    # Check parent runs (where Phase 2 tags should be)\n",
    "                    if parent_runs:\n",
    "                        sample_parent = parent_runs[0]\n",
    "                        tags = sample_parent.data.tags\n",
    "                        schema_tag = tags_registry.key(\"study\", \"key_schema_version\")\n",
    "                        data_fp_tag = tags_registry.key(\"fingerprint\", \"data\")\n",
    "                        eval_fp_tag = tags_registry.key(\"fingerprint\", \"eval\")\n",
    "                        study_key_tag = tags_registry.key(\"grouping\", \"study_key_hash\")\n",
    "                        \n",
    "                        print(f\"    Sample parent run:\")\n",
    "                        print(f\"      - schema_version: {tags.get(schema_tag, 'missing')}\")\n",
    "                        print(f\"      - data_fp: {'present' if tags.get(data_fp_tag) else 'missing'}\")\n",
    "                        print(f\"      - eval_fp: {'present' if tags.get(eval_fp_tag) else 'missing'}\")\n",
    "                        print(f\"      - study_key_hash: {'present' if tags.get(study_key_tag) else 'missing'}\")\n",
    "            \n",
    "            print(\"\\n\ud83d\udca1 Troubleshooting tips:\")\n",
    "            print(\"  1. **Artifact filter issue**: If you see 'Artifact filter removed X runs',\")\n",
    "            print(\"     the runs don't have 'code.artifact.available' tag set to 'true'.\")\n",
    "            print(\"     Options:\")\n",
    "            print(\"     a) Set require_artifact_available: false in config/best_model_selection.yaml\")\n",
    "            print(\"     b) Set code.artifact.available='true' tag on the runs (if artifacts exist)\")\n",
    "            print(\"  2. Ensure HPO runs have Phase 2 tags set (schema_version, fingerprints, etc.)\")\n",
    "            print(\"  3. Check that runs meet minimum trial requirements (min_trials_per_group in selection config)\")\n",
    "            print(\"  4. Check selection config in config/best_model_selection.yaml\")\n",
    "            print(\"\\n   You can still proceed to Step 7 if benchmark runs already exist from notebook 01.\")\n",
    "        else:\n",
    "            # Step 4: Convert champions to best_trials format for benchmarking\n",
    "            from pathlib import Path\n",
    "            from common.shared.platform_detection import detect_platform\n",
    "            \n",
    "            best_trials = {}\n",
    "            \n",
    "            for backbone, champion_data in champions.items():\n",
    "                champion = champion_data[\"champion\"]\n",
    "                # Use checkpoint_path as checkpoint_dir if available from local disk\n",
    "                checkpoint_path = champion.get(\"checkpoint_path\")\n",
    "                checkpoint_dir = str(checkpoint_path) if checkpoint_path and Path(checkpoint_path).exists() else None\n",
    "                \n",
    "                # If no local checkpoint, use single source of truth for acquisition\n",
    "                if not checkpoint_dir:\n",
    "                    try:\n",
    "                        from evaluation.selection.artifact_acquisition import acquire_best_model_checkpoint\n",
    "                        from common.shared.platform_detection import detect_platform\n",
    "                        from common.shared.yaml_utils import load_yaml\n",
    "                        \n",
    "                        run_id = champion.get(\"refit_run_id\") or champion.get(\"run_id\")  # Prefer refit_run_id for checkpoint acquisition\n",
    "                        study_key_hash = champion.get(\"study_key_hash\")\n",
    "                        trial_key_hash = champion.get(\"trial_key_hash\")\n",
    "                        \n",
    "                        if not run_id:\n",
    "                            continue\n",
    "                        \n",
    "                        print(f\"  Acquiring checkpoint for {backbone} from MLflow (run {run_id[:12]}...)\")\n",
    "                        \n",
    "                        # Load configs\n",
    "                        acquisition_config = load_yaml(CONFIG_DIR / \"artifact_acquisition.yaml\")\n",
    "                        original_priority = acquisition_config.get(\"priority\", [\"local\", \"mlflow\"])\n",
    "                        acquisition_config[\"priority\"] = [\"mlflow\"]  # Only MLflow for benchmarking\n",
    "                        \n",
    "                        # Use single source of truth: acquire_best_model_checkpoint()\n",
    "                        # It handles: local \u2192 drive \u2192 MLflow (in priority order)\n",
    "                        best_run_info = {\n",
    "                            \"run_id\": run_id,\n",
    "                            \"study_key_hash\": study_key_hash,\n",
    "                            \"trial_key_hash\": trial_key_hash,\n",
    "                            \"backbone\": backbone,\n",
    "                        }\n",
    "                        \n",
    "                        acquired_checkpoint_dir = acquire_best_model_checkpoint(\n",
    "                            best_run_info=best_run_info,\n",
    "                            root_dir=ROOT_DIR,\n",
    "                            config_dir=CONFIG_DIR,\n",
    "                            acquisition_config=acquisition_config,\n",
    "                            selection_config=selection_config,\n",
    "                            platform=detect_platform(),\n",
    "                            restore_from_drive=None,\n",
    "                            drive_store=None,\n",
    "                            in_colab=IN_COLAB,\n",
    "                        )\n",
    "                        \n",
    "                        if acquired_checkpoint_dir and Path(acquired_checkpoint_dir).exists():\n",
    "                            checkpoint_dir = str(acquired_checkpoint_dir)\n",
    "                            print(f\"    \u2713 Acquired checkpoint from MLflow\")\n",
    "                        else:\n",
    "                            print(f\"    \u26a0 Could not acquire checkpoint from MLflow\")\n",
    "                        \n",
    "                        # Restore original priority\n",
    "                        acquisition_config[\"priority\"] = original_priority\n",
    "                    except Exception as e:\n",
    "                        print(f\"    \u26a0 Error during artifact acquisition: {e}\")\n",
    "                        pass\n",
    "                    except Exception as e:\n",
    "                        print(f\"    \u26a0 Error during artifact acquisition: {e}\")\n",
    "                        pass\n",
    "                \n",
    "                best_trials[backbone] = {\n",
    "                    \"checkpoint_dir\": checkpoint_dir,  # From local disk or MLflow acquisition\n",
    "                    \"trial_name\": champion.get(\"trial_key_hash\", \"unknown\"),\n",
    "                    \"accuracy\": champion[\"metric\"],\n",
    "                    \"study_key_hash\": champion.get(\"study_key_hash\"),\n",
    "                    \"trial_key_hash\": champion.get(\"trial_key_hash\"),\n",
    "                    \"run_id\": champion[\"run_id\"],\n",
    "                }\n",
    "            \n",
    "            print(f\"\\n\u2713 Found {len(best_trials)} champion(s) to benchmark\")\n",
    "            \n",
    "            # Step 5: Load benchmark config\n",
    "            benchmark_config = load_yaml(CONFIG_DIR / \"benchmark.yaml\") if (CONFIG_DIR / \"benchmark.yaml\").exists() else {}\n",
    "            benchmark_settings = benchmark_config.get(\"benchmarking\", {})\n",
    "            \n",
    "            # Get benchmark parameters from config or use defaults\n",
    "            benchmark_batch_sizes = benchmark_settings.get(\"batch_sizes\", [1, 8, 16])\n",
    "            benchmark_iterations = benchmark_settings.get(\"iterations\", 100)\n",
    "            benchmark_warmup = benchmark_settings.get(\"warmup_iterations\", 10)\n",
    "            benchmark_max_length = benchmark_settings.get(\"max_length\", 512)\n",
    "            benchmark_device = benchmark_settings.get(\"device\")\n",
    "            \n",
    "            # Step 6: Setup test data path\n",
    "            from infrastructure.config.loader import load_experiment_config, load_all_configs\n",
    "            experiment_config = load_experiment_config(CONFIG_DIR, experiment_name)\n",
    "            configs = load_all_configs(experiment_config)\n",
    "            data_config = configs.get(\"data\", {})\n",
    "            \n",
    "            # Get dataset path from data config\n",
    "            local_path_str = data_config.get(\"local_path\", \"../dataset\")\n",
    "            DATASET_LOCAL_PATH = (CONFIG_DIR / local_path_str).resolve()\n",
    "            seed = data_config.get(\"seed\")\n",
    "            if seed is not None and \"dataset_tiny\" in str(DATASET_LOCAL_PATH):\n",
    "                DATASET_LOCAL_PATH = DATASET_LOCAL_PATH / f\"seed{seed}\"\n",
    "            \n",
    "            test_data_path = DATASET_LOCAL_PATH / \"test.json\"\n",
    "            \n",
    "            # Step 7: Create MLflow tracker and run benchmarking\n",
    "            if test_data_path.exists():\n",
    "                benchmark_experiment_name = f\"{experiment_name}-benchmark\"\n",
    "                benchmark_tracker = MLflowBenchmarkTracker(benchmark_experiment_name)\n",
    "                \n",
    "                environment = detect_platform()\n",
    "                hpo_config = configs.get(\"hpo\", {})\n",
    "                \n",
    "                print(f\"\\n\ud83d\udcca Running benchmarking on {len(best_trials)} champion(s)...\")\n",
    "                benchmark_results = benchmark_best_trials(\n",
    "                    best_trials=best_trials,\n",
    "                    test_data_path=test_data_path,\n",
    "                    root_dir=ROOT_DIR,\n",
    "                    environment=environment,\n",
    "                    data_config=data_config,\n",
    "                    hpo_config=hpo_config,\n",
    "                    benchmark_config=benchmark_config,\n",
    "                    benchmark_batch_sizes=benchmark_batch_sizes,\n",
    "                    benchmark_iterations=benchmark_iterations,\n",
    "                    benchmark_warmup=benchmark_warmup,\n",
    "                    benchmark_max_length=benchmark_max_length,\n",
    "                    benchmark_device=benchmark_device,\n",
    "                    benchmark_tracker=benchmark_tracker,\n",
    "                    backup_enabled=BACKUP_ENABLED,\n",
    "                    backup_to_drive=restore_from_drive if \"restore_from_drive\" in locals() else None,\n",
    "                    ensure_restored_from_drive=restore_from_drive if \"restore_from_drive\" in locals() else None,\n",
    "                )\n",
    "                \n",
    "                print(f\"\\n\u2713 Benchmarking complete. Results saved to MLflow experiment: {benchmark_experiment_name}\")\n",
    "            else:\n",
    "                print(f\"\u26a0 Test data not found at {test_data_path}. Skipping benchmarking.\")\n",
    "else:\n",
    "    print(\"\u23ed Skipping benchmarking (RUN_BENCHMARKING=False).\")\n",
    "    print(\"   If benchmark runs don't exist, set RUN_BENCHMARKING=True or run benchmarking in notebook 01.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Best Model Selection\n",
    "\n",
    "Query MLflow benchmark runs (created by `01_orchestrate_training_colab.ipynb` or Step 6 above using `evaluation.benchmarking.benchmark_best_trials`), join to training runs via grouping tags, and select the best model using normalized composite scoring.\n",
    "\n",
    "**Note**: Benchmark runs must exist in MLflow before running this step. If no benchmark runs are found, either:\n",
    "- Set `RUN_BENCHMARKING=True` in Step 6 above, or\n",
    "- Go back to `01_orchestrate_training_colab.ipynb` and run the benchmarking step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_74673/3313677413.py:1: DeprecationWarning: selection is deprecated, use evaluation.selection instead. This shim will be removed in 2 releases.\n",
      "  from selection.mlflow_selection import find_best_model_from_mlflow\n",
      "2026-01-12 23:45:58,539 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow\n",
      "2026-01-12 23:45:58,540 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: resume_ner_baseline-benchmark\n",
      "2026-01-12 23:45:58,541 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 2\n",
      "2026-01-12 23:45:58,543 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1\n",
      "2026-01-12 23:45:58,544 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30\n",
      "2026-01-12 23:45:58,545 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\ud83d\udccb Best Model Selection Mode: force_new\n",
      "  Mode is 'force_new' - skipping cache, querying MLflow...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-12 23:45:58,660 - evaluation.selection.mlflow_selection - INFO - Found 18 finished benchmark runs\n",
      "2026-01-12 23:45:58,661 - evaluation.selection.mlflow_selection - INFO - Found 18 benchmark runs with required metrics and grouping tags\n",
      "2026-01-12 23:45:58,662 - evaluation.selection.mlflow_selection - INFO - Preloading trial and refit runs from HPO experiments...\n",
      "2026-01-12 23:45:58,853 - evaluation.selection.mlflow_selection - INFO - Built trial lookup with 2 unique (study_hash, trial_hash) pairs\n",
      "2026-01-12 23:45:58,854 - evaluation.selection.mlflow_selection - INFO - Built refit lookup with 2 unique (study_hash, trial_hash) pairs\n",
      "2026-01-12 23:45:58,855 - evaluation.selection.mlflow_selection - INFO - Joining benchmark runs with trial runs and refit runs...\n",
      "2026-01-12 23:45:58,855 - evaluation.selection.mlflow_selection - INFO - Found 0 candidate(s) with both benchmark and training metrics\n",
      "2026-01-12 23:45:58,856 - evaluation.selection.mlflow_selection - WARNING - No candidates found with both benchmark and training metrics\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not find best model from MLflow.\n\nDiagnostics:\n  - Benchmark experiment 'resume_ner_baseline-benchmark': 18 finished run(s) found\n    - Unique (study_hash, trial_hash) pairs: 8\n  - HPO experiments:\n    - distilbert: 5 finished run(s) found\n    - distilroberta: 5 finished run(s) found\n    - HPO trial runs: 4 with 2 unique (study_hash, trial_hash) pairs\n    - HPO refit runs: 2 with 2 unique (study_hash, trial_hash) pairs\n\n  - Matching pairs: 0 out of 8 benchmark pairs\n\n  Sample benchmark (study_hash, trial_hash) pairs:\n    1. study=25cad2a25d5a991d..., trial=96bf728ba9b34f84...\n    2. study=584922cebb01e555..., trial=f302370a4cc43f41...\n    3. study=584922cebb01e555..., trial=f12b353c63a42e3f...\n\n  Sample HPO trial (study_hash, trial_hash) pairs:\n    1. study=712fbee85e89992e..., trial=ba1768feed3e6d36...\n    2. study=87caa65aa5d147ba..., trial=66030b67f3a57be3...\n\n  \u26a0\ufe0f  Hash mismatch detected! This usually means:\n     - Benchmark runs were created from different trials than current HPO runs\n     - Study or trial hashes changed between runs (e.g., Phase 2 migration)\n     - Solution: Re-run benchmarking on champions (Step 6) to create new benchmark runs\n\nPossible causes:\n  1. No benchmark runs have been executed yet. Run benchmark jobs first.\n  2. Benchmark runs exist but are missing required metrics or grouping tags.\n  3. HPO runs exist but are missing required metrics or grouping tags.\n  4. No matching runs found between benchmark and HPO experiments (hash mismatch).\n\nCheck the logs above for detailed information about what was found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 167\u001b[0m\n\u001b[1;32m    151\u001b[0m         error_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    152\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  \u26a0\ufe0f  Hash mismatch detected! This usually means:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m     - Benchmark runs were created from different trials than current HPO runs\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m     - Study or trial hashes changed between runs (e.g., Phase 2 migration)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m     - Solution: Re-run benchmarking on champions (Step 6) to create new benchmark runs\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         )\n\u001b[1;32m    158\u001b[0m     error_msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPossible causes:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  1. No benchmark runs have been executed yet. Run benchmark jobs first.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCheck the logs above for detailed information about what was found.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    165\u001b[0m     )\n\u001b[0;32m--> 167\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# Save to cache\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mselection\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcache\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m save_best_model_cache\n",
      "\u001b[0;31mValueError\u001b[0m: Could not find best model from MLflow.\n\nDiagnostics:\n  - Benchmark experiment 'resume_ner_baseline-benchmark': 18 finished run(s) found\n    - Unique (study_hash, trial_hash) pairs: 8\n  - HPO experiments:\n    - distilbert: 5 finished run(s) found\n    - distilroberta: 5 finished run(s) found\n    - HPO trial runs: 4 with 2 unique (study_hash, trial_hash) pairs\n    - HPO refit runs: 2 with 2 unique (study_hash, trial_hash) pairs\n\n  - Matching pairs: 0 out of 8 benchmark pairs\n\n  Sample benchmark (study_hash, trial_hash) pairs:\n    1. study=25cad2a25d5a991d..., trial=96bf728ba9b34f84...\n    2. study=584922cebb01e555..., trial=f302370a4cc43f41...\n    3. study=584922cebb01e555..., trial=f12b353c63a42e3f...\n\n  Sample HPO trial (study_hash, trial_hash) pairs:\n    1. study=712fbee85e89992e..., trial=ba1768feed3e6d36...\n    2. study=87caa65aa5d147ba..., trial=66030b67f3a57be3...\n\n  \u26a0\ufe0f  Hash mismatch detected! This usually means:\n     - Benchmark runs were created from different trials than current HPO runs\n     - Study or trial hashes changed between runs (e.g., Phase 2 migration)\n     - Solution: Re-run benchmarking on champions (Step 6) to create new benchmark runs\n\nPossible causes:\n  1. No benchmark runs have been executed yet. Run benchmark jobs first.\n  2. Benchmark runs exist but are missing required metrics or grouping tags.\n  3. HPO runs exist but are missing required metrics or grouping tags.\n  4. No matching runs found between benchmark and HPO experiments (hash mismatch).\n\nCheck the logs above for detailed information about what was found."
     ]
    }
   ],
   "source": [
    "from selection.mlflow_selection import find_best_model_from_mlflow\n",
    "from selection.artifact_acquisition import acquire_best_model_checkpoint\n",
    "from pathlib import Path\n",
    "from typing import Optional, Callable, Dict, Any\n",
    "\n",
    "# Validate experiments\n",
    "if benchmark_experiment is None:\n",
    "    raise ValueError(f\"Benchmark experiment '{benchmark_experiment_name}' not found. Run benchmark jobs first.\")\n",
    "if not hpo_experiments:\n",
    "    raise ValueError(f\"No HPO experiments found. Run HPO jobs first.\")\n",
    "\n",
    "# Check if we should reuse cached selection\n",
    "run_mode = selection_config.get(\"run\", {}).get(\"mode\", \"reuse_if_exists\")\n",
    "best_model = None\n",
    "cache_data = None\n",
    "\n",
    "print(f\"\\n\ud83d\udccb Best Model Selection Mode: {run_mode}\")\n",
    "\n",
    "if run_mode == \"reuse_if_exists\":\n",
    "    from selection.cache import load_cached_best_model\n",
    "    \n",
    "    tracking_uri = mlflow.get_tracking_uri()\n",
    "    cache_data = load_cached_best_model(\n",
    "        root_dir=ROOT_DIR,\n",
    "        config_dir=CONFIG_DIR,\n",
    "        experiment_name=experiment_name,\n",
    "        selection_config=selection_config,\n",
    "        tags_config=tags_config,\n",
    "        benchmark_experiment_id=benchmark_experiment[\"id\"],\n",
    "        tracking_uri=tracking_uri,\n",
    "    )\n",
    "    \n",
    "    if cache_data:\n",
    "        best_model = cache_data[\"best_model\"]\n",
    "        # Success message already printed by load_cached_best_model\n",
    "    else:\n",
    "        print(f\"\\n\u2139 Cache not available or invalid - will query MLflow for fresh selection\")\n",
    "elif run_mode == \"force_new\":\n",
    "    print(f\"  Mode is 'force_new' - skipping cache, querying MLflow...\")\n",
    "else:\n",
    "    print(f\"  \u26a0 Unknown run mode '{run_mode}', defaulting to querying MLflow...\")\n",
    "\n",
    "if best_model is None:\n",
    "    # Find best model\n",
    "    best_model = find_best_model_from_mlflow(\n",
    "        benchmark_experiment=benchmark_experiment,\n",
    "        hpo_experiments=hpo_experiments,\n",
    "        tags_config=tags_config,\n",
    "        selection_config=selection_config\n",
    "    )\n",
    "    \n",
    "    if best_model is None:\n",
    "        # Provide diagnostic information\n",
    "        from mlflow.tracking import MlflowClient\n",
    "        from infrastructure.naming.mlflow.tags_registry import load_tags_registry\n",
    "        \n",
    "        client = MlflowClient()\n",
    "        tags_registry = load_tags_registry(CONFIG_DIR)\n",
    "        study_key_tag = tags_registry.key(\"grouping\", \"study_key_hash\")\n",
    "        trial_key_tag = tags_registry.key(\"grouping\", \"trial_key_hash\")\n",
    "        \n",
    "        # Check benchmark experiment\n",
    "        benchmark_runs = client.search_runs(\n",
    "            experiment_ids=[benchmark_experiment[\"id\"]],\n",
    "            filter_string=\"\",\n",
    "            max_results=100,\n",
    "        )\n",
    "        finished_benchmark_runs = [r for r in benchmark_runs if r.info.status == \"FINISHED\"]\n",
    "        \n",
    "        # Check HPO experiments\n",
    "        hpo_run_counts = {}\n",
    "        hpo_trial_runs = []\n",
    "        hpo_refit_runs = []\n",
    "        stage_tag = tags_registry.key(\"process\", \"stage\")\n",
    "        \n",
    "        for backbone, exp_info in hpo_experiments.items():\n",
    "            hpo_runs = client.search_runs(\n",
    "                experiment_ids=[exp_info[\"id\"]],\n",
    "                filter_string=\"\",\n",
    "                max_results=100,\n",
    "            )\n",
    "            finished_hpo_runs = [r for r in hpo_runs if r.info.status == \"FINISHED\"]\n",
    "            hpo_run_counts[backbone] = len(finished_hpo_runs)\n",
    "            \n",
    "            # Separate trial and refit runs\n",
    "            for run in finished_hpo_runs:\n",
    "                stage = run.data.tags.get(stage_tag, \"\")\n",
    "                if stage == \"hpo\" or stage == \"hpo_trial\":\n",
    "                    hpo_trial_runs.append(run)\n",
    "                elif stage == \"hpo_refit\":\n",
    "                    hpo_refit_runs.append(run)\n",
    "        \n",
    "        # Collect unique (study_hash, trial_hash) pairs from benchmark runs\n",
    "        benchmark_pairs = set()\n",
    "        for run in finished_benchmark_runs:\n",
    "            study_hash = run.data.tags.get(study_key_tag)\n",
    "            trial_hash = run.data.tags.get(trial_key_tag)\n",
    "            if study_hash and trial_hash:\n",
    "                benchmark_pairs.add((study_hash, trial_hash))\n",
    "        \n",
    "        # Collect unique (study_hash, trial_hash) pairs from HPO trial runs\n",
    "        hpo_trial_pairs = set()\n",
    "        for run in hpo_trial_runs:\n",
    "            study_hash = run.data.tags.get(study_key_tag)\n",
    "            trial_hash = run.data.tags.get(trial_key_tag)\n",
    "            if study_hash and trial_hash:\n",
    "                hpo_trial_pairs.add((study_hash, trial_hash))\n",
    "        \n",
    "        # Collect unique (study_hash, trial_hash) pairs from HPO refit runs\n",
    "        hpo_refit_pairs = set()\n",
    "        for run in hpo_refit_runs:\n",
    "            study_hash = run.data.tags.get(study_key_tag)\n",
    "            trial_hash = run.data.tags.get(trial_key_tag)\n",
    "            if study_hash and trial_hash:\n",
    "                hpo_refit_pairs.add((study_hash, trial_hash))\n",
    "        \n",
    "        # Find matching pairs\n",
    "        matching_pairs = benchmark_pairs & hpo_trial_pairs\n",
    "        \n",
    "        error_msg = (\n",
    "            \"Could not find best model from MLflow.\\n\\n\"\n",
    "            \"Diagnostics:\\n\"\n",
    "            f\"  - Benchmark experiment '{benchmark_experiment['name']}': \"\n",
    "            f\"{len(finished_benchmark_runs)} finished run(s) found\\n\"\n",
    "            f\"    - Unique (study_hash, trial_hash) pairs: {len(benchmark_pairs)}\\n\"\n",
    "        )\n",
    "        \n",
    "        if hpo_run_counts:\n",
    "            error_msg += \"  - HPO experiments:\\n\"\n",
    "            for backbone, count in hpo_run_counts.items():\n",
    "                error_msg += f\"    - {backbone}: {count} finished run(s) found\\n\"\n",
    "            error_msg += (\n",
    "                f\"    - HPO trial runs: {len(hpo_trial_runs)} with {len(hpo_trial_pairs)} unique (study_hash, trial_hash) pairs\\n\"\n",
    "                f\"    - HPO refit runs: {len(hpo_refit_runs)} with {len(hpo_refit_pairs)} unique (study_hash, trial_hash) pairs\\n\"\n",
    "            )\n",
    "        \n",
    "        error_msg += (\n",
    "            f\"\\n  - Matching pairs: {len(matching_pairs)} out of {len(benchmark_pairs)} benchmark pairs\\n\"\n",
    "        )\n",
    "        \n",
    "        if len(matching_pairs) == 0 and len(benchmark_pairs) > 0 and len(hpo_trial_pairs) > 0:\n",
    "            # Show sample hashes for debugging\n",
    "            error_msg += \"\\n  Sample benchmark (study_hash, trial_hash) pairs:\\n\"\n",
    "            for i, (s, t) in enumerate(list(benchmark_pairs)[:3]):\n",
    "                error_msg += f\"    {i+1}. study={s[:16]}..., trial={t[:16]}...\\n\"\n",
    "            \n",
    "            error_msg += \"\\n  Sample HPO trial (study_hash, trial_hash) pairs:\\n\"\n",
    "            for i, (s, t) in enumerate(list(hpo_trial_pairs)[:3]):\n",
    "                error_msg += f\"    {i+1}. study={s[:16]}..., trial={t[:16]}...\\n\"\n",
    "            \n",
    "            error_msg += (\n",
    "                \"\\n  \u26a0\ufe0f  Hash mismatch detected! This usually means:\\n\"\n",
    "                \"     - Benchmark runs were created from different trials than current HPO runs\\n\"\n",
    "                \"     - Study or trial hashes changed between runs (e.g., Phase 2 migration)\\n\"\n",
    "                \"     - Solution: Re-run benchmarking on champions (Step 6) to create new benchmark runs\\n\"\n",
    "            )\n",
    "        \n",
    "        error_msg += (\n",
    "            \"\\nPossible causes:\\n\"\n",
    "            \"  1. No benchmark runs have been executed yet. Run benchmark jobs first.\\n\"\n",
    "            \"  2. Benchmark runs exist but are missing required metrics or grouping tags.\\n\"\n",
    "            \"  3. HPO runs exist but are missing required metrics or grouping tags.\\n\"\n",
    "            \"  4. No matching runs found between benchmark and HPO experiments (hash mismatch).\\n\"\n",
    "            \"\\nCheck the logs above for detailed information about what was found.\"\n",
    "        )\n",
    "        \n",
    "        raise ValueError(error_msg)\n",
    "    \n",
    "    # Save to cache\n",
    "    from selection.cache import save_best_model_cache\n",
    "    \n",
    "    tracking_uri = mlflow.get_tracking_uri()\n",
    "    # Note: inputs_summary could be enhanced if find_best_model_from_mlflow returns it\n",
    "    inputs_summary = {}\n",
    "    \n",
    "    timestamped_file, latest_file, index_file = save_best_model_cache(\n",
    "        root_dir=ROOT_DIR,\n",
    "        config_dir=CONFIG_DIR,\n",
    "        best_model=best_model,\n",
    "        experiment_name=experiment_name,\n",
    "        selection_config=selection_config,\n",
    "        tags_config=tags_config,\n",
    "        benchmark_experiment=benchmark_experiment,\n",
    "        hpo_experiments=hpo_experiments,\n",
    "        tracking_uri=tracking_uri,\n",
    "        inputs_summary=inputs_summary,\n",
    "    )\n",
    "    print(f\"\u2713 Saved best model selection to cache\")\n",
    "\n",
    "# Extract lineage information from best_model for final training tags\n",
    "from training_exec import extract_lineage_from_best_model\n",
    "lineage = extract_lineage_from_best_model(best_model)\n",
    "\n",
    "# Lineage extracted for final training tags\n",
    "\n",
    "# Acquire checkpoint\n",
    "best_checkpoint_dir = acquire_best_model_checkpoint(\n",
    "    best_run_info=best_model,\n",
    "    root_dir=ROOT_DIR,\n",
    "    config_dir=CONFIG_DIR,\n",
    "    acquisition_config=acquisition_config,\n",
    "    selection_config=selection_config,\n",
    "    platform=PLATFORM,\n",
    "    restore_from_drive=restore_from_drive if \"restore_from_drive\" in locals() else None,\n",
    "    drive_store=drive_store if \"drive_store\" in locals() else None,\n",
    "    in_colab=IN_COLAB,\n",
    ")\n",
    "\n",
    "print(f\"\\n\u2713 Best model checkpoint available at: {best_checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if selected run is already final training (skip retraining if so)\n",
    "stage_tag = tags_config.key(\"process\", \"stage\")\n",
    "trained_on_full_data_tag = tags_config.key(\"training\", \"trained_on_full_data\")\n",
    "\n",
    "is_final_training = best_model[\"tags\"].get(stage_tag) == \"final_training\"\n",
    "used_full_data = (\n",
    "    best_model[\"tags\"].get(trained_on_full_data_tag) == \"true\" or\n",
    "    best_model[\"params\"].get(\"use_combined_data\", \"false\").lower() == \"true\"\n",
    ")\n",
    "\n",
    "SKIP_FINAL_TRAINING = is_final_training and used_full_data\n",
    "\n",
    "if SKIP_FINAL_TRAINING:\n",
    "    final_checkpoint_dir = best_checkpoint_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Final Training\n",
    "\n",
    "Run final training with best configuration if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udd04 Starting final training with best configuration...\n",
      "\u2713 Final training config loaded from final_training.yaml\n",
      "\u2713 Output directory: /workspaces/resume-ner-azureml/outputs/final_training/local/distilroberta/spec-1e6acb58_exec-71e1ef24/v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-11 23:09:26,419 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}\n",
      "2026-01-11 23:09:26,420 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=final_training\n",
      "2026-01-11 23:09:26,678 - training.execution.mlflow_setup - INFO - \ud83c\udfc3 View run local_distilroberta_final_training_spec-1e6acb58_exec-71e1ef24_v1 at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/801daa4d-3a56-4952-a374-cf2c5a9c2846/runs/780e431b-06ae-4c03-b03f-46f34a8578b4\n",
      "2026-01-11 23:09:26,679 - training.execution.mlflow_setup - INFO - Created MLflow run: local_distilroberta_final_training_spec-1e6acb58_exec-71e1ef24_v1 (780e431b-06a...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Created MLflow run: local_distilroberta_final_training_spec-1e6acb58_exec-71e1ef24_v1 (780e431b-06a...)\n",
      "\ud83d\udd04 Running final training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-11 23:10:11,297 - training.execution.subprocess_runner - INFO - \ud83c\udfc3 View run local_distilroberta_final_training_spec-1e6acb58_exec-71e1ef24_v1 at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/801daa4d-3a56-4952-a374-cf2c5a9c2846/runs/780e431b-06ae-4c03-b03f-46f34a8578b4\n",
      "\ud83e\uddea View experiment at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/801daa4d-3a56-4952-a374-cf2c5a9c2846\n",
      "\n",
      "2026-01-11 23:10:11,298 - training.execution.subprocess_runner - WARNING - 2026-01-11 23:09:30,745 - infrastructure.tracking.mlflow.compatibility - INFO - Applied Azure ML artifact compatibility patch\n",
      "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/azureml/core/__init__.py:11: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "  [Training] Set MLflow tracking URI: azureml://germanywestcentral.api.azureml.ms/mlflow...\n",
      "  [Training] Set MLflow experiment: resume_ner_baseline-training\n",
      "  [Training] Using existing run: 780e431b-06a... (final training)\n",
      "  [Training] \u2713 Started run for artifact logging\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SKILL seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "  [Training] MLflow run detected: 780e431b-06a...\n",
      "2026-01-11 23:09:44,039 - infrastructure.tracking.mlflow.artifacts.uploader - INFO - Creating checkpoint archive from /workspaces/resume-ner-azureml/outputs/final_training/local/distilroberta/spec-1e6acb58_exec-71e1ef24/v1/checkpoint...\n",
      "2026-01-11 23:09:59,655 - infrastructure.tracking.mlflow.artifacts.manager - INFO - Created checkpoint archive: /tmp/checkpoint_n0_wab85.tar.gz (7 files, 315.6MB)\n",
      "2026-01-11 23:09:59,655 - infrastructure.tracking.mlflow._artifacts_file - INFO - Uploading checkpoint archive (289.8MB)...\n",
      "2026-01-11 23:10:06,988 - infrastructure.tracking.mlflow._artifacts_file - INFO - Successfully uploaded checkpoint archive: checkpoint_n0_wab85.tar.gz\n",
      "2026-01-11 23:10:06,989 - infrastructure.tracking.mlflow.artifacts.uploader - INFO - Successfully uploaded checkpoint archive: 7 files (315.6MB) for trial 0\n",
      "  [Training] \u2713 Logged checkpoint artifacts to MLflow\n",
      "  [Training] Ended independent run\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Saved metadata to: /workspaces/resume-ner-azureml/outputs/final_training/local/distilroberta/spec-1e6acb58_exec-71e1ef24/v1/metadata.json\n",
      "\u2713 Final training completed. Checkpoint: /workspaces/resume-ner-azureml/outputs/final_training/local/distilroberta/spec-1e6acb58_exec-71e1ef24/v1/checkpoint\n",
      "\u2713 MLflow run: 780e431b-06a...\n"
     ]
    }
   ],
   "source": [
    "if not SKIP_FINAL_TRAINING:\n",
    "    print(\"\ud83d\udd04 Starting final training with best configuration...\")\n",
    "    from training_exec import execute_final_training\n",
    "    # Execute final training (uses final_training.yaml via load_final_training_config)\n",
    "    # Will automatically reuse existing complete runs if run.mode: reuse_if_exists in final_training.yaml\n",
    "    final_checkpoint_dir = execute_final_training(\n",
    "        root_dir=ROOT_DIR,\n",
    "        config_dir=CONFIG_DIR,\n",
    "        best_model=best_model,\n",
    "        experiment_config=experiment_config,\n",
    "        lineage=lineage,\n",
    "        training_experiment_name=training_experiment_name,\n",
    "        platform=PLATFORM,\n",
    "    )\n",
    "else:\n",
    "    print(\"\u2713 Skipping final training - using selected checkpoint\")\n",
    "\n",
    "# Backup final checkpoint to Google Drive if in Colab\n",
    "if IN_COLAB and drive_store and final_checkpoint_dir:\n",
    "    checkpoint_path = Path(final_checkpoint_dir).resolve()\n",
    "    # Check if checkpoint is already in Drive\n",
    "    if str(checkpoint_path).startswith(\"/content/drive\"):\n",
    "        print(f\"\\n\u2713 Final training checkpoint is already in Google Drive\")\n",
    "        print(f\"  Drive path: {checkpoint_path}\")\n",
    "    else:\n",
    "        try:\n",
    "            print(f\"\\n\ud83d\udce6 Backing up final training checkpoint to Google Drive...\")\n",
    "            result = drive_store.backup(checkpoint_path, expect=\"dir\")\n",
    "            if result.ok:\n",
    "                print(f\"\u2713 Successfully backed up final checkpoint to Google Drive\")\n",
    "                print(f\"  Drive path: {result.dst}\")\n",
    "            else:\n",
    "                print(f\"\u26a0 Drive backup failed: {result.reason}\")\n",
    "                if result.error:\n",
    "                    print(f\"  Error: {result.error}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\u26a0 Drive backup error: {e}\")\n",
    "            print(f\"  Checkpoint is still available locally at: {final_checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Model Conversion & Optimization\n",
    "\n",
    "Convert the final trained model to ONNX format with optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Parent training: spec_fp=1e6acb58..., exec_fp=71e1ef24..., run_id=780e431b-06a...\n",
      "\n",
      "\ud83d\udd04 Starting model conversion...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_287188/2026750983.py:34: DeprecationWarning: conversion is deprecated, use deployment.conversion instead. This shim will be removed in 2 releases.\n",
      "  from conversion import execute_conversion\n",
      "[conversion.orchestration] Output directory: /workspaces/resume-ner-azureml/outputs/conversion/local/distilroberta/spec-1e6acb58_exec-71e1ef24/v1/conv-c62b1674\n",
      "2026-01-11 23:10:23,993 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}\n",
      "2026-01-11 23:10:23,994 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=conversion\n",
      "[conversion.orchestration] Created MLflow run: local_distilroberta_conversion_spec-1e6acb58_exec-71e1ef24_v1_conv-c62b1674 (8335e02b-59b...)\n",
      "[conversion.orchestration] Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /workspaces/resume-ner-azureml/outputs/final_training/local/distilroberta/spec-1e6acb58_exec-71e1ef24/v1/checkpoint --config-dir /workspaces/resume-ner-azureml/config --backbone distilroberta --output-dir /workspaces/resume-ner-azureml/outputs/conversion/local/distilroberta/spec-1e6acb58_exec-71e1ef24/v1/conv-c62b1674 --opset-version 18 --run-smoke-test\n",
      "[conversion.orchestration] 2026-01-11 23:10:27,537 - infrastructure.tracking.mlflow.compatibility - INFO - Applied Azure ML artifact compatibility patch\n",
      "[conversion.orchestration] [conversion.execution] Starting conversion: checkpoint='/workspaces/resume-ner-azureml/outputs/final_training/local/distilroberta/spec-1e6acb58_exec-71e1ef24/v1/checkpoint', backbone='distilroberta', quantize_int8=False, opset_version=18\n",
      "[conversion.orchestration] [conversion.execution] Resolving checkpoint directory from '/workspaces/resume-ner-azureml/outputs/final_training/local/distilroberta/spec-1e6acb58_exec-71e1ef24/v1/checkpoint'\n",
      "[conversion.orchestration] [conversion.execution] Output directory: '/workspaces/resume-ner-azureml/outputs/conversion/local/distilroberta/spec-1e6acb58_exec-71e1ef24/v1/conv-c62b1674/onnx_model'\n",
      "[conversion.orchestration] [conversion.execution] Azure ML scheme registration check failed (error: cannot import name '_tracking_store_registry' from 'mlflow.tracking._tracking_service.registry' (/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/registry.py)), but continuing with Azure ML run_id. Artifact logging may fail.\n",
      "[conversion.orchestration] [conversion.execution] Using MLflow run: 8335e02b-59b...\n",
      "[conversion.orchestration] [conversion.export] Starting ONNX export. quantize_int8=False\n",
      "[conversion.orchestration] [conversion.export] Output directory created at '/workspaces/resume-ner-azureml/outputs/conversion/local/distilroberta/spec-1e6acb58_exec-71e1ef24/v1/conv-c62b1674/onnx_model'\n",
      "[conversion.orchestration] [conversion.export] Loading tokenizer and model from checkpoint directory '/workspaces/resume-ner-azureml/outputs/final_training/local/distilroberta/spec-1e6acb58_exec-71e1ef24/v1/checkpoint'\n",
      "[conversion.orchestration] [conversion.export] Model and tokenizer successfully loaded; building example inputs for tracing\n",
      "[conversion.orchestration] [conversion.export] Exporting FP32 ONNX model to '/workspaces/resume-ner-azureml/outputs/conversion/local/distilroberta/spec-1e6acb58_exec-71e1ef24/v1/conv-c62b1674/onnx_model/model.onnx' (opset=18, dynamo=False)\n",
      "[conversion.orchestration] /opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:196: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "[conversion.orchestration]   inverted_mask = torch.tensor(1.0, dtype=dtype) - expanded_mask\n",
      "[conversion.orchestration] [conversion.export] FP32 ONNX export completed\n",
      "[conversion.orchestration] [conversion.export] Int8 quantization not requested; returning FP32 model\n",
      "[conversion.orchestration] [conversion.execution] Conversion completed. ONNX model: '/workspaces/resume-ner-azureml/outputs/conversion/local/distilroberta/spec-1e6acb58_exec-71e1ef24/v1/conv-c62b1674/onnx_model/model.onnx'\n",
      "[conversion.orchestration] [conversion.testing] Running ONNX smoke test for '/workspaces/resume-ner-azureml/outputs/conversion/local/distilroberta/spec-1e6acb58_exec-71e1ef24/v1/conv-c62b1674/onnx_model/model.onnx'\n",
      "[conversion.orchestration] [conversion.testing] ONNX smoke test completed successfully\n",
      "[conversion.orchestration] [conversion.execution] Smoke test passed\n",
      "[conversion.orchestration] [conversion.execution] Logged ONNX model to MLflow: /workspaces/resume-ner-azureml/outputs/conversion/local/distilroberta/spec-1e6acb58_exec-71e1ef24/v1/conv-c62b1674/onnx_model/model.onnx\n",
      "[conversion.orchestration] \ud83c\udfc3 View run local_distilroberta_conversion_spec-1e6acb58_exec-71e1ef24_v1_conv-c62b1674 at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/d97bba48-87dd-4664-a64f-ccb63be0279f/runs/8335e02b-59bf-4b85-a252-0b33578540c3\n",
      "[conversion.orchestration] \ud83e\uddea View experiment at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/d97bba48-87dd-4664-a64f-ccb63be0279f\n",
      "[conversion.orchestration] Conversion completed. ONNX model: /workspaces/resume-ner-azureml/outputs/conversion/local/distilroberta/spec-1e6acb58_exec-71e1ef24/v1/conv-c62b1674/onnx_model/model.onnx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u2713 Conversion completed successfully!\n",
      "  ONNX model: /workspaces/resume-ner-azureml/outputs/conversion/local/distilroberta/spec-1e6acb58_exec-71e1ef24/v1/conv-c62b1674/onnx_model/model.onnx\n",
      "  Model size: 311.14 MB\n"
     ]
    }
   ],
   "source": [
    "# Extract parent training information for conversion\n",
    "from common.shared.json_cache import load_json\n",
    "from pathlib import Path\n",
    "\n",
    "# Load metadata from final training output directory\n",
    "final_training_metadata_path = final_checkpoint_dir.parent / \"metadata.json\"\n",
    "\n",
    "if not final_training_metadata_path.exists():\n",
    "    raise ValueError(\n",
    "        f\"Metadata file not found: {final_training_metadata_path}\\n\"\n",
    "        \"Please ensure final training completed successfully.\"\n",
    "    )\n",
    "\n",
    "metadata = load_json(final_training_metadata_path)\n",
    "parent_spec_fp = metadata.get(\"spec_fp\")\n",
    "parent_exec_fp = metadata.get(\"exec_fp\")\n",
    "parent_training_run_id = metadata.get(\"mlflow\", {}).get(\"run_id\")\n",
    "\n",
    "if not parent_spec_fp or not parent_exec_fp:\n",
    "    raise ValueError(\n",
    "        f\"Missing required fingerprints in metadata: spec_fp={parent_spec_fp}, exec_fp={parent_exec_fp}\\n\"\n",
    "        \"Please ensure final training completed successfully.\"\n",
    "    )\n",
    "\n",
    "if parent_training_run_id:\n",
    "    print(f\"\u2713 Parent training: spec_fp={parent_spec_fp[:8]}..., exec_fp={parent_exec_fp[:8]}..., run_id={parent_training_run_id[:12]}...\")\n",
    "else:\n",
    "    print(f\"\u2713 Parent training: spec_fp={parent_spec_fp[:8]}..., exec_fp={parent_exec_fp[:8]}... (run_id not found)\")\n",
    "\n",
    "# Get parent training output directory (checkpoint parent)\n",
    "parent_training_output_dir = final_checkpoint_dir.parent\n",
    "\n",
    "print(f\"\\n\ud83d\udd04 Starting model conversion...\")\n",
    "from conversion import execute_conversion\n",
    "\n",
    "# Execute conversion (uses conversion.yaml via load_conversion_config)\n",
    "conversion_output_dir = execute_conversion(\n",
    "    root_dir=ROOT_DIR,\n",
    "    config_dir=CONFIG_DIR,\n",
    "    parent_training_output_dir=parent_training_output_dir,\n",
    "    parent_spec_fp=parent_spec_fp,\n",
    "    parent_exec_fp=parent_exec_fp,\n",
    "    experiment_config=experiment_config,\n",
    "    conversion_experiment_name=conversion_experiment_name,\n",
    "    platform=PLATFORM,\n",
    "    parent_training_run_id=parent_training_run_id,  # May be None, that's OK\n",
    ")\n",
    "\n",
    "# Find ONNX model file (search recursively, as model may be in onnx_model/ subdirectory)\n",
    "onnx_files = list(conversion_output_dir.rglob(\"*.onnx\"))\n",
    "if onnx_files:\n",
    "    onnx_model_path = onnx_files[0]\n",
    "    print(f\"\\n\u2713 Conversion completed successfully!\")\n",
    "    print(f\"  ONNX model: {onnx_model_path}\")\n",
    "    print(f\"  Model size: {onnx_model_path.stat().st_size / (1024 * 1024):.2f} MB\")\n",
    "else:\n",
    "    print(f\"\\n\u26a0 Warning: No ONNX model file found in {conversion_output_dir} (searched recursively)\")\n",
    "\n",
    "# Backup conversion output to Google Drive if in Colab\n",
    "if IN_COLAB and drive_store and conversion_output_dir:\n",
    "    output_path = Path(conversion_output_dir).resolve()\n",
    "    # Check if output is already in Drive\n",
    "    if str(output_path).startswith(\"/content/drive\"):\n",
    "        print(f\"\\n\u2713 Conversion output is already in Google Drive\")\n",
    "        print(f\"  Drive path: {output_path}\")\n",
    "    else:\n",
    "        try:\n",
    "            print(f\"\\n\ud83d\udce6 Backing up conversion output to Google Drive...\")\n",
    "            result = drive_store.backup(output_path, expect=\"dir\")\n",
    "            if result.ok:\n",
    "                print(f\"\u2713 Successfully backed up conversion output to Google Drive\")\n",
    "                print(f\"  Drive path: {result.dst}\")\n",
    "            else:\n",
    "                print(f\"\u26a0 Drive backup failed: {result.reason}\")\n",
    "                if result.error:\n",
    "                    print(f\"  Error: {result.error}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\u26a0 Drive backup error: {e}\")\n",
    "            print(f\"  Output is still available locally at: {conversion_output_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resume-ner-training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}