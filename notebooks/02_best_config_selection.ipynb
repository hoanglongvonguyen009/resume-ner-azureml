{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Best Configuration Selection (Local, Google Colab & Kaggle)\n",
    "\n",
    "This notebook automates the selection of the best model configuration from MLflow\n",
    "based on metrics and benchmarking results, then performs final training and model conversion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n",
    "**Prerequisites**: Run `01_orchestrate_training_colab.ipynb` first to:\n",
    "- Train models via HPO\n",
    "- Run benchmarking on best trials (using `evaluation.benchmarking.benchmark_best_trials`)\n",
    "\n",
    "Then this notebook:\n",
    "\n",
    "1. **Best Model Selection**: Query MLflow benchmark runs, join to training runs via grouping tags (`code.study_key_hash`, `code.trial_key_hash`), select best using normalized composite scoring\n",
    "2. **Artifact Acquisition**: Download the best model's checkpoint using fallback strategy (local disk ‚Üí drive restore ‚Üí MLflow download)\n",
    "3. **Final Training**: Optionally retrain with best config on full dataset (if not already final training)\n",
    "4. **Model Conversion**: Convert the final model to ONNX format using canonical path structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important\n",
    "\n",
    "- This notebook **executes on Local, Google Colab, or Kaggle** (not on Azure ML compute)\n",
    "- Requires MLflow tracking to be set up (Azure ML workspace or local SQLite)\n",
    "- All computation happens on the platform's GPU (if available) or CPU\n",
    "- **Storage & Persistence**:\n",
    "  - **Local**: Outputs saved to `outputs/` directory in repository root\n",
    "  - **Google Colab**: Checkpoints are automatically saved to Google Drive for persistence across sessions\n",
    "  - **Kaggle**: Outputs in `/kaggle/working/` are automatically persisted - no manual backup needed\n",
    "- The notebook must be **re-runnable end-to-end**\n",
    "- Uses the dataset path specified in the data config (from `config/data/*.yaml`), typically pointing to a local folder included in the repository\n",
    "- **Session Management**:\n",
    "  - **Local**: No session limits, outputs persist in repository\n",
    "  - **Colab**: Sessions timeout after 12-24 hours (depending on Colab plan). Checkpoints are saved to Drive automatically.\n",
    "  - **Kaggle**: Sessions have time limits based on your plan. All outputs are automatically saved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Detection\n",
    "\n",
    "The notebook automatically detects the execution environment (local, Google Colab, or Kaggle) and adapts its behavior accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Detected environment: LOCAL\n",
      "Platform: local\n",
      "Base directory: Current working directory\n",
      "Backup enabled: False\n"
     ]
    }
   ],
   "source": [
    "# Import environment detection functions\n",
    "# Bootstrap: Try to import, fallback to minimal detection if repo not cloned yet\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from common.shared.notebook_setup import (\n",
    "        get_platform_vars,\n",
    "        ensure_src_in_path,\n",
    "        detect_notebook_environment,\n",
    "        setup_notebook_paths,\n",
    "    )\n",
    "    \n",
    "    # Get platform variables and repository root\n",
    "    PLATFORM_VARS = get_platform_vars()\n",
    "    REPO_ROOT = ensure_src_in_path()\n",
    "    \n",
    "    # Get environment info\n",
    "    env = detect_notebook_environment()\n",
    "    PLATFORM = env.platform\n",
    "    IN_COLAB = env.is_colab\n",
    "    IN_KAGGLE = env.is_kaggle\n",
    "    IS_LOCAL = env.is_local\n",
    "    BASE_DIR = env.base_dir\n",
    "    BACKUP_ENABLED = env.backup_enabled\n",
    "    \n",
    "except ImportError:\n",
    "    # Bootstrap fallback: Minimal environment detection without imports\n",
    "    # This allows the notebook to work before repository is cloned\n",
    "    print(\"‚ö† Repository not cloned yet. Using bootstrap environment detection.\")\n",
    "    print(\"   Run the 'Repository Setup' cell to clone the repository.\")\n",
    "    \n",
    "    # Minimal environment detection\n",
    "    if \"COLAB_GPU\" in os.environ or \"COLAB_TPU\" in os.environ:\n",
    "        PLATFORM = \"colab\"\n",
    "        IN_COLAB = True\n",
    "        IN_KAGGLE = False\n",
    "        IS_LOCAL = False\n",
    "        BASE_DIR = Path(\"/content\")\n",
    "        BACKUP_ENABLED = True\n",
    "    elif \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ:\n",
    "        PLATFORM = \"kaggle\"\n",
    "        IN_COLAB = False\n",
    "        IN_KAGGLE = True\n",
    "        IS_LOCAL = False\n",
    "        BASE_DIR = Path(\"/kaggle/working\")\n",
    "        BACKUP_ENABLED = False\n",
    "    else:\n",
    "        PLATFORM = \"local\"\n",
    "        IN_COLAB = False\n",
    "        IN_KAGGLE = False\n",
    "        IS_LOCAL = True\n",
    "        BASE_DIR = Path.cwd()\n",
    "        BACKUP_ENABLED = False\n",
    "    \n",
    "    # Create minimal PLATFORM_VARS dict for compatibility\n",
    "    PLATFORM_VARS = {\n",
    "        \"platform\": PLATFORM,\n",
    "        \"is_colab\": IN_COLAB,\n",
    "        \"is_kaggle\": IN_KAGGLE,\n",
    "        \"is_local\": IS_LOCAL,\n",
    "        \"base_dir\": BASE_DIR,\n",
    "        \"backup_enabled\": BACKUP_ENABLED,\n",
    "    }\n",
    "    \n",
    "    REPO_ROOT = None\n",
    "\n",
    "print(f\"‚úì Detected environment: {PLATFORM.upper()}\")\n",
    "print(f\"Platform: {PLATFORM}\")\n",
    "print(f\"Base directory: {BASE_DIR if BASE_DIR else 'Current working directory'}\")\n",
    "print(f\"Backup enabled: {BACKUP_ENABLED}\")\n",
    "if REPO_ROOT is None:\n",
    "    print(\"‚ö† Repository root not found. Please run the Repository Setup cell.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Required Packages\n",
    "\n",
    "Install required packages based on the execution environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For local environment, please:\n",
      "1. Create conda environment: conda env create -f config/environment/conda.yaml\n",
      "2. Activate: conda activate resume-ner-training\n",
      "3. Restart kernel after activation\n",
      "\n",
      "If you've already done this, you can continue to the next cell.\n",
      "\n",
      "Installing Azure ML SDK (required for imports)...\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "if IS_LOCAL:\n",
    "    print(\"For local environment, please:\")\n",
    "    print(\"1. Create conda environment: conda env create -f config/environment/conda.yaml\")\n",
    "    print(\"2. Activate: conda activate resume-ner-training\")\n",
    "    print(\"3. Restart kernel after activation\")\n",
    "    print(\"\\nIf you've already done this, you can continue to the next cell.\")\n",
    "    print(\"\\nInstalling Azure ML SDK (required for imports)...\")\n",
    "    # Install Azure ML packages even for local (in case conda env not activated)\n",
    "    %pip install \"azure-ai-ml>=1.0.0\" --quiet\n",
    "    %pip install \"azure-identity>=1.12.0\" --quiet\n",
    "    %pip install azureml-defaults --quiet\n",
    "    %pip install azureml-mlflow --quiet\n",
    "else:\n",
    "    # Core ML libraries\n",
    "    %pip install \"transformers>=4.35.0,<5.0.0\" --quiet\n",
    "    %pip install \"safetensors>=0.4.0\" --quiet\n",
    "    %pip install \"datasets>=2.12.0\" --quiet\n",
    "\n",
    "    # ML utilities\n",
    "    %pip install \"numpy>=1.24.0,<2.0.0\" --quiet\n",
    "    %pip install \"pandas>=2.0.0\" --quiet\n",
    "    %pip install \"scikit-learn>=1.3.0\" --quiet\n",
    "\n",
    "    # Utilities\n",
    "    %pip install \"pyyaml>=6.0\" --quiet\n",
    "    %pip install \"tqdm>=4.65.0\" --quiet\n",
    "    %pip install \"seqeval>=1.2.2\" --quiet\n",
    "    %pip install \"sentencepiece>=0.1.99\" --quiet\n",
    "\n",
    "    # Experiment tracking\n",
    "    %pip install mlflow --quiet\n",
    "    %pip install optuna --quiet\n",
    "\n",
    "    # Azure ML SDK (required for orchestration imports)\n",
    "    %pip install \"azure-ai-ml>=1.0.0\" --quiet\n",
    "    %pip install \"azure-identity>=1.12.0\" --quiet\n",
    "    %pip install azureml-defaults --quiet\n",
    "    %pip install azureml-mlflow --quiet\n",
    "\n",
    "    # ONNX support\n",
    "    %pip install onnxruntime --quiet\n",
    "    %pip install \"onnx>=1.16.0\" --quiet\n",
    "    %pip install \"onnxscript>=0.1.0\" --quiet\n",
    "\n",
    "    print(\"‚úì All dependencies installed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Repository Setup\n",
    "\n",
    "**Note**: Repository setup is only needed for Colab/Kaggle environments. Local environments should already have the repository cloned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Local environment detected - detecting repository root...\n"
     ]
    }
   ],
   "source": [
    "# Repository setup - only needed for Colab/Kaggle\n",
    "# PLATFORM_VARS is set in Cell 2\n",
    "if not IS_LOCAL:\n",
    "    if IN_KAGGLE:\n",
    "        !git clone -b hpo_run_time_excl https://github.com/hoanglongvonguyen009/resume-ner-azureml.git /kaggle/working/resume-ner-azureml\n",
    "    elif IN_COLAB:\n",
    "        !git clone -b hpo_run_time_excl https://github.com/hoanglongvonguyen009/resume-ner-azureml.git /content/resume-ner-azureml\n",
    "else:\n",
    "    print(\"‚úì Local environment detected - detecting repository root...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Repository Setup\n",
    "\n",
    "Verify the repository structure exists:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get platform vars and repository root\n",
    "# PLATFORM_VARS is set in Cell 4\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "if 'REPO_ROOT' not in globals() or REPO_ROOT is None:\n",
    "    # Try to get repo root using ensure_src_in_path if available\n",
    "    if 'ensure_src_in_path' in globals():\n",
    "        try:\n",
    "            REPO_ROOT = ensure_src_in_path()\n",
    "        except (ImportError, ValueError):\n",
    "            REPO_ROOT = None\n",
    "    \n",
    "    # For local environments: manually detect repository root\n",
    "    if not REPO_ROOT and IS_LOCAL:\n",
    "        current_dir = Path.cwd()\n",
    "        # Check current directory first\n",
    "        if (current_dir / \"config\").exists() and (current_dir / \"src\").exists():\n",
    "            REPO_ROOT = current_dir\n",
    "        else:\n",
    "            # Check parent directories (in case notebook is in notebooks/ subdirectory)\n",
    "            for parent in current_dir.parents:\n",
    "                if (parent / \"config\").exists() and (parent / \"src\").exists():\n",
    "                    REPO_ROOT = parent\n",
    "                    break\n",
    "    \n",
    "    # Try expected location if not found (for Colab/Kaggle after cloning)\n",
    "    if not REPO_ROOT and not IS_LOCAL:\n",
    "        expected_path = BASE_DIR / \"resume-ner-azureml\"\n",
    "        if expected_path.exists() and (expected_path / \"config\").exists() and (expected_path / \"src\").exists():\n",
    "            src_dir = expected_path / \"src\"\n",
    "            if str(src_dir) not in sys.path:\n",
    "                sys.path.insert(0, str(src_dir))\n",
    "            REPO_ROOT = expected_path\n",
    "    \n",
    "    # Try to import and use setup functions if repo found\n",
    "    if REPO_ROOT:\n",
    "        try:\n",
    "            # Add src/ to path first so we can import setup_notebook_paths\n",
    "            src_dir = REPO_ROOT / \"src\"\n",
    "            if str(src_dir) not in sys.path:\n",
    "                sys.path.insert(0, str(src_dir))\n",
    "            \n",
    "            from common.shared.notebook_setup import setup_notebook_paths\n",
    "            paths = setup_notebook_paths(root_dir=REPO_ROOT, add_src_to_path=True)\n",
    "            ROOT_DIR = paths.root_dir\n",
    "            CONFIG_DIR = paths.config_dir\n",
    "            SRC_DIR = paths.src_dir\n",
    "            print(f\"‚úì Repository: {ROOT_DIR} (config={CONFIG_DIR.name}, src={SRC_DIR.name})\")\n",
    "            print(\"‚úì Repository structure verified\")\n",
    "        except (ImportError, ValueError) as e:\n",
    "            print(f\"‚ö† Could not setup paths: {e}\")\n",
    "            REPO_ROOT = None\n",
    "\n",
    "if 'REPO_ROOT' not in globals() or REPO_ROOT is None:\n",
    "    print(\"‚ö† Repository not found. Please run the Repository Setup cell (Cell 8) to clone the repository.\")\n",
    "    print(\"   After cloning, re-run this cell to verify the repository structure.\")\n",
    "    ROOT_DIR = None\n",
    "    CONFIG_DIR = None\n",
    "    SRC_DIR = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Configuration\n",
    "\n",
    "Load experiment configuration and define experiment naming convention.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded configs: experiment=resume_ner_baseline, tags, selection, conversion, acquisition, benchmark\n",
      "‚úì Experiment names: benchmark=resume_ner_baseline-benchmark, training=resume_ner_baseline-training, conversion=resume_ner_baseline-conversion\n"
     ]
    }
   ],
   "source": [
    "from infrastructure.config.loader import load_experiment_config\n",
    "from common.constants import EXPERIMENT_NAME\n",
    "from common.shared.yaml_utils import load_yaml\n",
    "from infrastructure.naming.mlflow.tags_registry import load_tags_registry\n",
    "\n",
    "# Load experiment config\n",
    "experiment_config = load_experiment_config(CONFIG_DIR, EXPERIMENT_NAME)\n",
    "\n",
    "# Load best model selection configs\n",
    "tags_config = load_tags_registry(CONFIG_DIR)\n",
    "selection_config = load_yaml(CONFIG_DIR / \"best_model_selection.yaml\")\n",
    "conversion_config = load_yaml(CONFIG_DIR / \"conversion.yaml\")\n",
    "acquisition_config = load_yaml(CONFIG_DIR / \"artifact_acquisition.yaml\")\n",
    "benchmark_config = load_yaml(CONFIG_DIR / \"benchmark.yaml\")\n",
    "\n",
    "print(f\"‚úì Loaded configs: experiment={experiment_config.name}, tags, selection, conversion, acquisition, benchmark\")\n",
    "\n",
    "# Define experiment names (discovery happens after MLflow setup in Cell 4)\n",
    "experiment_name = experiment_config.name\n",
    "benchmark_experiment_name = f\"{experiment_name}-benchmark\"\n",
    "training_experiment_name = f\"{experiment_name}-training\"  # For final training runs\n",
    "conversion_experiment_name = f\"{experiment_name}-conversion\"\n",
    "\n",
    "print(f\"‚úì Experiment names: benchmark={benchmark_experiment_name}, training={training_experiment_name}, conversion={conversion_experiment_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Setup MLflow\n",
    "\n",
    "Setup MLflow tracking with fallback to local if Azure ML is unavailable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-18 21:41:44,213 - common.shared.mlflow_setup - INFO - Azure ML enabled in config, attempting to connect...\n",
      "2026-01-18 21:41:44,215 - common.shared.mlflow_setup - WARNING - [DEBUG] Initial env check - subscription_id: True, resource_group: True, client_id: True, client_secret: True, tenant_id: True\n",
      "2026-01-18 21:41:44,216 - common.shared.mlflow_setup - WARNING - [DEBUG] Platform detected: local\n",
      "2026-01-18 21:41:44,216 - common.shared.mlflow_setup - WARNING - [DEBUG] Service Principal check - client_id present: True, client_secret present: True, tenant_id present: True, has_service_principal: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì azureml.mlflow is available - Azure ML tracking will be used if configured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-18 21:41:44,217 - common.shared.mlflow_setup - INFO - Using Service Principal authentication (from config.env)\n",
      "2026-01-18 21:41:44,408 - common.shared.mlflow_setup - INFO - Successfully connected to Azure ML workspace: resume-ner-ws\n",
      "2026-01-18 21:43:45,591 - common.shared.mlflow_setup - INFO - Using Azure ML workspace tracking\n",
      "2026-01-18 21:43:45,673 - evaluation.selection.experiment_discovery - INFO - Discovered 2 HPO experiment(s) for resume_ner_baseline\n",
      "2026-01-18 21:43:45,776 - evaluation.selection.experiment_discovery - INFO - Found benchmark experiment: resume_ner_baseline-benchmark\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì MLflow tracking URI: azureml://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws\n",
      "‚úì MLflow experiment: resume_ner_baseline-training\n",
      "‚úì Experiments: 2 HPO (distilbert, distilroberta), benchmark=found, training=resume_ner_baseline-training, conversion=resume_ner_baseline-conversion\n"
     ]
    }
   ],
   "source": [
    "# Check if azureml.mlflow is available\n",
    "try:\n",
    "    import azureml.mlflow  # noqa: F401\n",
    "    print(\"‚úì azureml.mlflow is available - Azure ML tracking will be used if configured\")\n",
    "except ImportError:\n",
    "    print(\"‚ö† azureml.mlflow is not available - will fallback to local SQLite tracking\")\n",
    "    print(\"  To use Azure ML tracking, install: pip install azureml-mlflow\")\n",
    "    print(\"  Then restart the kernel and re-run this cell\")\n",
    "\n",
    "from common.shared.mlflow_setup import setup_mlflow_from_config\n",
    "import mlflow\n",
    "\n",
    "# Setup MLflow tracking (use training experiment for setup - actual queries use discovered experiments)\n",
    "tracking_uri = setup_mlflow_from_config(\n",
    "    experiment_name=training_experiment_name,\n",
    "    config_dir=CONFIG_DIR,\n",
    "    fallback_to_local=True,\n",
    ")\n",
    "\n",
    "print(f\"‚úì MLflow tracking URI: {tracking_uri}\")\n",
    "print(f\"‚úì MLflow experiment: {training_experiment_name}\")\n",
    "\n",
    "# Discover HPO and benchmark experiments from MLflow (after setup)\n",
    "# NOTE: This cell is the SINGLE SOURCE OF TRUTH for hpo_experiments and benchmark_experiment\n",
    "# These variables are reused in:\n",
    "#   - Cell 16 (Step 6: Benchmarking) - uses hpo_experiments and benchmark_experiment\n",
    "#   - Cell 18 (Step 7: Best Model Selection) - uses hpo_experiments and benchmark_experiment\n",
    "# Do not rebuild these variables elsewhere - always reference them from this cell.\n",
    "from mlflow.tracking import MlflowClient\n",
    "from evaluation.selection.experiment_discovery import discover_all_experiments\n",
    "\n",
    "mlflow_client = MlflowClient()\n",
    "experiments = discover_all_experiments(\n",
    "    experiment_name=experiment_name,\n",
    "    mlflow_client=mlflow_client,\n",
    "    create_benchmark_if_missing=False,  # Don't auto-create, let benchmarking step handle it\n",
    ")\n",
    "\n",
    "hpo_experiments = experiments[\"hpo_experiments\"]\n",
    "benchmark_experiment = experiments[\"benchmark_experiment\"]\n",
    "\n",
    "hpo_backbones = \", \".join(hpo_experiments.keys())\n",
    "print(f\"‚úì Experiments: {len(hpo_experiments)} HPO ({hpo_backbones}), benchmark={'found' if benchmark_experiment else 'not found'}, training={training_experiment_name}, conversion={conversion_experiment_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Drive Backup Setup (Colab Only)\n",
    "\n",
    "Setup Google Drive backup/restore for Colab environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Local environment detected - outputs will be saved to repository (no Drive backup needed)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Fix numpy/pandas compatibility before importing infrastructure modules\n",
    "try:\n",
    "    from infrastructure.storage.drive import create_colab_store\n",
    "except (ValueError, ImportError) as e:\n",
    "    if \"numpy.dtype size changed\" in str(e) or \"numpy\" in str(e).lower():\n",
    "        print(\"‚ö† Numpy/pandas compatibility issue detected. Fixing...\")\n",
    "        import subprocess\n",
    "        import sys\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"--force-reinstall\", \"--no-cache-dir\", \"numpy>=1.24.0,<2.0.0\", \"pandas>=2.0.0\", \"--quiet\"])\n",
    "        print(\"‚úì Numpy/pandas reinstalled. Please restart the kernel and re-run this cell.\")\n",
    "        raise RuntimeError(\"Please restart kernel after numpy/pandas fix\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# Mount Google Drive and create backup store (Colab only - Kaggle doesn't need this)\n",
    "DRIVE_BACKUP_DIR = None\n",
    "drive_store = None\n",
    "restore_from_drive = None\n",
    "\n",
    "if IN_COLAB:\n",
    "    drive_store = create_colab_store(ROOT_DIR, CONFIG_DIR)\n",
    "    if drive_store:\n",
    "        BACKUP_ENABLED = True\n",
    "        DRIVE_BACKUP_DIR = drive_store.backup_root\n",
    "        # Create restore function wrapper\n",
    "        def restore_from_drive(local_path: Path, is_directory: bool = False) -> bool:\n",
    "            \"\"\"Restore file/directory from Drive backup.\"\"\"\n",
    "            try:\n",
    "                expect = \"dir\" if is_directory else \"file\"\n",
    "                result = drive_store.restore(local_path, expect=expect)\n",
    "                return result.ok\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö† Drive restore failed: {e}\")\n",
    "                return False\n",
    "        \n",
    "        # Create backup_to_drive wrapper function (standardized backup pattern)\n",
    "        def backup_to_drive(source_path: Path, is_directory: bool = False) -> bool:\n",
    "            \"\"\"Backup file/directory to Drive using DriveBackupStore.\"\"\"\n",
    "            if not BACKUP_ENABLED or drive_store is None:\n",
    "                return False\n",
    "            # Map is_directory to expect parameter\n",
    "            expect = \"dir\" if is_directory else \"file\"\n",
    "            result = drive_store.backup(source_path, expect=expect)\n",
    "            if result.ok:\n",
    "                print(f\"‚úì Backed up: {source_path.name}\")\n",
    "            return result.ok\n",
    "        \n",
    "        print(f\"‚úì Google Drive mounted\")\n",
    "        print(f\"‚úì Backup base directory: {DRIVE_BACKUP_DIR}\")\n",
    "        print(f\"\\nNote: All outputs/ will be mirrored to: {DRIVE_BACKUP_DIR / 'outputs'}\")\n",
    "    else:\n",
    "        BACKUP_ENABLED = False\n",
    "        print(\"‚ö† Warning: Could not mount Google Drive. Backup to Google Drive will be disabled.\")\n",
    "elif IN_KAGGLE:\n",
    "    print(\"‚úì Kaggle environment detected - outputs are automatically persisted (no Drive mount needed)\")\n",
    "    BACKUP_ENABLED = False\n",
    "else:\n",
    "    # Local environment\n",
    "    print(\"‚úì Local environment detected - outputs will be saved to repository (no Drive backup needed)\")\n",
    "    BACKUP_ENABLED = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Run Benchmarking on Champions (Optional)\n",
    "\n",
    "**Optional Step**: If you haven't run benchmarking in `01_orchestrate_training_colab.ipynb`, you can run it here before selecting the best model. This step will:\n",
    "1. Select champions (best trials) from HPO runs using Phase 2 selection logic\n",
    "2. Run benchmarking on each champion to measure inference performance\n",
    "3. Save benchmark results to MLflow for use in Step 7\n",
    "\n",
    "**Note**: If benchmark runs already exist in MLflow, you can skip this step and proceed directly to Step 7.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-18 21:43:45,877 - evaluation.selection.workflows.benchmarking_workflow - INFO - Running benchmarking workflow on champions\n",
      "2026-01-18 21:43:45,878 - evaluation.selection.workflows.benchmarking_workflow - INFO - Selecting champions for 2 backbone(s)\n",
      "2026-01-18 21:43:46,570 - evaluation.selection.trial_finder.mlflow_queries - INFO - No runs found with stage='hpo_trial' for distilbert, trying legacy stage='hpo'\n",
      "2026-01-18 21:43:47,076 - evaluation.selection.trial_finder.mlflow_queries - INFO - Found 38 runs with stage tag for distilbert (backbone=distilbert)\n",
      "2026-01-18 21:43:47,077 - evaluation.selection.trial_finder.champion_selection - INFO - Filtered out 13 parent run(s) (only child/trial runs have metrics). 25 child runs remaining.\n",
      "2026-01-18 21:43:47,078 - evaluation.selection.trial_finder.champion_selection - INFO - Grouped runs for distilbert: 0 v1 group(s), 2 v2 group(s)\n",
      "2026-01-18 21:43:47,087 - evaluation.selection.trial_finder.champion_selection - INFO - Found 2 eligible group(s) for distilbert (0 skipped due to min_trials requirement)\n",
      "2026-01-18 21:43:47,523 - evaluation.selection.artifact_unified.selectors - INFO - Found refit run 0c67e62a-a31... for trial 1c82a9ea-5c6... (selected latest from 1 refit run(s))\n",
      "2026-01-18 21:43:47,523 - evaluation.selection.trial_finder.champion_selection - INFO - Found refit run 0c67e62a-a31... for champion trial 1c82a9ea-5c6... (using SSOT selector: refit_preferred)\n",
      "2026-01-18 21:43:47,843 - evaluation.selection.trial_finder.mlflow_queries - INFO - No runs found with stage='hpo_trial' for distilroberta, trying legacy stage='hpo'\n",
      "2026-01-18 21:43:48,020 - evaluation.selection.trial_finder.mlflow_queries - INFO - Found 8 runs with stage tag for distilroberta (backbone=distilroberta)\n",
      "2026-01-18 21:43:48,021 - evaluation.selection.trial_finder.champion_selection - INFO - Filtered out 4 parent run(s) (only child/trial runs have metrics). 4 child runs remaining.\n",
      "2026-01-18 21:43:48,022 - evaluation.selection.trial_finder.champion_selection - INFO - Grouped runs for distilroberta: 0 v1 group(s), 1 v2 group(s)\n",
      "2026-01-18 21:43:48,022 - evaluation.selection.trial_finder.champion_selection - INFO - Found 1 eligible group(s) for distilroberta (0 skipped due to min_trials requirement)\n",
      "2026-01-18 21:43:48,330 - evaluation.selection.artifact_unified.selectors - INFO - Found refit run 09397cbe-140... for trial 1918d385-692... (selected latest from 1 refit run(s))\n",
      "2026-01-18 21:43:48,331 - evaluation.selection.trial_finder.champion_selection - INFO - Found refit run 09397cbe-140... for champion trial 1918d385-692... (using SSOT selector: refit_preferred)\n",
      "2026-01-18 21:43:48,561 - evaluation.benchmarking.existence_checker - WARNING - Found 1 finished benchmark run(s) by hash (fallback). Consider setting benchmark_key tag for more reliable idempotency. trial_key_hash=18075ea4fc583b8b...\n",
      "2026-01-18 21:43:48,561 - evaluation.benchmarking.filter - INFO - Skipping distilbert - benchmark already exists (trial_key_hash=18075ea4fc583b8b...)\n",
      "2026-01-18 21:43:48,683 - evaluation.benchmarking.existence_checker - WARNING - Found 1 finished benchmark run(s) by hash (fallback). Consider setting benchmark_key tag for more reliable idempotency. trial_key_hash=681375f8734ec607...\n",
      "2026-01-18 21:43:48,684 - evaluation.benchmarking.filter - INFO - Skipping distilroberta - benchmark already exists (trial_key_hash=681375f8734ec607...)\n",
      "2026-01-18 21:43:48,684 - evaluation.selection.workflows.benchmarking_workflow - INFO - Skipping 2 already-benchmarked champion(s)\n",
      "2026-01-18 21:43:48,685 - evaluation.selection.workflows.benchmarking_workflow - INFO - All champions already benchmarked\n"
     ]
    }
   ],
   "source": [
    "# Optional: Run benchmarking on champions if not already done\n",
    "# Skip this cell if benchmark runs already exist in MLflow\n",
    "\n",
    "RUN_BENCHMARKING = True  # Set to True to run benchmarking\n",
    "\n",
    "if RUN_BENCHMARKING:\n",
    "    # Reload module to ensure latest function signature (in case kernel has cached version)\n",
    "    import importlib\n",
    "    import evaluation.selection.workflows.benchmarking_workflow\n",
    "    importlib.reload(evaluation.selection.workflows.benchmarking_workflow)\n",
    "    \n",
    "    from evaluation.selection.workflows import run_benchmarking_workflow\n",
    "    from infrastructure.tracking.mlflow.trackers import MLflowBenchmarkTracker\n",
    "    from infrastructure.config.loader import load_all_configs\n",
    "    \n",
    "    # Load all configs\n",
    "    configs = load_all_configs(experiment_config)\n",
    "    data_config = configs.get(\"data\", {})\n",
    "    hpo_config = configs.get(\"hpo\", {})\n",
    "    \n",
    "    # Use benchmark_experiment from Cell 12 (single source of truth) if available\n",
    "    benchmark_experiment_name = f\"{experiment_name}-benchmark\"\n",
    "    if \"benchmark_experiment\" not in globals() or benchmark_experiment is None:\n",
    "        from evaluation.selection.experiment_discovery import discover_benchmark_experiment\n",
    "        benchmark_experiment = discover_benchmark_experiment(\n",
    "            experiment_name=experiment_name,\n",
    "            mlflow_client=mlflow_client,\n",
    "            create_if_missing=True,\n",
    "        )\n",
    "    \n",
    "    # Setup benchmark tracker\n",
    "    benchmark_tracker = MLflowBenchmarkTracker(benchmark_experiment_name)\n",
    "    \n",
    "    # Run benchmarking workflow\n",
    "    champions_to_benchmark = run_benchmarking_workflow(\n",
    "        hpo_experiments=hpo_experiments,\n",
    "        selection_config=selection_config,\n",
    "        benchmark_config=benchmark_config,\n",
    "        data_config=data_config,\n",
    "        hpo_config=hpo_config,\n",
    "        root_dir=ROOT_DIR,\n",
    "        config_dir=CONFIG_DIR,\n",
    "        experiment_name=experiment_name,\n",
    "        mlflow_client=mlflow_client,\n",
    "        benchmark_experiment=benchmark_experiment,\n",
    "        benchmark_tracker=benchmark_tracker,\n",
    "        backup_enabled=BACKUP_ENABLED,\n",
    "        backup_to_drive=backup_to_drive if \"backup_to_drive\" in locals() else None,\n",
    "        restore_from_drive=restore_from_drive if \"restore_from_drive\" in locals() else None,\n",
    "        in_colab=IN_COLAB,\n",
    "        platform=PLATFORM,\n",
    "    )\n",
    "    \n",
    "    # Store benchmarked champions for checkpoint reuse in Step 7\n",
    "    # Index by refit_run_id (primary) and (backbone, study_key_hash, trial_key_hash) (fallback)\n",
    "    BENCHMARKED_CHAMPIONS_BY_REFIT = {}\n",
    "    BENCHMARKED_CHAMPIONS_BY_KEYS = {}\n",
    "    \n",
    "    for backbone, champion_data in champions_to_benchmark.items():\n",
    "        champion = champion_data.get(\"champion\", {})\n",
    "        refit_run_id = champion.get(\"refit_run_id\")\n",
    "        checkpoint_path = champion.get(\"checkpoint_path\")\n",
    "        \n",
    "        if refit_run_id and checkpoint_path:\n",
    "            # Primary index: refit_run_id (most reliable)\n",
    "            BENCHMARKED_CHAMPIONS_BY_REFIT[refit_run_id] = {\n",
    "                \"checkpoint_path\": Path(checkpoint_path),\n",
    "                \"backbone\": backbone,\n",
    "                \"champion\": champion,\n",
    "            }\n",
    "            \n",
    "            # Fallback index: (backbone, study_key_hash, trial_key_hash)\n",
    "            study_key_hash = champion.get(\"study_key_hash\")\n",
    "            trial_key_hash = champion.get(\"trial_key_hash\")\n",
    "            if study_key_hash and trial_key_hash:\n",
    "                BENCHMARKED_CHAMPIONS_BY_KEYS[(backbone, study_key_hash, trial_key_hash)] = {\n",
    "                    \"checkpoint_path\": Path(checkpoint_path),\n",
    "                    \"refit_run_id\": refit_run_id,\n",
    "                    \"champion\": champion,\n",
    "                }\n",
    "    \n",
    "    if BENCHMARKED_CHAMPIONS_BY_REFIT:\n",
    "        print(f\"üíæ Stored {len(BENCHMARKED_CHAMPIONS_BY_REFIT)} benchmarked champion(s) for checkpoint reuse\")\n",
    "else:\n",
    "    print(\"‚è≠ Skipping benchmarking (RUN_BENCHMARKING=False).\")\n",
    "    print(\"   If benchmark runs don't exist, set RUN_BENCHMARKING=True or run benchmarking in notebook 01.\")\n",
    "    BENCHMARKED_CHAMPIONS_BY_REFIT = {}\n",
    "    BENCHMARKED_CHAMPIONS_BY_KEYS = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Best Model Selection\n",
    "\n",
    "Query MLflow benchmark runs (created by `01_orchestrate_training_colab.ipynb` or Step 6 above using `evaluation.benchmarking.benchmark_best_trials`), join to training runs via grouping tags, and select the best model using normalized composite scoring.\n",
    "\n",
    "**Note**: Benchmark runs must exist in MLflow before running this step. If no benchmark runs are found, either:\n",
    "- Set `RUN_BENCHMARKING=True` in Step 6 above, or\n",
    "- Go back to `01_orchestrate_training_colab.ipynb` and run the benchmarking step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-18 21:43:48,693 - evaluation.selection.workflows.selection_workflow - INFO - Best Model Selection Mode: force_new\n",
      "2026-01-18 21:43:48,695 - evaluation.selection.workflows.selection_workflow - INFO - Mode is 'force_new' - querying MLflow for fresh selection\n",
      "2026-01-18 21:43:48,697 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow\n",
      "2026-01-18 21:43:48,698 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: resume_ner_baseline-benchmark\n",
      "2026-01-18 21:43:48,699 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 2\n",
      "2026-01-18 21:43:48,699 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1\n",
      "2026-01-18 21:43:48,700 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30\n",
      "2026-01-18 21:43:48,700 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from config file, applied when multiple benchmark runs exist with same benchmark_key)\n",
      "2026-01-18 21:43:48,700 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...\n",
      "2026-01-18 21:43:48,765 - evaluation.selection.mlflow_selection - INFO - Found 2 finished benchmark runs\n",
      "2026-01-18 21:43:48,766 - evaluation.selection.mlflow_selection - INFO - Found 2 benchmark runs with required metrics and grouping tags\n",
      "2026-01-18 21:43:48,766 - evaluation.selection.mlflow_selection - INFO - Preloading trial and refit runs from HPO experiments...\n",
      "2026-01-18 21:43:49,069 - evaluation.selection.mlflow_selection - INFO - Built trial lookup with 29 unique (study_hash, trial_hash) pairs\n",
      "2026-01-18 21:43:49,069 - evaluation.selection.mlflow_selection - INFO - Built refit lookup with 16 unique (study_hash, trial_hash) pairs\n",
      "2026-01-18 21:43:49,070 - evaluation.selection.mlflow_selection - INFO - Grouped 2 benchmark runs into 2 unique groups (by study_key_hash, trial_key_hash, benchmark_key)\n",
      "2026-01-18 21:43:49,071 - evaluation.selection.mlflow_selection - INFO - Found 2 candidate(s) with both benchmark and training metrics\n",
      "2026-01-18 21:43:49,071 - evaluation.selection.mlflow_selection - INFO - Best model selected: backbone=distilbert, f1=0.5247, latency=172.19ms, composite=1.0000\n",
      "2026-01-18 21:43:49,074 - evaluation.selection.cache - INFO - Saved best model selection cache: /workspaces/resume-ner-azureml/outputs/cache/best_model_selection/latest_best_model_selection_cache.json\n",
      "2026-01-18 21:43:49,074 - evaluation.selection.workflows.selection_workflow - INFO - Saved best model selection to cache\n",
      "2026-01-18 21:43:49,075 - evaluation.selection.workflows.selection_workflow - INFO - Acquiring checkpoint for best model...\n",
      "2026-01-18 21:43:49,075 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: yes)\n",
      "2026-01-18 21:43:49,139 - evaluation.selection.artifact_unified.acquisition - INFO - Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=1c82a9ea-5c6..., study_key_hash=c3659fea..., trial_key_hash=18075ea4...\n",
      "2026-01-18 21:43:49,204 - evaluation.selection.artifact_unified.selectors - INFO - Found refit run 0c67e62a-a31... for trial 1c82a9ea-5c6... (selected latest from 1 refit run(s))\n",
      "2026-01-18 21:43:49,205 - evaluation.selection.artifact_unified.acquisition - INFO - Selected artifact run: artifact_run_id=0c67e62a-a31..., trial_run_id=1c82a9ea-5c6...\n",
      "2026-01-18 21:43:49,206 - evaluation.selection.artifact_unified.acquisition - INFO - Artifact acquisition config for checkpoint: priority=['local', 'drive', 'mlflow'], local.enabled=True, local.validate=True, drive.enabled=True, mlflow.enabled=True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Best model selected:\n",
      "   Run ID: 0c67e62a-a317-4c85-9628-1ee50e3bc07a\n",
      "   Experiment: resume_ner_baseline-hpo-distilbert\n",
      "   Backbone: distilbert\n",
      "   F1 Score: 0.5247\n",
      "   Latency: 172.19 ms\n",
      "   Composite Score: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-18 21:43:49,284 - evaluation.selection.artifact_unified.acquisition - INFO - Acquiring artifact from mlflow: run_id=0c67e62a-a31..., artifact_kind=checkpoint, backbone=distilbert\n",
      "2026-01-18 21:43:49,286 - evaluation.selection.artifact_unified.acquisition - INFO - Downloading artifact from MLflow: run_id=0c67e62a-a31..., destination=/workspaces/resume-ner-azureml/outputs/best_model_selection/local/distilbert/checkpoint_c3659fea_18075ea4\n",
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:04<00:00,  4.98s/it]\n",
      "2026-01-18 21:43:54,898 - evaluation.selection.artifact_unified.acquisition - INFO - Successfully downloaded artifact from MLflow to: /workspaces/resume-ner-azureml/outputs/best_model_selection/local/distilbert/checkpoint_c3659fea_18075ea4/best_trial_checkpoint.tar.gz\n",
      "2026-01-18 21:43:58,870 - evaluation.selection.artifact_unified.acquisition - INFO - Directory is a valid checkpoint: /workspaces/resume-ner-azureml/outputs/best_model_selection/local/distilbert/checkpoint_c3659fea_18075ea4\n",
      "2026-01-18 21:43:58,871 - evaluation.selection.artifact_unified.acquisition - INFO - Artifact validation passed: /workspaces/resume-ner-azureml/outputs/best_model_selection/local/distilbert/checkpoint_c3659fea_18075ea4\n",
      "2026-01-18 21:43:58,872 - evaluation.selection.workflows.selection_workflow - INFO - Best model checkpoint available at: /workspaces/resume-ner-azureml/outputs/best_model_selection/local/distilbert/checkpoint_c3659fea_18075ea4\n"
     ]
    }
   ],
   "source": [
    "from evaluation.selection.workflows import run_selection_workflow\n",
    "from training.execution import extract_lineage_from_best_model\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize dictionaries if not already created (e.g., if benchmarking was skipped)\n",
    "if \"BENCHMARKED_CHAMPIONS_BY_REFIT\" not in globals():\n",
    "    BENCHMARKED_CHAMPIONS_BY_REFIT = {}\n",
    "if \"BENCHMARKED_CHAMPIONS_BY_KEYS\" not in globals():\n",
    "    BENCHMARKED_CHAMPIONS_BY_KEYS = {}\n",
    "\n",
    "# Run selection workflow\n",
    "best_model, best_checkpoint_dir = run_selection_workflow(\n",
    "    benchmark_experiment=benchmark_experiment,\n",
    "    hpo_experiments=hpo_experiments,\n",
    "    selection_config=selection_config,\n",
    "    tags_config=tags_config,\n",
    "    root_dir=ROOT_DIR,\n",
    "    config_dir=CONFIG_DIR,\n",
    "    experiment_name=experiment_name,\n",
    "    acquisition_config=acquisition_config,\n",
    "    platform=PLATFORM,\n",
    "    backup_enabled=BACKUP_ENABLED,\n",
    "    backup_to_drive=backup_to_drive if \"backup_to_drive\" in locals() else None,\n",
    "    restore_from_drive=restore_from_drive if \"restore_from_drive\" in locals() else None,\n",
    "    in_colab=IN_COLAB,\n",
    "    benchmarked_champions_by_refit=BENCHMARKED_CHAMPIONS_BY_REFIT if \"BENCHMARKED_CHAMPIONS_BY_REFIT\" in globals() else None,\n",
    "    benchmarked_champions_by_keys=BENCHMARKED_CHAMPIONS_BY_KEYS if \"BENCHMARKED_CHAMPIONS_BY_KEYS\" in globals() else None,\n",
    ")\n",
    "\n",
    "# Extract lineage from best model for final training\n",
    "lineage = extract_lineage_from_best_model(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if selected run is already final training (skip retraining if so)\n",
    "stage_tag = tags_config.key(\"process\", \"stage\")\n",
    "trained_on_full_data_tag = tags_config.key(\"training\", \"trained_on_full_data\")\n",
    "\n",
    "is_final_training = best_model[\"tags\"].get(stage_tag) == \"final_training\"\n",
    "used_full_data = (\n",
    "    best_model[\"tags\"].get(trained_on_full_data_tag) == \"true\" or\n",
    "    best_model[\"params\"].get(\"use_combined_data\", \"false\").lower() == \"true\"\n",
    ")\n",
    "\n",
    "SKIP_FINAL_TRAINING = is_final_training and used_full_data\n",
    "\n",
    "if SKIP_FINAL_TRAINING:\n",
    "    final_checkpoint_dir = best_checkpoint_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Final Training\n",
    "\n",
    "Run final training with best configuration if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-18 21:44:01,597 - training.execution.executor - INFO - Final training config loaded from final_training.yaml\n",
      "2026-01-18 21:44:01,597 - training.execution.executor - INFO - Output directory: /workspaces/resume-ner-azureml/outputs/final_training/local/distilbert/spec-1e6acb58_exec-02136b6b/v1\n",
      "2026-01-18 21:44:01,653 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}\n",
      "2026-01-18 21:44:01,653 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=final_training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Starting final training with best configuration...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-18 21:44:01,826 - training.execution.mlflow_setup - INFO - üèÉ View run local_distilbert_final_training_spec-1e6acb58_exec-02136b6b_v1 at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/801daa4d-3a56-4952-a374-cf2c5a9c2846/runs/f3e57780-66ba-4bfc-beab-4f0516a65a8a\n",
      "2026-01-18 21:44:01,826 - training.execution.mlflow_setup - INFO - Created MLflow run: local_distilbert_final_training_spec-1e6acb58_exec-02136b6b_v1 (f3e57780-66b...)\n",
      "2026-01-18 21:44:01,827 - training.execution.executor - INFO - Created MLflow run: local_distilbert_final_training_spec-1e6acb58_exec-02136b6b_v1 (f3e57780-66b...)\n",
      "2026-01-18 21:44:01,878 - training.execution.executor - INFO - Running final training...\n",
      "2026-01-18 21:44:41,674 - training.execution.subprocess_runner - INFO - üèÉ View run local_distilbert_final_training_spec-1e6acb58_exec-02136b6b_v1 at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/801daa4d-3a56-4952-a374-cf2c5a9c2846/runs/f3e57780-66ba-4bfc-beab-4f0516a65a8a\n",
      "üß™ View experiment at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/801daa4d-3a56-4952-a374-cf2c5a9c2846\n",
      "\n",
      "2026-01-18 21:44:41,675 - training.execution.subprocess_runner - WARNING - 2026-01-18 21:44:08,621 - infrastructure.tracking.mlflow.compatibility - INFO - Applied Azure ML artifact compatibility patch\n",
      "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/azureml/core/__init__.py:11: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "  [Training] Set MLflow tracking URI: azureml://germanywestcentral.api.azureml.ms/mlflow...\n",
      "  [Training] Set MLflow experiment: resume_ner_baseline-training\n",
      "  [Training] Using existing run: f3e57780-66b... (final training)\n",
      "  [Training] ‚úì Started run for artifact logging\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: SKILL seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: NAME seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: EMAIL seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "  [Training] MLflow run detected: f3e57780-66b...\n",
      "2026-01-18 21:44:20,076 - infrastructure.tracking.mlflow.artifacts.uploader - INFO - Creating checkpoint archive from /workspaces/resume-ner-azureml/outputs/final_training/local/distilbert/spec-1e6acb58_exec-02136b6b/v1/checkpoint...\n",
      "2026-01-18 21:44:32,276 - infrastructure.tracking.mlflow.artifacts.manager - INFO - Created checkpoint archive: /tmp/checkpoint_am9zvm9_.tar.gz (6 files, 254.1MB)\n",
      "2026-01-18 21:44:32,276 - infrastructure.tracking.mlflow._artifacts_file - INFO - Uploading checkpoint archive (233.8MB)...\n",
      "2026-01-18 21:44:37,585 - infrastructure.tracking.mlflow._artifacts_file - INFO - Successfully uploaded checkpoint archive: checkpoint_am9zvm9_.tar.gz\n",
      "2026-01-18 21:44:37,585 - infrastructure.tracking.mlflow.artifacts.uploader - INFO - Successfully uploaded checkpoint archive: 6 files (254.1MB) for trial 0\n",
      "  [Training] ‚úì Logged checkpoint artifacts to MLflow\n",
      "  [Training] Ended independent run\n",
      "\n",
      "2026-01-18 21:44:41,780 - training.execution.executor - INFO - Saved metadata to: /workspaces/resume-ner-azureml/outputs/final_training/local/distilbert/spec-1e6acb58_exec-02136b6b/v1/metadata.json\n",
      "2026-01-18 21:44:41,781 - training.execution.executor - INFO - Final training completed. Checkpoint: /workspaces/resume-ner-azureml/outputs/final_training/local/distilbert/spec-1e6acb58_exec-02136b6b/v1/checkpoint\n",
      "2026-01-18 21:44:41,782 - training.execution.executor - INFO - MLflow run: f3e57780-66b...\n"
     ]
    }
   ],
   "source": [
    "if not SKIP_FINAL_TRAINING:\n",
    "    print(\"üîÑ Starting final training with best configuration...\")\n",
    "    from training.execution import run_final_training_workflow\n",
    "    # Execute final training (uses final_training.yaml via load_final_training_config)\n",
    "    # Will automatically reuse existing complete runs if run.mode: reuse_if_exists in final_training.yaml\n",
    "    # Backup to Drive is handled automatically by run_final_training_workflow() using standardized backup pattern\n",
    "    final_checkpoint_dir = run_final_training_workflow(\n",
    "        root_dir=ROOT_DIR,\n",
    "        config_dir=CONFIG_DIR,\n",
    "        best_model=best_model,\n",
    "        experiment_config=experiment_config,\n",
    "        lineage=lineage,\n",
    "        training_experiment_name=training_experiment_name,\n",
    "        platform=PLATFORM,\n",
    "        backup_enabled=BACKUP_ENABLED,\n",
    "        backup_to_drive=backup_to_drive if \"backup_to_drive\" in locals() else None,\n",
    "        restore_from_drive=restore_from_drive if \"restore_from_drive\" in locals() else None,\n",
    "        in_colab=IN_COLAB,\n",
    "    )\n",
    "else:\n",
    "    print(\"‚úì Skipping final training - using selected checkpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Model Conversion & Optimization\n",
    "\n",
    "Convert the final trained model to ONNX format with optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-18 21:44:41,798 - deployment.conversion.orchestration - INFO - Output directory: /workspaces/resume-ner-azureml/outputs/conversion/local/distilbert/spec-1e6acb58_exec-02136b6b/v1/conv-3a36b9a8\n",
      "2026-01-18 21:44:41,861 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}\n",
      "2026-01-18 21:44:41,862 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=conversion\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Parent training: spec_fp=1e6acb58..., exec_fp=02136b6b..., run_id=f3e57780-66b...\n",
      "\n",
      "üîÑ Starting model conversion...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-18 21:44:42,025 - deployment.conversion.orchestration - INFO - Created MLflow run: local_distilbert_conversion_spec-1e6acb58_exec-02136b6b_v1_conv-3a36b9a8 (71a5e2b8-7a5...)\n",
      "2026-01-18 21:44:42,025 - deployment.conversion.orchestration - INFO - Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /workspaces/resume-ner-azureml/outputs/final_training/local/distilbert/spec-1e6acb58_exec-02136b6b/v1/checkpoint --config-dir /workspaces/resume-ner-azureml/config --backbone distilbert --output-dir /workspaces/resume-ner-azureml/outputs/conversion/local/distilbert/spec-1e6acb58_exec-02136b6b/v1/conv-3a36b9a8 --opset-version 18 --run-smoke-test\n",
      "2026-01-18 21:44:46,687 - deployment.conversion.orchestration - WARNING - 2026-01-18 21:44:46,687 - infrastructure.tracking.mlflow.compatibility - INFO - Applied Azure ML artifact compatibility patch\n",
      "2026-01-18 21:44:51,337 - deployment.conversion.orchestration - WARNING - [conversion.execution] Starting conversion: checkpoint='/workspaces/resume-ner-azureml/outputs/final_training/local/distilbert/spec-1e6acb58_exec-02136b6b/v1/checkpoint', backbone='distilbert', quantize_int8=False, opset_version=18\n",
      "2026-01-18 21:44:51,339 - deployment.conversion.orchestration - WARNING - [conversion.execution] Resolving checkpoint directory from '/workspaces/resume-ner-azureml/outputs/final_training/local/distilbert/spec-1e6acb58_exec-02136b6b/v1/checkpoint'\n",
      "2026-01-18 21:44:51,339 - deployment.conversion.orchestration - WARNING - [conversion.execution] Output directory: '/workspaces/resume-ner-azureml/outputs/conversion/local/distilbert/spec-1e6acb58_exec-02136b6b/v1/conv-3a36b9a8/onnx_model'\n",
      "2026-01-18 21:44:51,340 - deployment.conversion.orchestration - WARNING - [conversion.execution] Azure ML scheme registration check failed (error: cannot import name '_tracking_store_registry' from 'mlflow.tracking._tracking_service.registry' (/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/registry.py)), but continuing with Azure ML run_id. Artifact logging may fail.\n",
      "2026-01-18 21:44:51,341 - deployment.conversion.orchestration - WARNING - [conversion.execution] Using MLflow run: 71a5e2b8-7a5...\n",
      "2026-01-18 21:44:52,735 - deployment.conversion.orchestration - WARNING - [conversion.export] Starting ONNX export. quantize_int8=False\n",
      "2026-01-18 21:44:52,736 - deployment.conversion.orchestration - WARNING - [conversion.export] Output directory created at '/workspaces/resume-ner-azureml/outputs/conversion/local/distilbert/spec-1e6acb58_exec-02136b6b/v1/conv-3a36b9a8/onnx_model'\n",
      "2026-01-18 21:44:52,736 - deployment.conversion.orchestration - WARNING - [conversion.export] Loading tokenizer and model from checkpoint directory '/workspaces/resume-ner-azureml/outputs/final_training/local/distilbert/spec-1e6acb58_exec-02136b6b/v1/checkpoint'\n",
      "2026-01-18 21:44:53,200 - deployment.conversion.orchestration - WARNING - [conversion.export] Model and tokenizer successfully loaded; building example inputs for tracing\n",
      "2026-01-18 21:44:53,201 - deployment.conversion.orchestration - WARNING - [conversion.export] Exporting FP32 ONNX model to '/workspaces/resume-ner-azureml/outputs/conversion/local/distilbert/spec-1e6acb58_exec-02136b6b/v1/conv-3a36b9a8/onnx_model/model.onnx' (opset=18, dynamo=False)\n",
      "2026-01-18 21:44:53,370 - deployment.conversion.orchestration - WARNING - /opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:196: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "2026-01-18 21:44:53,371 - deployment.conversion.orchestration - WARNING -   inverted_mask = torch.tensor(1.0, dtype=dtype) - expanded_mask\n",
      "2026-01-18 21:44:56,060 - deployment.conversion.orchestration - WARNING - [conversion.export] FP32 ONNX export completed\n",
      "2026-01-18 21:44:56,061 - deployment.conversion.orchestration - WARNING - [conversion.export] Int8 quantization not requested; returning FP32 model\n",
      "2026-01-18 21:44:56,075 - deployment.conversion.orchestration - WARNING - [conversion.execution] Conversion completed. ONNX model: '/workspaces/resume-ner-azureml/outputs/conversion/local/distilbert/spec-1e6acb58_exec-02136b6b/v1/conv-3a36b9a8/onnx_model/model.onnx'\n",
      "2026-01-18 21:44:56,076 - deployment.conversion.orchestration - WARNING - [conversion.testing] Running ONNX smoke test for '/workspaces/resume-ner-azureml/outputs/conversion/local/distilbert/spec-1e6acb58_exec-02136b6b/v1/conv-3a36b9a8/onnx_model/model.onnx'\n",
      "2026-01-18 21:44:56,553 - deployment.conversion.orchestration - WARNING - [conversion.testing] ONNX smoke test completed successfully\n",
      "2026-01-18 21:44:56,576 - deployment.conversion.orchestration - WARNING - [conversion.execution] Smoke test passed\n",
      "2026-01-18 21:45:03,711 - deployment.conversion.orchestration - WARNING - [conversion.execution] Logged ONNX model to MLflow: /workspaces/resume-ner-azureml/outputs/conversion/local/distilbert/spec-1e6acb58_exec-02136b6b/v1/conv-3a36b9a8/onnx_model/model.onnx\n",
      "2026-01-18 21:45:03,839 - deployment.conversion.orchestration - INFO - üèÉ View run local_distilbert_conversion_spec-1e6acb58_exec-02136b6b_v1_conv-3a36b9a8 at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/d97bba48-87dd-4664-a64f-ccb63be0279f/runs/71a5e2b8-7a57-4448-a588-e851012300ab\n",
      "2026-01-18 21:45:03,841 - deployment.conversion.orchestration - INFO - üß™ View experiment at: https://germanywestcentral.api.azureml.ms/mlflow/v2.0/subscriptions/50c06ef8-627b-46d5-b779-d07c9b398f75/resourceGroups/resume_ner_2026-01-02-16-47-05/providers/Microsoft.MachineLearningServices/workspaces/resume-ner-ws/#/experiments/d97bba48-87dd-4664-a64f-ccb63be0279f\n",
      "2026-01-18 21:45:06,752 - deployment.conversion.orchestration - INFO - Conversion completed. ONNX model: /workspaces/resume-ner-azureml/outputs/conversion/local/distilbert/spec-1e6acb58_exec-02136b6b/v1/conv-3a36b9a8/onnx_model/model.onnx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Conversion completed successfully!\n",
      "  ONNX model: /workspaces/resume-ner-azureml/outputs/conversion/local/distilbert/spec-1e6acb58_exec-02136b6b/v1/conv-3a36b9a8/onnx_model/model.onnx\n",
      "  Model size: 253.29 MB\n"
     ]
    }
   ],
   "source": [
    "# Extract parent training information for conversion\n",
    "from common.shared.json_cache import load_json\n",
    "from pathlib import Path\n",
    "\n",
    "# Load metadata from final training output directory\n",
    "final_training_metadata_path = final_checkpoint_dir.parent / \"metadata.json\"\n",
    "\n",
    "if not final_training_metadata_path.exists():\n",
    "    raise ValueError(\n",
    "        f\"Metadata file not found: {final_training_metadata_path}\\n\"\n",
    "        \"Please ensure final training completed successfully.\"\n",
    "    )\n",
    "\n",
    "metadata = load_json(final_training_metadata_path)\n",
    "parent_spec_fp = metadata.get(\"spec_fp\")\n",
    "parent_exec_fp = metadata.get(\"exec_fp\")\n",
    "parent_training_run_id = metadata.get(\"mlflow\", {}).get(\"run_id\")\n",
    "\n",
    "if not parent_spec_fp or not parent_exec_fp:\n",
    "    raise ValueError(\n",
    "        f\"Missing required fingerprints in metadata: spec_fp={parent_spec_fp}, exec_fp={parent_exec_fp}\\n\"\n",
    "        \"Please ensure final training completed successfully.\"\n",
    "    )\n",
    "\n",
    "if parent_training_run_id:\n",
    "    print(f\"‚úì Parent training: spec_fp={parent_spec_fp[:8]}..., exec_fp={parent_exec_fp[:8]}..., run_id={parent_training_run_id[:12]}...\")\n",
    "else:\n",
    "    print(f\"‚úì Parent training: spec_fp={parent_spec_fp[:8]}..., exec_fp={parent_exec_fp[:8]}... (run_id not found)\")\n",
    "\n",
    "# Get parent training output directory (checkpoint parent)\n",
    "parent_training_output_dir = final_checkpoint_dir.parent\n",
    "\n",
    "print(f\"\\nüîÑ Starting model conversion...\")\n",
    "from deployment.conversion import run_conversion_workflow\n",
    "\n",
    "# Execute conversion (uses conversion.yaml via load_conversion_config)\n",
    "# Backup to Drive is handled automatically by run_conversion_workflow() using standardized backup pattern\n",
    "conversion_output_dir = run_conversion_workflow(\n",
    "    root_dir=ROOT_DIR,\n",
    "    config_dir=CONFIG_DIR,\n",
    "    parent_training_output_dir=parent_training_output_dir,\n",
    "    parent_spec_fp=parent_spec_fp,\n",
    "    parent_exec_fp=parent_exec_fp,\n",
    "    experiment_config=experiment_config,\n",
    "    conversion_experiment_name=conversion_experiment_name,\n",
    "    platform=PLATFORM,\n",
    "    parent_training_run_id=parent_training_run_id,  # May be None, that's OK\n",
    "    backup_enabled=BACKUP_ENABLED,\n",
    "    backup_to_drive=backup_to_drive if \"backup_to_drive\" in locals() else None,\n",
    "    restore_from_drive=restore_from_drive if \"restore_from_drive\" in locals() else None,\n",
    "    in_colab=IN_COLAB,\n",
    ")\n",
    "\n",
    "# Find ONNX model file (search recursively, as model may be in onnx_model/ subdirectory)\n",
    "onnx_files = list(conversion_output_dir.rglob(\"*.onnx\"))\n",
    "if onnx_files:\n",
    "    onnx_model_path = onnx_files[0]\n",
    "    print(f\"\\n‚úì Conversion completed successfully!\")\n",
    "    print(f\"  ONNX model: {onnx_model_path}\")\n",
    "    print(f\"  Model size: {onnx_model_path.stat().st_size / (1024 * 1024):.2f} MB\")\n",
    "else:\n",
    "    print(f\"\\n‚ö† Warning: No ONNX model file found in {conversion_output_dir} (searched recursively)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resume-ner-training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
