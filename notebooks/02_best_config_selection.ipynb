{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Best Configuration Selection (Local, Google Colab & Kaggle)\n",
    "\n",
    "This notebook automates the selection of the best model configuration from MLflow\n",
    "based on metrics and benchmarking results, then performs final training and model conversion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n",
    "**Prerequisites**: Run `01_orchestrate_training_colab.ipynb` first to:\n",
    "- Train models via HPO\n",
    "- Run benchmarking on best trials (using `evaluation.benchmarking.benchmark_best_trials`)\n",
    "\n",
    "Then this notebook:\n",
    "\n",
    "1. **Best Model Selection**: Query MLflow benchmark runs, join to training runs via grouping tags (`code.study_key_hash`, `code.trial_key_hash`), select best using normalized composite scoring\n",
    "2. **Artifact Acquisition**: Download the best model's checkpoint using fallback strategy (local disk ‚Üí drive restore ‚Üí MLflow download)\n",
    "3. **Final Training**: Optionally retrain with best config on full dataset (if not already final training)\n",
    "4. **Model Conversion**: Convert the final model to ONNX format using canonical path structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important\n",
    "\n",
    "- This notebook **executes on Local, Google Colab, or Kaggle** (not on Azure ML compute)\n",
    "- Requires MLflow tracking to be set up (Azure ML workspace or local SQLite)\n",
    "- All computation happens on the platform's GPU (if available) or CPU\n",
    "- **Storage & Persistence**:\n",
    "  - **Local**: Outputs saved to `outputs/` directory in repository root\n",
    "  - **Google Colab**: Checkpoints are automatically saved to Google Drive for persistence across sessions\n",
    "  - **Kaggle**: Outputs in `/kaggle/working/` are automatically persisted - no manual backup needed\n",
    "- The notebook must be **re-runnable end-to-end**\n",
    "- Uses the dataset path specified in the data config (from `config/data/*.yaml`), typically pointing to a local folder included in the repository\n",
    "- **Session Management**:\n",
    "  - **Local**: No session limits, outputs persist in repository\n",
    "  - **Colab**: Sessions timeout after 12-24 hours (depending on Colab plan). Checkpoints are saved to Drive automatically.\n",
    "  - **Kaggle**: Sessions have time limits based on your plan. All outputs are automatically saved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Detection\n",
    "\n",
    "The notebook automatically detects the execution environment (local, Google Colab, or Kaggle) and adapts its behavior accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Bootstrap: Find repository root and add src/ to Python path\n",
    "# This must happen before importing from common or infrastructure\n",
    "def find_repo_root() -> Path:\n",
    "    \"\"\"Find repository root by searching for config/ and src/ directories.\"\"\"\n",
    "    current_dir = Path.cwd()\n",
    "    # Check current directory first\n",
    "    if (current_dir / \"config\").exists() and (current_dir / \"src\").exists():\n",
    "        return current_dir\n",
    "    # Search up the directory tree\n",
    "    for parent in current_dir.parents:\n",
    "        if (parent / \"config\").exists() and (parent / \"src\").exists():\n",
    "            return parent\n",
    "    raise ValueError(f\"Could not find repository root. Searched from: {current_dir}\")\n",
    "\n",
    "# Find repo root and add src to path\n",
    "ROOT_DIR = find_repo_root()\n",
    "SRC_DIR = ROOT_DIR / \"src\"\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "# Now we can import from common\n",
    "from common.shared.notebook_setup import detect_notebook_environment\n",
    "\n",
    "# Detect execution environment\n",
    "env = detect_notebook_environment()\n",
    "PLATFORM = env.platform\n",
    "IN_COLAB = env.is_colab\n",
    "IN_KAGGLE = env.is_kaggle\n",
    "IS_LOCAL = env.is_local\n",
    "BASE_DIR = env.base_dir\n",
    "BACKUP_ENABLED = env.backup_enabled\n",
    "\n",
    "print(f\"‚úì Detected environment: {PLATFORM.upper()}\")\n",
    "print(f\"Platform: {PLATFORM}\")\n",
    "print(f\"Base directory: {BASE_DIR if BASE_DIR else 'Current working directory'}\")\n",
    "print(f\"Backup enabled: {BACKUP_ENABLED}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Required Packages\n",
    "\n",
    "Install required packages based on the execution environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "if IS_LOCAL:\n",
    "    print(\"For local environment, please:\")\n",
    "    print(\"1. Create conda environment: conda env create -f config/environment/conda.yaml\")\n",
    "    print(\"2. Activate: conda activate resume-ner-training\")\n",
    "    print(\"3. Restart kernel after activation\")\n",
    "    print(\"\\nIf you've already done this, you can continue to the next cell.\")\n",
    "    print(\"\\nInstalling Azure ML SDK (required for imports)...\")\n",
    "    # Install Azure ML packages even for local (in case conda env not activated)\n",
    "    %pip install \"azure-ai-ml>=1.0.0\" --quiet\n",
    "    %pip install \"azure-identity>=1.12.0\" --quiet\n",
    "    %pip install azureml-defaults --quiet\n",
    "    %pip install azureml-mlflow --quiet\n",
    "else:\n",
    "    # Core ML libraries\n",
    "    %pip install \"transformers>=4.35.0,<5.0.0\" --quiet\n",
    "    %pip install \"safetensors>=0.4.0\" --quiet\n",
    "    %pip install \"datasets>=2.12.0\" --quiet\n",
    "\n",
    "    # ML utilities\n",
    "    %pip install \"numpy>=1.24.0,<2.0.0\" --quiet\n",
    "    %pip install \"pandas>=2.0.0\" --quiet\n",
    "    %pip install \"scikit-learn>=1.3.0\" --quiet\n",
    "\n",
    "    # Utilities\n",
    "    %pip install \"pyyaml>=6.0\" --quiet\n",
    "    %pip install \"tqdm>=4.65.0\" --quiet\n",
    "    %pip install \"seqeval>=1.2.2\" --quiet\n",
    "    %pip install \"sentencepiece>=0.1.99\" --quiet\n",
    "\n",
    "    # Experiment tracking\n",
    "    %pip install mlflow --quiet\n",
    "    %pip install optuna --quiet\n",
    "\n",
    "    # Azure ML SDK (required for orchestration imports)\n",
    "    %pip install \"azure-ai-ml>=1.0.0\" --quiet\n",
    "    %pip install \"azure-identity>=1.12.0\" --quiet\n",
    "    %pip install azureml-defaults --quiet\n",
    "    %pip install azureml-mlflow --quiet\n",
    "\n",
    "    # ONNX support\n",
    "    %pip install onnxruntime --quiet\n",
    "    %pip install \"onnx>=1.16.0\" --quiet\n",
    "    %pip install \"onnxscript>=0.1.0\" --quiet\n",
    "\n",
    "    print(\"‚úì All dependencies installed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Repository Setup\n",
    "\n",
    "**Note**: Repository setup is only needed for Colab/Kaggle environments. Local environments should already have the repository cloned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repository setup - only needed for Colab/Kaggle\n",
    "if not IS_LOCAL:\n",
    "    if IN_KAGGLE:\n",
    "        !git clone -b gg_final_training_2 https://github.com/longdang193/resume-ner-azureml.git /kaggle/working/resume-ner-azureml\n",
    "    elif IN_COLAB:\n",
    "        !git clone -b gg_final_training_2 https://github.com/longdang193/resume-ner-azureml.git /content/resume-ner-azureml\n",
    "else:\n",
    "    print(\"‚úì Local environment detected - detecting repository root...\")\n",
    "\n",
    "# Set up paths using extracted utility\n",
    "from common.shared.notebook_setup import setup_notebook_paths\n",
    "\n",
    "paths = setup_notebook_paths(add_src_to_path=True)\n",
    "ROOT_DIR = paths.root_dir\n",
    "CONFIG_DIR = paths.config_dir\n",
    "SRC_DIR = paths.src_dir\n",
    "\n",
    "print(f\"‚úì Repository: {ROOT_DIR} (config={CONFIG_DIR.name}, src={SRC_DIR.name})\")\n",
    "print(\"‚úì Repository structure verified\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Configuration\n",
    "\n",
    "Load experiment configuration and define experiment naming convention.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from infrastructure.config.loader import load_experiment_config\n",
    "from common.constants import EXPERIMENT_NAME\n",
    "from common.shared.yaml_utils import load_yaml\n",
    "from infrastructure.naming.mlflow.tags_registry import load_tags_registry\n",
    "\n",
    "# Load experiment config\n",
    "experiment_config = load_experiment_config(CONFIG_DIR, EXPERIMENT_NAME)\n",
    "\n",
    "# Load best model selection configs\n",
    "tags_config = load_tags_registry(CONFIG_DIR)\n",
    "selection_config = load_yaml(CONFIG_DIR / \"best_model_selection.yaml\")\n",
    "conversion_config = load_yaml(CONFIG_DIR / \"conversion.yaml\")\n",
    "acquisition_config = load_yaml(CONFIG_DIR / \"artifact_acquisition.yaml\")\n",
    "benchmark_config = load_yaml(CONFIG_DIR / \"benchmark.yaml\")\n",
    "\n",
    "print(f\"‚úì Loaded configs: experiment={experiment_config.name}, tags, selection, conversion, acquisition, benchmark\")\n",
    "\n",
    "# Define experiment names (discovery happens after MLflow setup in Cell 4)\n",
    "experiment_name = experiment_config.name\n",
    "benchmark_experiment_name = f\"{experiment_name}-benchmark\"\n",
    "training_experiment_name = f\"{experiment_name}-training\"  # For final training runs\n",
    "conversion_experiment_name = f\"{experiment_name}-conversion\"\n",
    "\n",
    "print(f\"‚úì Experiment names: benchmark={benchmark_experiment_name}, training={training_experiment_name}, conversion={conversion_experiment_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Setup MLflow\n",
    "\n",
    "Setup MLflow tracking with fallback to local if Azure ML is unavailable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if azureml.mlflow is available\n",
    "try:\n",
    "    import azureml.mlflow  # noqa: F401\n",
    "    print(\"‚úì azureml.mlflow is available - Azure ML tracking will be used if configured\")\n",
    "except ImportError:\n",
    "    print(\"‚ö† azureml.mlflow is not available - will fallback to local SQLite tracking\")\n",
    "    print(\"  To use Azure ML tracking, install: pip install azureml-mlflow\")\n",
    "    print(\"  Then restart the kernel and re-run this cell\")\n",
    "\n",
    "from common.shared.mlflow_setup import setup_mlflow_from_config\n",
    "import mlflow\n",
    "\n",
    "# Setup MLflow tracking (use training experiment for setup - actual queries use discovered experiments)\n",
    "tracking_uri = setup_mlflow_from_config(\n",
    "    experiment_name=training_experiment_name,\n",
    "    config_dir=CONFIG_DIR,\n",
    "    fallback_to_local=True,\n",
    ")\n",
    "\n",
    "print(f\"‚úì MLflow tracking URI: {tracking_uri}\")\n",
    "print(f\"‚úì MLflow experiment: {training_experiment_name}\")\n",
    "\n",
    "# Discover HPO and benchmark experiments from MLflow (after setup)\n",
    "# NOTE: This cell is the SINGLE SOURCE OF TRUTH for hpo_experiments and benchmark_experiment\n",
    "# These variables are reused in:\n",
    "#   - Cell 16 (Step 6: Benchmarking) - uses hpo_experiments and benchmark_experiment\n",
    "#   - Cell 18 (Step 7: Best Model Selection) - uses hpo_experiments and benchmark_experiment\n",
    "# Do not rebuild these variables elsewhere - always reference them from this cell.\n",
    "from mlflow.tracking import MlflowClient\n",
    "from evaluation.selection.experiment_discovery import discover_all_experiments\n",
    "\n",
    "mlflow_client = MlflowClient()\n",
    "experiments = discover_all_experiments(\n",
    "    experiment_name=experiment_name,\n",
    "    mlflow_client=mlflow_client,\n",
    "    create_benchmark_if_missing=False,  # Don't auto-create, let benchmarking step handle it\n",
    ")\n",
    "\n",
    "hpo_experiments = experiments[\"hpo_experiments\"]\n",
    "benchmark_experiment = experiments[\"benchmark_experiment\"]\n",
    "\n",
    "hpo_backbones = \", \".join(hpo_experiments.keys())\n",
    "print(f\"‚úì Experiments: {len(hpo_experiments)} HPO ({hpo_backbones}), benchmark={'found' if benchmark_experiment else 'not found'}, training={training_experiment_name}, conversion={conversion_experiment_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Drive Backup Setup (Colab Only)\n",
    "\n",
    "Setup Google Drive backup/restore for Colab environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Fix numpy/pandas compatibility before importing orchestration modules\n",
    "try:\n",
    "    from infrastructure.storage.drive import create_colab_store\n",
    "except (ValueError, ImportError) as e:\n",
    "    if \"numpy.dtype size changed\" in str(e) or \"numpy\" in str(e).lower():\n",
    "        print(\"‚ö† Numpy/pandas compatibility issue detected. Fixing...\")\n",
    "        import subprocess\n",
    "        import sys\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"--force-reinstall\", \"--no-cache-dir\", \"numpy>=1.24.0,<2.0.0\", \"pandas>=2.0.0\", \"--quiet\"])\n",
    "        print(\"‚úì Numpy/pandas reinstalled. Please restart the kernel and re-run this cell.\")\n",
    "        raise RuntimeError(\"Please restart kernel after numpy/pandas fix\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# Mount Google Drive and create backup store (Colab only - Kaggle doesn't need this)\n",
    "DRIVE_BACKUP_DIR = None\n",
    "drive_store = None\n",
    "restore_from_drive = None\n",
    "\n",
    "if IN_COLAB:\n",
    "    drive_store = create_colab_store(ROOT_DIR, CONFIG_DIR)\n",
    "    if drive_store:\n",
    "        BACKUP_ENABLED = True\n",
    "        DRIVE_BACKUP_DIR = drive_store.backup_root\n",
    "        # Create restore function wrapper\n",
    "        def restore_from_drive(local_path: Path, is_directory: bool = False) -> bool:\n",
    "            \"\"\"Restore file/directory from Drive backup.\"\"\"\n",
    "            try:\n",
    "                expect = \"dir\" if is_directory else \"file\"\n",
    "                result = drive_store.restore(local_path, expect=expect)\n",
    "                return result.ok\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö† Drive restore failed: {e}\")\n",
    "                return False\n",
    "        print(f\"‚úì Google Drive mounted\")\n",
    "        print(f\"‚úì Backup base directory: {DRIVE_BACKUP_DIR}\")\n",
    "        print(f\"\\nNote: All outputs/ will be mirrored to: {DRIVE_BACKUP_DIR / 'outputs'}\")\n",
    "    else:\n",
    "        BACKUP_ENABLED = False\n",
    "        print(\"‚ö† Warning: Could not mount Google Drive. Backup to Google Drive will be disabled.\")\n",
    "elif IN_KAGGLE:\n",
    "    print(\"‚úì Kaggle environment detected - outputs are automatically persisted (no Drive mount needed)\")\n",
    "    BACKUP_ENABLED = False\n",
    "else:\n",
    "    # Local environment\n",
    "    print(\"‚úì Local environment detected - outputs will be saved to repository (no Drive backup needed)\")\n",
    "    BACKUP_ENABLED = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Run Benchmarking on Champions (Optional)\n",
    "\n",
    "**Optional Step**: If you haven't run benchmarking in `01_orchestrate_training_colab.ipynb`, you can run it here before selecting the best model. This step will:\n",
    "1. Select champions (best trials) from HPO runs using Phase 2 selection logic\n",
    "2. Run benchmarking on each champion to measure inference performance\n",
    "3. Save benchmark results to MLflow for use in Step 7\n",
    "\n",
    "**Note**: If benchmark runs already exist in MLflow, you can skip this step and proceed directly to Step 7.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Run benchmarking on champions if not already done\n",
    "# Skip this cell if benchmark runs already exist in MLflow\n",
    "\n",
    "RUN_BENCHMARKING = True  # Set to True to run benchmarking\n",
    "\n",
    "if RUN_BENCHMARKING:\n",
    "    from evaluation.selection.workflows import run_benchmarking_workflow\n",
    "    from infrastructure.tracking.mlflow.trackers import MLflowBenchmarkTracker\n",
    "    from infrastructure.config.loader import load_all_configs\n",
    "    \n",
    "    # Load all configs\n",
    "    configs = load_all_configs(experiment_config)\n",
    "    data_config = configs.get(\"data\", {})\n",
    "    hpo_config = configs.get(\"hpo\", {})\n",
    "    \n",
    "    # Use benchmark_experiment from Cell 12 (single source of truth) if available\n",
    "    benchmark_experiment_name = f\"{experiment_name}-benchmark\"\n",
    "    if \"benchmark_experiment\" not in globals() or benchmark_experiment is None:\n",
    "        from evaluation.selection.experiment_discovery import discover_benchmark_experiment\n",
    "        benchmark_experiment = discover_benchmark_experiment(\n",
    "            experiment_name=experiment_name,\n",
    "            mlflow_client=mlflow_client,\n",
    "            create_if_missing=True,\n",
    "        )\n",
    "    \n",
    "    # Setup benchmark tracker\n",
    "    benchmark_tracker = MLflowBenchmarkTracker(benchmark_experiment_name)\n",
    "    \n",
    "    # Run benchmarking workflow\n",
    "    champions_to_benchmark = run_benchmarking_workflow(\n",
    "        hpo_experiments=hpo_experiments,\n",
    "        selection_config=selection_config,\n",
    "        benchmark_config=benchmark_config,\n",
    "        data_config=data_config,\n",
    "        hpo_config=hpo_config,\n",
    "        root_dir=ROOT_DIR,\n",
    "        config_dir=CONFIG_DIR,\n",
    "        experiment_name=experiment_name,\n",
    "        mlflow_client=mlflow_client,\n",
    "        benchmark_experiment=benchmark_experiment,\n",
    "        benchmark_tracker=benchmark_tracker,\n",
    "        backup_enabled=BACKUP_ENABLED,\n",
    "        backup_to_drive=restore_from_drive if \"restore_from_drive\" in locals() else None,\n",
    "        restore_from_drive=restore_from_drive if \"restore_from_drive\" in locals() else None,\n",
    "        in_colab=IN_COLAB,\n",
    "    )\n",
    "    \n",
    "    # Store benchmarked champions for checkpoint reuse in Step 7\n",
    "    # Index by refit_run_id (primary) and (backbone, study_key_hash, trial_key_hash) (fallback)\n",
    "    BENCHMARKED_CHAMPIONS_BY_REFIT = {}\n",
    "    BENCHMARKED_CHAMPIONS_BY_KEYS = {}\n",
    "    \n",
    "    for backbone, champion_data in champions_to_benchmark.items():\n",
    "        champion = champion_data.get(\"champion\", {})\n",
    "        refit_run_id = champion.get(\"refit_run_id\")\n",
    "        checkpoint_path = champion.get(\"checkpoint_path\")\n",
    "        \n",
    "        if refit_run_id and checkpoint_path:\n",
    "            # Primary index: refit_run_id (most reliable)\n",
    "            BENCHMARKED_CHAMPIONS_BY_REFIT[refit_run_id] = {\n",
    "                \"checkpoint_path\": Path(checkpoint_path),\n",
    "                \"backbone\": backbone,\n",
    "                \"champion\": champion,\n",
    "            }\n",
    "            \n",
    "            # Fallback index: (backbone, study_key_hash, trial_key_hash)\n",
    "            study_key_hash = champion.get(\"study_key_hash\")\n",
    "            trial_key_hash = champion.get(\"trial_key_hash\")\n",
    "            if study_key_hash and trial_key_hash:\n",
    "                BENCHMARKED_CHAMPIONS_BY_KEYS[(backbone, study_key_hash, trial_key_hash)] = {\n",
    "                    \"checkpoint_path\": Path(checkpoint_path),\n",
    "                    \"refit_run_id\": refit_run_id,\n",
    "                    \"champion\": champion,\n",
    "                }\n",
    "    \n",
    "    if BENCHMARKED_CHAMPIONS_BY_REFIT:\n",
    "        print(f\"üíæ Stored {len(BENCHMARKED_CHAMPIONS_BY_REFIT)} benchmarked champion(s) for checkpoint reuse\")\n",
    "else:\n",
    "    print(\"‚è≠ Skipping benchmarking (RUN_BENCHMARKING=False).\")\n",
    "    print(\"   If benchmark runs don't exist, set RUN_BENCHMARKING=True or run benchmarking in notebook 01.\")\n",
    "    BENCHMARKED_CHAMPIONS_BY_REFIT = {}\n",
    "    BENCHMARKED_CHAMPIONS_BY_KEYS = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Best Model Selection\n",
    "\n",
    "Query MLflow benchmark runs (created by `01_orchestrate_training_colab.ipynb` or Step 6 above using `evaluation.benchmarking.benchmark_best_trials`), join to training runs via grouping tags, and select the best model using normalized composite scoring.\n",
    "\n",
    "**Note**: Benchmark runs must exist in MLflow before running this step. If no benchmark runs are found, either:\n",
    "- Set `RUN_BENCHMARKING=True` in Step 6 above, or\n",
    "- Go back to `01_orchestrate_training_colab.ipynb` and run the benchmarking step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.selection.workflows import run_selection_workflow\n",
    "from training.execution import extract_lineage_from_best_model\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize dictionaries if not already created (e.g., if benchmarking was skipped)\n",
    "if \"BENCHMARKED_CHAMPIONS_BY_REFIT\" not in globals():\n",
    "    BENCHMARKED_CHAMPIONS_BY_REFIT = {}\n",
    "if \"BENCHMARKED_CHAMPIONS_BY_KEYS\" not in globals():\n",
    "    BENCHMARKED_CHAMPIONS_BY_KEYS = {}\n",
    "\n",
    "# Run selection workflow\n",
    "best_model, best_checkpoint_dir = run_selection_workflow(\n",
    "    benchmark_experiment=benchmark_experiment,\n",
    "    hpo_experiments=hpo_experiments,\n",
    "    selection_config=selection_config,\n",
    "    tags_config=tags_config,\n",
    "    root_dir=ROOT_DIR,\n",
    "    config_dir=CONFIG_DIR,\n",
    "    experiment_name=experiment_name,\n",
    "    acquisition_config=acquisition_config,\n",
    "    platform=PLATFORM,\n",
    "    restore_from_drive=restore_from_drive if \"restore_from_drive\" in locals() else None,\n",
    "    drive_store=drive_store if \"drive_store\" in locals() else None,\n",
    "    in_colab=IN_COLAB,\n",
    "    benchmarked_champions_by_refit=BENCHMARKED_CHAMPIONS_BY_REFIT if \"BENCHMARKED_CHAMPIONS_BY_REFIT\" in globals() else None,\n",
    "    benchmarked_champions_by_keys=BENCHMARKED_CHAMPIONS_BY_KEYS if \"BENCHMARKED_CHAMPIONS_BY_KEYS\" in globals() else None,\n",
    ")\n",
    "\n",
    "# Extract lineage from best model for final training\n",
    "lineage = extract_lineage_from_best_model(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if selected run is already final training (skip retraining if so)\n",
    "stage_tag = tags_config.key(\"process\", \"stage\")\n",
    "trained_on_full_data_tag = tags_config.key(\"training\", \"trained_on_full_data\")\n",
    "\n",
    "is_final_training = best_model[\"tags\"].get(stage_tag) == \"final_training\"\n",
    "used_full_data = (\n",
    "    best_model[\"tags\"].get(trained_on_full_data_tag) == \"true\" or\n",
    "    best_model[\"params\"].get(\"use_combined_data\", \"false\").lower() == \"true\"\n",
    ")\n",
    "\n",
    "SKIP_FINAL_TRAINING = is_final_training and used_full_data\n",
    "\n",
    "if SKIP_FINAL_TRAINING:\n",
    "    final_checkpoint_dir = best_checkpoint_dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Final Training\n",
    "\n",
    "Run final training with best configuration if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_FINAL_TRAINING:\n",
    "    print(\"üîÑ Starting final training with best configuration...\")\n",
    "    from training.execution import execute_final_training\n",
    "    # Execute final training (uses final_training.yaml via load_final_training_config)\n",
    "    # Will automatically reuse existing complete runs if run.mode: reuse_if_exists in final_training.yaml\n",
    "    final_checkpoint_dir = execute_final_training(\n",
    "        root_dir=ROOT_DIR,\n",
    "        config_dir=CONFIG_DIR,\n",
    "        best_model=best_model,\n",
    "        experiment_config=experiment_config,\n",
    "        lineage=lineage,\n",
    "        training_experiment_name=training_experiment_name,\n",
    "        platform=PLATFORM,\n",
    "    )\n",
    "else:\n",
    "    print(\"‚úì Skipping final training - using selected checkpoint\")\n",
    "\n",
    "# Backup final checkpoint to Google Drive if in Colab\n",
    "if IN_COLAB and drive_store and final_checkpoint_dir:\n",
    "    checkpoint_path = Path(final_checkpoint_dir).resolve()\n",
    "    # Check if checkpoint is already in Drive\n",
    "    if str(checkpoint_path).startswith(\"/content/drive\"):\n",
    "        print(f\"\\n‚úì Final training checkpoint is already in Google Drive\")\n",
    "        print(f\"  Drive path: {checkpoint_path}\")\n",
    "    else:\n",
    "        try:\n",
    "            print(f\"\\nüì¶ Backing up final training checkpoint to Google Drive...\")\n",
    "            result = drive_store.backup(checkpoint_path, expect=\"dir\")\n",
    "            if result.ok:\n",
    "                print(f\"‚úì Successfully backed up final checkpoint to Google Drive\")\n",
    "                print(f\"  Drive path: {result.dst}\")\n",
    "            else:\n",
    "                print(f\"‚ö† Drive backup failed: {result.reason}\")\n",
    "                if result.error:\n",
    "                    print(f\"  Error: {result.error}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö† Drive backup error: {e}\")\n",
    "            print(f\"  Checkpoint is still available locally at: {final_checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Model Conversion & Optimization\n",
    "\n",
    "Convert the final trained model to ONNX format with optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract parent training information for conversion\n",
    "from common.shared.json_cache import load_json\n",
    "from pathlib import Path\n",
    "\n",
    "# Load metadata from final training output directory\n",
    "final_training_metadata_path = final_checkpoint_dir.parent / \"metadata.json\"\n",
    "\n",
    "if not final_training_metadata_path.exists():\n",
    "    raise ValueError(\n",
    "        f\"Metadata file not found: {final_training_metadata_path}\\n\"\n",
    "        \"Please ensure final training completed successfully.\"\n",
    "    )\n",
    "\n",
    "metadata = load_json(final_training_metadata_path)\n",
    "parent_spec_fp = metadata.get(\"spec_fp\")\n",
    "parent_exec_fp = metadata.get(\"exec_fp\")\n",
    "parent_training_run_id = metadata.get(\"mlflow\", {}).get(\"run_id\")\n",
    "\n",
    "if not parent_spec_fp or not parent_exec_fp:\n",
    "    raise ValueError(\n",
    "        f\"Missing required fingerprints in metadata: spec_fp={parent_spec_fp}, exec_fp={parent_exec_fp}\\n\"\n",
    "        \"Please ensure final training completed successfully.\"\n",
    "    )\n",
    "\n",
    "if parent_training_run_id:\n",
    "    print(f\"‚úì Parent training: spec_fp={parent_spec_fp[:8]}..., exec_fp={parent_exec_fp[:8]}..., run_id={parent_training_run_id[:12]}...\")\n",
    "else:\n",
    "    print(f\"‚úì Parent training: spec_fp={parent_spec_fp[:8]}..., exec_fp={parent_exec_fp[:8]}... (run_id not found)\")\n",
    "\n",
    "# Get parent training output directory (checkpoint parent)\n",
    "parent_training_output_dir = final_checkpoint_dir.parent\n",
    "\n",
    "print(f\"\\nüîÑ Starting model conversion...\")\n",
    "from deployment.conversion import execute_conversion\n",
    "\n",
    "# Execute conversion (uses conversion.yaml via load_conversion_config)\n",
    "conversion_output_dir = execute_conversion(\n",
    "    root_dir=ROOT_DIR,\n",
    "    config_dir=CONFIG_DIR,\n",
    "    parent_training_output_dir=parent_training_output_dir,\n",
    "    parent_spec_fp=parent_spec_fp,\n",
    "    parent_exec_fp=parent_exec_fp,\n",
    "    experiment_config=experiment_config,\n",
    "    conversion_experiment_name=conversion_experiment_name,\n",
    "    platform=PLATFORM,\n",
    "    parent_training_run_id=parent_training_run_id,  # May be None, that's OK\n",
    ")\n",
    "\n",
    "# Find ONNX model file (search recursively, as model may be in onnx_model/ subdirectory)\n",
    "onnx_files = list(conversion_output_dir.rglob(\"*.onnx\"))\n",
    "if onnx_files:\n",
    "    onnx_model_path = onnx_files[0]\n",
    "    print(f\"\\n‚úì Conversion completed successfully!\")\n",
    "    print(f\"  ONNX model: {onnx_model_path}\")\n",
    "    print(f\"  Model size: {onnx_model_path.stat().st_size / (1024 * 1024):.2f} MB\")\n",
    "else:\n",
    "    print(f\"\\n‚ö† Warning: No ONNX model file found in {conversion_output_dir} (searched recursively)\")\n",
    "\n",
    "# Backup conversion output to Google Drive if in Colab\n",
    "if IN_COLAB and drive_store and conversion_output_dir:\n",
    "    output_path = Path(conversion_output_dir).resolve()\n",
    "    # Check if output is already in Drive\n",
    "    if str(output_path).startswith(\"/content/drive\"):\n",
    "        print(f\"\\n‚úì Conversion output is already in Google Drive\")\n",
    "        print(f\"  Drive path: {output_path}\")\n",
    "    else:\n",
    "        try:\n",
    "            print(f\"\\nüì¶ Backing up conversion output to Google Drive...\")\n",
    "            result = drive_store.backup(output_path, expect=\"dir\")\n",
    "            if result.ok:\n",
    "                print(f\"‚úì Successfully backed up conversion output to Google Drive\")\n",
    "                print(f\"  Drive path: {result.dst}\")\n",
    "            else:\n",
    "                print(f\"‚ö† Drive backup failed: {result.reason}\")\n",
    "                if result.error:\n",
    "                    print(f\"  Error: {result.error}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö† Drive backup error: {e}\")\n",
    "            print(f\"  Output is still available locally at: {conversion_output_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "resume-ner-training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
