
============================================================
[INFO] Pytest log file: /workspaces/resume-ner-azureml/outputs/pytest_logs/pytest_20260118_162545.log
============================================================

============================= test session starts ==============================
platform linux -- Python 3.10.19, pytest-9.0.2, pluggy-1.6.0
rootdir: /workspaces/resume-ner-azureml
configfile: pytest.ini
plugins: xdist-3.8.0, cov-7.0.0, asyncio-1.3.0, mock-3.15.1, anyio-4.12.1
asyncio: mode=auto, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function

----------------------------- live log collection ------------------------------
2026-01-18 16:25:47 INFO     infrastructure.tracking.mlflow.compatibility: Applied Azure ML artifact compatibility patch
collected 1409 items

tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_missing_benchmark_yaml_uses_defaults PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_empty_batch_sizes_handled_gracefully PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_non_integer_batch_sizes_type_check PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_negative_iterations_handled PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_zero_iterations_handled PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_non_integer_iterations_type_check PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_negative_warmup_iterations_handled PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_non_integer_warmup_type_check PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_negative_max_length_handled PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_zero_max_length_handled PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_non_integer_max_length_type_check PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_invalid_device_value_handled PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_test_data_nonexistent_path_handled PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_test_data_resolution_fallback_logic PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_output_filename_with_path_separators PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_missing_benchmarking_section_uses_defaults PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_missing_output_section_uses_defaults PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_benchmark_best_trials_handles_missing_test_data 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: Skipping benchmarking (test data not available)
PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_run_benchmarking_handles_missing_benchmark_script Warning: Benchmark script not found: /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_handles_0/src/evaluation/benchmarking/cli.py
PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_run_benchmarking_handles_subprocess_failure 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_handles_1/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_handles_1/checkpoint --test-data /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_handles_1/test.json --batch-sizes 1 8 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_handles_1/benchmark.json
2026-01-18 16:25:49 ERROR    evaluation.benchmarking.utils: Benchmarking failed with return code 1
PASSED
tests/benchmarking/integration/test_benchmark_mlflow_tracking.py::TestBenchmarkMlflowTrackingWithTrialId::test_benchmark_passes_trial_id_to_run_benchmarking 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: Benchmarking distilbert (trial-25d03eeb)...
2026-01-18 16:25:49 WARNING  evaluation.benchmarking.orchestrator_original: [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=xyz789uvw012...
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: Benchmark completed: /tmp/pytest-of-codespace/pytest-120/test_benchmark_passes_trial_id0/outputs/benchmarking/test/benchmark.json
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: Benchmarking complete. 1/1 trials benchmarked.
PASSED
tests/benchmarking/integration/test_benchmark_mlflow_tracking.py::TestBenchmarkMlflowTrackingWithTrialId::test_benchmark_passes_trial_id_old_format 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: Benchmarking distilbert (trial_1_20251231_161745)...
2026-01-18 16:25:49 WARNING  evaluation.benchmarking.orchestrator_original: [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=xyz789uvw012...
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: Benchmark completed: /tmp/pytest-of-codespace/pytest-120/test_benchmark_passes_trial_id1/outputs/benchmarking/test/benchmark.json
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: Benchmarking complete. 1/1 trials benchmarked.
PASSED
tests/benchmarking/integration/test_benchmark_mlflow_tracking.py::TestBenchmarkMlflowTrackingWithTrialId::test_run_benchmarking_mlflow_tracking_with_trial_id 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_mlflow_t0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_mlflow_t0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_mlflow_t0/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_mlflow_t0/benchmark.json
2026-01-18 16:25:49 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Building run name: trial_id=trial-25d03eeb, root_dir=/tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_mlflow_t0, config_dir=/tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_mlflow_t0/config
2026-01-18 16:25:49 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Generated run name: local_distilbert_benchmark_trial-25d03eeb
PASSED
tests/benchmarking/integration/test_benchmark_mlflow_tracking.py::TestBenchmarkMlflowTrackingWithTrialId::test_run_benchmarking_mlflow_tracking_fallback_to_trial_key_hash 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_mlflow_t1/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_mlflow_t1/checkpoint --test-data /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_mlflow_t1/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_mlflow_t1/benchmark.json
2026-01-18 16:25:49 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Constructed trial_id from trial_key_hash: trial-25d03eeb
2026-01-18 16:25:49 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Building run name: trial_id=trial-25d03eeb, root_dir=/tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_mlflow_t1, config_dir=/tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_mlflow_t1/config
2026-01-18 16:25:49 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Generated run name: local_distilbert_benchmark_trial-25d03eeb
PASSED
tests/benchmarking/integration/test_benchmark_orchestrator.py::TestBenchmarkOrchestratorConfigUsage::test_benchmark_best_trials_uses_config_batch_sizes 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: Benchmarking distilbert (trial_0)...
2026-01-18 16:25:49 WARNING  evaluation.benchmarking.orchestrator_original: [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: Benchmark completed: /tmp/pytest-of-codespace/pytest-120/test_benchmark_best_trials_use0/outputs/benchmarking/test/benchmark.json
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: Benchmarking complete. 1/1 trials benchmarked.
PASSED
tests/benchmarking/integration/test_benchmark_orchestrator.py::TestBenchmarkOrchestratorConfigUsage::test_benchmark_best_trials_uses_config_iterations 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: Benchmarking distilbert (trial_0)...
2026-01-18 16:25:49 WARNING  evaluation.benchmarking.orchestrator_original: [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: Benchmark completed: /tmp/pytest-of-codespace/pytest-120/test_benchmark_best_trials_use1/outputs/benchmarking/test/benchmark.json
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: Benchmarking complete. 1/1 trials benchmarked.
PASSED
tests/benchmarking/integration/test_benchmark_orchestrator.py::TestBenchmarkOrchestratorConfigUsage::test_benchmark_best_trials_uses_config_warmup 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: Benchmarking distilbert (trial_0)...
2026-01-18 16:25:49 WARNING  evaluation.benchmarking.orchestrator_original: [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: Benchmark completed: /tmp/pytest-of-codespace/pytest-120/test_benchmark_best_trials_use2/outputs/benchmarking/test/benchmark.json
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: Benchmarking complete. 1/1 trials benchmarked.
PASSED
tests/benchmarking/integration/test_benchmark_orchestrator.py::TestBenchmarkOrchestratorConfigUsage::test_benchmark_best_trials_uses_config_max_length 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: Benchmarking distilbert (trial_0)...
2026-01-18 16:25:49 WARNING  evaluation.benchmarking.orchestrator_original: [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: Benchmark completed: /tmp/pytest-of-codespace/pytest-120/test_benchmark_best_trials_use3/outputs/benchmarking/test/benchmark.json
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: Benchmarking complete. 1/1 trials benchmarked.
PASSED
tests/benchmarking/integration/test_benchmark_orchestrator.py::TestBenchmarkOrchestratorConfigUsage::test_benchmark_best_trials_uses_config_device 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: Benchmarking distilbert (trial_0)...
2026-01-18 16:25:49 WARNING  evaluation.benchmarking.orchestrator_original: [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: Benchmark completed: /tmp/pytest-of-codespace/pytest-120/test_benchmark_best_trials_use4/outputs/benchmarking/test/custom_benchmark.json
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: Benchmarking complete. 1/1 trials benchmarked.
PASSED
tests/benchmarking/integration/test_benchmark_orchestrator.py::TestBenchmarkOrchestratorConfigUsage::test_benchmark_best_trials_uses_output_filename 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: Benchmarking distilbert (trial_0)...
2026-01-18 16:25:49 WARNING  evaluation.benchmarking.orchestrator_original: [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: Benchmark completed: /tmp/pytest-of-codespace/pytest-120/test_benchmark_best_trials_use5/outputs/benchmarking/test/benchmark.json
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: Benchmarking complete. 1/1 trials benchmarked.
PASSED
tests/benchmarking/integration/test_benchmark_orchestrator.py::TestBenchmarkOrchestratorConfigUsage::test_benchmark_best_trials_uses_custom_output_filename 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: Benchmarking distilbert (trial_0)...
2026-01-18 16:25:49 WARNING  evaluation.benchmarking.orchestrator_original: [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: Benchmark completed: /tmp/pytest-of-codespace/pytest-120/test_benchmark_best_trials_use6/outputs/benchmarking/test/custom_benchmark.json
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: Benchmarking complete. 1/1 trials benchmarked.
PASSED
tests/benchmarking/integration/test_benchmark_orchestrator.py::TestBenchmarkOrchestratorConfigUsage::test_benchmark_best_trials_all_config_options_together 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: Benchmarking distilbert (trial_0)...
2026-01-18 16:25:49 WARNING  evaluation.benchmarking.orchestrator_original: [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: Benchmark completed: /tmp/pytest-of-codespace/pytest-120/test_benchmark_best_trials_all0/outputs/benchmarking/test/custom_benchmark.json
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: Benchmarking complete. 1/1 trials benchmarked.
PASSED
tests/benchmarking/integration/test_benchmark_orchestrator.py::TestBenchmarkOrchestratorConfigUsage::test_benchmark_best_trials_defaults_when_config_missing 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 WARNING  infrastructure.paths.resolve: Failed to load paths.yaml config: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_benchmark_best_trials_def0/outputs/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.. Using fallback logic.
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: Benchmarking distilbert (trial_0)...
2026-01-18 16:25:49 WARNING  evaluation.benchmarking.orchestrator_original: [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
Warning: Benchmark script not found: /tmp/pytest-of-codespace/pytest-120/test_benchmark_best_trials_def0/outputs/src/evaluation/benchmarking/cli.py
2026-01-18 16:25:49 ERROR    evaluation.benchmarking.orchestrator_original: Benchmark failed for distilbert
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: Benchmarking complete. 0/1 trials benchmarked.
PASSED
tests/benchmarking/integration/test_benchmark_run_mode_idempotency.py::TestRunModeIntegration::test_force_new_returns_all_champions 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 INFO     evaluation.benchmarking.filter: Run mode is 'force_new' - skipping idempotency check, will benchmark all champions
PASSED
tests/benchmarking/integration/test_benchmark_run_mode_idempotency.py::TestRunModeIntegration::test_reuse_if_exists_filters_existing_benchmarks 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 INFO     evaluation.benchmarking.filter: Skipping distilbert - benchmark already exists (trial_key_hash=trial_hash_456...)
PASSED
tests/benchmarking/integration/test_benchmark_run_mode_idempotency.py::TestRunModeIntegration::test_get_benchmark_run_mode_from_config PASSED
tests/benchmarking/integration/test_benchmark_run_mode_idempotency.py::TestBenchmarkKeyIdempotencyIntegration::test_benchmark_key_primary_check_succeeds PASSED
tests/benchmarking/integration/test_benchmark_run_mode_idempotency.py::TestBenchmarkKeyIdempotencyIntegration::test_benchmark_key_primary_check_fails_then_fallback 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 WARNING  evaluation.benchmarking.existence_checker: Found 1 finished benchmark run(s) by hash (fallback). Consider setting benchmark_key tag for more reliable idempotency. trial_key_hash=trial_hash_456...
PASSED
tests/benchmarking/integration/test_benchmark_run_mode_idempotency.py::TestBenchmarkKeyIdempotencyIntegration::test_benchmark_key_changes_with_config_creates_new_benchmark PASSED
tests/benchmarking/integration/test_benchmark_run_mode_idempotency.py::TestBenchmarkKeyIdempotencyIntegration::test_benchmark_key_same_config_reuses_benchmark PASSED
tests/benchmarking/integration/test_benchmark_run_mode_idempotency.py::TestBackwardCompatibilityFallback::test_fallback_to_hash_when_benchmark_key_tag_missing 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 WARNING  evaluation.benchmarking.existence_checker: Found 1 finished benchmark run(s) by hash (fallback). Consider setting benchmark_key tag for more reliable idempotency. trial_key_hash=trial_hash_456...
PASSED
tests/benchmarking/integration/test_benchmark_run_mode_idempotency.py::TestBackwardCompatibilityFallback::test_fallback_requires_both_hashes PASSED
tests/benchmarking/integration/test_benchmark_run_mode_idempotency.py::TestCompleteWorkflowScenarios::test_scenario_reuse_if_exists_with_existing_benchmark 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 INFO     evaluation.benchmarking.filter: Skipping distilbert - benchmark already exists (trial_key_hash=trial_hash_456...)
PASSED
tests/benchmarking/integration/test_benchmark_run_mode_idempotency.py::TestCompleteWorkflowScenarios::test_scenario_reuse_if_exists_without_existing_benchmark PASSED
tests/benchmarking/integration/test_benchmark_run_mode_idempotency.py::TestCompleteWorkflowScenarios::test_scenario_force_new_always_creates 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 INFO     evaluation.benchmarking.filter: Run mode is 'force_new' - skipping idempotency check, will benchmark all champions
PASSED
tests/benchmarking/integration/test_benchmark_run_mode_idempotency.py::TestCompleteWorkflowScenarios::test_scenario_config_change_creates_new_benchmark PASSED
tests/benchmarking/integration/test_benchmark_utils.py::TestBenchmarkUtilsConfigUsage::test_run_benchmarking_uses_batch_sizes 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_bat0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_bat0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_bat0/test.json --batch-sizes 1 8 16 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_bat0/benchmark.json
PASSED
tests/benchmarking/integration/test_benchmark_utils.py::TestBenchmarkUtilsConfigUsage::test_run_benchmarking_uses_iterations 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_ite0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_ite0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_ite0/test.json --batch-sizes 1 8 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_ite0/benchmark.json
PASSED
tests/benchmarking/integration/test_benchmark_utils.py::TestBenchmarkUtilsConfigUsage::test_run_benchmarking_uses_warmup_iterations 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_war0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_war0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_war0/test.json --batch-sizes 1 8 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_war0/benchmark.json
PASSED
tests/benchmarking/integration/test_benchmark_utils.py::TestBenchmarkUtilsConfigUsage::test_run_benchmarking_uses_max_length 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_max0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_max0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_max0/test.json --batch-sizes 1 8 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_max0/benchmark.json
PASSED
tests/benchmarking/integration/test_benchmark_utils.py::TestBenchmarkUtilsConfigUsage::test_run_benchmarking_uses_device_when_provided 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_dev0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_dev0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_dev0/test.json --batch-sizes 1 8 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_dev0/benchmark.json --device cuda
PASSED
tests/benchmarking/integration/test_benchmark_utils.py::TestBenchmarkUtilsConfigUsage::test_run_benchmarking_skips_device_when_null 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_skips_de0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_skips_de0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_skips_de0/test.json --batch-sizes 1 8 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_skips_de0/benchmark.json
PASSED
tests/benchmarking/integration/test_benchmark_utils.py::TestBenchmarkUtilsConfigUsage::test_run_benchmarking_uses_output_path 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_out0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_out0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_out0/test.json --batch-sizes 1 8 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_out0/benchmark.json
PASSED
tests/benchmarking/integration/test_benchmark_utils.py::TestBenchmarkUtilsConfigUsage::test_run_benchmarking_all_config_options_together 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_all_conf0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_all_conf0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_all_conf0/test.json --batch-sizes 2 4 32 --iterations 200 --warmup 20 --max-length 256 --output /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_all_conf0/custom_benchmark.json --device cuda
PASSED
tests/benchmarking/integration/test_benchmark_workflow.py::TestBenchmarkWorkflow::test_workflow_loads_config_and_uses_all_options 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: Benchmarking distilbert (trial_0)...
2026-01-18 16:25:49 WARNING  evaluation.benchmarking.orchestrator_original: [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: Benchmark completed: /tmp/pytest-of-codespace/pytest-120/test_workflow_loads_config_and0/outputs/benchmarking/test/benchmark.json
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: Benchmarking complete. 1/1 trials benchmarked.
PASSED
tests/benchmarking/integration/test_benchmark_workflow.py::TestBenchmarkWorkflow::test_workflow_custom_config_values 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-120/test_workflow_custom_config_va0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-120/test_workflow_custom_config_va0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-120/test_workflow_custom_config_va0/test.json --batch-sizes 2 4 32 --iterations 200 --warmup 20 --max-length 256 --output /tmp/pytest-of-codespace/pytest-120/test_workflow_custom_config_va0/custom_benchmark.json --device cuda
PASSED
tests/benchmarking/integration/test_benchmark_workflow.py::TestBenchmarkWorkflow::test_workflow_defaults_when_config_missing 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: Benchmarking distilbert (trial_0)...
2026-01-18 16:25:49 WARNING  evaluation.benchmarking.orchestrator_original: [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: Benchmark completed: /tmp/pytest-of-codespace/pytest-120/test_workflow_defaults_when_co0/outputs/benchmarking/test/benchmark.json
2026-01-18 16:25:49 INFO     evaluation.benchmarking.orchestrator_original: Benchmarking complete. 1/1 trials benchmarked.
PASSED
tests/benchmarking/unit/test_benchmark_config.py::TestBenchmarkConfigLoading::test_load_experiment_config_includes_benchmark PASSED
tests/benchmarking/unit/test_benchmark_config.py::TestBenchmarkConfigLoading::test_load_experiment_config_defaults_to_benchmark_yaml PASSED
tests/benchmarking/unit/test_benchmark_config.py::TestBenchmarkConfigLoading::test_load_all_configs_loads_benchmark_if_exists PASSED
tests/benchmarking/unit/test_benchmark_config.py::TestBenchmarkConfigLoading::test_load_all_configs_skips_benchmark_if_not_exists PASSED
tests/benchmarking/unit/test_benchmark_config.py::TestBenchmarkConfigLoading::test_load_benchmark_config_structure PASSED
tests/benchmarking/unit/test_benchmark_config.py::TestBenchmarkConfigLoading::test_load_benchmark_config_with_custom_values PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_batch_sizes_extraction PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_batch_sizes_default PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_batch_sizes_custom_values PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_iterations_extraction PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_iterations_default PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_iterations_custom_value PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_warmup_iterations_extraction PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_warmup_iterations_default PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_warmup_iterations_custom_value PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_max_length_extraction PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_max_length_default PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_max_length_custom_value PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_device_extraction_null PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_device_extraction_cuda PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_device_extraction_cpu PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_device_default PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_test_data_extraction_null PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_test_data_extraction_relative_path PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_test_data_extraction_absolute_path PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_test_data_default PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_output_filename_extraction PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_output_filename_default PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_output_filename_custom_value PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_all_options_together PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestRunModeConfig::test_run_mode_extraction_reuse_if_exists PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestRunModeConfig::test_run_mode_extraction_force_new PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestRunModeConfig::test_run_mode_default_when_missing PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestRunModeConfig::test_run_mode_default_when_run_section_missing PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestRunModeConfig::test_get_benchmark_run_mode_uses_config PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestRunModeConfig::test_get_benchmark_run_mode_default PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestRunModeBehavior::test_force_new_skips_filtering 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 INFO     evaluation.benchmarking.filter: Run mode is 'force_new' - skipping idempotency check, will benchmark all champions
PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestRunModeBehavior::test_reuse_if_exists_filters_existing 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 INFO     evaluation.benchmarking.filter: Skipping distilbert - benchmark already exists (trial_key_hash=trial_hash_456...)
2026-01-18 16:25:49 INFO     evaluation.benchmarking.filter: Skipping deberta - benchmark already exists (trial_key_hash=trial_hash_789...)
PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestBenchmarkKeyIdempotency::test_build_benchmark_key_includes_config_hash PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestBenchmarkKeyIdempotency::test_benchmark_key_changes_with_config_change PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestBenchmarkKeyIdempotency::test_benchmark_key_same_with_same_config PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestBenchmarkKeyIdempotency::test_benchmark_key_used_as_primary_check PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestBenchmarkKeyFallback::test_fallback_to_hash_when_benchmark_key_not_found PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestBenchmarkKeyFallback::test_fallback_requires_both_hashes PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestIdempotencySuccessCases::test_success_benchmark_exists_by_benchmark_key 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 INFO     evaluation.benchmarking.filter: Skipping distilbert - benchmark already exists (trial_key_hash=trial_hash_456...)
PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestIdempotencySuccessCases::test_success_benchmark_exists_by_hash_fallback 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 INFO     evaluation.benchmarking.filter: Skipping distilbert - benchmark already exists (trial_key_hash=trial_hash_456...)
PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestIdempotencySuccessCases::test_success_benchmark_not_exists_creates_new PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestIdempotencyFailureCases::test_failure_missing_champion_run_id 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 WARNING  evaluation.benchmarking.filter: No run_id found for champion distilbert, skipping idempotency check
PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestIdempotencyFailureCases::test_failure_mlflow_client_unavailable 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_failure_mlflow_client_una0/config/tags.yaml, using defaults
PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestIdempotencyFailureCases::test_failure_mlflow_check_raises_exception PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestConfigDocumentationCoverage::test_run_mode_documentation_covered PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestConfigDocumentationCoverage::test_idempotency_documentation_covered PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestConfigDocumentationCoverage::test_independence_from_hpo_config PASSED
tests/benchmarking/unit/test_benchmark_trial_id_extraction.py::TestTrialIdExtraction::test_trial_id_from_parameter_old_format 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-120/test_trial_id_from_parameter_o0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-120/test_trial_id_from_parameter_o0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-120/test_trial_id_from_parameter_o0/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-120/test_trial_id_from_parameter_o0/benchmark.json
2026-01-18 16:25:49 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Building run name: trial_id=1_20251231_161745, root_dir=/tmp/pytest-of-codespace/pytest-120/test_trial_id_from_parameter_o0, config_dir=/tmp/pytest-of-codespace/pytest-120/test_trial_id_from_parameter_o0/config
2026-01-18 16:25:49 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-120/test_trial_id_from_parameter_o0/config/naming.yaml, using empty policy
2026-01-18 16:25:49 WARNING  evaluation.benchmarking.utils: Could not log benchmark results to MLflow: Naming policy not available or formatting failed. Legacy fallback removed. Process type: <Mock name='create_naming_context().process_type' id='123304183202432'>
PASSED
tests/benchmarking/unit/test_benchmark_trial_id_extraction.py::TestTrialIdExtraction::test_trial_id_from_parameter_new_format 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-120/test_trial_id_from_parameter_n0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-120/test_trial_id_from_parameter_n0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-120/test_trial_id_from_parameter_n0/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-120/test_trial_id_from_parameter_n0/benchmark.json
2026-01-18 16:25:49 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Building run name: trial_id=trial-25d03eeb, root_dir=/tmp/pytest-of-codespace/pytest-120/test_trial_id_from_parameter_n0, config_dir=/tmp/pytest-of-codespace/pytest-120/test_trial_id_from_parameter_n0/config
2026-01-18 16:25:49 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-120/test_trial_id_from_parameter_n0/config/naming.yaml, using empty policy
2026-01-18 16:25:49 WARNING  evaluation.benchmarking.utils: Could not log benchmark results to MLflow: Naming policy not available or formatting failed. Legacy fallback removed. Process type: <Mock name='create_naming_context().process_type' id='123304183352960'>
PASSED
tests/benchmarking/unit/test_benchmark_trial_id_extraction.py::TestTrialIdExtraction::test_trial_id_extraction_from_path_old_format 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-120/test_trial_id_extraction_from_0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-120/test_trial_id_extraction_from_0/hpo/local/distilbert/study-abc123/trial_1_20251231_161745/checkpoint --test-data /tmp/pytest-of-codespace/pytest-120/test_trial_id_extraction_from_0/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-120/test_trial_id_extraction_from_0/benchmark.json
2026-01-18 16:25:49 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Extracted trial_id from checkpoint path: trial_1_20251231_161745 (at level 1)
2026-01-18 16:25:49 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Building run name: trial_id=trial_1_20251231_161745, root_dir=/tmp/pytest-of-codespace/pytest-120/test_trial_id_extraction_from_0, config_dir=/tmp/pytest-of-codespace/pytest-120/test_trial_id_extraction_from_0/config
2026-01-18 16:25:49 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-120/test_trial_id_extraction_from_0/config/naming.yaml, using empty policy
2026-01-18 16:25:49 WARNING  evaluation.benchmarking.utils: Could not log benchmark results to MLflow: Naming policy not available or formatting failed. Legacy fallback removed. Process type: <Mock name='create_naming_context().process_type' id='123304183329840'>
PASSED
tests/benchmarking/unit/test_benchmark_trial_id_extraction.py::TestTrialIdExtraction::test_trial_id_extraction_from_path_new_format 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-120/test_trial_id_extraction_from_1/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-120/test_trial_id_extraction_from_1/hpo/local/distilbert/study-abc123/trial-25d03eeb/checkpoint --test-data /tmp/pytest-of-codespace/pytest-120/test_trial_id_extraction_from_1/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-120/test_trial_id_extraction_from_1/benchmark.json
2026-01-18 16:25:49 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Extracted trial_id from checkpoint path: trial-25d03eeb (at level 1)
2026-01-18 16:25:49 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Building run name: trial_id=trial-25d03eeb, root_dir=/tmp/pytest-of-codespace/pytest-120/test_trial_id_extraction_from_1, config_dir=/tmp/pytest-of-codespace/pytest-120/test_trial_id_extraction_from_1/config
2026-01-18 16:25:49 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-120/test_trial_id_extraction_from_1/config/naming.yaml, using empty policy
2026-01-18 16:25:49 WARNING  evaluation.benchmarking.utils: Could not log benchmark results to MLflow: Naming policy not available or formatting failed. Legacy fallback removed. Process type: <Mock name='create_naming_context().process_type' id='123304183386496'>
PASSED
tests/benchmarking/unit/test_benchmark_trial_id_extraction.py::TestTrialIdExtraction::test_trial_id_extraction_from_refit_checkpoint_path 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-120/test_trial_id_extraction_from_2/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-120/test_trial_id_extraction_from_2/hpo/local/distilbert/study-abc123/trial-25d03eeb/refit/checkpoint --test-data /tmp/pytest-of-codespace/pytest-120/test_trial_id_extraction_from_2/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-120/test_trial_id_extraction_from_2/benchmark.json
2026-01-18 16:25:49 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Extracted trial_id from checkpoint path: trial-25d03eeb (at level 2)
2026-01-18 16:25:49 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Building run name: trial_id=trial-25d03eeb, root_dir=/tmp/pytest-of-codespace/pytest-120/test_trial_id_extraction_from_2, config_dir=/tmp/pytest-of-codespace/pytest-120/test_trial_id_extraction_from_2/config
2026-01-18 16:25:49 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-120/test_trial_id_extraction_from_2/config/naming.yaml, using empty policy
2026-01-18 16:25:49 WARNING  evaluation.benchmarking.utils: Could not log benchmark results to MLflow: Naming policy not available or formatting failed. Legacy fallback removed. Process type: <Mock name='create_naming_context().process_type' id='123304266146064'>
PASSED
tests/benchmarking/unit/test_benchmark_trial_id_extraction.py::TestTrialIdExtraction::test_trial_id_fallback_to_trial_key_hash 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-120/test_trial_id_fallback_to_tria0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-120/test_trial_id_fallback_to_tria0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-120/test_trial_id_fallback_to_tria0/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-120/test_trial_id_fallback_to_tria0/benchmark.json
2026-01-18 16:25:49 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Constructed trial_id from trial_key_hash: trial-25d03eeb
2026-01-18 16:25:49 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Building run name: trial_id=trial-25d03eeb, root_dir=/tmp/pytest-of-codespace/pytest-120/test_trial_id_fallback_to_tria0, config_dir=/tmp/pytest-of-codespace/pytest-120/test_trial_id_fallback_to_tria0/config
2026-01-18 16:25:49 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-120/test_trial_id_fallback_to_tria0/config/naming.yaml, using empty policy
2026-01-18 16:25:49 WARNING  evaluation.benchmarking.utils: Could not log benchmark results to MLflow: Naming policy not available or formatting failed. Legacy fallback removed. Process type: <Mock name='create_naming_context().process_type' id='123304181559360'>
PASSED
tests/benchmarking/unit/test_benchmark_trial_id_extraction.py::TestTrialIdExtraction::test_trial_id_parameter_overrides_path_extraction 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-120/test_trial_id_parameter_overri0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-120/test_trial_id_parameter_overri0/hpo/local/distilbert/study-abc123/trial-25d03eeb/checkpoint --test-data /tmp/pytest-of-codespace/pytest-120/test_trial_id_parameter_overri0/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-120/test_trial_id_parameter_overri0/benchmark.json
2026-01-18 16:25:49 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Building run name: trial_id=trial-custom123, root_dir=/tmp/pytest-of-codespace/pytest-120/test_trial_id_parameter_overri0, config_dir=/tmp/pytest-of-codespace/pytest-120/test_trial_id_parameter_overri0/config
2026-01-18 16:25:49 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-120/test_trial_id_parameter_overri0/config/naming.yaml, using empty policy
2026-01-18 16:25:49 WARNING  evaluation.benchmarking.utils: Could not log benchmark results to MLflow: Naming policy not available or formatting failed. Legacy fallback removed. Process type: <Mock name='create_naming_context().process_type' id='123304182914624'>
PASSED
tests/config/integration/test_config_integration.py::TestEndToEndScenarios::test_hpo_trial_workflow 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_hpo_trial_workflow0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 16:25:49 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 16:25:49 INFO     infrastructure.tracking.mlflow.index.version_counter: [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:e0fbae020bc656b7e6425344765f3df149e5f6831d2b7..., root_dir=/tmp/pytest-of-codespace/pytest-120/test_hpo_trial_workflow0, config_dir=/tmp/pytest-of-codespace/pytest-120/test_hpo_trial_workflow0/config, counter_path=/tmp/pytest-of-codespace/pytest-120/test_hpo_trial_workflow0/outputs/cache/run_name_counter.json
2026-01-18 16:25:49 INFO     infrastructure.tracking.mlflow.index.version_counter: [Reserve Version] Loaded 0 existing allocations from /tmp/pytest-of-codespace/pytest-120/test_hpo_trial_workflow0/outputs/cache/run_name_counter.json
2026-01-18 16:25:49 INFO     infrastructure.tracking.mlflow.index.version_counter: [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0
2026-01-18 16:25:49 INFO     infrastructure.tracking.mlflow.index.version_counter: [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)
2026-01-18 16:25:49 INFO     infrastructure.tracking.mlflow.index.version_counter: [Reserve Version]  Successfully reserved version 1 for counter_key resume-ner:hpo:e0fbae020bc656b7e6425344765f3df149e... (run_id: pending_2026...)
PASSED
tests/config/integration/test_config_integration.py::TestEndToEndScenarios::test_final_training_workflow 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:49 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_final_training_workflow0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 16:25:49 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=final_training
PASSED
tests/config/integration/test_config_integration.py::TestEndToEndScenarios::test_benchmarking_workflow 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:50 WARNING  infrastructure.naming.display_policy: [Naming Policy] No pattern for process_type: benchmarking
2026-01-18 16:25:50 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_benchmarking_workflow0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 16:25:50 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=benchmarking
2026-01-18 16:25:50 WARNING  infrastructure.naming.mlflow.run_names: Could not reserve version for run name: Benchmarking requires trial_id for run_key, using base name without version
PASSED
tests/config/integration/test_config_integration.py::TestConfigurationConsistency::test_paths_match_naming_patterns 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:50 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_paths_match_naming_patter0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 16:25:50 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 16:25:50 INFO     infrastructure.tracking.mlflow.index.version_counter: [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:fa924e9d2ac5675b719f2281bdcff80badd52b78fb7ef..., root_dir=/tmp/pytest-of-codespace/pytest-120/test_paths_match_naming_patter0, config_dir=/tmp/pytest-of-codespace/pytest-120/test_paths_match_naming_patter0/config, counter_path=/tmp/pytest-of-codespace/pytest-120/test_paths_match_naming_patter0/outputs/cache/run_name_counter.json
2026-01-18 16:25:50 INFO     infrastructure.tracking.mlflow.index.version_counter: [Reserve Version] Loaded 0 existing allocations from /tmp/pytest-of-codespace/pytest-120/test_paths_match_naming_patter0/outputs/cache/run_name_counter.json
2026-01-18 16:25:50 INFO     infrastructure.tracking.mlflow.index.version_counter: [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0
2026-01-18 16:25:50 INFO     infrastructure.tracking.mlflow.index.version_counter: [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)
2026-01-18 16:25:50 INFO     infrastructure.tracking.mlflow.index.version_counter: [Reserve Version]  Successfully reserved version 1 for counter_key resume-ner:hpo:fa924e9d2ac5675b719f2281bdcff80badd... (run_id: pending_2026...)
PASSED
tests/config/integration/test_config_integration.py::TestConfigurationConsistency::test_tag_keys_from_tags_yaml PASSED
tests/config/integration/test_config_integration.py::TestConfigurationConsistency::test_naming_patterns_from_naming_yaml 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:50 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_naming_patterns_from_nami0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 16:25:50 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 16:25:50 INFO     infrastructure.tracking.mlflow.index.version_counter: [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:e0fbae020bc656b7e6425344765f3df149e5f6831d2b7..., root_dir=/tmp/pytest-of-codespace/pytest-120/test_naming_patterns_from_nami0, config_dir=/tmp/pytest-of-codespace/pytest-120/test_naming_patterns_from_nami0/config, counter_path=/tmp/pytest-of-codespace/pytest-120/test_naming_patterns_from_nami0/outputs/cache/run_name_counter.json
2026-01-18 16:25:50 INFO     infrastructure.tracking.mlflow.index.version_counter: [Reserve Version] Loaded 0 existing allocations from /tmp/pytest-of-codespace/pytest-120/test_naming_patterns_from_nami0/outputs/cache/run_name_counter.json
2026-01-18 16:25:50 INFO     infrastructure.tracking.mlflow.index.version_counter: [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0
2026-01-18 16:25:50 INFO     infrastructure.tracking.mlflow.index.version_counter: [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)
2026-01-18 16:25:50 INFO     infrastructure.tracking.mlflow.index.version_counter: [Reserve Version]  Successfully reserved version 1 for counter_key resume-ner:hpo:e0fbae020bc656b7e6425344765f3df149e... (run_id: pending_2026...)
PASSED
tests/config/integration/test_config_integration.py::TestCrossProcessConsistency::test_same_model_environment_consistent_paths PASSED
tests/config/integration/test_config_integration.py::TestCrossProcessConsistency::test_tag_keys_consistent_across_processes PASSED
tests/config/integration/test_config_integration.py::TestCrossProcessConsistency::test_naming_conventions_consistent 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:50 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_naming_conventions_consis0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 16:25:50 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 16:25:50 INFO     infrastructure.tracking.mlflow.index.version_counter: [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:e0fbae020bc656b7e6425344765f3df149e5f6831d2b7..., root_dir=/tmp/pytest-of-codespace/pytest-120/test_naming_conventions_consis0, config_dir=/tmp/pytest-of-codespace/pytest-120/test_naming_conventions_consis0/config, counter_path=/tmp/pytest-of-codespace/pytest-120/test_naming_conventions_consis0/outputs/cache/run_name_counter.json
2026-01-18 16:25:50 INFO     infrastructure.tracking.mlflow.index.version_counter: [Reserve Version] Loaded 0 existing allocations from /tmp/pytest-of-codespace/pytest-120/test_naming_conventions_consis0/outputs/cache/run_name_counter.json
2026-01-18 16:25:50 INFO     infrastructure.tracking.mlflow.index.version_counter: [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0
2026-01-18 16:25:50 INFO     infrastructure.tracking.mlflow.index.version_counter: [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)
2026-01-18 16:25:50 INFO     infrastructure.tracking.mlflow.index.version_counter: [Reserve Version]  Successfully reserved version 1 for counter_key resume-ner:hpo:e0fbae020bc656b7e6425344765f3df149e... (run_id: pending_2026...)
PASSED
tests/config/unit/test_config_loader.py::TestConfigHashComputation::test_compute_config_hash_length PASSED
tests/config/unit/test_config_loader.py::TestConfigHashComputation::test_compute_config_hash_deterministic PASSED
tests/config/unit/test_config_loader.py::TestConfigHashComputation::test_compute_config_hash_different_configs PASSED
tests/config/unit/test_config_loader.py::TestConfigHashComputation::test_compute_config_hash_no_randomness PASSED
tests/config/unit/test_config_loader.py::TestConfigHashComputation::test_compute_config_hash_order_independent PASSED
tests/config/unit/test_config_loader.py::TestConfigHashComputation::test_compute_config_hashes_all_domains PASSED
tests/config/unit/test_config_loader.py::TestConfigHashComputation::test_compute_config_hashes_deterministic PASSED
tests/config/unit/test_config_loader.py::TestConfigLoading::test_load_experiment_config PASSED
tests/config/unit/test_config_loader.py::TestConfigLoading::test_load_experiment_config_defaults PASSED
tests/config/unit/test_config_loader.py::TestConfigLoading::test_load_all_configs PASSED
tests/config/unit/test_config_loader.py::TestConfigLoading::test_load_all_configs_with_benchmark PASSED
tests/config/unit/test_config_loader.py::TestConfigMetadata::test_create_config_metadata PASSED
tests/config/unit/test_config_loader.py::TestConfigImmutability::test_snapshot_configs PASSED
tests/config/unit/test_config_loader.py::TestConfigImmutability::test_validate_config_immutability_unchanged PASSED
tests/config/unit/test_config_loader.py::TestConfigImmutability::test_validate_config_immutability_changed PASSED
tests/config/unit/test_config_loader.py::TestConfigImmutability::test_validate_config_immutability_multiple_changes PASSED
tests/config/unit/test_data_config.py::TestDataConfigLoading::test_load_complete_data_config PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_name_option PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_version_option PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_description_option PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_local_path_option PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_seed_option PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_splitting_train_test_ratio PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_splitting_stratified_true PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_splitting_stratified_false PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_splitting_random_seed PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_schema_format PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_schema_annotation_format PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_schema_entity_types PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_schema_stats_median_sentence_length PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_schema_stats_mean_sentence_length PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_schema_stats_p95_sentence_length PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_schema_stats_suggested_sequence_length PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_schema_stats_entity_density PASSED
tests/config/unit/test_data_config.py::TestDataConfigIntegration::test_data_config_via_experiment_config PASSED
tests/config/unit/test_data_config.py::TestDataConfigIntegration::test_build_label_list_from_data_config PASSED
tests/config/unit/test_data_config.py::TestDataConfigValidation::test_data_config_missing_optional_sections PASSED
tests/config/unit/test_data_config.py::TestDataConfigValidation::test_data_config_partial_splitting PASSED
tests/config/unit/test_data_config.py::TestDataConfigValidation::test_data_config_partial_schema PASSED
tests/config/unit/test_data_config.py::TestDataConfigValidation::test_data_config_partial_stats PASSED
tests/config/unit/test_data_config.py::TestDataConfigValidation::test_data_config_numeric_types PASSED
tests/config/unit/test_data_config.py::TestDataConfigValidation::test_data_config_boolean_types PASSED
tests/config/unit/test_data_config.py::TestDataConfigValidation::test_data_config_entity_types_list PASSED
tests/config/unit/test_data_config.py::TestDataConfigRealFiles::test_load_real_resume_tiny_config PASSED
tests/config/unit/test_data_config.py::TestDataConfigRealFiles::test_load_real_resume_v1_config PASSED
tests/config/unit/test_data_config.py::TestDataConfigRealFiles::test_all_real_data_configs_have_required_sections SKIPPED
tests/config/unit/test_experiment_config.py::TestExperimentConfigLoading::test_load_complete_experiment_config PASSED
tests/config/unit/test_experiment_config.py::TestExperimentConfigLoading::test_load_experiment_config_with_defaults PASSED
tests/config/unit/test_experiment_config.py::TestExperimentConfigOptions::test_experiment_name_option PASSED
tests/config/unit/test_experiment_config.py::TestExperimentConfigOptions::test_experiment_name_fallback PASSED
tests/config/unit/test_experiment_config.py::TestExperimentConfigOptions::test_data_config_option PASSED
tests/config/unit/test_experiment_config.py::TestExperimentConfigOptions::test_model_config_option PASSED
tests/config/unit/test_experiment_config.py::TestExperimentConfigOptions::test_train_config_option PASSED
tests/config/unit/test_experiment_config.py::TestExperimentConfigOptions::test_hpo_config_option PASSED
tests/config/unit/test_experiment_config.py::TestExperimentConfigOptions::test_env_config_option PASSED
tests/config/unit/test_experiment_config.py::TestExperimentConfigOptions::test_benchmark_config_option PASSED
tests/config/unit/test_experiment_config.py::TestExperimentConfigOptions::test_benchmark_config_default PASSED
tests/config/unit/test_experiment_config.py::TestStagesConfiguration::test_stages_smoke_aml_experiment PASSED
tests/config/unit/test_experiment_config.py::TestStagesConfiguration::test_stages_smoke_hpo_config PASSED
tests/config/unit/test_experiment_config.py::TestStagesConfiguration::test_stages_smoke_both_options PASSED
tests/config/unit/test_experiment_config.py::TestStagesConfiguration::test_stages_hpo_aml_experiment PASSED
tests/config/unit/test_experiment_config.py::TestStagesConfiguration::test_stages_hpo_hpo_config PASSED
tests/config/unit/test_experiment_config.py::TestStagesConfiguration::test_stages_hpo_both_options PASSED
tests/config/unit/test_experiment_config.py::TestStagesConfiguration::test_stages_training_aml_experiment PASSED
tests/config/unit/test_experiment_config.py::TestStagesConfiguration::test_stages_training_backbones_single PASSED
tests/config/unit/test_experiment_config.py::TestStagesConfiguration::test_stages_training_backbones_multiple PASSED
tests/config/unit/test_experiment_config.py::TestStagesConfiguration::test_stages_training_both_options PASSED
tests/config/unit/test_experiment_config.py::TestStagesConfiguration::test_stages_all_stages_together PASSED
tests/config/unit/test_experiment_config.py::TestNamingConfiguration::test_naming_include_backbone_in_experiment_true PASSED
tests/config/unit/test_experiment_config.py::TestNamingConfiguration::test_naming_include_backbone_in_experiment_false PASSED
tests/config/unit/test_experiment_config.py::TestNamingConfiguration::test_naming_missing_defaults_to_empty PASSED
tests/config/unit/test_experiment_config.py::TestExperimentConfigIntegration::test_load_all_configs_with_experiment_config PASSED
tests/config/unit/test_experiment_config.py::TestExperimentConfigIntegration::test_experiment_config_stages_preserved PASSED
tests/config/unit/test_experiment_config.py::TestExperimentConfigIntegration::test_experiment_config_naming_preserved PASSED
tests/config/unit/test_experiment_config.py::TestExperimentConfigRealFile::test_load_real_resume_ner_baseline_config SKIPPED
tests/config/unit/test_experiment_config.py::TestExperimentConfigRealFile::test_real_resume_ner_baseline_has_all_sections SKIPPED
tests/config/unit/test_fingerprint_placeholder_fallback.py::TestFingerprintPlaceholderFallback::test_compute_fingerprints_returns_placeholders_on_import_error PASSED
tests/config/unit/test_fingerprint_placeholder_fallback.py::TestFingerprintPlaceholderFallback::test_compute_fingerprints_import_error_handling PASSED
tests/config/unit/test_fingerprint_placeholder_fallback.py::TestFingerprintPlaceholderFallback::test_placeholder_values_are_short_enough_for_naming PASSED
tests/config/unit/test_fingerprints.py::test_compute_spec_fp_deterministic PASSED
tests/config/unit/test_fingerprints.py::test_compute_spec_fp_without_seed PASSED
tests/config/unit/test_fingerprints.py::test_compute_spec_fp_different_seeds PASSED
tests/config/unit/test_fingerprints.py::test_compute_exec_fp_basic PASSED
tests/config/unit/test_fingerprints.py::test_compute_exec_fp_auto_detect PASSED
tests/config/unit/test_fingerprints.py::test_compute_exec_fp_different_precision PASSED
tests/config/unit/test_fingerprints.py::test_compute_conv_fp PASSED
tests/config/unit/test_fingerprints.py::test_compute_conv_fp_different_parents PASSED
tests/config/unit/test_fingerprints.py::test_compute_bench_fp PASSED
tests/config/unit/test_fingerprints.py::test_compute_hardware_fp PASSED
tests/config/unit/test_fingerprints.py::test_compute_hardware_fp_empty PASSED
tests/config/unit/test_mlflow_yaml.py::TestAzureMLConfiguration::test_azure_ml_enabled PASSED
tests/config/unit/test_mlflow_yaml.py::TestAzureMLConfiguration::test_azure_ml_disabled PASSED
tests/config/unit/test_mlflow_yaml.py::TestAzureMLConfiguration::test_azure_ml_workspace_name PASSED
tests/config/unit/test_mlflow_yaml.py::TestTrackingConfiguration::test_tracking_benchmark_enabled PASSED
tests/config/unit/test_mlflow_yaml.py::TestTrackingConfiguration::test_tracking_benchmark_disabled PASSED
tests/config/unit/test_mlflow_yaml.py::TestTrackingConfiguration::test_tracking_benchmark_log_artifacts PASSED
tests/config/unit/test_mlflow_yaml.py::TestTrackingConfiguration::test_tracking_training_enabled PASSED
tests/config/unit/test_mlflow_yaml.py::TestTrackingConfiguration::test_tracking_training_disabled PASSED
tests/config/unit/test_mlflow_yaml.py::TestTrackingConfiguration::test_tracking_training_log_checkpoint PASSED
tests/config/unit/test_mlflow_yaml.py::TestTrackingConfiguration::test_tracking_training_log_metrics_json PASSED
tests/config/unit/test_mlflow_yaml.py::TestTrackingConfiguration::test_tracking_conversion_enabled PASSED
tests/config/unit/test_mlflow_yaml.py::TestTrackingConfiguration::test_tracking_conversion_disabled PASSED
tests/config/unit/test_mlflow_yaml.py::TestTrackingConfiguration::test_tracking_conversion_log_onnx_model PASSED
tests/config/unit/test_mlflow_yaml.py::TestTrackingConfiguration::test_tracking_conversion_log_conversion_log PASSED
tests/config/unit/test_mlflow_yaml.py::TestNamingConfiguration::test_naming_project_name PASSED
tests/config/unit/test_mlflow_yaml.py::TestNamingConfiguration::test_naming_tags_max_length PASSED
tests/config/unit/test_mlflow_yaml.py::TestNamingConfiguration::test_naming_tags_sanitize PASSED
tests/config/unit/test_mlflow_yaml.py::TestNamingConfiguration::test_naming_run_name_max_length PASSED
tests/config/unit/test_mlflow_yaml.py::TestNamingConfiguration::test_naming_run_name_shorten_fingerprints PASSED
tests/config/unit/test_mlflow_yaml.py::TestNamingConfiguration::test_naming_run_name_auto_increment_enabled 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:50 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_naming_run_name_auto_incr0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 16:25:50 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
PASSED
tests/config/unit/test_mlflow_yaml.py::TestNamingConfiguration::test_naming_run_name_auto_increment_processes_hpo 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:50 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_naming_run_name_auto_incr1/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 16:25:50 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
PASSED
tests/config/unit/test_mlflow_yaml.py::TestNamingConfiguration::test_naming_run_name_auto_increment_processes_benchmarking 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:50 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_naming_run_name_auto_incr2/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 16:25:50 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=benchmarking
PASSED
tests/config/unit/test_mlflow_yaml.py::TestNamingConfiguration::test_naming_run_name_auto_increment_format 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:50 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_naming_run_name_auto_incr3/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 16:25:50 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
PASSED
tests/config/unit/test_mlflow_yaml.py::TestIndexConfiguration::test_index_enabled PASSED
tests/config/unit/test_mlflow_yaml.py::TestIndexConfiguration::test_index_disabled PASSED
tests/config/unit/test_mlflow_yaml.py::TestIndexConfiguration::test_index_max_entries PASSED
tests/config/unit/test_mlflow_yaml.py::TestIndexConfiguration::test_index_file_name PASSED
tests/config/unit/test_mlflow_yaml.py::TestRunFinderConfiguration::test_run_finder_strict_mode_default_true PASSED
tests/config/unit/test_mlflow_yaml.py::TestRunFinderConfiguration::test_run_finder_strict_mode_default_false PASSED
tests/config/unit/test_model_config.py::TestModelConfigLoading::test_load_distilbert_config PASSED
tests/config/unit/test_model_config.py::TestModelConfigLoading::test_load_distilroberta_config PASSED
tests/config/unit/test_model_config.py::TestModelConfigLoading::test_load_deberta_config PASSED
tests/config/unit/test_model_config.py::TestModelConfigOptions::test_backbone_option PASSED
tests/config/unit/test_model_config.py::TestModelConfigOptions::test_tokenizer_option PASSED
tests/config/unit/test_model_config.py::TestModelConfigOptions::test_preprocessing_sequence_length PASSED
tests/config/unit/test_model_config.py::TestModelConfigOptions::test_preprocessing_max_length PASSED
tests/config/unit/test_model_config.py::TestModelConfigOptions::test_preprocessing_tokenization PASSED
tests/config/unit/test_model_config.py::TestModelConfigOptions::test_preprocessing_replace_rare_with_unk PASSED
tests/config/unit/test_model_config.py::TestModelConfigOptions::test_preprocessing_unk_frequency_threshold PASSED
tests/config/unit/test_model_config.py::TestModelConfigOptions::test_preprocessing_keep_stopwords PASSED
tests/config/unit/test_model_config.py::TestModelConfigOptions::test_decoding_use_crf PASSED
tests/config/unit/test_model_config.py::TestModelConfigOptions::test_decoding_crf_learning_rate PASSED
tests/config/unit/test_model_config.py::TestModelConfigOptions::test_loss_use_class_weights PASSED
tests/config/unit/test_model_config.py::TestModelConfigOptions::test_loss_class_weight_smoothing PASSED
tests/config/unit/test_model_config.py::TestModelConfigOptions::test_loss_ignore_index PASSED
tests/config/unit/test_model_config.py::TestModelConfigIntegration::test_model_config_via_experiment_config PASSED
tests/config/unit/test_model_config.py::TestModelConfigIntegration::test_model_config_in_training_config_building PASSED
tests/config/unit/test_model_config.py::TestModelConfigValidation::test_model_config_missing_sections PASSED
tests/config/unit/test_model_config.py::TestModelConfigValidation::test_model_config_partial_preprocessing PASSED
tests/config/unit/test_model_config.py::TestModelConfigValidation::test_model_config_partial_decoding PASSED
tests/config/unit/test_model_config.py::TestModelConfigValidation::test_model_config_partial_loss PASSED
tests/config/unit/test_model_config.py::TestModelConfigValidation::test_model_config_numeric_types PASSED
tests/config/unit/test_model_config.py::TestModelConfigValidation::test_model_config_boolean_types PASSED
tests/config/unit/test_model_config.py::TestModelConfigRealFiles::test_load_real_distilbert_config PASSED
tests/config/unit/test_model_config.py::TestModelConfigRealFiles::test_load_real_distilroberta_config PASSED
tests/config/unit/test_model_config.py::TestModelConfigRealFiles::test_load_real_deberta_config PASSED
tests/config/unit/test_model_config.py::TestModelConfigRealFiles::test_all_real_model_configs_have_required_sections SKIPPED
tests/config/unit/test_naming_yaml.py::TestSchemaVersion::test_schema_version_is_loaded PASSED
tests/config/unit/test_naming_yaml.py::TestSeparatorsExplicit::test_separator_field PASSED
tests/config/unit/test_naming_yaml.py::TestSeparatorsExplicit::test_separator_component PASSED
tests/config/unit/test_naming_yaml.py::TestSeparatorsExplicit::test_separator_version PASSED
tests/config/unit/test_naming_yaml.py::TestVersionFormatExplicit::test_version_format PASSED
tests/config/unit/test_naming_yaml.py::TestVersionFormatExplicit::test_version_separator PASSED
tests/config/unit/test_naming_yaml.py::TestNormalizeExplicit::test_normalize_env_replace PASSED
tests/config/unit/test_naming_yaml.py::TestNormalizeExplicit::test_normalize_env_lowercase PASSED
tests/config/unit/test_naming_yaml.py::TestNormalizeExplicit::test_normalize_model_replace PASSED
tests/config/unit/test_naming_yaml.py::TestNormalizeExplicit::test_normalize_model_lowercase PASSED
tests/config/unit/test_naming_yaml.py::TestValidateExplicit::test_validate_max_length PASSED
tests/config/unit/test_naming_yaml.py::TestValidateExplicit::test_validate_forbidden_chars PASSED
tests/config/unit/test_naming_yaml.py::TestValidateExplicit::test_validate_warn_length PASSED
tests/config/unit/test_naming_yaml.py::TestRunNamesComponentOptions::test_hpo_trial_component_options PASSED
tests/config/unit/test_naming_yaml.py::TestRunNamesComponentOptions::test_hpo_trial_fold_component_options PASSED
tests/config/unit/test_naming_yaml.py::TestRunNamesComponentOptions::test_hpo_refit_component_options PASSED
tests/config/unit/test_naming_yaml.py::TestRunNamesComponentOptions::test_hpo_sweep_semantic_suffix_options PASSED
tests/config/unit/test_naming_yaml.py::TestRunNamesComponentOptions::test_final_training_component_options PASSED
tests/config/unit/test_naming_yaml.py::TestRunNamesComponentOptions::test_benchmarking_component_options PASSED
tests/config/unit/test_naming_yaml.py::TestRunNamesComponentOptions::test_conversion_component_options PASSED
tests/config/unit/test_paths.py::TestLoadPathsConfig::test_load_paths_config_with_file PASSED
tests/config/unit/test_paths.py::TestLoadPathsConfig::test_load_paths_config_without_file PASSED
tests/config/unit/test_paths.py::TestResolveOutputPath::test_resolve_simple_path PASSED
tests/config/unit/test_paths.py::TestResolveOutputPath::test_resolve_cache_subdirectory PASSED
tests/config/unit/test_paths.py::TestResolveOutputPath::test_resolve_path_with_pattern PASSED
tests/config/unit/test_paths.py::TestGetCacheFilePath::test_get_latest_cache_file PASSED
tests/config/unit/test_paths.py::TestGetCacheFilePath::test_get_index_cache_file PASSED
tests/config/unit/test_paths.py::TestGetTimestampedCacheFilename::test_generate_best_config_filename PASSED
tests/config/unit/test_paths.py::TestGetTimestampedCacheFilename::test_generate_final_training_filename PASSED
tests/config/unit/test_paths.py::TestGetCacheStrategyConfig::test_get_strategy_config PASSED
tests/config/unit/test_paths.py::TestSaveCacheWithDualStrategy::test_save_cache_creates_all_files PASSED
tests/config/unit/test_paths.py::TestLoadCacheFile::test_load_latest_cache PASSED
tests/config/unit/test_paths.py::TestLoadCacheFile::test_load_specific_timestamp PASSED
tests/config/unit/test_paths.py::TestLoadCacheFile::test_load_returns_none_when_not_found PASSED
tests/config/unit/test_paths.py::TestPathBuildingV2::test_path_building_v2_hpo PASSED
tests/config/unit/test_paths.py::TestPathBuildingV2::test_path_building_v2_normalized PASSED
tests/config/unit/test_paths.py::TestPathBuildingV2::test_path_building_v2_all_storage_envs PASSED
tests/config/unit/test_paths.py::TestPathBuildingV2::test_path_building_v2_study8_trial8_format PASSED
tests/config/unit/test_paths.py::TestResolveProjectPaths::test_trusts_provided_config_dir PASSED
tests/config/unit/test_paths.py::TestResolveProjectPaths::test_infers_from_output_dir_when_config_dir_none PASSED
tests/config/unit/test_paths.py::TestResolveProjectPaths::test_infers_from_start_path_as_fallback PASSED
tests/config/unit/test_paths.py::TestResolveProjectPaths::test_handles_config_dir_with_different_name PASSED
tests/config/unit/test_paths.py::TestResolveProjectPaths::test_returns_none_when_inference_fails PASSED
tests/config/unit/test_paths.py::TestResolveProjectPaths::test_prioritizes_config_dir_over_output_dir PASSED
tests/config/unit/test_paths.py::TestResolveProjectPaths::test_handles_none_inputs PASSED
tests/config/unit/test_paths.py::TestResolveProjectPaths::test_derives_config_dir_from_root_dir PASSED
tests/config/unit/test_paths.py::TestResolveProjectPaths::test_handles_output_dir_without_outputs_parent PASSED
tests/config/unit/test_paths_yaml.py::TestBaseDirectories::test_base_outputs PASSED
tests/config/unit/test_paths_yaml.py::TestBaseDirectories::test_base_notebooks PASSED
tests/config/unit/test_paths_yaml.py::TestBaseDirectories::test_base_config PASSED
tests/config/unit/test_paths_yaml.py::TestBaseDirectories::test_base_src PASSED
tests/config/unit/test_paths_yaml.py::TestBaseDirectories::test_base_tests PASSED
tests/config/unit/test_paths_yaml.py::TestBaseDirectories::test_base_mlruns PASSED
tests/config/unit/test_paths_yaml.py::TestOutputsSubdirectories::test_outputs_hpo_tests PASSED
tests/config/unit/test_paths_yaml.py::TestOutputsSubdirectories::test_outputs_dry_run PASSED
tests/config/unit/test_paths_yaml.py::TestOutputsSubdirectories::test_outputs_e2e_test PASSED
tests/config/unit/test_paths_yaml.py::TestOutputsSubdirectories::test_outputs_pytest_logs PASSED
tests/config/unit/test_paths_yaml.py::TestFilesConfiguration::test_files_metrics PASSED
tests/config/unit/test_paths_yaml.py::TestFilesConfiguration::test_files_benchmark PASSED
tests/config/unit/test_paths_yaml.py::TestFilesConfiguration::test_files_checkpoint_dir PASSED
tests/config/unit/test_paths_yaml.py::TestFilesConfiguration::test_files_cache_best_config_latest PASSED
tests/config/unit/test_paths_yaml.py::TestFilesConfiguration::test_files_cache_best_config_index PASSED
tests/config/unit/test_paths_yaml.py::TestFilesConfiguration::test_files_cache_final_training_latest PASSED
tests/config/unit/test_paths_yaml.py::TestFilesConfiguration::test_files_cache_final_training_index PASSED
tests/config/unit/test_paths_yaml.py::TestFilesConfiguration::test_files_cache_best_model_selection_latest PASSED
tests/config/unit/test_paths_yaml.py::TestFilesConfiguration::test_files_cache_best_model_selection_index PASSED
tests/config/unit/test_paths_yaml.py::TestFilesConfiguration::test_files_cache_conversion_cache PASSED
tests/config/unit/test_paths_yaml.py::TestCacheStrategiesConfiguration::test_cache_strategies_best_configurations_timestamped_enabled PASSED
tests/config/unit/test_paths_yaml.py::TestCacheStrategiesConfiguration::test_cache_strategies_best_configurations_latest_include_timestamped_ref PASSED
tests/config/unit/test_paths_yaml.py::TestCacheStrategiesConfiguration::test_cache_strategies_best_configurations_index_max_entries PASSED
tests/config/unit/test_paths_yaml.py::TestCacheStrategiesConfiguration::test_cache_strategies_final_training_all_options PASSED
tests/config/unit/test_paths_yaml.py::TestCacheStrategiesConfiguration::test_cache_strategies_best_model_selection_all_options PASSED
tests/config/unit/test_paths_yaml.py::TestDriveConfiguration::test_drive_mount_point PASSED
tests/config/unit/test_paths_yaml.py::TestDriveConfiguration::test_drive_backup_base_dir PASSED
tests/config/unit/test_paths_yaml.py::TestDriveConfiguration::test_drive_auto_restore_on_startup PASSED
tests/config/unit/test_paths_yaml.py::TestDriveConfiguration::test_drive_auto_restore_on_startup_true PASSED
tests/config/unit/test_paths_yaml.py::TestNormalizePathsConfiguration::test_normalize_paths_replace PASSED
tests/config/unit/test_paths_yaml.py::TestNormalizePathsConfiguration::test_normalize_paths_lowercase PASSED
tests/config/unit/test_paths_yaml.py::TestNormalizePathsConfiguration::test_normalize_paths_lowercase_true PASSED
tests/config/unit/test_paths_yaml.py::TestNormalizePathsConfiguration::test_normalize_paths_max_component_length PASSED
tests/config/unit/test_paths_yaml.py::TestNormalizePathsConfiguration::test_normalize_paths_max_path_length PASSED
tests/config/unit/test_paths_yaml.py::TestPatternsConfiguration::test_patterns_best_config_file PASSED
tests/config/unit/test_paths_yaml.py::TestPatternsConfiguration::test_patterns_final_training_cache_file PASSED
tests/config/unit/test_paths_yaml.py::TestPatternsConfiguration::test_patterns_best_model_selection_cache_file PASSED
tests/config/unit/test_run_mode.py::TestGetRunMode::test_get_run_mode_explicit_force_new PASSED
tests/config/unit/test_run_mode.py::TestGetRunMode::test_get_run_mode_explicit_reuse_if_exists PASSED
tests/config/unit/test_run_mode.py::TestGetRunMode::test_get_run_mode_explicit_resume_if_incomplete PASSED
tests/config/unit/test_run_mode.py::TestGetRunMode::test_get_run_mode_default_when_missing PASSED
tests/config/unit/test_run_mode.py::TestGetRunMode::test_get_run_mode_default_when_run_section_missing PASSED
tests/config/unit/test_run_mode.py::TestGetRunMode::test_get_run_mode_custom_default PASSED
tests/config/unit/test_run_mode.py::TestGetRunMode::test_get_run_mode_nested_config PASSED
tests/config/unit/test_run_mode.py::TestGetRunMode::test_get_run_mode_combined_config PASSED
tests/config/unit/test_run_mode.py::TestIsForceNew::test_is_force_new_true PASSED
tests/config/unit/test_run_mode.py::TestIsForceNew::test_is_force_new_false PASSED
tests/config/unit/test_run_mode.py::TestIsForceNew::test_is_force_new_default PASSED
tests/config/unit/test_run_mode.py::TestIsReuseIfExists::test_is_reuse_if_exists_true PASSED
tests/config/unit/test_run_mode.py::TestIsReuseIfExists::test_is_reuse_if_exists_false PASSED
tests/config/unit/test_run_mode.py::TestIsReuseIfExists::test_is_reuse_if_exists_default PASSED
tests/config/unit/test_run_mode.py::TestIsResumeIfIncomplete::test_is_resume_if_incomplete_true PASSED
tests/config/unit/test_run_mode.py::TestIsResumeIfIncomplete::test_is_resume_if_incomplete_false PASSED
tests/config/unit/test_run_mode.py::TestIsResumeIfIncomplete::test_is_resume_if_incomplete_default PASSED
tests/config/unit/test_run_mode.py::TestRunModeIntegration::test_hpo_config_with_run_mode PASSED
tests/config/unit/test_run_mode.py::TestRunModeIntegration::test_combined_hpo_checkpoint_config PASSED
tests/config/unit/test_run_mode.py::TestRunModeIntegration::test_final_training_config_with_run_mode PASSED
tests/config/unit/test_variants.py::TestComputeNextVariant::test_compute_next_variant_hpo_no_existing PASSED
tests/config/unit/test_variants.py::TestComputeNextVariant::test_compute_next_variant_hpo_with_existing PASSED
tests/config/unit/test_variants.py::TestComputeNextVariant::test_compute_next_variant_hpo_custom_base_name PASSED
tests/config/unit/test_variants.py::TestComputeNextVariant::test_compute_next_variant_final_training_no_existing PASSED
tests/config/unit/test_variants.py::TestComputeNextVariant::test_compute_next_variant_final_training_with_existing PASSED
tests/config/unit/test_variants.py::TestFindExistingVariants::test_find_existing_variants_hpo_none PASSED
tests/config/unit/test_variants.py::TestFindExistingVariants::test_find_existing_variants_hpo_implicit_variant_1 PASSED
tests/config/unit/test_variants.py::TestFindExistingVariants::test_find_existing_variants_hpo_explicit_variants PASSED
tests/config/unit/test_variants.py::TestFindExistingVariants::test_find_existing_variants_hpo_mixed_patterns PASSED
tests/config/unit/test_variants.py::TestFindExistingVariants::test_find_existing_variants_final_training PASSED
tests/config/unit/test_variants.py::TestFindExistingVariants::test_find_existing_variants_invalid_process_type PASSED
tests/config/unit/test_variants.py::TestVariantsIntegration::test_hpo_variant_sequence PASSED
tests/config/unit/test_variants.py::TestVariantsIntegration::test_hpo_variant_with_custom_study_name PASSED
tests/conversion/integration/test_conversion_config.py::TestConversionConfig::test_opset_version_passed_to_subprocess 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:50 INFO     deployment.conversion.orchestration: Output directory: /tmp/pytest-of-codespace/pytest-120/test_opset_version_passed_to_s0/outputs/conversion/test
2026-01-18 16:25:50 INFO     deployment.conversion.orchestration: Created MLflow run: test_run_name (test_run_id...)
2026-01-18 16:25:50 INFO     deployment.conversion.orchestration: Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /path/to/checkpoint --config-dir /tmp/pytest-of-codespace/pytest-120/test_opset_version_passed_to_s0/config --backbone distilbert-base-uncased --output-dir /tmp/pytest-of-codespace/pytest-120/test_opset_version_passed_to_s0/outputs/conversion/test --opset-version 19 --run-smoke-test
2026-01-18 16:25:50 INFO     infrastructure.tracking.mlflow.lifecycle: Run test_run_id... already terminated with status <Mock name='create_mlflow_client().get_run().info.status' id='123304180903712'> (expected FINISHED)
2026-01-18 16:25:50 INFO     deployment.conversion.orchestration: Conversion completed. ONNX model: /tmp/pytest-of-codespace/pytest-120/test_opset_version_passed_to_s0/outputs/conversion/test/model.onnx
PASSED
tests/conversion/integration/test_conversion_config.py::TestConversionConfig::test_quantization_int8_adds_flag 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:50 INFO     deployment.conversion.orchestration: Output directory: /tmp/pytest-of-codespace/pytest-120/test_quantization_int8_adds_fl0/outputs/conversion/test
2026-01-18 16:25:50 INFO     deployment.conversion.orchestration: Created MLflow run: test_run_name (test_run_id...)
2026-01-18 16:25:50 INFO     deployment.conversion.orchestration: Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /path/to/checkpoint --config-dir /tmp/pytest-of-codespace/pytest-120/test_quantization_int8_adds_fl0/config --backbone distilbert-base-uncased --output-dir /tmp/pytest-of-codespace/pytest-120/test_quantization_int8_adds_fl0/outputs/conversion/test --opset-version 18 --quantize-int8 --run-smoke-test
2026-01-18 16:25:50 INFO     infrastructure.tracking.mlflow.lifecycle: Run test_run_id... already terminated with status <Mock name='create_mlflow_client().get_run().info.status' id='123304183178112'> (expected FINISHED)
2026-01-18 16:25:50 INFO     deployment.conversion.orchestration: Conversion completed. ONNX model: /tmp/pytest-of-codespace/pytest-120/test_quantization_int8_adds_fl0/outputs/conversion/test/model_int8.onnx
PASSED
tests/conversion/integration/test_conversion_config.py::TestConversionConfig::test_quantization_none_no_flag 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:50 INFO     deployment.conversion.orchestration: Output directory: /tmp/pytest-of-codespace/pytest-120/test_quantization_none_no_flag0/outputs/conversion/test
2026-01-18 16:25:50 INFO     deployment.conversion.orchestration: Created MLflow run: test_run_name (test_run_id...)
2026-01-18 16:25:50 INFO     deployment.conversion.orchestration: Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /path/to/checkpoint --config-dir /tmp/pytest-of-codespace/pytest-120/test_quantization_none_no_flag0/config --backbone distilbert-base-uncased --output-dir /tmp/pytest-of-codespace/pytest-120/test_quantization_none_no_flag0/outputs/conversion/test --opset-version 18 --run-smoke-test
2026-01-18 16:25:50 INFO     infrastructure.tracking.mlflow.lifecycle: Run test_run_id... already terminated with status <Mock name='create_mlflow_client().get_run().info.status' id='123304181236256'> (expected FINISHED)
2026-01-18 16:25:50 INFO     deployment.conversion.orchestration: Conversion completed. ONNX model: /tmp/pytest-of-codespace/pytest-120/test_quantization_none_no_flag0/outputs/conversion/test/model.onnx
PASSED
tests/conversion/integration/test_conversion_config.py::TestConversionConfig::test_run_smoke_test_true_adds_flag 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:50 INFO     deployment.conversion.orchestration: Output directory: /tmp/pytest-of-codespace/pytest-120/test_run_smoke_test_true_adds_0/outputs/conversion/test
2026-01-18 16:25:50 INFO     deployment.conversion.orchestration: Created MLflow run: test_run_name (test_run_id...)
2026-01-18 16:25:50 INFO     deployment.conversion.orchestration: Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /path/to/checkpoint --config-dir /tmp/pytest-of-codespace/pytest-120/test_run_smoke_test_true_adds_0/config --backbone distilbert-base-uncased --output-dir /tmp/pytest-of-codespace/pytest-120/test_run_smoke_test_true_adds_0/outputs/conversion/test --opset-version 18 --run-smoke-test
2026-01-18 16:25:50 INFO     infrastructure.tracking.mlflow.lifecycle: Run test_run_id... already terminated with status <Mock name='create_mlflow_client().get_run().info.status' id='123304162646992'> (expected FINISHED)
2026-01-18 16:25:50 INFO     deployment.conversion.orchestration: Conversion completed. ONNX model: /tmp/pytest-of-codespace/pytest-120/test_run_smoke_test_true_adds_0/outputs/conversion/test/model.onnx
PASSED
tests/conversion/integration/test_conversion_config.py::TestConversionConfig::test_run_smoke_test_false_no_flag 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:50 INFO     deployment.conversion.orchestration: Output directory: /tmp/pytest-of-codespace/pytest-120/test_run_smoke_test_false_no_f0/outputs/conversion/test
2026-01-18 16:25:50 INFO     deployment.conversion.orchestration: Created MLflow run: test_run_name (test_run_id...)
2026-01-18 16:25:50 INFO     deployment.conversion.orchestration: Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /path/to/checkpoint --config-dir /tmp/pytest-of-codespace/pytest-120/test_run_smoke_test_false_no_f0/config --backbone distilbert-base-uncased --output-dir /tmp/pytest-of-codespace/pytest-120/test_run_smoke_test_false_no_f0/outputs/conversion/test --opset-version 18
2026-01-18 16:25:50 INFO     infrastructure.tracking.mlflow.lifecycle: Run test_run_id... already terminated with status <Mock name='create_mlflow_client().get_run().info.status' id='123304181216160'> (expected FINISHED)
2026-01-18 16:25:50 INFO     deployment.conversion.orchestration: Conversion completed. ONNX model: /tmp/pytest-of-codespace/pytest-120/test_run_smoke_test_false_no_f0/outputs/conversion/test/model.onnx
PASSED
tests/conversion/integration/test_conversion_config.py::TestConversionConfig::test_filename_pattern_used_in_find_onnx_model 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:50 INFO     deployment.conversion.orchestration: Output directory: /tmp/pytest-of-codespace/pytest-120/test_filename_pattern_used_in_0/outputs/conversion/test
2026-01-18 16:25:50 INFO     deployment.conversion.orchestration: Created MLflow run: test_run_name (test_run_id...)
2026-01-18 16:25:50 INFO     deployment.conversion.orchestration: Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /path/to/checkpoint --config-dir /tmp/pytest-of-codespace/pytest-120/test_filename_pattern_used_in_0/config --backbone distilbert-base-uncased --output-dir /tmp/pytest-of-codespace/pytest-120/test_filename_pattern_used_in_0/outputs/conversion/test --opset-version 18 --run-smoke-test
2026-01-18 16:25:50 INFO     infrastructure.tracking.mlflow.lifecycle: Run test_run_id... already terminated with status <Mock name='create_mlflow_client().get_run().info.status' id='123304162672256'> (expected FINISHED)
2026-01-18 16:25:50 INFO     deployment.conversion.orchestration: Conversion completed. ONNX model: /tmp/pytest-of-codespace/pytest-120/test_filename_pattern_used_in_0/outputs/conversion/test/custom_fp32_model.onnx
PASSED
tests/conversion/unit/test_conversion_config.py::TestConversionConfigLoading::test_load_yaml_loads_conversion_config PASSED
tests/conversion/unit/test_conversion_config.py::TestConversionConfigLoading::test_conversion_config_structure PASSED
tests/conversion/unit/test_conversion_config.py::TestConversionConfigLoading::test_conversion_config_default_values PASSED
tests/conversion/unit/test_conversion_config.py::TestConversionConfigLoading::test_conversion_config_custom_values PASSED
tests/conversion/unit/test_conversion_config.py::TestConversionConfigLoading::test_conversion_config_missing_sections PASSED
tests/conversion/unit/test_conversion_config.py::TestConversionConfigLoading::test_conversion_config_types PASSED
tests/conversion/unit/test_conversion_config.py::TestConversionConfigLoading::test_load_actual_conversion_yaml PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_target_format_extraction PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_target_format_default PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_onnx_opset_version_extraction PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_onnx_opset_version_custom PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_onnx_opset_version_default PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_onnx_quantization_extraction PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_onnx_quantization_int8 PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_onnx_quantization_dynamic PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_onnx_quantization_default PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_onnx_run_smoke_test_extraction PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_onnx_run_smoke_test_false PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_onnx_run_smoke_test_default PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_output_filename_pattern_extraction PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_output_filename_pattern_custom PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_output_filename_pattern_default PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_all_options_together PASSED
tests/evaluation/selection/integration/test_benchmarking_checkpoint_resolution.py::TestCheckpointResolutionFromLocalDisk::test_resolve_checkpoint_from_champion_checkpoint_path PASSED
tests/evaluation/selection/integration/test_benchmarking_checkpoint_resolution.py::TestCheckpointResolutionFromLocalDisk::test_resolve_checkpoint_from_trial_dir PASSED
tests/evaluation/selection/integration/test_benchmarking_checkpoint_resolution.py::TestCheckpointResolutionFromMLflow::test_resolve_from_mlflow_refit_run PASSED
tests/evaluation/selection/integration/test_benchmarking_checkpoint_resolution.py::TestCheckpointResolutionFromMLflow::test_resolve_from_mlflow_parent_run PASSED
tests/evaluation/selection/integration/test_benchmarking_checkpoint_resolution.py::TestBenchmarkingWorkflowIntegration::test_benchmarking_with_local_checkpoint PASSED
tests/evaluation/selection/integration/test_benchmarking_checkpoint_resolution.py::TestBenchmarkingWorkflowIntegration::test_benchmarking_with_mlflow_checkpoint PASSED
tests/evaluation/selection/integration/test_checkpoint_acquisition_for_benchmarking.py::TestRefitRunDiscovery::test_find_refit_run_by_hash_match PASSED
tests/evaluation/selection/integration/test_checkpoint_acquisition_for_benchmarking.py::TestRefitRunDiscovery::test_find_refit_run_by_study_hash_only PASSED
tests/evaluation/selection/integration/test_checkpoint_acquisition_for_benchmarking.py::TestRefitRunDiscovery::test_find_refit_run_via_parent_relationship PASSED
tests/evaluation/selection/integration/test_checkpoint_acquisition_for_benchmarking.py::TestRefitRunDiscovery::test_find_any_refit_run_as_last_resort PASSED
tests/evaluation/selection/integration/test_checkpoint_acquisition_for_benchmarking.py::TestCheckpointArtifactDiscovery::test_checkpoint_in_refit_run PASSED
tests/evaluation/selection/integration/test_checkpoint_acquisition_for_benchmarking.py::TestCheckpointArtifactDiscovery::test_checkpoint_in_parent_hpo_run PASSED
tests/evaluation/selection/integration/test_checkpoint_acquisition_for_benchmarking.py::TestCheckpointArtifactDiscovery::test_no_checkpoint_in_trial_run PASSED
tests/evaluation/selection/integration/test_checkpoint_acquisition_for_benchmarking.py::TestCheckpointArtifactDiscovery::test_checkpoint_artifact_paths PASSED
tests/evaluation/selection/integration/test_checkpoint_acquisition_for_benchmarking.py::TestCheckpointAcquisitionWorkflow::test_acquire_from_refit_run_success PASSED
tests/evaluation/selection/integration/test_checkpoint_acquisition_for_benchmarking.py::TestCheckpointAcquisitionWorkflow::test_acquire_from_parent_hpo_run_fallback PASSED
tests/evaluation/selection/integration/test_checkpoint_acquisition_for_benchmarking.py::TestCheckpointAcquisitionWorkflow::test_no_checkpoint_found_anywhere PASSED
tests/evaluation/selection/integration/test_checkpoint_acquisition_for_benchmarking.py::TestEdgeCases::test_parent_run_id_not_available PASSED
tests/evaluation/selection/integration/test_checkpoint_acquisition_for_benchmarking.py::TestEdgeCases::test_refit_run_without_trial_key_hash PASSED
tests/evaluation/selection/integration/test_checkpoint_acquisition_for_benchmarking.py::TestEdgeCases::test_multiple_refit_runs PASSED
tests/evaluation/selection/test_champion_selection.py::TestSelectChampionPerBackbone::test_successful_selection_v2 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:50 INFO     evaluation.selection.trial_finder.mlflow_queries: Found 3 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 16:25:50 INFO     evaluation.selection.trial_finder.champion_selection: Grouped runs for distilbert: 0 v1 group(s), 1 v2 group(s)
2026-01-18 16:25:50 INFO     evaluation.selection.trial_finder.champion_selection: Found 1 eligible group(s) for distilbert (0 skipped due to min_trials requirement)
2026-01-18 16:25:50 INFO     evaluation.selection.artifact_unified.selectors: Found refit run refit-run-12... for trial run2... (selected latest from 1 refit run(s))
2026-01-18 16:25:50 INFO     evaluation.selection.trial_finder.champion_selection: Found refit run refit-run-12... for champion trial run2... (using SSOT selector: refit_preferred)
PASSED
tests/evaluation/selection/test_champion_selection.py::TestSelectChampionPerBackbone::test_no_runs_returns_none 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:50 INFO     evaluation.selection.trial_finder.mlflow_queries: No runs found with stage='hpo_trial' for distilbert, trying legacy stage='hpo'
2026-01-18 16:25:50 INFO     evaluation.selection.trial_finder.mlflow_queries: Found 0 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 16:25:50 INFO     evaluation.selection.trial_finder.champion_selection: Grouped runs for distilbert: 0 v1 group(s), 0 v2 group(s)
2026-01-18 16:25:50 WARNING  evaluation.selection.trial_finder.mlflow_queries: No v2 groups found and v1 is no longer supported. Returning empty groups.
2026-01-18 16:25:50 WARNING  evaluation.selection.trial_finder.champion_selection: No valid groups found for distilbert. No trial runs found in HPO experiment 'test_hpo_experiment'. This may indicate:
  - HPO was not run for this backbone
  - Runs exist but don't have required tags (stage='hpo_trial' or 'hpo', backbone tag)
  - Runs exist but were filtered out (missing metrics, artifacts, or grouping tags)
Skipping champion selection for distilbert.
PASSED
tests/evaluation/selection/test_champion_selection.py::TestSelectChampionPerBackbone::test_insufficient_trials_returns_none 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:50 INFO     evaluation.selection.trial_finder.mlflow_queries: Found 2 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 16:25:50 INFO     evaluation.selection.trial_finder.champion_selection: Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)
2026-01-18 16:25:50 WARNING  evaluation.selection.trial_finder.mlflow_queries: No v2 groups found and v1 is no longer supported. Returning empty groups.
2026-01-18 16:25:50 WARNING  evaluation.selection.trial_finder.champion_selection: No valid groups found for distilbert. Found 1 v1 group(s) and 0 v2 group(s), but none matched selection criteria (schema_version preference: auto). This may indicate:
  - Groups don't meet min_trials requirement
  - Schema version mismatch (prefer_schema_version=auto)
  - Groups filtered out by other criteria
Skipping champion selection for distilbert.
PASSED
tests/evaluation/selection/test_champion_selection.py::TestSelectChampionPerBackbone::test_missing_metrics_filtered 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:50 INFO     evaluation.selection.trial_finder.mlflow_queries: Found 3 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 16:25:50 INFO     evaluation.selection.trial_finder.champion_selection: Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)
2026-01-18 16:25:50 WARNING  evaluation.selection.trial_finder.mlflow_queries: No v2 groups found and v1 is no longer supported. Returning empty groups.
2026-01-18 16:25:50 WARNING  evaluation.selection.trial_finder.champion_selection: No valid groups found for distilbert. Found 1 v1 group(s) and 0 v2 group(s), but none matched selection criteria (schema_version preference: auto). This may indicate:
  - Groups don't meet min_trials requirement
  - Schema version mismatch (prefer_schema_version=auto)
  - Groups filtered out by other criteria
Skipping champion selection for distilbert.
PASSED
tests/evaluation/selection/test_champion_selection.py::TestSelectChampionPerBackbone::test_nan_metrics_filtered 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:50 INFO     evaluation.selection.trial_finder.mlflow_queries: Found 3 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 16:25:50 INFO     evaluation.selection.trial_finder.champion_selection: Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)
2026-01-18 16:25:50 WARNING  evaluation.selection.trial_finder.mlflow_queries: No v2 groups found and v1 is no longer supported. Returning empty groups.
2026-01-18 16:25:50 WARNING  evaluation.selection.trial_finder.champion_selection: No valid groups found for distilbert. Found 1 v1 group(s) and 0 v2 group(s), but none matched selection criteria (schema_version preference: auto). This may indicate:
  - Groups don't meet min_trials requirement
  - Schema version mismatch (prefer_schema_version=auto)
  - Groups filtered out by other criteria
Skipping champion selection for distilbert.
PASSED
tests/evaluation/selection/test_champion_selection.py::TestSelectChampionPerBackbone::test_artifact_availability_filter 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:50 INFO     evaluation.selection.trial_finder.mlflow_queries: Found 3 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 16:25:50 WARNING  evaluation.selection.trial_finder.champion_selection: Artifact filter: 1 run(s) have code.artifact.available='false' (explicitly marked as unavailable)
2026-01-18 16:25:50 WARNING  evaluation.selection.trial_finder.champion_selection: Artifact filter: 1 run(s) excluded (1 explicitly false, 0 missing/legacy allowed)
2026-01-18 16:25:50 WARNING  evaluation.selection.trial_finder.champion_selection: Artifact filter removed 1 runs for distilbert (2 remaining). Check that runs have 'code.artifact.available' tag set to 'true'.
2026-01-18 16:25:50 INFO     evaluation.selection.trial_finder.champion_selection: Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)
2026-01-18 16:25:50 WARNING  evaluation.selection.trial_finder.mlflow_queries: No v2 groups found and v1 is no longer supported. Returning empty groups.
2026-01-18 16:25:50 WARNING  evaluation.selection.trial_finder.champion_selection: No valid groups found for distilbert. Found 1 v1 group(s) and 0 v2 group(s), but none matched selection criteria (schema_version preference: auto). This may indicate:
  - Groups don't meet min_trials requirement
  - Schema version mismatch (prefer_schema_version=auto)
  - Groups filtered out by other criteria
Skipping champion selection for distilbert.
PASSED
tests/evaluation/selection/test_champion_selection.py::TestSelectChampionPerBackbone::test_no_artifact_requirement 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:50 INFO     evaluation.selection.trial_finder.mlflow_queries: Found 3 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 16:25:50 INFO     evaluation.selection.trial_finder.champion_selection: Grouped runs for distilbert: 0 v1 group(s), 1 v2 group(s)
2026-01-18 16:25:50 INFO     evaluation.selection.trial_finder.champion_selection: Found 1 eligible group(s) for distilbert (0 skipped due to min_trials requirement)
2026-01-18 16:25:50 INFO     evaluation.selection.artifact_unified.selectors: Found refit run refit-run-12... for trial run2... (selected latest from 1 refit run(s))
2026-01-18 16:25:50 INFO     evaluation.selection.trial_finder.champion_selection: Found refit run refit-run-12... for champion trial run2... (using SSOT selector: refit_preferred)
PASSED
tests/evaluation/selection/test_champion_selection.py::TestSelectChampionPerBackbone::test_never_mix_v1_v2_when_disabled 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:50 INFO     evaluation.selection.trial_finder.mlflow_queries: Found 5 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 16:25:50 INFO     evaluation.selection.trial_finder.champion_selection: Grouped runs for distilbert: 1 v1 group(s), 1 v2 group(s)
2026-01-18 16:25:50 INFO     evaluation.selection.trial_finder.champion_selection: Found both v1 and v2 runs for distilbert. Using 2.0 groups only (never mixing versions).
2026-01-18 16:25:50 INFO     evaluation.selection.trial_finder.champion_selection: Found 1 eligible group(s) for distilbert (0 skipped due to min_trials requirement)
2026-01-18 16:25:50 INFO     evaluation.selection.artifact_unified.selectors: Found refit run refit-run-12... for trial run3... (selected latest from 1 refit run(s))
2026-01-18 16:25:50 INFO     evaluation.selection.trial_finder.champion_selection: Found refit run refit-run-12... for champion trial run3... (using SSOT selector: refit_preferred)
PASSED
tests/evaluation/selection/test_champion_selection.py::TestSelectChampionPerBackbone::test_minimize_objective 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:50 INFO     evaluation.selection.trial_finder.mlflow_queries: Found 3 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 16:25:50 INFO     evaluation.selection.trial_finder.champion_selection: Grouped runs for distilbert: 0 v1 group(s), 1 v2 group(s)
2026-01-18 16:25:50 INFO     evaluation.selection.trial_finder.champion_selection: Found 1 eligible group(s) for distilbert (0 skipped due to min_trials requirement)
2026-01-18 16:25:50 INFO     evaluation.selection.artifact_unified.selectors: Found refit run refit-run-12... for trial run3... (selected latest from 1 refit run(s))
2026-01-18 16:25:50 INFO     evaluation.selection.trial_finder.champion_selection: Found refit run refit-run-12... for champion trial run3... (using SSOT selector: refit_preferred)
PASSED
tests/evaluation/selection/test_champion_selection.py::TestSelectChampionPerBackbone::test_multiple_groups_selects_best 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:50 INFO     evaluation.selection.trial_finder.mlflow_queries: Found 6 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 16:25:50 INFO     evaluation.selection.trial_finder.champion_selection: Grouped runs for distilbert: 0 v1 group(s), 2 v2 group(s)
2026-01-18 16:25:50 INFO     evaluation.selection.trial_finder.champion_selection: Found 2 eligible group(s) for distilbert (0 skipped due to min_trials requirement)
2026-01-18 16:25:50 INFO     evaluation.selection.artifact_unified.selectors: Found refit run refit-run-12... for trial run6... (selected latest from 1 refit run(s))
2026-01-18 16:25:50 INFO     evaluation.selection.trial_finder.champion_selection: Found refit run refit-run-12... for champion trial run6... (using SSOT selector: refit_preferred)
PASSED
tests/evaluation/selection/test_champion_selection.py::TestSelectChampionPerBackbone::test_stable_score_computation 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:50 INFO     evaluation.selection.trial_finder.mlflow_queries: Found 5 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 16:25:50 INFO     evaluation.selection.trial_finder.champion_selection: Grouped runs for distilbert: 0 v1 group(s), 1 v2 group(s)
2026-01-18 16:25:50 INFO     evaluation.selection.trial_finder.champion_selection: Found 1 eligible group(s) for distilbert (0 skipped due to min_trials requirement)
2026-01-18 16:25:50 INFO     evaluation.selection.artifact_unified.selectors: Found refit run refit-run-12... for trial run3... (selected latest from 1 refit run(s))
2026-01-18 16:25:50 INFO     evaluation.selection.trial_finder.champion_selection: Found refit run refit-run-12... for champion trial run3... (using SSOT selector: refit_preferred)
PASSED
tests/final_training/integration/test_final_training_component.py::test_execute_final_training_reuse_if_exists_skips_training 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:50 INFO     training.execution.executor: Final training config loaded from final_training.yaml
2026-01-18 16:25:50 INFO     training.execution.executor: Output directory: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_re0/outputs/v1
2026-01-18 16:25:50 INFO     training.execution.executor: Found existing completed final training run
2026-01-18 16:25:50 INFO     training.execution.executor:   Output directory: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_re0/outputs/v1
2026-01-18 16:25:50 INFO     training.execution.executor:   Checkpoint: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_re0/outputs/v1/checkpoint
2026-01-18 16:25:50 INFO     training.execution.executor:   Reusing existing checkpoint (run.mode: reuse_if_exists)
PASSED
tests/final_training/integration/test_final_training_component.py::test_execute_final_training_force_new_runs_training 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:50 INFO     training.execution.executor: Final training config loaded from final_training.yaml
2026-01-18 16:25:50 INFO     training.execution.executor: Output directory: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_fo0/outputs/v1
2026-01-18 16:25:50 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_fo0/config/tags.yaml, using defaults
2026-01-18 16:25:50 INFO     training.execution.mlflow_setup:  View run test_run_name at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/640045798020977743/runs/4e8515fdfbd7415484a0ae2c159266bf
2026-01-18 16:25:50 INFO     training.execution.mlflow_setup: Created MLflow run: test_run_name (4e8515fdfbd7...)
2026-01-18 16:25:50 INFO     training.execution.executor: Created MLflow run: test_run_name (4e8515fdfbd7...)
2026-01-18 16:25:50 INFO     training.execution.executor: Running final training...
2026-01-18 16:25:50 INFO     infrastructure.tracking.mlflow.lifecycle: Run 4e8515fdfbd7... already terminated with status <Mock name='mock.get_run().info.status' id='123304183349888'> (expected FINISHED)
2026-01-18 16:25:50 INFO     training.execution.executor: Saved metadata to: None
2026-01-18 16:25:50 INFO     training.execution.executor: Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_fo0/outputs/v1/checkpoint
2026-01-18 16:25:50 INFO     training.execution.executor: MLflow run: 4e8515fdfbd7...
PASSED
tests/final_training/integration/test_final_training_component.py::test_execute_final_training_resume_if_incomplete_continues 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:50 INFO     training.execution.executor: Final training config loaded from final_training.yaml
2026-01-18 16:25:50 INFO     training.execution.executor: Output directory: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_re1/outputs/v1
2026-01-18 16:25:50 WARNING  training.execution.executor: Could not search for existing runs: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_re1/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.
2026-01-18 16:25:50 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_re1/config/tags.yaml, using defaults
2026-01-18 16:25:50 INFO     training.execution.mlflow_setup:  View run test_run_name at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/640045798020977743/runs/4887db959d2d47059c04709512f7268c
2026-01-18 16:25:50 INFO     training.execution.mlflow_setup: Created MLflow run: test_run_name (4887db959d2d...)
2026-01-18 16:25:50 INFO     training.execution.executor: Created MLflow run: test_run_name (4887db959d2d...)
2026-01-18 16:25:50 INFO     training.execution.executor: Running final training...
2026-01-18 16:25:50 INFO     infrastructure.tracking.mlflow.lifecycle: Run 4887db959d2d... already terminated with status <Mock name='mock.get_run().info.status' id='123304183277184'> (expected FINISHED)
2026-01-18 16:25:50 INFO     training.execution.executor: Saved metadata to: None
2026-01-18 16:25:50 INFO     training.execution.executor: Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_re1/outputs/v1/checkpoint
2026-01-18 16:25:50 INFO     training.execution.executor: MLflow run: 4887db959d2d...
PASSED
tests/final_training/integration/test_final_training_component.py::test_execute_final_training_missing_dataset_raises_error 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:50 INFO     training.execution.executor: Final training config loaded from final_training.yaml
2026-01-18 16:25:50 INFO     training.execution.executor: Output directory: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_mi0/outputs/v1
PASSED
tests/final_training/integration/test_final_training_component.py::test_execute_final_training_local_path_override 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:50 INFO     training.execution.executor: Final training config loaded from final_training.yaml
2026-01-18 16:25:50 INFO     training.execution.executor: Output directory: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_lo0/outputs/v1
2026-01-18 16:25:50 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_lo0/config/tags.yaml, using defaults
2026-01-18 16:25:50 INFO     training.execution.mlflow_setup:  View run test_run_name at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/640045798020977743/runs/9dd5bd306dba4f08936de341740583db
2026-01-18 16:25:50 INFO     training.execution.mlflow_setup: Created MLflow run: test_run_name (9dd5bd306dba...)
2026-01-18 16:25:50 INFO     training.execution.executor: Created MLflow run: test_run_name (9dd5bd306dba...)
2026-01-18 16:25:50 INFO     training.execution.executor: Running final training...
2026-01-18 16:25:50 INFO     infrastructure.tracking.mlflow.lifecycle: Run 9dd5bd306dba... already terminated with status <Mock name='mock.get_run().info.status' id='123304183274832'> (expected FINISHED)
2026-01-18 16:25:50 INFO     training.execution.executor: Saved metadata to: None
2026-01-18 16:25:50 INFO     training.execution.executor: Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_lo0/outputs/v1/checkpoint
2026-01-18 16:25:50 INFO     training.execution.executor: MLflow run: 9dd5bd306dba...
PASSED
tests/final_training/integration/test_final_training_component.py::test_execute_final_training_training_failure_marks_run_failed 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:50 INFO     training.execution.executor: Final training config loaded from final_training.yaml
2026-01-18 16:25:50 INFO     training.execution.executor: Output directory: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_tr0/outputs/v1
2026-01-18 16:25:50 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_tr0/config/tags.yaml, using defaults
2026-01-18 16:25:50 INFO     training.execution.mlflow_setup:  View run test_run_name at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/640045798020977743/runs/8a0786b0aea047fca7155fe23dfe91e0
2026-01-18 16:25:50 INFO     training.execution.mlflow_setup: Created MLflow run: test_run_name (8a0786b0aea0...)
2026-01-18 16:25:50 INFO     training.execution.executor: Created MLflow run: test_run_name (8a0786b0aea0...)
2026-01-18 16:25:50 INFO     training.execution.executor: Running final training...
2026-01-18 16:25:50 INFO     infrastructure.tracking.mlflow.lifecycle: Run 8a0786b0aea0... already has status <Mock name='mock.get_run().info.status' id='123304183273104'>, skipping termination (expected RUNNING)
PASSED
tests/final_training/integration/test_final_training_component.py::test_execute_final_training_mlflow_disabled_skips_tracking 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:50 INFO     training.execution.executor: Final training config loaded from final_training.yaml
2026-01-18 16:25:50 INFO     training.execution.executor: Output directory: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_ml0/outputs/v1
2026-01-18 16:25:50 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_ml0/config/tags.yaml, using defaults
2026-01-18 16:25:50 INFO     training.execution.mlflow_setup:  View run test_run_name at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/640045798020977743/runs/61bc9f38fb1b4a148ecccb9748845c9a
2026-01-18 16:25:50 INFO     training.execution.mlflow_setup: Created MLflow run: test_run_name (61bc9f38fb1b...)
2026-01-18 16:25:50 INFO     training.execution.executor: Created MLflow run: test_run_name (61bc9f38fb1b...)
2026-01-18 16:25:50 INFO     training.execution.executor: Running final training...
2026-01-18 16:25:50 INFO     infrastructure.tracking.mlflow.lifecycle: Run 61bc9f38fb1b... already terminated with status <Mock name='mock.get_run().info.status' id='123304183282128'> (expected FINISHED)
2026-01-18 16:25:50 INFO     training.execution.executor: Saved metadata to: None
2026-01-18 16:25:50 INFO     training.execution.executor: Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_ml0/outputs/v1/checkpoint
2026-01-18 16:25:50 INFO     training.execution.executor: MLflow run: 61bc9f38fb1b...
PASSED
tests/final_training/integration/test_final_training_component.py::test_execute_final_training_source_scratch_no_checkpoint 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:50 INFO     training.execution.executor: Final training config loaded from final_training.yaml
2026-01-18 16:25:50 INFO     training.execution.executor: Output directory: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_so0/outputs/v1
2026-01-18 16:25:50 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_so0/config/tags.yaml, using defaults
2026-01-18 16:25:50 INFO     training.execution.mlflow_setup:  View run test_run_name at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/640045798020977743/runs/a6c39d411f31484b9acd07f9b5f0162e
2026-01-18 16:25:50 INFO     training.execution.mlflow_setup: Created MLflow run: test_run_name (a6c39d411f31...)
2026-01-18 16:25:50 INFO     training.execution.executor: Created MLflow run: test_run_name (a6c39d411f31...)
2026-01-18 16:25:50 INFO     training.execution.executor: Running final training...
2026-01-18 16:25:50 INFO     infrastructure.tracking.mlflow.lifecycle: Run a6c39d411f31... already terminated with status <Mock name='mock.get_run().info.status' id='123304183175856'> (expected FINISHED)
2026-01-18 16:25:50 INFO     training.execution.executor: Saved metadata to: None
2026-01-18 16:25:50 INFO     training.execution.executor: Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_so0/outputs/v1/checkpoint
2026-01-18 16:25:50 INFO     training.execution.executor: MLflow run: a6c39d411f31...
PASSED
tests/final_training/integration/test_final_training_component.py::test_execute_final_training_source_final_training_with_checkpoint 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:50 INFO     training.execution.executor: Final training config loaded from final_training.yaml
2026-01-18 16:25:50 INFO     training.execution.executor: Output directory: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_so1/outputs/v1
2026-01-18 16:25:50 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_so1/config/tags.yaml, using defaults
2026-01-18 16:25:50 INFO     training.execution.mlflow_setup:  View run test_run_name at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/640045798020977743/runs/1fa8bf30980a4f46aee1b5202b85b616
2026-01-18 16:25:50 INFO     training.execution.mlflow_setup: Created MLflow run: test_run_name (1fa8bf30980a...)
2026-01-18 16:25:50 INFO     training.execution.executor: Created MLflow run: test_run_name (1fa8bf30980a...)
2026-01-18 16:25:50 INFO     training.execution.executor: Running final training...
2026-01-18 16:25:50 INFO     infrastructure.tracking.mlflow.lifecycle: Run 1fa8bf30980a... already terminated with status <Mock name='mock.get_run().info.status' id='123304183272960'> (expected FINISHED)
2026-01-18 16:25:50 INFO     training.execution.executor: Saved metadata to: None
2026-01-18 16:25:50 INFO     training.execution.executor: Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_so1/outputs/v1/checkpoint
2026-01-18 16:25:50 INFO     training.execution.executor: MLflow run: 1fa8bf30980a...
PASSED
tests/final_training/integration/test_final_training_component.py::test_execute_final_training_hyperparameter_precedence 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:50 INFO     training.execution.executor: Final training config loaded from final_training.yaml
2026-01-18 16:25:50 INFO     training.execution.executor: Output directory: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_hy0/outputs/v1
2026-01-18 16:25:50 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_hy0/config/tags.yaml, using defaults
2026-01-18 16:25:51 INFO     training.execution.mlflow_setup:  View run test_run_name at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/640045798020977743/runs/971f69509d254cad87f42dc3a9d014e5
2026-01-18 16:25:51 INFO     training.execution.mlflow_setup: Created MLflow run: test_run_name (971f69509d25...)
2026-01-18 16:25:51 INFO     training.execution.executor: Created MLflow run: test_run_name (971f69509d25...)
2026-01-18 16:25:51 INFO     training.execution.executor: Running final training...
2026-01-18 16:25:51 INFO     infrastructure.tracking.mlflow.lifecycle: Run 971f69509d25... already terminated with status <Mock name='mock.get_run().info.status' id='123304183166480'> (expected FINISHED)
2026-01-18 16:25:51 INFO     training.execution.executor: Saved metadata to: None
2026-01-18 16:25:51 INFO     training.execution.executor: Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_hy0/outputs/v1/checkpoint
2026-01-18 16:25:51 INFO     training.execution.executor: MLflow run: 971f69509d25...
PASSED
tests/final_training/integration/test_final_training_component.py::test_execute_final_training_mlflow_overrides 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:51 INFO     training.execution.executor: Final training config loaded from final_training.yaml
2026-01-18 16:25:51 INFO     training.execution.executor: Output directory: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_ml1/outputs/v1
2026-01-18 16:25:51 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_ml1/config/tags.yaml, using defaults
2026-01-18 16:25:51 INFO     training.execution.mlflow_setup:  View run default_run_name at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/170515528070219461/runs/2066149438eb44b2996c5d1d2498a40d
2026-01-18 16:25:51 INFO     training.execution.mlflow_setup: Created MLflow run: default_run_name (2066149438eb...)
2026-01-18 16:25:51 INFO     training.execution.executor: Created MLflow run: default_run_name (2066149438eb...)
2026-01-18 16:25:51 INFO     training.execution.executor: Running final training...
2026-01-18 16:25:51 INFO     infrastructure.tracking.mlflow.lifecycle: Run 2066149438eb... already terminated with status <Mock name='mock.get_run().info.status' id='123304183270896'> (expected FINISHED)
2026-01-18 16:25:51 INFO     training.execution.executor: Saved metadata to: None
2026-01-18 16:25:51 INFO     training.execution.executor: Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_ml1/outputs/v1/checkpoint
2026-01-18 16:25:51 INFO     training.execution.executor: MLflow run: 2066149438eb...
PASSED
tests/final_training/integration/test_final_training_logging_intervals.py::test_logging_eval_interval_loaded_from_config 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:51 INFO     training.execution.executor: Final training config loaded from final_training.yaml
2026-01-18 16:25:51 INFO     training.execution.executor: Output directory: /tmp/pytest-of-codespace/pytest-120/test_logging_eval_interval_loa0/outputs/v1
2026-01-18 16:25:51 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_logging_eval_interval_loa0/config/tags.yaml, using defaults
2026-01-18 16:25:51 INFO     training.execution.mlflow_setup:  View run test_run_name at: file:///tmp/mlflow/#/experiments/640045798020977743/runs/fc2eb2b090e14495b65e1701fa107924
2026-01-18 16:25:51 INFO     training.execution.mlflow_setup: Created MLflow run: test_run_name (fc2eb2b090e1...)
2026-01-18 16:25:51 INFO     training.execution.executor: Created MLflow run: test_run_name (fc2eb2b090e1...)
2026-01-18 16:25:51 INFO     training.execution.executor: Running final training...
2026-01-18 16:25:51 INFO     infrastructure.tracking.mlflow.lifecycle: Run fc2eb2b090e1... already terminated with status <Mock name='mock.get_run().info.status' id='123304183166624'> (expected FINISHED)
2026-01-18 16:25:51 INFO     training.execution.executor: Saved metadata to: None
2026-01-18 16:25:51 INFO     training.execution.executor: Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-120/test_logging_eval_interval_loa0/outputs/v1/checkpoint
2026-01-18 16:25:51 INFO     training.execution.executor: MLflow run: fc2eb2b090e1...
PASSED
tests/final_training/integration/test_final_training_logging_intervals.py::test_logging_save_interval_loaded_from_config 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:51 INFO     training.execution.executor: Final training config loaded from final_training.yaml
2026-01-18 16:25:51 INFO     training.execution.executor: Output directory: /tmp/pytest-of-codespace/pytest-120/test_logging_save_interval_loa0/outputs/v1
2026-01-18 16:25:51 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_logging_save_interval_loa0/config/tags.yaml, using defaults
2026-01-18 16:25:51 INFO     training.execution.mlflow_setup:  View run test_run_name at: file:///tmp/mlflow/#/experiments/640045798020977743/runs/82e68a4be85640649f5e4c6932315072
2026-01-18 16:25:51 INFO     training.execution.mlflow_setup: Created MLflow run: test_run_name (82e68a4be856...)
2026-01-18 16:25:51 INFO     training.execution.executor: Created MLflow run: test_run_name (82e68a4be856...)
2026-01-18 16:25:51 INFO     training.execution.executor: Running final training...
2026-01-18 16:25:51 INFO     infrastructure.tracking.mlflow.lifecycle: Run 82e68a4be856... already terminated with status <Mock name='mock.get_run().info.status' id='123304183166912'> (expected FINISHED)
2026-01-18 16:25:51 INFO     training.execution.executor: Saved metadata to: None
2026-01-18 16:25:51 INFO     training.execution.executor: Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-120/test_logging_save_interval_loa0/outputs/v1/checkpoint
2026-01-18 16:25:51 INFO     training.execution.executor: MLflow run: 82e68a4be856...
PASSED
tests/final_training/integration/test_final_training_logging_intervals.py::test_logging_intervals_both_loaded_from_config 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:51 INFO     training.execution.executor: Final training config loaded from final_training.yaml
2026-01-18 16:25:51 INFO     training.execution.executor: Output directory: /tmp/pytest-of-codespace/pytest-120/test_logging_intervals_both_lo0/outputs/v1
2026-01-18 16:25:51 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_logging_intervals_both_lo0/config/tags.yaml, using defaults
2026-01-18 16:25:51 INFO     training.execution.mlflow_setup:  View run test_run_name at: file:///tmp/mlflow/#/experiments/640045798020977743/runs/43a201ece3564efc8b815a1abaa3ea9e
2026-01-18 16:25:51 INFO     training.execution.mlflow_setup: Created MLflow run: test_run_name (43a201ece356...)
2026-01-18 16:25:51 INFO     training.execution.executor: Created MLflow run: test_run_name (43a201ece356...)
2026-01-18 16:25:51 INFO     training.execution.executor: Running final training...
2026-01-18 16:25:51 INFO     infrastructure.tracking.mlflow.lifecycle: Run 43a201ece356... already terminated with status <Mock name='mock.get_run().info.status' id='123304183162400'> (expected FINISHED)
2026-01-18 16:25:51 INFO     training.execution.executor: Saved metadata to: None
2026-01-18 16:25:51 INFO     training.execution.executor: Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-120/test_logging_intervals_both_lo0/outputs/v1/checkpoint
2026-01-18 16:25:51 INFO     training.execution.executor: MLflow run: 43a201ece356...
PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestSourceParentDictFormat::test_source_parent_dict_format_resolves_checkpoint PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestSourceParentDictFormat::test_source_parent_dict_format_with_validation_false PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestCheckpointSource::test_checkpoint_source_string_path PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestCheckpointSource::test_checkpoint_source_relative_path PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestCheckpointSource::test_checkpoint_source_dict_format PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestCheckpointSource::test_checkpoint_source_validation_fails PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestCheckpointSource::test_checkpoint_source_validation_false_allows_invalid PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestSeedRandomSeed::test_seed_random_seed_override PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestSeedRandomSeed::test_seed_falls_back_to_train_config PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestSeedRandomSeed::test_seed_falls_back_to_default PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestSeedRandomSeed::test_seed_precedence_final_training_over_best_config PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestVariantNumber::test_variant_number_explicit_override PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestVariantNumber::test_variant_number_ignored_when_force_new PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestVariantNumber::test_variant_number_none_auto_increments PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestEarlyStopping::test_early_stopping_enabled_override PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestEarlyStopping::test_early_stopping_patience_override PASSED
tests/hpo/integration/test_best_trial_selection_component.py::TestBestTrialSelection::test_best_trial_selected_by_optuna PASSED
tests/hpo/integration/test_best_trial_selection_component.py::TestBestTrialSelection::test_best_trial_extraction PASSED
tests/hpo/integration/test_best_trial_selection_component.py::TestBestTrialSelection::test_best_trial_with_cv_statistics PASSED
tests/hpo/integration/test_best_trial_selection_component.py::TestBestTrialSelection::test_best_trial_minimization_direction PASSED
tests/hpo/integration/test_best_trial_selection_component.py::TestBestTrialSelectionWithCriteria::test_selection_with_accuracy_threshold PASSED
tests/hpo/integration/test_best_trial_selection_component.py::TestBestTrialSelectionWithCriteria::test_selection_with_min_accuracy_gain PASSED
tests/hpo/integration/test_best_trial_selection_component.py::TestBestTrialSelectionWithCriteria::test_selection_accuracy_only_when_outside_threshold PASSED
tests/hpo/integration/test_best_trial_selection_component.py::TestBestTrialSelectionWithCriteria::test_selection_smoke_yaml_parameters PASSED
tests/hpo/integration/test_best_trial_selection_component.py::TestBestTrialIntegration::test_best_trial_used_for_refit PASSED
tests/hpo/integration/test_best_trial_selection_component.py::TestBestTrialIntegration::test_best_trial_with_no_completed_trials PASSED
tests/hpo/integration/test_best_trial_selection_component.py::TestBestTrialIntegration::test_best_trial_preserves_hyperparameters PASSED
tests/hpo/integration/test_early_termination.py::TestPrunerCreation::test_create_pruner_bandit_policy PASSED
tests/hpo/integration/test_early_termination.py::TestPrunerCreation::test_create_pruner_median_policy PASSED
tests/hpo/integration/test_early_termination.py::TestPrunerCreation::test_create_pruner_no_early_termination PASSED
tests/hpo/integration/test_early_termination.py::TestPrunerCreation::test_create_pruner_smoke_yaml_params PASSED
tests/hpo/integration/test_early_termination.py::TestPruningBehavior::test_pruner_delays_evaluation PASSED
tests/hpo/integration/test_early_termination.py::TestPruningBehavior::test_pruner_prunes_poor_trials PASSED
tests/hpo/integration/test_early_termination.py::TestPruningBehavior::test_pruner_evaluation_interval PASSED
tests/hpo/integration/test_early_termination.py::TestPruningBehavior::test_pruner_with_study_manager FAILED
tests/hpo/integration/test_early_termination.py::TestPruningIntegration::test_pruning_preserves_best_trials PASSED
tests/hpo/integration/test_early_termination.py::TestPruningIntegration::test_pruning_with_checkpoint_resume FAILED
tests/hpo/integration/test_early_termination.py::TestPruningIntegration::test_pruning_disabled_behavior PASSED
tests/hpo/integration/test_early_termination.py::TestPruningSmokeYaml::test_pruning_smoke_yaml_config PASSED
tests/hpo/integration/test_error_handling.py::TestTrialExecutionErrors::test_training_subprocess_failure PASSED
tests/hpo/integration/test_error_handling.py::TestTrialExecutionErrors::test_training_module_not_found PASSED
tests/hpo/integration/test_error_handling.py::TestTrialExecutionErrors::test_missing_metrics_file 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:53 ERROR    training.hpo.trial.metrics: metrics.json not found at expected location: /tmp/pytest-of-codespace/pytest-120/test_missing_metrics_file0/outputs/hpo/local/distilbert/study-abc12345/trial-def67890/metrics.json. Trial output dir: /tmp/pytest-of-codespace/pytest-120/test_missing_metrics_file0/outputs/hpo/local/distilbert/study-abc12345/trial-def67890, Root dir: /tmp/pytest-of-codespace/pytest-120/test_missing_metrics_file0
PASSED
tests/hpo/integration/test_error_handling.py::TestTrialExecutionErrors::test_missing_objective_metric_in_metrics PASSED
tests/hpo/integration/test_error_handling.py::TestCVOrchestratorErrors::test_cv_trial_failure_propagates_error 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:53 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-120/test_cv_trial_failure_propagat0/config/naming.yaml, using empty policy
2026-01-18 16:25:53 WARNING  training.hpo.execution.local.cv: Could not build systematic run name and tags: Naming policy not available or formatting failed. Legacy fallback removed. Process type: hpo_trial, using fallback
2026-01-18 16:25:53 WARNING  training.hpo.execution.local.cv: Could not create trial run: Could not find experiment with ID exp_123
2026-01-18 16:25:53 WARNING  infrastructure.paths.resolve: Failed to load paths.yaml config: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_cv_trial_failure_propagat0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.. Using fallback logic.
2026-01-18 16:25:53 INFO     training.hpo.execution.local.cv: [CV] Created trial folder: /workspaces/resume-ner-azureml/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-10fa119c (trial 0)
PASSED
tests/hpo/integration/test_error_handling.py::TestCVOrchestratorErrors::test_cv_missing_trial_key_hash_raises_error 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:53 WARNING  infrastructure.tracking.mlflow.hash_utils: Could not get or compute study_key_hash. study_key_hash=missing, hpo_parent_run_id=provided, configs=missing
2026-01-18 16:25:53 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-120/test_cv_missing_trial_key_hash0/config/naming.yaml, using empty policy
2026-01-18 16:25:53 WARNING  training.hpo.execution.local.cv: Could not build systematic run name and tags: HPO trial run name built without study_key_hash; check study identity propagation., using fallback
2026-01-18 16:25:53 WARNING  training.hpo.execution.local.cv: Could not create trial run: Could not find experiment with ID exp_123
2026-01-18 16:25:53 WARNING  infrastructure.tracking.mlflow.hash_utils: Could not get or compute study_key_hash. study_key_hash=missing, hpo_parent_run_id=provided, configs=missing
2026-01-18 16:25:53 ERROR    training.hpo.execution.local.cv: In v2 study folder study-abc12345 but trial_key_hash is None. study_key_hash=NO. Will attempt to compute hash.
PASSED
tests/hpo/integration/test_error_handling.py::TestRefitExecutionErrors::test_refit_subprocess_failure 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:53 INFO     training.hpo.execution.local.refit: [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'batch_size': 4, 'dropout': 0.2, 'weight_decay': 0.05, 'backbone': 'distilbert'}
2026-01-18 16:25:53 INFO     training.hpo.execution.local.refit: [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 16:25:53 WARNING  infrastructure.paths.resolve: Failed to load paths.yaml config: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_refit_subprocess_failure0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.. Using fallback logic.
2026-01-18 16:25:53 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-120/test_refit_subprocess_failure0/config/naming.yaml, using empty policy
FAILED
tests/hpo/integration/test_error_handling.py::TestRefitExecutionErrors::test_refit_non_v2_study_folder_raises_error 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:53 INFO     training.hpo.execution.local.refit: [REFIT] Starting refit training for trial 0 with hyperparameters: {'backbone': 'distilbert'}
2026-01-18 16:25:53 INFO     training.hpo.execution.local.refit: [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 16:25:53 WARNING  training.hpo.execution.local.refit: Could not construct v2 refit folder, falling back to legacy: 'NoneType' object has no attribute 'mkdir'
PASSED
tests/hpo/integration/test_error_handling.py::TestStudyManagerErrors::test_study_creation_with_invalid_storage_uri PASSED
tests/hpo/integration/test_error_handling.py::TestBestTrialSelectionErrors::test_extract_best_config_with_no_completed_trials PASSED
tests/hpo/integration/test_error_handling.py::TestBestTrialSelectionErrors::test_selection_logic_with_empty_candidates PASSED
tests/hpo/integration/test_error_handling.py::TestSearchSpaceErrors::test_invalid_search_space_type PASSED
tests/hpo/integration/test_error_handling.py::TestSearchSpaceErrors::test_invalid_float_range PASSED
tests/hpo/integration/test_error_handling.py::TestMLflowErrors::test_mlflow_run_creation_failure_handled_gracefully PASSED
tests/hpo/integration/test_error_handling.py::TestConfigurationErrors::test_missing_objective_metric_in_config PASSED
tests/hpo/integration/test_error_handling.py::TestConfigurationErrors::test_invalid_sampling_algorithm PASSED
tests/hpo/integration/test_error_handling.py::TestPathResolutionErrors::test_missing_config_dir_handled_gracefully PASSED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointCreation::test_checkpoint_storage_path_resolution PASSED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointCreation::test_checkpoint_storage_path_with_backbone_placeholder PASSED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointCreation::test_checkpoint_disabled_returns_none PASSED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointCreation::test_storage_uri_conversion PASSED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointResume::test_resume_from_existing_checkpoint 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:53 WARNING  evaluation.selection.trial_finder.discovery: No v2 study folders found in /tmp/pytest-of-codespace/pytest-120/test_resume_from_existing_chec0/outputs/hpo (resolved from /tmp/pytest-of-codespace/pytest-120/test_resume_from_existing_chec0/outputs/hpo)
2026-01-18 16:25:53 INFO     training.hpo.core.study: [HPO] Resuming optimization for distilbert from checkpoint...
2026-01-18 16:25:53 INFO     training.hpo.core.study: Loaded 0 existing trials (0 completed, 0 marked as failed)
FAILED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointResume::test_resume_preserves_trials 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:54 WARNING  evaluation.selection.trial_finder.discovery: No v2 study folders found in /tmp/pytest-of-codespace/pytest-120/test_resume_preserves_trials0/outputs/hpo (resolved from /tmp/pytest-of-codespace/pytest-120/test_resume_preserves_trials0/outputs/hpo)
2026-01-18 16:25:54 INFO     training.hpo.core.study: [HPO] Resuming optimization for distilbert from checkpoint...
2026-01-18 16:25:54 INFO     training.hpo.core.study: Loaded 0 existing trials (0 completed, 0 marked as failed)
FAILED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointResume::test_resume_marks_running_trials_as_failed 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:54 WARNING  evaluation.selection.trial_finder.discovery: No v2 study folders found in /tmp/pytest-of-codespace/pytest-120/test_resume_marks_running_tria0/outputs/hpo (resolved from /tmp/pytest-of-codespace/pytest-120/test_resume_marks_running_tria0/outputs/hpo)
2026-01-18 16:25:54 INFO     training.hpo.core.study: [HPO] Resuming optimization for distilbert from checkpoint...
2026-01-18 16:25:54 INFO     training.hpo.core.study: Loaded 0 existing trials (0 completed, 0 marked as failed)
FAILED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointResume::test_resume_with_auto_resume_false_raises_error 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:54 WARNING  evaluation.selection.trial_finder.discovery: No v2 study folders found in /tmp/pytest-of-codespace/pytest-120/test_resume_with_auto_resume_f0/outputs/hpo (resolved from /tmp/pytest-of-codespace/pytest-120/test_resume_with_auto_resume_f0/outputs/hpo)
2026-01-18 16:25:54 INFO     training.hpo.core.study: [HPO] auto_resume=false: Creating new study 'hpo_distilbert_no_resume_test' (ignoring existing study)
2026-01-18 16:25:54 INFO     training.hpo.core.study: [HPO] Starting optimization for distilbert with checkpointing...
FAILED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointResume::test_resume_continues_trial_numbering 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:54 WARNING  evaluation.selection.trial_finder.discovery: No v2 study folders found in /tmp/pytest-of-codespace/pytest-120/test_resume_continues_trial_nu0/outputs/hpo (resolved from /tmp/pytest-of-codespace/pytest-120/test_resume_continues_trial_nu0/outputs/hpo)
2026-01-18 16:25:54 INFO     training.hpo.core.study: [HPO] Resuming optimization for distilbert from checkpoint...
2026-01-18 16:25:54 INFO     training.hpo.core.study: Loaded 0 existing trials (0 completed, 0 marked as failed)
FAILED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointSmokeYaml::test_checkpoint_smoke_yaml_study_name_template PASSED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointSmokeYaml::test_checkpoint_smoke_yaml_storage_path_template PASSED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointSmokeYaml::test_checkpoint_smoke_yaml_auto_resume_true 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:54 WARNING  evaluation.selection.trial_finder.discovery: No v2 study folders found in /tmp/pytest-of-codespace/pytest-120/test_checkpoint_smoke_yaml_aut0/outputs/hpo (resolved from /tmp/pytest-of-codespace/pytest-120/test_checkpoint_smoke_yaml_aut0/outputs/hpo)
2026-01-18 16:25:54 INFO     training.hpo.core.study: [HPO] Resuming optimization for distilbert from checkpoint...
2026-01-18 16:25:54 INFO     training.hpo.core.study: Loaded 0 existing trials (0 completed, 0 marked as failed)
FAILED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointFileIO::test_checkpoint_file_exists_after_study_creation PASSED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointFileIO::test_checkpoint_file_persists_trials PASSED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointFileIO::test_checkpoint_file_can_be_moved_and_loaded PASSED
tests/hpo/integration/test_hpo_full_workflow.py::TestFullHPOWorkflow::test_full_hpo_workflow_with_cv_and_refit FAILED
tests/hpo/integration/test_hpo_full_workflow.py::TestFullHPOWorkflow::test_full_hpo_workflow_no_cv_no_refit FAILED
tests/hpo/integration/test_hpo_full_workflow.py::TestFullHPOWorkflow::test_full_hpo_workflow_creates_correct_path_structure FAILED
tests/hpo/integration/test_hpo_resume_workflow.py::TestHPOResumeWorkflow::test_resume_workflow_preserves_trials FAILED
tests/hpo/integration/test_hpo_resume_workflow.py::TestHPOResumeWorkflow::test_resume_workflow_with_different_run_id FAILED
tests/hpo/integration/test_hpo_resume_workflow.py::TestHPOResumeWorkflow::test_resume_workflow_with_cv FAILED
tests/hpo/integration/test_hpo_run_mode_variants.py::TestHPORunModeForceNew::test_force_new_creates_variant_1_on_first_run PASSED
tests/hpo/integration/test_hpo_run_mode_variants.py::TestHPORunModeForceNew::test_force_new_creates_variant_2_on_second_run PASSED
tests/hpo/integration/test_hpo_run_mode_variants.py::TestHPORunModeForceNew::test_force_new_creates_variant_3_on_third_run PASSED
tests/hpo/integration/test_hpo_run_mode_variants.py::TestHPORunModeForceNew::test_force_new_with_custom_study_name PASSED
tests/hpo/integration/test_hpo_run_mode_variants.py::TestHPORunModeReuseIfExists::test_reuse_if_exists_uses_base_name PASSED
tests/hpo/integration/test_hpo_run_mode_variants.py::TestHPORunModeReuseIfExists::test_reuse_if_exists_even_with_existing_variants PASSED
tests/hpo/integration/test_hpo_run_mode_variants.py::TestStudyManagerWithRunMode::test_study_manager_extracts_run_mode PASSED
tests/hpo/integration/test_hpo_run_mode_variants.py::TestStudyManagerWithRunMode::test_study_manager_passes_run_mode_to_create_study_name FAILED
tests/hpo/integration/test_hpo_run_mode_variants.py::TestSmokeYamlRunModeConfig::test_smoke_yaml_run_mode_default PASSED
tests/hpo/integration/test_hpo_run_mode_variants.py::TestSmokeYamlRunModeConfig::test_smoke_yaml_run_mode_force_new PASSED
tests/hpo/integration/test_hpo_run_mode_variants.py::TestSmokeYamlRunModeConfig::test_smoke_yaml_study_name_null_auto_generate PASSED
tests/hpo/integration/test_hpo_run_mode_variants.py::TestSmokeYamlRunModeConfig::test_smoke_yaml_variant_sequence PASSED
tests/hpo/integration/test_hpo_studies_dict_storage.py::TestHPOStudiesDictStorage::test_notebook_loop_stores_all_backbones PASSED
tests/hpo/integration/test_hpo_studies_dict_storage.py::TestHPOStudiesDictStorage::test_validate_hpo_studies_dict_helper PASSED
tests/hpo/integration/test_hpo_sweep_setup.py::TestHPOSweepSetup::test_load_configs_from_smoke_yaml PASSED
tests/hpo/integration/test_hpo_sweep_setup.py::TestHPOSweepSetup::test_setup_hpo_mlflow_run_creates_parent_run 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:55 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 16:25:55 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 16:25:55 INFO     infrastructure.tracking.mlflow.index.version_counter: [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:7cc3f049175243f692c605769296b4e812adcb6c65bbf..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-18 16:25:55 INFO     infrastructure.tracking.mlflow.index.version_counter: [Reserve Version] Loaded 30 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-18 16:25:55 INFO     infrastructure.tracking.mlflow.index.version_counter: [Reserve Version] Found 12 allocations for counter_key (after deduplication): committed=[], reserved=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], expired=[], max_committed_version=0
2026-01-18 16:25:55 INFO     infrastructure.tracking.mlflow.index.version_counter: [Reserve Version] Allocation details: [(1, 'reserved', 'pending_2026'), (2, 'reserved', 'pending_2026'), (3, 'reserved', 'pending_2026'), (4, 'reserved', 'pending_2026'), (5, 'reserved', 'pending_2026'), (6, 'reserved', 'pending_2026'), (7, 'reserved', 'pending_2026'), (8, 'reserved', 'pending_2026'), (9, 'reserved', 'pending_2026'), (10, 'reserved', 'pending_2026'), (11, 'reserved', 'pending_2026'), (12, 'reserved', 'pending_2026')]
2026-01-18 16:25:55 INFO     infrastructure.tracking.mlflow.index.version_counter: [Reserve Version] Reserving next version: 13 (incremented from max_committed=0, skipped 12 reserved/expired versions)
2026-01-18 16:25:55 INFO     infrastructure.tracking.mlflow.index.version_counter: [Reserve Version]  Successfully reserved version 13 for counter_key resume-ner:hpo:7cc3f049175243f692c605769296b4e812a... (run_id: pending_2026...)
PASSED
tests/hpo/integration/test_hpo_sweep_setup.py::TestHPOSweepSetup::test_setup_hpo_mlflow_run_computes_study_key_hash 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:55 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_setup_hpo_mlflow_run_comp0/config, raw_auto_inc_config={}
2026-01-18 16:25:55 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
PASSED
tests/hpo/integration/test_hpo_sweep_setup.py::TestHPOSweepSetup::test_create_study_name_from_checkpoint_config PASSED
tests/hpo/integration/test_hpo_sweep_setup.py::TestHPOSweepSetup::test_create_study_name_without_checkpoint PASSED
tests/hpo/integration/test_hpo_sweep_setup.py::TestHPOSweepSetup::test_setup_hpo_mlflow_run_tags 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:55 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 16:25:55 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 16:25:55 INFO     infrastructure.tracking.mlflow.index.version_counter: [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:9aece94eee717121a4a3d5b5d8008953a198f5d1de41f..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-18 16:25:55 INFO     infrastructure.tracking.mlflow.index.version_counter: [Reserve Version] Loaded 31 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-18 16:25:55 INFO     infrastructure.tracking.mlflow.index.version_counter: [Reserve Version] Found 11 allocations for counter_key (after deduplication): committed=[], reserved=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], expired=[], max_committed_version=0
2026-01-18 16:25:55 INFO     infrastructure.tracking.mlflow.index.version_counter: [Reserve Version] Allocation details: [(1, 'reserved', 'pending_2026'), (2, 'reserved', 'pending_2026'), (3, 'reserved', 'pending_2026'), (4, 'reserved', 'pending_2026'), (5, 'reserved', 'pending_2026'), (6, 'reserved', 'pending_2026'), (7, 'reserved', 'pending_2026'), (8, 'reserved', 'pending_2026'), (9, 'reserved', 'pending_2026'), (10, 'reserved', 'pending_2026'), (11, 'reserved', 'pending_2026')]
2026-01-18 16:25:55 INFO     infrastructure.tracking.mlflow.index.version_counter: [Reserve Version] Reserving next version: 12 (incremented from max_committed=0, skipped 11 reserved/expired versions)
2026-01-18 16:25:55 INFO     infrastructure.tracking.mlflow.index.version_counter: [Reserve Version]  Successfully reserved version 12 for counter_key resume-ner:hpo:9aece94eee717121a4a3d5b5d8008953a19... (run_id: pending_2026...)
PASSED
tests/hpo/integration/test_hpo_sweep_setup.py::TestHPOSweepSetup::test_checkpoint_file_created PASSED
tests/hpo/integration/test_hpo_sweep_setup.py::TestHPOSweepSetup::test_checkpoint_file_created_v2_hash_based PASSED
tests/hpo/integration/test_hpo_sweep_setup.py::TestHPOSweepSetup::test_study_key_hash_and_family_hash_computed PASSED
tests/hpo/integration/test_hpo_sweep_setup.py::TestHPOSweepSetup::test_setup_hpo_mlflow_run_trusts_provided_config_dir 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:55 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_setup_hpo_mlflow_run_trus0/project1/config, raw_auto_inc_config={}
2026-01-18 16:25:55 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
PASSED
tests/hpo/integration/test_hpo_sweep_setup.py::TestHPOSweepSetup::test_setup_hpo_mlflow_run_infers_config_dir_when_none 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:55 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_setup_hpo_mlflow_run_infe0/project/config, raw_auto_inc_config={}
2026-01-18 16:25:55 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunHierarchy::test_hpo_parent_run_has_correct_tags PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunHierarchy::test_trial_run_is_child_of_hpo_parent PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunHierarchy::test_fold_run_is_child_of_trial_run PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunHierarchy::test_refit_run_is_child_of_hpo_parent PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunTags::test_hpo_parent_run_tags PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunTags::test_trial_run_tags PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunTags::test_refit_run_tags PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunTags::test_trial_run_inherits_study_key_hash_from_parent PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunMetrics::test_trial_run_logs_metrics PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunMetrics::test_trial_run_logs_hyperparameters PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunMetrics::test_cv_trial_run_logs_aggregated_metrics PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunMetrics::test_refit_run_logs_metrics PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunStructure::test_mlflow_structure_no_cv PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunStructure::test_mlflow_structure_with_cv PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunStructure::test_mlflow_runs_have_grouping_tags PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunStatus::test_trial_run_status_transitions PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunStatus::test_refit_run_status_finished_after_upload PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureV2::test_study_folder_naming PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureV2::test_trial_folder_naming PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureV2::test_refit_folder_structure PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureV2::test_cv_fold_folder_structure PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureV2::test_checkpoint_location_in_trial PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureV2::test_checkpoint_location_in_refit PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureV2::test_checkpoint_location_in_cv_fold PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureValidation::test_is_v2_path_detects_trial_folder PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureValidation::test_find_study_folder PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureValidation::test_find_trial_folder PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureWithBuildOutputPath::test_build_output_path_hpo_sweep PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureWithBuildOutputPath::test_build_output_path_hpo_trial PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureWithBuildOutputPath::test_build_output_path_hpo_refit PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureFiles::test_study_db_location PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureFiles::test_trial_meta_json_location PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureFiles::test_fold_splits_json_location PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureFiles::test_metrics_json_location_in_trial PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureFiles::test_metrics_json_location_in_refit PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureSmokeYaml::test_path_structure_matches_smoke_yaml PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureSmokeYaml::test_path_structure_study8_trial8_format PASSED
tests/hpo/integration/test_refit_training.py::TestRefitTrainingSetup::test_refit_uses_best_trial_hyperparameters 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:56 INFO     training.hpo.execution.local.refit: [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'batch_size': 4, 'dropout': 0.2, 'weight_decay': 0.05, 'backbone': 'distilbert'}
2026-01-18 16:25:56 INFO     training.hpo.execution.local.refit: [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 16:25:56 WARNING  infrastructure.paths.resolve: Failed to load paths.yaml config: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_refit_uses_best_trial_hyp0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.. Using fallback logic.
2026-01-18 16:25:56 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-120/test_refit_uses_best_trial_hyp0/config/naming.yaml, using empty policy
FAILED
tests/hpo/integration/test_refit_training.py::TestRefitTrainingSetup::test_refit_creates_mlflow_run 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:56 INFO     training.hpo.execution.local.refit: [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-18 16:25:56 INFO     training.hpo.execution.local.refit: [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 16:25:56 WARNING  infrastructure.paths.resolve: Failed to load paths.yaml config: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_refit_creates_mlflow_run0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.. Using fallback logic.
2026-01-18 16:25:56 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-120/test_refit_creates_mlflow_run0/config/naming.yaml, using empty policy
FAILED
tests/hpo/integration/test_refit_training.py::TestRefitTrainingSetup::test_refit_creates_v2_output_directory 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:56 INFO     training.hpo.execution.local.refit: [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-18 16:25:56 INFO     training.hpo.execution.local.refit: [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 16:25:56 WARNING  infrastructure.paths.resolve: Failed to load paths.yaml config: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_refit_creates_v2_output_d0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.. Using fallback logic.
2026-01-18 16:25:56 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-120/test_refit_creates_v2_output_d0/config/naming.yaml, using empty policy
FAILED
tests/hpo/integration/test_refit_training.py::TestRefitTrainingExecution::test_refit_reads_metrics_from_file 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:56 INFO     training.hpo.execution.local.refit: [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-18 16:25:56 INFO     training.hpo.execution.local.refit: [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 16:25:56 WARNING  infrastructure.paths.resolve: Failed to load paths.yaml config: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_refit_reads_metrics_from_0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.. Using fallback logic.
2026-01-18 16:25:56 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-120/test_refit_reads_metrics_from_0/config/naming.yaml, using empty policy
FAILED
tests/hpo/integration/test_refit_training.py::TestRefitTrainingExecution::test_refit_logs_metrics_to_mlflow 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:56 INFO     training.hpo.execution.local.refit: [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-18 16:25:56 INFO     training.hpo.execution.local.refit: [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 16:25:56 WARNING  infrastructure.paths.resolve: Failed to load paths.yaml config: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_refit_logs_metrics_to_mlf0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.. Using fallback logic.
2026-01-18 16:25:56 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-120/test_refit_logs_metrics_to_mlf0/config/naming.yaml, using empty policy
FAILED
tests/hpo/integration/test_refit_training.py::TestRefitTrainingExecution::test_refit_creates_checkpoint_directory 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:56 INFO     training.hpo.execution.local.refit: [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-18 16:25:56 INFO     training.hpo.execution.local.refit: [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 16:25:56 WARNING  infrastructure.paths.resolve: Failed to load paths.yaml config: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_refit_creates_checkpoint_0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.. Using fallback logic.
2026-01-18 16:25:56 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-120/test_refit_creates_checkpoint_0/config/naming.yaml, using empty policy
FAILED
tests/hpo/integration/test_refit_training.py::TestRefitTrainingSmokeYaml::test_refit_enabled_in_smoke_yaml 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:56 INFO     training.hpo.execution.local.refit: [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-18 16:25:56 INFO     training.hpo.execution.local.refit: [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 16:25:56 WARNING  infrastructure.paths.resolve: Failed to load paths.yaml config: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_refit_enabled_in_smoke_ya0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.. Using fallback logic.
2026-01-18 16:25:56 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-120/test_refit_enabled_in_smoke_ya0/config/naming.yaml, using empty policy
FAILED
tests/hpo/integration/test_refit_training.py::TestRefitTrainingSmokeYaml::test_refit_uses_full_epochs 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:56 INFO     training.hpo.execution.local.refit: [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-18 16:25:56 INFO     training.hpo.execution.local.refit: [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 16:25:56 WARNING  infrastructure.paths.resolve: Failed to load paths.yaml config: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_refit_uses_full_epochs0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.. Using fallback logic.
2026-01-18 16:25:56 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-120/test_refit_uses_full_epochs0/config/naming.yaml, using empty policy
FAILED
tests/hpo/integration/test_refit_training.py::TestRefitCheckpointDuplicationPrevention::test_refit_skips_checkpoint_folder_logging FAILED
tests/hpo/integration/test_refit_training.py::TestRefitCheckpointDuplicationPrevention::test_refit_prevents_duplication_only_archive_uploaded 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:56 INFO     training.hpo.execution.local.refit: [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-18 16:25:56 INFO     training.hpo.execution.local.refit: [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 16:25:56 WARNING  infrastructure.paths.resolve: Failed to load paths.yaml config: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_refit_prevents_duplicatio0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.. Using fallback logic.
2026-01-18 16:25:56 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-120/test_refit_prevents_duplicatio0/config/naming.yaml, using empty policy
FAILED
tests/hpo/integration/test_smoke_yaml_options.py::TestTimeoutMinutes::test_timeout_minutes_stops_study_after_timeout PASSED
tests/hpo/integration/test_smoke_yaml_options.py::TestTimeoutMinutes::test_timeout_minutes_conversion_to_seconds PASSED
tests/hpo/integration/test_smoke_yaml_options.py::TestSaveOnlyBest::test_save_only_best_deletes_non_best_checkpoints PASSED
tests/hpo/integration/test_smoke_yaml_options.py::TestSaveOnlyBest::test_save_only_best_false_preserves_all_checkpoints PASSED
tests/hpo/integration/test_smoke_yaml_options.py::TestLogBestCheckpoint::test_log_best_checkpoint_config_enabled PASSED
tests/hpo/integration/test_smoke_yaml_options.py::TestLogBestCheckpoint::test_log_best_checkpoint_config_disabled PASSED
tests/hpo/integration/test_smoke_yaml_options.py::TestLogBestCheckpoint::test_log_best_checkpoint_conditional_call PASSED
tests/hpo/integration/test_smoke_yaml_options.py::TestDisableAutoCleanup::test_disable_auto_cleanup_false_enables_cleanup PASSED
tests/hpo/integration/test_smoke_yaml_options.py::TestDisableAutoCleanup::test_disable_auto_cleanup_true_disables_cleanup PASSED
tests/hpo/integration/test_smoke_yaml_options.py::TestDisableAutoCleanup::test_disable_auto_cleanup_default_is_disabled PASSED
tests/hpo/integration/test_smoke_yaml_options.py::TestDisableAutoOptunaMark::test_disable_auto_optuna_mark_false_enables_marking FAILED
tests/hpo/integration/test_smoke_yaml_options.py::TestDisableAutoOptunaMark::test_disable_auto_optuna_mark_true_skips_marking PASSED
tests/hpo/integration/test_trial_execution.py::TestTrialExecutionNoCV::test_trial_execution_no_cv_creates_single_run 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:57 INFO     training.hpo.execution.local.trial: [TRIAL] Training completed. Objective metric 'macro-f1': 0.75
PASSED
tests/hpo/integration/test_trial_execution.py::TestTrialExecutionNoCV::test_trial_execution_no_cv_output_path 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:57 INFO     training.hpo.execution.local.trial: [TRIAL] Training completed. Objective metric 'macro-f1': 0.8
PASSED
tests/hpo/integration/test_trial_execution.py::TestTrialExecutionNoCV::test_trial_execution_no_cv_metrics_file 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:57 INFO     training.hpo.execution.local.trial: [TRIAL] Training completed. Objective metric 'macro-f1': 0.75
PASSED
tests/hpo/integration/test_trial_execution.py::TestTrialExecutionWithCV::test_trial_execution_with_cv_creates_nested_runs 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:57 WARNING  infrastructure.tracking.mlflow.hash_utils: Could not get or compute study_key_hash. study_key_hash=missing, hpo_parent_run_id=provided, configs=missing
2026-01-18 16:25:57 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-120/test_trial_execution_with_cv_c0/config/naming.yaml, using empty policy
2026-01-18 16:25:57 WARNING  training.hpo.execution.local.cv: Could not build systematic run name and tags: Naming policy not available or formatting failed. Legacy fallback removed. Process type: hpo_trial, using fallback
2026-01-18 16:25:57 WARNING  training.hpo.execution.local.cv: Could not create trial run: expected string or bytes-like object
2026-01-18 16:25:57 WARNING  infrastructure.tracking.mlflow.hash_utils: Could not get or compute study_key_hash. study_key_hash=missing, hpo_parent_run_id=provided, configs=missing
2026-01-18 16:25:57 ERROR    training.hpo.execution.local.cv: In v2 study folder study-abc12345 but trial_key_hash is None. study_key_hash=NO. Will attempt to compute hash.
FAILED
tests/hpo/integration/test_trial_execution.py::TestTrialExecutionWithCV::test_trial_execution_with_cv_creates_fold_runs 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:57 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-120/test_trial_execution_with_cv_c1/config/naming.yaml, using empty policy
2026-01-18 16:25:57 WARNING  training.hpo.execution.local.cv: Could not build systematic run name and tags: Naming policy not available or formatting failed. Legacy fallback removed. Process type: hpo_trial, using fallback
2026-01-18 16:25:57 WARNING  training.hpo.execution.local.cv: Could not create trial run: expected string or bytes-like object
2026-01-18 16:25:57 WARNING  infrastructure.paths.resolve: Failed to load paths.yaml config: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_trial_execution_with_cv_c1/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.. Using fallback logic.
2026-01-18 16:25:57 INFO     training.hpo.execution.local.cv: [CV] Created trial folder: /tmp/pytest-of-codespace/pytest-120/test_trial_execution_with_cv_c1/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-1b593325 (trial 0)
FAILED
tests/hpo/integration/test_trial_execution.py::TestTrialExecutionWithCV::test_trial_execution_with_cv_aggregates_metrics 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:57 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-120/test_trial_execution_with_cv_a0/config/naming.yaml, using empty policy
2026-01-18 16:25:57 WARNING  training.hpo.execution.local.cv: Could not build systematic run name and tags: Naming policy not available or formatting failed. Legacy fallback removed. Process type: hpo_trial, using fallback
2026-01-18 16:25:57 WARNING  training.hpo.execution.local.cv: Could not create trial run: expected string or bytes-like object
2026-01-18 16:25:57 WARNING  infrastructure.tracking.mlflow.hash_utils: Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
2026-01-18 16:25:57 WARNING  training.hpo.execution.local.cv: Could not compute trial_key_hash
2026-01-18 16:25:57 ERROR    training.hpo.execution.local.cv: In v2 study folder study-abc12345 but trial_key_hash is None. study_key_hash=YES. Will attempt to compute hash.
2026-01-18 16:25:57 WARNING  training.hpo.execution.local.cv: In v2 study folder study-abc12345 but trial_key_hash is None. Attempting to recompute from trial_params...
2026-01-18 16:25:57 WARNING  infrastructure.tracking.mlflow.hash_utils: Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
FAILED
tests/hpo/integration/test_trial_execution.py::TestTrialExecutionWithCV::test_trial_execution_with_cv_output_paths 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:57 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-120/test_trial_execution_with_cv_o0/config/naming.yaml, using empty policy
2026-01-18 16:25:57 WARNING  training.hpo.execution.local.cv: Could not build systematic run name and tags: Naming policy not available or formatting failed. Legacy fallback removed. Process type: hpo_trial, using fallback
2026-01-18 16:25:57 WARNING  training.hpo.execution.local.cv: Could not create trial run: expected string or bytes-like object
2026-01-18 16:25:57 WARNING  infrastructure.tracking.mlflow.hash_utils: Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
2026-01-18 16:25:57 WARNING  training.hpo.execution.local.cv: Could not compute trial_key_hash
2026-01-18 16:25:57 ERROR    training.hpo.execution.local.cv: In v2 study folder study-abc12345 but trial_key_hash is None. study_key_hash=YES. Will attempt to compute hash.
2026-01-18 16:25:57 WARNING  training.hpo.execution.local.cv: In v2 study folder study-abc12345 but trial_key_hash is None. Attempting to recompute from trial_params...
2026-01-18 16:25:57 WARNING  infrastructure.tracking.mlflow.hash_utils: Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
FAILED
tests/hpo/integration/test_trial_execution.py::TestTrialExecutionWithCV::test_trial_execution_with_cv_smoke_yaml_params 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:57 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-120/test_trial_execution_with_cv_s0/config/naming.yaml, using empty policy
2026-01-18 16:25:57 WARNING  training.hpo.execution.local.cv: Could not build systematic run name and tags: Naming policy not available or formatting failed. Legacy fallback removed. Process type: hpo_trial, using fallback
2026-01-18 16:25:57 WARNING  training.hpo.execution.local.cv: Could not create trial run: expected string or bytes-like object
2026-01-18 16:25:57 WARNING  infrastructure.tracking.mlflow.hash_utils: Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
2026-01-18 16:25:57 WARNING  training.hpo.execution.local.cv: Could not compute trial_key_hash
2026-01-18 16:25:57 ERROR    training.hpo.execution.local.cv: In v2 study folder study-abc12345 but trial_key_hash is None. study_key_hash=YES. Will attempt to compute hash.
2026-01-18 16:25:57 WARNING  training.hpo.execution.local.cv: In v2 study folder study-abc12345 but trial_key_hash is None. Attempting to recompute from trial_params...
2026-01-18 16:25:57 WARNING  infrastructure.tracking.mlflow.hash_utils: Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
FAILED
tests/hpo/unit/test_hpo_variant_generation.py::TestCreateStudyNameWithVariants::test_create_study_name_force_new_no_existing PASSED
tests/hpo/unit/test_hpo_variant_generation.py::TestCreateStudyNameWithVariants::test_create_study_name_force_new_with_existing PASSED
tests/hpo/unit/test_hpo_variant_generation.py::TestCreateStudyNameWithVariants::test_create_study_name_reuse_if_exists PASSED
tests/hpo/unit/test_hpo_variant_generation.py::TestCreateStudyNameWithVariants::test_create_study_name_custom_study_name_force_new PASSED
tests/hpo/unit/test_hpo_variant_generation.py::TestCreateStudyNameWithVariants::test_create_study_name_custom_study_name_reuse_if_exists PASSED
tests/hpo/unit/test_hpo_variant_generation.py::TestCreateStudyNameWithVariants::test_create_study_name_checkpoint_disabled_force_new PASSED
tests/hpo/unit/test_hpo_variant_generation.py::TestFindStudyVariants::test_find_study_variants_none PASSED
tests/hpo/unit/test_hpo_variant_generation.py::TestFindStudyVariants::test_find_study_variants_implicit_variant_1 PASSED
tests/hpo/unit/test_hpo_variant_generation.py::TestFindStudyVariants::test_find_study_variants_explicit_variants PASSED
tests/hpo/unit/test_hpo_variant_generation.py::TestFindStudyVariants::test_find_study_variants_ignores_other_folders PASSED
tests/hpo/unit/test_hpo_variant_generation.py::TestFindStudyVariants::test_find_study_variants_different_backbone PASSED
tests/hpo/unit/test_search_space.py::TestSearchSpaceTranslation::test_translate_smoke_yaml_search_space PASSED
tests/hpo/unit/test_search_space.py::TestSearchSpaceTranslation::test_backbone_choice_values PASSED
tests/hpo/unit/test_search_space.py::TestSearchSpaceTranslation::test_learning_rate_loguniform_range PASSED
tests/hpo/unit/test_search_space.py::TestSearchSpaceTranslation::test_batch_size_choice_values PASSED
tests/hpo/unit/test_search_space.py::TestSearchSpaceTranslation::test_dropout_uniform_range PASSED
tests/hpo/unit/test_search_space.py::TestSearchSpaceTranslation::test_weight_decay_loguniform_range PASSED
tests/hpo/unit/test_search_space.py::TestSearchSpaceTranslation::test_exclude_params PASSED
tests/hpo/unit/test_search_space.py::TestSearchSpaceTranslation::test_unsupported_search_space_type PASSED
tests/hpo/unit/test_search_space.py::TestSearchSpaceTranslation::test_search_space_translator_class PASSED
tests/hpo/unit/test_trial_selection.py::TestSelectionCriteria::test_selection_faster_model_within_threshold PASSED
tests/hpo/unit/test_trial_selection.py::TestSelectionCriteria::test_selection_slower_model_when_accuracy_better PASSED
tests/hpo/unit/test_trial_selection.py::TestSelectionCriteria::test_selection_min_accuracy_gain_respected PASSED
tests/hpo/unit/test_trial_selection.py::TestSelectionCriteria::test_selection_tie_breaking_deterministic PASSED
tests/hpo/unit/test_trial_selection.py::TestSelectionCriteria::test_selection_relative_threshold_calculation PASSED
tests/hpo/unit/test_trial_selection.py::TestSelectionCriteria::test_selection_smoke_yaml_parameters PASSED
tests/hpo/unit/test_trial_selection.py::TestSelectionCriteria::test_selection_no_candidates_raises_error PASSED
tests/hpo/unit/test_trial_selection.py::TestSelectionCriteria::test_selection_single_candidate PASSED
tests/hpo/unit/test_trial_selection.py::TestSelectionCriteria::test_selection_no_threshold_accuracy_only PASSED
tests/hpo/unit/test_trial_selection.py::TestSelectionCriteria::test_selection_normalize_speed_scores PASSED
tests/hpo/unit/test_trial_selection.py::TestSelectionCriteria::test_selection_apply_threshold_logic PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_force_new_with_exists_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_force_new_without_exists_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_force_new_with_exists_and_complete_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_force_new_with_exists_and_incomplete_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_reuse_if_exists_hpo_with_exists_returns_true PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_reuse_if_exists_hpo_without_exists_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_reuse_if_exists_final_training_with_exists_and_complete_returns_true PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_reuse_if_exists_final_training_with_exists_and_incomplete_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_reuse_if_exists_final_training_without_exists_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_resume_if_incomplete_with_exists_and_incomplete_returns_true PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_resume_if_incomplete_with_exists_and_complete_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_resume_if_incomplete_without_exists_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_resume_if_incomplete_hpo_treats_as_reuse_if_exists PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_default_with_exists_returns_true PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_default_without_exists_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_not_exists_always_returns_false_regardless_of_mode PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestGetLoadIfExistsFlag::test_force_new_with_checkpoint_enabled_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestGetLoadIfExistsFlag::test_force_new_with_checkpoint_disabled_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestGetLoadIfExistsFlag::test_reuse_if_exists_with_checkpoint_enabled_returns_true PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestGetLoadIfExistsFlag::test_reuse_if_exists_with_checkpoint_disabled_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestGetLoadIfExistsFlag::test_default_with_checkpoint_enabled_returns_true PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestGetLoadIfExistsFlag::test_default_with_checkpoint_disabled_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestGetLoadIfExistsFlag::test_resume_if_incomplete_with_checkpoint_enabled_returns_true PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestGetLoadIfExistsFlag::test_resume_if_incomplete_with_checkpoint_disabled_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestGetLoadIfExistsFlag::test_checkpoint_disabled_always_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestProcessTypeIntegration::test_hpo_process_type_ignores_completeness PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestProcessTypeIntegration::test_final_training_process_type_checks_completeness PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestProcessTypeIntegration::test_selection_process_type_ignores_completeness PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestProcessTypeIntegration::test_benchmarking_process_type_ignores_completeness PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetObjectiveDirection::test_direction_key_maximize PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetObjectiveDirection::test_direction_key_minimize PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetObjectiveDirection::test_default_when_missing PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetObjectiveDirection::test_default_when_objective_empty PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetChampionSelectionConfig::test_default_values PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetChampionSelectionConfig::test_custom_values PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetChampionSelectionConfig::test_top_k_clamping_when_greater_than_min_trials 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:57 WARNING  infrastructure.config.selection: top_k_for_stable_score (5) > min_trials_per_group (3). Clamping top_k to 3.
PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetChampionSelectionConfig::test_top_k_equal_to_min_trials PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetChampionSelectionConfig::test_top_k_less_than_min_trials PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetChampionSelectionConfig::test_require_artifact_available_false PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetChampionSelectionConfig::test_artifact_check_source_disk PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetChampionSelectionConfig::test_prefer_schema_version_2_0 PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetChampionSelectionConfig::test_allow_mixed_schema_groups_true PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetChampionSelectionConfig::test_complete_config PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestComputeDataFingerprint::test_content_hash_priority PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestComputeDataFingerprint::test_manifest_hash_priority PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestComputeDataFingerprint::test_content_hash_over_manifest_hash PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestComputeDataFingerprint::test_semantic_fallback PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestComputeDataFingerprint::test_semantic_fallback_deterministic PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestComputeDataFingerprint::test_empty_config PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestComputeDataFingerprint::test_minimal_config PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestComputeEvalFingerprint::test_with_eval_config PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestComputeEvalFingerprint::test_deterministic PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestComputeEvalFingerprint::test_empty_config PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestComputeEvalFingerprint::test_with_different_configs PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestBuildHpoStudyKeyV2::test_basic_structure PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestBuildHpoStudyKeyV2::test_includes_fingerprints PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestBuildHpoStudyKeyV2::test_deterministic PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestBuildHpoStudyKeyV2::test_different_models_produce_different_keys PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestBuildHpoStudyKeyV2::test_with_benchmark_config PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestBuildHpoStudyKeyV2::test_without_benchmark_config PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestBuildHpoStudyKeyV2::test_hash_function PASSED
tests/infrastructure/naming/test_semantic_suffix.py::TestSemanticSuffixBehavior::test_auto_generated_study_name_returns_empty_semantic_suffix 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:57 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_auto_generated_study_name0/config, raw_auto_inc_config={}
2026-01-18 16:25:57 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
PASSED
tests/infrastructure/naming/test_semantic_suffix.py::TestSemanticSuffixBehavior::test_custom_study_name_includes_semantic_suffix 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:57 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_custom_study_name_include0/config, raw_auto_inc_config={}
2026-01-18 16:25:57 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
PASSED
tests/infrastructure/naming/test_semantic_suffix.py::TestSemanticSuffixBehavior::test_auto_generated_with_variant_returns_empty_semantic_suffix 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:57 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_auto_generated_with_varia0/config, raw_auto_inc_config={}
2026-01-18 16:25:57 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
PASSED
tests/infrastructure/paths/test_repo_root_detection.py::TestDetectRepoRoot::test_detects_from_config_dir PASSED
tests/infrastructure/paths/test_repo_root_detection.py::TestDetectRepoRoot::test_detects_from_output_dir PASSED
tests/infrastructure/paths/test_repo_root_detection.py::TestDetectRepoRoot::test_detects_from_start_path PASSED
tests/infrastructure/paths/test_repo_root_detection.py::TestDetectRepoRoot::test_detects_from_current_directory PASSED
tests/infrastructure/paths/test_repo_root_detection.py::TestDetectRepoRoot::test_detects_from_parent_directories PASSED
tests/infrastructure/paths/test_repo_root_detection.py::TestDetectRepoRoot::test_raises_value_error_when_not_found 
-------------------------------- live log call ---------------------------------
2026-01-18 16:25:57 WARNING  infrastructure.paths.repo: Could not find repository root. Falling back to current working directory: /tmp/pytest-of-codespace/pytest-120/test_raises_value_error_when_n0/random/structure
PASSED
tests/infrastructure/paths/test_repo_root_detection.py::TestDetectRepoRoot::test_prioritizes_config_dir_over_output_dir PASSED
tests/infrastructure/paths/test_repo_root_detection.py::TestDetectRepoRoot::test_works_with_config_file PASSED
tests/infrastructure/paths/test_repo_root_detection.py::TestValidateRepoRoot::test_validates_with_required_markers PASSED
tests/infrastructure/paths/test_repo_root_detection.py::TestValidateRepoRoot::test_rejects_without_config_dir PASSED
tests/infrastructure/paths/test_repo_root_detection.py::TestValidateRepoRoot::test_rejects_without_src_dir PASSED
tests/infrastructure/paths/test_repo_root_detection.py::TestValidateRepoRoot::test_validates_with_optional_markers PASSED
tests/infrastructure/paths/test_repo_root_detection.py::TestValidateRepoRoot::test_rejects_non_directory PASSED
tests/infrastructure/paths/test_repo_root_detection.py::TestHelperFunctions::test_infer_config_dir_uses_unified_function PASSED
tests/infrastructure/paths/test_repo_root_detection.py::TestHelperFunctions::test_resolve_project_paths_uses_unified_function PASSED
tests/infrastructure/paths/test_repo_root_detection.py::TestConfigLoading::test_loads_repository_root_config PASSED
tests/infrastructure/paths/test_repo_root_detection.py::TestConfigLoading::test_derives_markers_from_base PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestQueryRunsByTags::test_filters_finished_runs PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestQueryRunsByTags::test_filters_by_required_tags PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestQueryRunsByTags::test_handles_missing_tags PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestQueryRunsByTags::test_passes_filter_string PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestQueryRunsByTags::test_passes_max_results PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestQueryRunsByTags::test_empty_result PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestFindBestRunByMetric::test_maximize_metric PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestFindBestRunByMetric::test_minimize_metric PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestFindBestRunByMetric::test_filters_runs_without_metric PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestFindBestRunByMetric::test_returns_none_when_no_runs_have_metric PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestFindBestRunByMetric::test_empty_runs_list PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestFindBestRunByMetric::test_single_run PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestGroupRunsByVariant::test_groups_by_variant_tag PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestGroupRunsByVariant::test_default_variant_for_missing_tag PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestGroupRunsByVariant::test_custom_variant_tag PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestGroupRunsByVariant::test_empty_runs_list PASSED
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerV2HashComputation::test_v2_hash_computation_success FAILED
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerV2HashComputation::test_v2_hash_computation_missing_train_config FAILED
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerV2HashComputation::test_v2_hash_computation_empty_eval_config FAILED
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerBestTrialSearch::test_best_trial_search_by_study_key_hash_success FAILED
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerBestTrialSearch::test_best_trial_search_filters_by_parent_run_id FAILED
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerBestTrialSearch::test_best_trial_search_no_parent_study_key_hash FAILED
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerBestTrialSearch::test_best_trial_search_no_matching_runs FAILED
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerBestTrialSearch::test_best_trial_search_exception_handling FAILED
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerTrialNumberExtraction::test_extract_trial_number_from_tag FAILED
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerTrialNumberExtraction::test_extract_trial_number_from_run_name FAILED
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerTrialNumberExtraction::test_extract_trial_number_returns_none_when_not_found FAILED
tests/integration/api/test_api.py::TestHealthEndpoint::test_health_check PASSED
tests/integration/api/test_api.py::TestHealthEndpoint::test_model_info_not_loaded PASSED
tests/integration/api/test_api.py::TestHealthEndpoint::test_model_info_loaded PASSED
tests/integration/api/test_api.py::TestPredictEndpoint::test_predict_not_loaded PASSED
tests/integration/api/test_api.py::TestPredictEndpoint::test_predict_success PASSED
tests/integration/api/test_api.py::TestPredictEndpoint::test_predict_batch_success PASSED
tests/integration/api/test_api.py::TestPredictEndpoint::test_predict_batch_size_exceeded PASSED
tests/integration/api/test_api.py::TestFileEndpoints::test_predict_file_not_loaded PASSED
tests/integration/api/test_api.py::TestFileEndpoints::test_predict_file_pdf PASSED
tests/integration/api/test_api.py::TestFileEndpoints::test_predict_file_image PASSED
tests/integration/api/test_api_local_server.py::TestServerLifecycle::test_server_startup_with_valid_model SKIPPED
tests/integration/api/test_api_local_server.py::TestServerLifecycle::test_server_startup_with_invalid_model_path PASSED
tests/integration/api/test_api_local_server.py::TestServerLifecycle::test_server_graceful_shutdown SKIPPED
tests/integration/api/test_api_local_server.py::TestHealthEndpoints::test_health_check_model_loaded SKIPPED
tests/integration/api/test_api_local_server.py::TestHealthEndpoints::test_model_info_loaded SKIPPED
tests/integration/api/test_api_local_server.py::TestSingleTextPrediction::test_predict_valid_text SKIPPED
tests/integration/api/test_api_local_server.py::TestSingleTextPrediction::test_predict_empty_text SKIPPED
tests/integration/api/test_api_local_server.py::TestSingleTextPrediction::test_predict_unicode_text SKIPPED
tests/integration/api/test_api_local_server.py::TestSingleTextPrediction::test_predict_long_text SKIPPED
tests/integration/api/test_api_local_server.py::TestSingleTextPrediction::test_predict_special_characters SKIPPED
tests/integration/api/test_api_local_server.py::TestSingleTextPrediction::test_predict_whitespace_only SKIPPED
tests/integration/api/test_api_local_server.py::TestSingleTextPrediction::test_predict_non_string_value SKIPPED
tests/integration/api/test_api_local_server.py::TestBatchTextPrediction::test_predict_batch_small SKIPPED
tests/integration/api/test_api_local_server.py::TestBatchTextPrediction::test_predict_batch_medium SKIPPED
tests/integration/api/test_api_local_server.py::TestBatchTextPrediction::test_predict_batch_empty SKIPPED
tests/integration/api/test_api_local_server.py::TestBatchTextPrediction::test_predict_batch_mixed SKIPPED
tests/integration/api/test_api_local_server.py::TestBatchTextPrediction::test_predict_batch_size_exceeded SKIPPED
tests/integration/api/test_api_local_server.py::TestBatchTextPrediction::test_predict_batch_with_empty_text SKIPPED
tests/integration/api/test_api_local_server.py::TestBatchTextPrediction::test_predict_batch_missing_texts_field SKIPPED
tests/integration/api/test_api_local_server.py::TestBatchTextPrediction::test_predict_batch_non_list_value SKIPPED
tests/integration/api/test_api_local_server.py::TestFileUpload::test_predict_file_pdf SKIPPED
tests/integration/api/test_api_local_server.py::TestFileUpload::test_predict_file_png SKIPPED
tests/integration/api/test_api_local_server.py::TestFileUpload::test_predict_file_larger_pdf SKIPPED
tests/integration/api/test_api_local_server.py::TestFileUpload::test_predict_file_small_pdf SKIPPED
tests/integration/api/test_api_local_server.py::TestFileUpload::test_predict_file_missing_file SKIPPED
tests/integration/api/test_api_local_server.py::TestBatchFileUpload::test_predict_file_batch_small SKIPPED
tests/integration/api/test_api_local_server.py::TestBatchFileUpload::test_predict_file_batch_mixed_types SKIPPED
tests/integration/api/test_api_local_server.py::TestBatchFileUpload::test_predict_file_batch_medium SKIPPED
tests/integration/api/test_api_local_server.py::TestBatchFileUpload::test_predict_file_batch_empty SKIPPED
tests/integration/api/test_api_local_server.py::TestBatchFileUpload::test_predict_file_batch_size_exceeded SKIPPED
tests/integration/api/test_api_local_server.py::TestDebugEndpoint::test_predict_debug SKIPPED
tests/integration/api/test_api_local_server.py::TestErrorHandling::test_invalid_json SKIPPED
tests/integration/api/test_api_local_server.py::TestErrorHandling::test_missing_required_fields SKIPPED
tests/integration/api/test_api_local_server.py::TestErrorHandling::test_invalid_file_type SKIPPED
tests/integration/api/test_api_local_server.py::TestPerformance::test_predict_latency SKIPPED
tests/integration/api/test_api_local_server.py::TestPerformance::test_predict_batch_latency SKIPPED
tests/integration/api/test_api_local_server.py::TestStability::test_repeated_predictions_consistency SKIPPED
tests/integration/api/test_api_local_server.py::TestStability::test_repeated_file_processing SKIPPED
tests/integration/api/test_api_local_server.py::TestStability::test_comprehensive_multi_file_multi_iteration SKIPPED
tests/integration/api/test_inference_direct.py::test_direct_inference SKIPPED
tests/integration/api/test_inference_performance.py::TestInferencePerformanceIntegration::test_real_inference_performance SKIPPED
tests/integration/api/test_inference_performance.py::TestInferencePerformanceIntegration::test_tokenization_consistency_mock PASSED
tests/integration/api/test_onnx_inference.py::test_onnx_inference_speed SKIPPED
tests/integration/api/test_tokenization_speed.py::test_tokenization_speed SKIPPED
tests/orchestration/jobs/hpo/local/test_backup.py::TestBackupHpoStudyToDrive::test_backup_v2_study_folder_found_locally PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestBackupHpoStudyToDrive::test_backup_skips_when_already_in_drive PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestBackupHpoStudyToDrive::test_backup_uses_v2_folder_only PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestBackupHpoStudyToDrive::test_backup_warns_when_study_folder_not_found PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestBackupHpoStudyToDrive::test_backup_checks_file_existence_not_just_path PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestBackupHpoStudyToDrive::test_backup_disabled_skips_all_operations PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestIncrementalBackupCallback::test_create_incremental_backup_callback_file PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestIncrementalBackupCallback::test_create_incremental_backup_callback_directory PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestIncrementalBackupCallback::test_incremental_backup_callback_skips_when_disabled PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestIncrementalBackupCallback::test_incremental_backup_callback_skips_drive_paths PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestIncrementalBackupCallback::test_incremental_backup_callback_skips_nonexistent_path PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestIncrementalBackupCallback::test_incremental_backup_callback_skips_non_complete_trials PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestIncrementalBackupCallback::test_incremental_backup_callback_handles_errors_gracefully PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestIncrementalBackupCallback::test_create_study_db_backup_callback PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestImmediateBackupIfNeeded::test_immediate_backup_succeeds_with_enabled_backup_local_path PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestImmediateBackupIfNeeded::test_immediate_backup_succeeds_with_directory PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestImmediateBackupIfNeeded::test_immediate_backup_skips_when_disabled PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestImmediateBackupIfNeeded::test_immediate_backup_skips_when_path_is_drive PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestImmediateBackupIfNeeded::test_immediate_backup_skips_when_path_missing PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestImmediateBackupIfNeeded::test_immediate_backup_skips_when_backup_to_drive_is_none PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestImmediateBackupIfNeeded::test_immediate_backup_handles_backup_failure_gracefully PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestImmediateBackupIfNeeded::test_immediate_backup_handles_exception_gracefully PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestShouldSkipBackup::test_should_skip_when_backup_disabled PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestShouldSkipBackup::test_should_skip_when_path_is_drive PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestShouldSkipBackup::test_should_skip_when_path_missing PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestShouldSkipBackup::test_should_not_skip_when_all_conditions_met PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestShouldSkipBackup::test_should_skip_priority_order PASSED
tests/selection/integration/test_artifact_acquisition_edge_cases.py::TestArtifactAcquisitionEdgeCases::test_empty_priority_list PASSED
tests/selection/integration/test_artifact_acquisition_edge_cases.py::TestArtifactAcquisitionEdgeCases::test_invalid_priority_values PASSED
tests/selection/integration/test_artifact_acquisition_edge_cases.py::TestArtifactAcquisitionEdgeCases::test_missing_local_section PASSED
tests/selection/integration/test_artifact_acquisition_edge_cases.py::TestArtifactAcquisitionEdgeCases::test_missing_drive_section PASSED
tests/selection/integration/test_artifact_acquisition_edge_cases.py::TestArtifactAcquisitionEdgeCases::test_missing_mlflow_section PASSED
tests/selection/integration/test_artifact_acquisition_edge_cases.py::TestArtifactAcquisitionEdgeCases::test_validation_false_allows_invalid_checkpoints PASSED
tests/selection/integration/test_artifact_acquisition_edge_cases.py::TestArtifactAcquisitionEdgeCases::test_validation_true_rejects_invalid_checkpoints PASSED
tests/selection/integration/test_artifact_acquisition_edge_cases.py::TestArtifactAcquisitionEdgeCases::test_priority_order_affects_strategy_selection PASSED
tests/selection/integration/test_artifact_acquisition_edge_cases.py::TestArtifactAcquisitionEdgeCases::test_duplicate_priority_values PASSED
tests/selection/integration/test_artifact_acquisition_edge_cases.py::TestArtifactAcquisitionEdgeCases::test_priority_with_only_one_source PASSED
tests/selection/integration/test_artifact_acquisition_edge_cases.py::TestArtifactAcquisitionEdgeCases::test_missing_study_trial_hashes_skips_local PASSED
tests/selection/integration/test_artifact_acquisition_edge_cases.py::TestArtifactAcquisitionEdgeCases::test_config_with_all_optional_fields PASSED
tests/selection/integration/test_artifact_acquisition_logic.py::TestArtifactAcquisitionConfig::test_priority_order_local_first PASSED
tests/selection/integration/test_artifact_acquisition_logic.py::TestArtifactAcquisitionConfig::test_priority_order_mlflow_first PASSED
tests/selection/integration/test_artifact_acquisition_logic.py::TestArtifactAcquisitionConfig::test_local_validate_controls_validation PASSED
tests/selection/integration/test_artifact_acquisition_logic.py::TestArtifactAcquisitionConfig::test_drive_enabled_controls_drive_strategy 
 Backing up best model checkpoint to Google Drive...
 Successfully backed up checkpoint to Google Drive
  Drive path: <Mock name='mock.backup().dst' id='123304056293120'>
PASSED
tests/selection/integration/test_artifact_acquisition_logic.py::TestArtifactAcquisitionConfig::test_drive_validate_controls_validation 
 Backing up best model checkpoint to Google Drive...
 Successfully backed up checkpoint to Google Drive
  Drive path: <Mock name='mock.backup().dst' id='123304056305840'>

 Backing up best model checkpoint to Google Drive...
 Successfully backed up checkpoint to Google Drive
  Drive path: <Mock name='mock.backup().dst' id='123304056305840'>
PASSED
tests/selection/integration/test_artifact_acquisition_logic.py::TestArtifactAcquisitionConfig::test_mlflow_enabled_controls_mlflow_strategy PASSED
tests/selection/integration/test_artifact_acquisition_logic.py::TestArtifactAcquisitionConfig::test_mlflow_validate_controls_validation PASSED
tests/selection/integration/test_artifact_acquisition_logic.py::TestArtifactAcquisitionConfig::test_priority_order_with_some_sources_disabled PASSED
tests/selection/integration/test_artifact_acquisition_logic.py::TestArtifactAcquisitionConfig::test_all_strategies_fail_gracefully_when_disabled PASSED
tests/selection/integration/test_artifact_acquisition_unified_config.py::TestSearchRootsIntegration::test_search_roots_used_in_local_discovery FAILED
tests/selection/integration/test_artifact_acquisition_unified_config.py::TestSearchRootsIntegration::test_search_roots_default_when_missing PASSED
tests/selection/integration/test_artifact_acquisition_unified_config.py::TestArtifactKindsPriorityIntegration::test_artifact_kinds_priority_overrides_global PASSED
tests/selection/integration/test_artifact_acquisition_unified_config.py::TestArtifactKindsPriorityIntegration::test_artifact_kinds_fallback_to_global_priority FAILED
tests/selection/integration/test_artifact_acquisition_unified_config.py::TestMlflowRequireArtifactTagIntegration::test_require_artifact_tag_config_extracted PASSED
tests/selection/integration/test_artifact_acquisition_unified_config.py::TestConfigOptionCombinations::test_all_config_options_together PASSED
tests/selection/integration/test_artifact_acquisition_unified_config.py::TestConfigOptionCombinations::test_config_with_disabled_sources FAILED
tests/selection/integration/test_artifact_acquisition_workflow.py::TestArtifactAcquisitionWorkflow::test_complete_workflow_with_default_config PASSED
tests/selection/integration/test_artifact_acquisition_workflow.py::TestArtifactAcquisitionWorkflow::test_workflow_with_custom_priority_order PASSED
tests/selection/integration/test_artifact_acquisition_workflow.py::TestArtifactAcquisitionWorkflow::test_workflow_with_validation_disabled PASSED
tests/selection/integration/test_artifact_acquisition_workflow.py::TestArtifactAcquisitionWorkflow::test_workflow_with_all_sources_enabled PASSED
tests/selection/integration/test_artifact_acquisition_workflow.py::TestArtifactAcquisitionWorkflow::test_workflow_with_all_sources_disabled PASSED
tests/selection/integration/test_artifact_acquisition_workflow.py::TestArtifactAcquisitionWorkflow::test_workflow_mlflow_fallback_to_manual PASSED
tests/selection/integration/test_best_model_cache.py::test_cache_dual_file_strategy_creates_all_files FAILED
tests/selection/integration/test_best_model_cache.py::test_cache_load_valid_cache_with_mlflow_validation FAILED
tests/selection/integration/test_best_model_cache.py::test_cache_load_cache_key_mismatch_returns_none FAILED
tests/selection/integration/test_best_model_cache.py::test_cache_partial_write_recovery FAILED
tests/selection/integration/test_best_model_cache.py::test_cache_missing_metrics_handling FAILED
tests/selection/integration/test_best_model_selection_config_integration.py::TestLatencyAggregationIntegration::test_latency_aggregation_latest_strategy FAILED
tests/selection/integration/test_best_model_selection_config_integration.py::TestLatencyAggregationIntegration::test_latency_aggregation_median_strategy FAILED
tests/selection/integration/test_best_model_selection_config_integration.py::TestLatencyAggregationIntegration::test_latency_aggregation_mean_strategy FAILED
tests/selection/integration/test_best_model_selection_config_integration.py::TestLatencyAggregationIntegration::test_latency_aggregation_default_when_missing PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestObjectiveDirectionMigrationIntegration::test_objective_direction_default_when_missing PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestChampionSelectionConfigIntegration::test_min_trials_per_group_extraction PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestChampionSelectionConfigIntegration::test_top_k_for_stable_score_extraction PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestChampionSelectionConfigIntegration::test_require_artifact_available_extraction PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestChampionSelectionConfigIntegration::test_artifact_check_source_extraction PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestChampionSelectionConfigIntegration::test_prefer_schema_version_extraction PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestChampionSelectionConfigIntegration::test_allow_mixed_schema_groups_extraction PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestScoringConfigIntegration::test_scoring_weights_used_in_composite_score FAILED
tests/selection/integration/test_best_model_selection_config_integration.py::TestScoringConfigIntegration::test_normalize_weights_controls_normalization FAILED
tests/selection/integration/test_best_model_selection_config_integration.py::TestBenchmarkRequiredMetricsIntegration::test_required_metrics_filters_benchmark_runs FAILED
tests/selection/integration/test_best_model_selection_config_integration.py::TestCompleteConfigWorkflow::test_all_config_options_used_together FAILED
tests/selection/integration/test_best_model_selection_config_integration.py::TestCompleteConfigWorkflow::test_config_with_missing_sections_uses_defaults PASSED
tests/selection/integration/test_mlflow_selection_config.py::TestMLflowSelectionConfigUsage::test_find_best_model_uses_objective_metric FAILED
tests/selection/integration/test_mlflow_selection_config.py::TestMLflowSelectionConfigUsage::test_find_best_model_uses_scoring_weights FAILED
tests/selection/integration/test_mlflow_selection_config.py::TestMLflowSelectionConfigUsage::test_find_best_model_uses_benchmark_required_metrics FAILED
tests/selection/integration/test_mlflow_selection_config.py::TestMLflowSelectionConfigUsage::test_find_best_model_weight_normalization FAILED
tests/selection/integration/test_mlflow_selection_config.py::TestMLflowSelectionConfigUsage::test_find_best_model_composite_score_calculation FAILED
tests/selection/integration/test_mlflow_selection_config.py::TestMLflowSelectionConfigUsage::test_find_best_model_all_config_options_together FAILED
tests/selection/integration/test_mlflow_selection_config.py::TestMLflowSelectionConfigUsage::test_find_best_model_custom_config_values FAILED
tests/selection/integration/test_selection_cache_config.py::TestSelectionCacheConfig::test_compute_selection_cache_key_includes_selection_config PASSED
tests/selection/integration/test_selection_cache_config.py::TestSelectionCacheConfig::test_compute_selection_cache_key_deterministic PASSED
tests/selection/integration/test_selection_cache_config.py::TestSelectionCacheConfig::test_load_cached_best_model_validates_cache_key FAILED
tests/selection/integration/test_selection_cache_config.py::TestSelectionCacheConfig::test_load_cached_best_model_cache_key_mismatch   Mode is 'force_new' - skipping cache
PASSED
tests/selection/integration/test_selection_cache_config.py::TestSelectionCacheConfig::test_save_best_model_cache_includes_selection_config PASSED
tests/selection/integration/test_selection_cache_config.py::TestSelectionCacheConfig::test_run_mode_reuse_if_exists_behavior FAILED
tests/selection/integration/test_selection_cache_config.py::TestSelectionCacheConfig::test_run_mode_force_new_behavior   Mode is 'force_new' - skipping cache
PASSED
tests/selection/integration/test_selection_edge_cases.py::TestSelectionConfigEdgeCases::test_missing_run_section_uses_defaults PASSED
tests/selection/integration/test_selection_edge_cases.py::TestSelectionConfigEdgeCases::test_missing_objective_section_handled PASSED
tests/selection/integration/test_selection_edge_cases.py::TestSelectionConfigEdgeCases::test_missing_scoring_section_uses_defaults PASSED
tests/selection/integration/test_selection_edge_cases.py::TestSelectionConfigEdgeCases::test_negative_weights_handled PASSED
tests/selection/integration/test_selection_edge_cases.py::TestSelectionConfigEdgeCases::test_zero_weights_handled PASSED
tests/selection/integration/test_selection_edge_cases.py::TestSelectionConfigEdgeCases::test_weight_normalization_zero_sum PASSED
tests/selection/integration/test_selection_edge_cases.py::TestSelectionConfigEdgeCases::test_empty_required_metrics_list PASSED
tests/selection/integration/test_selection_edge_cases.py::TestSelectionConfigEdgeCases::test_missing_benchmark_section_uses_defaults PASSED
tests/selection/integration/test_selection_edge_cases.py::TestSelectionConfigEdgeCases::test_cache_key_with_missing_sections PASSED
tests/selection/integration/test_selection_edge_cases.py::TestSelectionConfigEdgeCases::test_find_best_model_missing_required_metrics FAILED
tests/selection/integration/test_selection_edge_cases.py::TestSelectionConfigEdgeCases::test_find_best_model_empty_required_metrics FAILED
tests/selection/integration/test_selection_edge_cases.py::TestSelectionConfigEdgeCases::test_invalid_run_mode_value PASSED
tests/selection/integration/test_selection_workflow.py::TestSelectionWorkflow::test_workflow_loads_config_and_uses_all_options FAILED
tests/selection/integration/test_selection_workflow.py::TestSelectionWorkflow::test_workflow_custom_config_values FAILED
tests/selection/integration/test_selection_workflow.py::TestSelectionWorkflow::test_workflow_cache_key_computation PASSED
tests/selection/integration/test_selection_workflow.py::TestSelectionWorkflow::test_workflow_cache_loading_with_config FAILED
tests/selection/unit/test_artifact_acquisition_config.py::TestArtifactAcquisitionConfigLoading::test_load_yaml_loads_artifact_acquisition_config PASSED
tests/selection/unit/test_artifact_acquisition_config.py::TestArtifactAcquisitionConfigLoading::test_artifact_acquisition_config_structure PASSED
tests/selection/unit/test_artifact_acquisition_config.py::TestArtifactAcquisitionConfigLoading::test_artifact_acquisition_config_default_values PASSED
tests/selection/unit/test_artifact_acquisition_config.py::TestArtifactAcquisitionConfigLoading::test_artifact_acquisition_config_custom_values PASSED
tests/selection/unit/test_artifact_acquisition_config.py::TestArtifactAcquisitionConfigLoading::test_artifact_acquisition_config_missing_sections PASSED
tests/selection/unit/test_artifact_acquisition_config.py::TestArtifactAcquisitionConfigLoading::test_artifact_acquisition_config_types PASSED
tests/selection/unit/test_artifact_acquisition_config.py::TestArtifactAcquisitionConfigLoading::test_load_actual_artifact_acquisition_yaml PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestSearchRootsConfig::test_search_roots_extraction PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestSearchRootsConfig::test_search_roots_custom_order PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestSearchRootsConfig::test_search_roots_default PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestSearchRootsConfig::test_search_roots_empty_list PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestSearchRootsConfig::test_search_roots_passed_to_discovery PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestArtifactKindsConfig::test_artifact_kinds_extraction PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestArtifactKindsConfig::test_artifact_kinds_priority_overrides_global PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestArtifactKindsConfig::test_artifact_kinds_fallback_to_global_priority PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestArtifactKindsConfig::test_artifact_kinds_all_artifact_types PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestArtifactKindsConfig::test_artifact_kinds_missing_priority PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestMlflowRequireArtifactTag::test_require_artifact_tag_extraction PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestMlflowRequireArtifactTag::test_require_artifact_tag_true PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestMlflowRequireArtifactTag::test_require_artifact_tag_default PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestConfigIntegrationSuccessCases::test_success_with_all_config_options PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestConfigIntegrationSuccessCases::test_success_with_custom_artifact_kinds_priority PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestConfigIntegrationSuccessCases::test_success_with_custom_search_roots PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestConfigIntegrationFailureCases::test_failure_missing_search_roots_uses_defaults PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestConfigIntegrationFailureCases::test_failure_missing_artifact_kinds_uses_global_priority PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestConfigIntegrationFailureCases::test_failure_empty_priority_list PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestConfigIntegrationFailureCases::test_failure_all_sources_disabled PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestConfigIntegrationFailureCases::test_failure_invalid_priority_source PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestConfigOptionTypes::test_all_config_option_types PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestConfigDefaults::test_all_defaults_match_config_file PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestConfigCompleteCoverage::test_all_config_sections_present PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestConfigCompleteCoverage::test_all_local_options_present PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestConfigCompleteCoverage::test_all_drive_options_present PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestConfigCompleteCoverage::test_all_mlflow_options_present PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_priority_extraction PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_priority_custom_order PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_priority_default PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_local_validate_extraction PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_local_validate_false PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_local_validate_default PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_local_match_strategy_extraction PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_local_match_strategy_metadata_run_id PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_local_match_strategy_spec_fp PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_local_require_exact_match_extraction PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_local_require_exact_match_false PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_drive_enabled_extraction PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_drive_enabled_false PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_drive_enabled_default PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_drive_validate_extraction PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_drive_validate_false PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_drive_validate_default PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_drive_folder_path_extraction PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_drive_folder_path_custom PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_mlflow_enabled_extraction PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_mlflow_enabled_false PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_mlflow_enabled_default PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_mlflow_validate_extraction PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_mlflow_validate_false PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_mlflow_validate_default PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_mlflow_download_timeout_extraction PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_mlflow_download_timeout_custom PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_mlflow_download_timeout_default PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_all_options_together PASSED
tests/selection/unit/test_best_model_selection_config.py::TestBestModelSelectionConfigLoading::test_load_best_model_selection_config PASSED
tests/selection/unit/test_best_model_selection_config.py::TestBestModelSelectionConfigLoading::test_load_config_with_custom_values PASSED
tests/selection/unit/test_best_model_selection_config.py::TestBestModelSelectionConfigLoading::test_load_config_structure_validation PASSED
tests/selection/unit/test_best_model_selection_config.py::TestBestModelSelectionConfigLoading::test_load_config_matches_actual_file PASSED
tests/selection/unit/test_best_model_selection_config.py::TestBestModelSelectionConfigLoading::test_load_config_with_champion_selection_section PASSED
tests/selection/unit/test_best_model_selection_config.py::TestBestModelSelectionConfigLoading::test_load_config_champion_selection_custom_values PASSED
tests/selection/unit/test_best_model_selection_config.py::TestBestModelSelectionConfigLoading::test_load_config_champion_selection_structure_validation PASSED
tests/selection/unit/test_best_model_selection_config.py::TestBestModelSelectionConfigLoading::test_load_config_champion_selection_with_objective_direction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestRunModeConfig::test_run_mode_extraction_force_new PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestRunModeConfig::test_run_mode_extraction_reuse_if_exists PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestRunModeConfig::test_run_mode_default PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestRunModeConfig::test_run_mode_invalid_value PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestObjectiveConfig::test_objective_metric_extraction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestObjectiveConfig::test_objective_metric_default PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestObjectiveConfig::test_objective_direction_extraction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestObjectiveConfig::test_objective_direction_minimize PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_min_trials_per_group_extraction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_min_trials_per_group_default PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_min_trials_per_group_custom_value PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_top_k_for_stable_score_extraction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_top_k_for_stable_score_default PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_require_artifact_available_extraction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_require_artifact_available_false PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_artifact_check_source_extraction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_artifact_check_source_disk PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_prefer_schema_version_extraction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_prefer_schema_version_2_0 PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_allow_mixed_schema_groups_extraction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_allow_mixed_schema_groups_true PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_all_champion_selection_options_together PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestScoringConfig::test_f1_weight_extraction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestScoringConfig::test_f1_weight_default PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestScoringConfig::test_latency_weight_extraction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestScoringConfig::test_latency_weight_default PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestScoringConfig::test_normalize_weights_extraction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestScoringConfig::test_normalize_weights_false PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestScoringConfig::test_normalize_weights_default PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestBenchmarkConfig::test_required_metrics_extraction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestBenchmarkConfig::test_required_metrics_multiple PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestBenchmarkConfig::test_required_metrics_default PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestBenchmarkConfig::test_latency_aggregation_extraction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestBenchmarkConfig::test_latency_aggregation_median PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestBenchmarkConfig::test_latency_aggregation_mean PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestBenchmarkConfig::test_latency_aggregation_default PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestBenchmarkConfig::test_latency_aggregation_invalid_value PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigIntegrationSuccessCases::test_all_config_options_together PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigIntegrationSuccessCases::test_config_with_custom_latency_aggregation PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigIntegrationSuccessCases::test_config_with_all_champion_selection_options PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigIntegrationFailureCases::test_failure_missing_run_section PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigIntegrationFailureCases::test_failure_missing_objective_section PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigIntegrationFailureCases::test_failure_missing_champion_selection_section PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigIntegrationFailureCases::test_failure_missing_scoring_section PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigIntegrationFailureCases::test_failure_missing_benchmark_section PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigIntegrationFailureCases::test_failure_empty_required_metrics PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigIntegrationFailureCases::test_failure_zero_weights PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigTypeValidation::test_all_config_option_types PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigDefaults::test_all_defaults_match_config_file PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigCompleteCoverage::test_all_config_sections_present PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigCompleteCoverage::test_all_run_options_present PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigCompleteCoverage::test_all_objective_options_present PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigCompleteCoverage::test_all_champion_selection_options_present PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigCompleteCoverage::test_all_scoring_options_present PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigCompleteCoverage::test_all_benchmark_options_present PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_run_mode_extraction PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_run_mode_default PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_run_mode_reuse_if_exists PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_objective_metric_extraction PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_objective_metric_default PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_objective_goal_extraction PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_objective_goal_minimize PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_objective_goal_default PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_scoring_f1_weight_extraction PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_scoring_f1_weight_default PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_scoring_latency_weight_extraction PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_scoring_latency_weight_default PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_scoring_normalize_weights_extraction_true PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_scoring_normalize_weights_extraction_false PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_scoring_normalize_weights_default PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_benchmark_required_metrics_extraction PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_benchmark_required_metrics_multiple PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_benchmark_required_metrics_default PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_all_options_together PASSED
tests/selection/unit/test_study_summary.py::TestStudySummaryMultipleBackbones::test_print_study_summaries_with_multiple_backbones PASSED
tests/selection/unit/test_study_summary.py::TestStudySummaryMultipleBackbones::test_print_study_summaries_with_missing_from_dict PASSED
tests/selection/unit/test_study_summary.py::TestStudySummaryMultipleBackbones::test_print_study_summaries_skips_already_printed PASSED
tests/selection/unit/test_study_summary.py::TestStudySummaryMultipleBackbones::test_format_study_summary_line_with_cv_stats PASSED
tests/selection/unit/test_study_summary.py::TestStudySummaryMultipleBackbones::test_format_study_summary_line_without_cv_stats PASSED
tests/selection/unit/test_study_summary.py::TestStudySummaryMultipleBackbones::test_extract_cv_statistics PASSED
tests/selection/unit/test_study_summary.py::TestStudySummaryMultipleBackbones::test_extract_cv_statistics_missing PASSED
tests/shared/unit/test_drive_backup.py::TestBackupResult::test_backup_result_str_success PASSED
tests/shared/unit/test_drive_backup.py::TestBackupResult::test_backup_result_str_error PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_init_validation PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_drive_path_for_valid_path PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_drive_path_for_outside_root PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_drive_path_for_outside_outputs PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_drive_path_for_allows_outputs_when_disabled PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_backup_file PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_backup_directory PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_backup_nonexistent_file PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_backup_type_mismatch PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_backup_type_inference PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_backup_dry_run PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_restore_file PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_restore_directory PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_restore_nonexistent_backup PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_restore_overwrites_existing PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_ensure_local_exists PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_ensure_local_restores_if_missing PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_backup_exists_true PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_backup_exists_false PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_as_restore_callback PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_as_backup_callback PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_drive_path_for_rejects_drive_paths PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_backup_rejects_drive_paths PASSED
tests/shared/unit/test_drive_backup.py::TestEnsureLocalOptions::test_default_options PASSED
tests/shared/unit/test_drive_backup.py::TestColabMounting::test_mount_colab_drive_success PASSED
tests/shared/unit/test_drive_backup.py::TestColabMounting::test_mount_colab_drive_import_error PASSED
tests/shared/unit/test_drive_backup.py::TestColabMounting::test_create_colab_store_success Using configured backup location: /tmp/pytest-of-codespace/pytest-120/test_create_colab_store_succes0/drive/MyDrive/resume-ner-checkpoints
PASSED
tests/shared/unit/test_drive_backup.py::TestColabMounting::test_create_colab_store_mount_fails  Warning: Could not mount Drive: Not in Colab
PASSED
tests/shared/unit/test_drive_backup.py::TestColabMounting::test_create_colab_store_default_path FAILED
tests/shared/unit/test_platform_detection.py::TestIsDrivePath::test_drive_path_string PASSED
tests/shared/unit/test_platform_detection.py::TestIsDrivePath::test_drive_path_path_object PASSED
tests/shared/unit/test_platform_detection.py::TestIsDrivePath::test_local_path_string PASSED
tests/shared/unit/test_platform_detection.py::TestIsDrivePath::test_local_path_path_object PASSED
tests/shared/unit/test_platform_detection.py::TestIsDrivePath::test_relative_path PASSED
tests/shared/unit/test_platform_detection.py::TestIsDrivePath::test_none_input PASSED
tests/tracking/integration/test_azureml_artifact_upload_integration.py::TestAzureMLArtifactUploadIntegration::test_artifact_upload_to_refit_run_with_monkey_patch SKIPPED
tests/tracking/integration/test_azureml_artifact_upload_integration.py::TestAzureMLArtifactUploadIntegration::test_refit_run_completion_after_upload SKIPPED
tests/tracking/integration/test_azureml_artifact_upload_integration.py::TestMonkeyPatchBehavior::test_patch_handles_tracking_uri_error PASSED
tests/tracking/integration/test_naming_integration.py::test_end_to_end_final_training PASSED
tests/tracking/integration/test_naming_integration.py::test_end_to_end_conversion PASSED
tests/tracking/integration/test_naming_integration.py::test_cross_platform_same_spec_fp PASSED
tests/tracking/integration/test_tracking_config_enabled.py::TestBenchmarkTrackingEnabled::test_benchmark_tracking_enabled_creates_run FAILED
tests/tracking/integration/test_tracking_config_enabled.py::TestBenchmarkTrackingEnabled::test_benchmark_tracking_disabled_skips_run PASSED
tests/tracking/integration/test_tracking_config_enabled.py::TestTrainingTrackingEnabled::test_training_tracking_enabled_creates_run FAILED
tests/tracking/integration/test_tracking_config_enabled.py::TestTrainingTrackingEnabled::test_training_tracking_disabled_skips_run PASSED
tests/tracking/integration/test_tracking_config_enabled.py::TestConversionTrackingEnabled::test_conversion_tracking_enabled_creates_run FAILED
tests/tracking/integration/test_tracking_config_enabled.py::TestConversionTrackingEnabled::test_conversion_tracking_disabled_skips_run PASSED
tests/tracking/integration/test_tracking_config_enabled.py::TestLogArtifactsOptions::test_benchmark_log_artifacts_disabled_skips_logging PASSED
tests/tracking/integration/test_tracking_config_enabled.py::TestLogArtifactsOptions::test_training_log_checkpoint_disabled_skips_logging PASSED
tests/tracking/integration/test_tracking_config_enabled.py::TestLogArtifactsOptions::test_training_log_metrics_json_disabled_skips_logging PASSED
tests/tracking/integration/test_tracking_config_enabled.py::TestLogArtifactsOptions::test_conversion_log_onnx_model_disabled_skips_logging PASSED
tests/tracking/integration/test_tracking_config_enabled.py::TestLogArtifactsOptions::test_conversion_log_conversion_log_disabled_skips_logging PASSED
tests/tracking/scripts/test_artifact_upload_manual.py::test_monkey_patch_registration Testing monkey-patch registration...
 Azure ML builder registered: <function azureml_artifacts_builder at 0x702505e23eb0>
 Builder is patched (has __wrapped__ attribute)
PASSED
tests/tracking/scripts/test_artifact_upload_manual.py::test_artifact_upload_to_child_run 
Testing artifact upload to child run...
Tracking URI: sqlite:////workspaces/resume-ner-azureml/mlruns/mlflow.db
 No active MLflow run, skipping test
PASSED
tests/tracking/scripts/test_artifact_upload_manual.py::test_refit_run_completion 
Testing refit run completion logic...
 No active MLflow run, skipping test
PASSED
tests/tracking/unit/test_azureml_artifact_upload.py::TestAzureMLArtifactBuilderPatch::test_patch_registered_on_import PASSED
tests/tracking/unit/test_azureml_artifact_upload.py::TestAzureMLArtifactBuilderPatch::test_patch_handles_tracking_uri_parameter PASSED
tests/tracking/unit/test_azureml_artifact_upload.py::TestAzureMLArtifactBuilderPatch::test_patch_auto_applies_on_module_import PASSED
tests/tracking/unit/test_azureml_artifact_upload.py::TestArtifactUploadToChildRun::test_upload_to_refit_run_when_available PASSED
tests/tracking/unit/test_azureml_artifact_upload.py::TestArtifactUploadToChildRun::test_upload_to_parent_run_when_refit_not_available PASSED
tests/tracking/unit/test_azureml_artifact_upload.py::TestRefitRunFinishedStatus::test_refit_run_marked_finished_after_successful_upload PASSED
tests/tracking/unit/test_azureml_artifact_upload.py::TestRefitRunFinishedStatus::test_refit_run_marked_failed_after_upload_failure PASSED
tests/tracking/unit/test_azureml_artifact_upload.py::TestRefitRunFinishedStatus::test_refit_run_not_terminated_if_already_finished PASSED
tests/tracking/unit/test_azureml_artifact_upload.py::TestAzureMLCompatibility::test_azureml_mlflow_imported PASSED
tests/tracking/unit/test_azureml_artifact_upload.py::TestAzureMLCompatibility::test_artifact_repository_registry_has_azureml PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestAzureMLConfiguration::test_azure_ml_enabled PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestAzureMLConfiguration::test_azure_ml_disabled PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestExperimentNaming::test_build_mlflow_experiment_name_format PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestExperimentNaming::test_build_mlflow_experiment_name_all_stages PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestExperimentNaming::test_build_mlflow_experiment_name_special_characters PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestStageSpecificTrackingConfiguration::test_tracking_benchmark_enabled PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestStageSpecificTrackingConfiguration::test_tracking_training_config PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestStageSpecificTrackingConfiguration::test_tracking_conversion_config PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestNamingConfigurationDetails::test_naming_project_name PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestNamingConfigurationDetails::test_naming_project_name_default PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestNamingConfigurationDetails::test_naming_tags_config PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestNamingConfigurationDetails::test_naming_run_name_config PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestIndexCacheConfiguration::test_index_enabled PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestIndexCacheConfiguration::test_index_disabled PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestRunFinderConfiguration::test_run_finder_strict_mode_default_true PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestRunFinderConfiguration::test_run_finder_strict_mode_default_false PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestRunFinderConfiguration::test_run_finder_default PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestArtifactUploadUtilities::test_log_artifact_safe_with_active_run PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestArtifactUploadUtilities::test_log_artifact_safe_with_explicit_run_id PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestArtifactUploadUtilities::test_log_artifact_safe_handles_errors PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestArtifactUploadUtilities::test_log_artifacts_safe_with_directory PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestArtifactUploadUtilities::test_upload_checkpoint_archive PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestRunLifecycleUtilities::test_terminate_run_safe_with_running_run PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestRunLifecycleUtilities::test_terminate_run_safe_with_already_terminated PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestRunLifecycleUtilities::test_terminate_run_safe_with_tags PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestRunLifecycleUtilities::test_terminate_run_with_tags PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestRunLifecycleUtilities::test_ensure_run_terminated PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestRunLifecycleUtilities::test_ensure_run_terminated_already_finished PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestRunCreationUtilities::test_get_or_create_experiment_existing PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestRunCreationUtilities::test_get_or_create_experiment_new PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestRunCreationUtilities::test_resolve_experiment_id_from_parent PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestRunCreationUtilities::test_resolve_experiment_id_from_name PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestURLUtilities::test_get_mlflow_run_url_azureml PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestURLUtilities::test_get_mlflow_run_url_standard PASSED
tests/tracking/unit/test_mlflow_utils_config_inference.py::TestInferConfigDir::test_finds_config_in_parent_chain PASSED
tests/tracking/unit/test_mlflow_utils_config_inference.py::TestInferConfigDir::test_finds_config_at_root_level PASSED
tests/tracking/unit/test_mlflow_utils_config_inference.py::TestInferConfigDir::test_falls_back_to_cwd_when_not_found PASSED
tests/tracking/unit/test_mlflow_utils_config_inference.py::TestInferConfigDir::test_handles_none_path PASSED
tests/tracking/unit/test_mlflow_utils_config_inference.py::TestInferConfigDir::test_finds_first_config_in_parent_chain PASSED
tests/tracking/unit/test_mlflow_utils_config_inference.py::TestInferConfigDir::test_not_in_outputs_subdirectory PASSED
tests/tracking/unit/test_naming_centralized.py::test_naming_context_validation PASSED
tests/tracking/unit/test_naming_centralized.py::test_naming_context_final_training_requires_fingerprints PASSED
tests/tracking/unit/test_naming_centralized.py::test_naming_context_conversion_requires_parent_and_conv_fp PASSED
tests/tracking/unit/test_naming_centralized.py::test_create_naming_context_auto_detect PASSED
tests/tracking/unit/test_naming_centralized.py::test_build_output_path_hpo FAILED
tests/tracking/unit/test_naming_centralized.py::test_build_output_path_benchmarking PASSED
tests/tracking/unit/test_naming_centralized.py::test_build_output_path_final_training PASSED
tests/tracking/unit/test_naming_centralized.py::test_build_output_path_final_training_variant PASSED
tests/tracking/unit/test_naming_centralized.py::test_build_output_path_conversion PASSED
tests/tracking/unit/test_naming_centralized.py::test_build_output_path_best_configurations PASSED
tests/tracking/unit/test_naming_centralized.py::test_build_parent_training_id PASSED
tests/tracking/unit/test_naming_centralized.py::TestRunNameGeneration::test_hpo_trial_run_name PASSED
tests/tracking/unit/test_naming_centralized.py::TestRunNameGeneration::test_hpo_trial_fold_run_name PASSED
tests/tracking/unit/test_naming_centralized.py::TestRunNameGeneration::test_hpo_refit_run_name PASSED
tests/tracking/unit/test_naming_centralized.py::TestRunNameGeneration::test_hpo_sweep_run_name PASSED
tests/tracking/unit/test_naming_centralized.py::TestRunNameGeneration::test_run_name_max_length PASSED
tests/tracking/unit/test_naming_centralized.py::TestRunNameGeneration::test_run_name_forbidden_chars_removed PASSED
tests/tracking/unit/test_naming_placeholder_truncation.py::TestNamingPlaceholderTruncation::test_final_training_naming_with_unknown_placeholders PASSED
tests/tracking/unit/test_naming_placeholder_truncation.py::TestNamingPlaceholderTruncation::test_final_training_naming_pattern_with_unknown_placeholders PASSED
tests/tracking/unit/test_naming_placeholder_truncation.py::TestNamingPlaceholderTruncation::test_placeholder_truncation_to_8_chars PASSED
tests/tracking/unit/test_naming_placeholder_truncation.py::TestNamingPlaceholderTruncation::test_old_placeholder_behavior_would_truncate PASSED
tests/tracking/unit/test_naming_placeholder_truncation.py::TestNamingPlaceholderTruncation::test_naming_context_accepts_unknown_placeholders PASSED
tests/tracking/unit/test_naming_placeholder_truncation.py::TestNamingPlaceholderTruncation::test_token_values_with_unknown_placeholders PASSED
tests/tracking/unit/test_naming_policy.py::TestStudyKeyHashComputation::test_build_hpo_study_key PASSED
tests/tracking/unit/test_naming_policy.py::TestStudyKeyHashComputation::test_build_hpo_study_key_hash_length PASSED
tests/tracking/unit/test_naming_policy.py::TestStudyKeyHashComputation::test_build_hpo_study_key_hash_deterministic PASSED
tests/tracking/unit/test_naming_policy.py::TestStudyKeyHashComputation::test_build_hpo_study_key_hash_different_inputs PASSED
tests/tracking/unit/test_naming_policy.py::TestStudyKeyHashComputation::test_build_hpo_study_key_includes_all_config_components PASSED
tests/tracking/unit/test_naming_policy.py::TestStudyKeyHashComputation::test_build_hpo_study_key_without_benchmark PASSED
tests/tracking/unit/test_naming_policy.py::TestStudyKeyHashComputation::test_build_hpo_study_family_key PASSED
tests/tracking/unit/test_naming_policy.py::TestStudyKeyHashComputation::test_build_hpo_study_family_hash PASSED
tests/tracking/unit/test_naming_policy.py::TestTrialKeyHashComputation::test_build_hpo_trial_key PASSED
tests/tracking/unit/test_naming_policy.py::TestTrialKeyHashComputation::test_build_hpo_trial_key_hash_length PASSED
tests/tracking/unit/test_naming_policy.py::TestTrialKeyHashComputation::test_build_hpo_trial_key_hash_includes_study_hash PASSED
tests/tracking/unit/test_naming_policy.py::TestTrialKeyHashComputation::test_build_hpo_trial_key_same_params_same_hash PASSED
tests/tracking/unit/test_naming_policy.py::TestTrialKeyHashComputation::test_build_hpo_trial_key_different_params_different_hash PASSED
tests/tracking/unit/test_naming_policy.py::TestTrialKeyHashComputation::test_build_hpo_trial_key_normalizes_hyperparameters PASSED
tests/tracking/unit/test_naming_policy.py::TestTrialKeyHashComputation::test_build_hpo_trial_key_normalizes_strings PASSED
tests/tracking/unit/test_naming_policy.py::TestTrialKeyHashComputation::test_build_hpo_trial_key_with_smoke_yaml_params PASSED
tests/tracking/unit/test_naming_policy_details.py::TestComponentConfiguration::test_zero_pad_trial_number PASSED
tests/tracking/unit/test_naming_policy_details.py::TestComponentConfiguration::test_component_default_values PASSED
tests/tracking/unit/test_naming_policy_details.py::TestComponentConfiguration::test_component_length_truncation PASSED
tests/tracking/unit/test_naming_policy_details.py::TestSemanticSuffix::test_semantic_suffix_enabled PASSED
tests/tracking/unit/test_naming_policy_details.py::TestSemanticSuffix::test_semantic_suffix_max_length PASSED
tests/tracking/unit/test_naming_policy_details.py::TestSemanticSuffix::test_semantic_suffix_sanitization PASSED
tests/tracking/unit/test_naming_policy_details.py::TestVersionFormat::test_version_format_parsing PASSED
tests/tracking/unit/test_naming_policy_details.py::TestSeparatorPolicy::test_separator_field PASSED
tests/tracking/unit/test_naming_policy_details.py::TestNormalizationRules::test_normalization_env_replace PASSED
tests/tracking/unit/test_naming_policy_details.py::TestNormalizationRules::test_normalization_model_replace PASSED
tests/tracking/unit/test_naming_policy_details.py::TestValidationRules::test_validate_max_length PASSED
tests/tracking/unit/test_naming_policy_details.py::TestValidationRules::test_validate_forbidden_chars PASSED
tests/tracking/unit/test_naming_policy_details.py::TestValidationRules::test_validate_warn_length PASSED
tests/tracking/unit/test_sweep_tracker_config_inference.py::TestSweepTrackerConfigInference::test_log_best_trial_id_infers_config_from_output_dir FAILED
tests/tracking/unit/test_sweep_tracker_config_inference.py::TestSweepTrackerConfigInference::test_log_best_trial_id_finds_config_in_parent_chain FAILED
tests/tracking/unit/test_sweep_tracker_config_inference.py::TestSweepTrackerConfigInference::test_log_best_trial_id_falls_back_to_cwd_config_when_not_found FAILED
tests/tracking/unit/test_sweep_tracker_config_inference.py::TestSweepTrackerConfigInference::test_log_final_metrics_infers_config_correctly PASSED
tests/tracking/unit/test_sweep_tracker_config_inference.py::TestSweepTrackerConfigInference::test_log_final_metrics_uses_hpo_output_dir_when_output_dir_none PASSED
tests/tracking/unit/test_sweep_tracker_config_inference.py::TestSweepTrackerConfigInference::test_config_dir_not_in_outputs_directory FAILED
tests/tracking/unit/test_tags_comprehensive.py::TestTagsRegistry::test_load_tags_registry_from_file PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagsRegistry::test_load_tags_registry_fallback_to_defaults PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagsRegistry::test_load_tags_registry_merges_with_defaults PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagsRegistry::test_load_tags_registry_module_level_caching PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagsRegistry::test_load_tags_registry_schema_version_defaults_to_0 PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagsRegistry::test_tags_registry_key_access_all_sections PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagsRegistry::test_tags_registry_raises_tagkeyerror_for_missing_keys PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagsRegistry::test_tags_registry_handles_invalid_section_types PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagsRegistry::test_tags_registry_handles_invalid_key_value_types PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagBuilding::test_build_mlflow_tags_minimal_tags PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagBuilding::test_build_mlflow_tags_hpo_process PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagBuilding::test_build_mlflow_tags_hpo_refit_process PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagBuilding::test_build_mlflow_tags_benchmarking_process PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagBuilding::test_build_mlflow_tags_final_training_process PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagBuilding::test_build_mlflow_tags_conversion_process PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagBuilding::test_build_mlflow_tags_optional_tags PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagSanitization::test_sanitize_tag_value_truncates_exceeding_max_length PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagSanitization::test_sanitize_tag_value_adds_indicator_when_truncated PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagSanitization::test_sanitize_tag_value_preserves_values_within_limit PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagSanitization::test_sanitize_tag_value_handles_empty_strings PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagSanitization::test_sanitize_tag_value_uses_max_length_from_config PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagKeyResolution::test_get_tag_key_loads_from_registry PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagKeyResolution::test_get_tag_key_falls_back_to_fallback PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagKeyResolution::test_get_tag_key_raises_when_missing_and_no_fallback PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagKeyResolution::test_get_tag_key_handles_registry_loading_failures PASSED
tests/tracking/unit/test_tags_registry.py::test_tags_registry_key_access PASSED
tests/tracking/unit/test_tags_registry.py::test_tags_registry_missing_key PASSED
tests/tracking/unit/test_tags_registry.py::test_tags_registry_validation_required_keys PASSED
tests/tracking/unit/test_tags_registry.py::test_load_tags_registry_from_file PASSED
tests/tracking/unit/test_tags_registry.py::test_load_tags_registry_missing_file PASSED
tests/tracking/unit/test_tags_registry.py::test_load_tags_registry_caching PASSED
tests/tracking/unit/test_tags_registry.py::test_load_tags_registry_merge_with_defaults PASSED
tests/tracking/unit/test_tags_registry.py::test_load_tags_registry_default_schema_version PASSED
tests/tracking/unit/test_tags_registry.py::test_tags_registry_invalid_section_type PASSED
tests/tracking/unit/test_tags_registry.py::test_tags_registry_invalid_key_type PASSED
tests/unit/api/test_extractors.py::TestFileTypeDetection::test_detect_pdf PASSED
tests/unit/api/test_extractors.py::TestFileTypeDetection::test_detect_png PASSED
tests/unit/api/test_extractors.py::TestFileTypeDetection::test_detect_jpeg PASSED
tests/unit/api/test_extractors.py::TestFileTypeDetection::test_invalid_file_type PASSED
tests/unit/api/test_extractors.py::TestPDFExtraction::test_extract_pdf_pymupdf PASSED
tests/unit/api/test_extractors.py::TestPDFExtraction::test_extract_pdf_pdfplumber PASSED
tests/unit/api/test_extractors.py::TestPDFExtraction::test_extract_pdf_invalid_extractor PASSED
tests/unit/api/test_extractors.py::TestImageExtraction::test_extract_image_easyocr PASSED
tests/unit/api/test_extractors.py::TestImageExtraction::test_extract_image_pytesseract PASSED
tests/unit/api/test_extractors.py::TestImageExtraction::test_extract_image_invalid_extractor PASSED
tests/unit/api/test_extractors.py::TestFileValidation::test_validate_file_success PASSED
tests/unit/api/test_extractors.py::TestFileValidation::test_validate_file_size_exceeded PASSED
tests/unit/api/test_inference.py::TestONNXInferenceEngine::test_load_model_success PASSED
tests/unit/api/test_inference.py::TestONNXInferenceEngine::test_load_model_file_not_found PASSED
tests/unit/api/test_inference.py::TestONNXInferenceEngine::test_predict_tokens PASSED
tests/unit/api/test_inference.py::TestONNXInferenceEngine::test_model_not_loaded_error PASSED
tests/unit/api/test_inference_fixes.py::TestInferenceFixes::test_tokenization_returns_numpy_arrays PASSED
tests/unit/api/test_inference_fixes.py::TestInferenceFixes::test_offset_mapping_extraction PASSED
tests/unit/api/test_inference_fixes.py::TestInferenceFixes::test_entity_extraction_with_offsets PASSED
tests/unit/api/test_inference_fixes.py::TestInferenceFixes::test_no_hanging_on_special_tokens PASSED
tests/unit/api/test_inference_performance.py::TestInferencePerformance::test_tokenization_does_not_hang PASSED
tests/unit/api/test_inference_performance.py::TestInferencePerformance::test_entity_extraction_with_offset_mapping PASSED
tests/unit/api/test_inference_performance.py::TestInferencePerformance::test_empty_text_handling PASSED
tests/unit/api/test_inference_performance.py::TestInferencePerformance::test_special_characters_handling PASSED
tests/unit/api/test_inference_performance.py::TestInferencePerformance::test_tokenization_consistency PASSED
tests/unit/shared/test_mlflow_setup.py::TestGetLocalTrackingUri::test_local_platform PASSED
tests/unit/shared/test_mlflow_setup.py::TestGetLocalTrackingUri::test_colab_with_drive PASSED
tests/unit/shared/test_mlflow_setup.py::TestGetLocalTrackingUri::test_colab_without_drive PASSED
tests/unit/shared/test_mlflow_setup.py::TestGetLocalTrackingUri::test_kaggle_platform PASSED
tests/unit/shared/test_mlflow_setup.py::TestGetAzureMlTrackingUri::test_success PASSED
tests/unit/shared/test_mlflow_setup.py::TestGetAzureMlTrackingUri::test_import_error PASSED
tests/unit/shared/test_mlflow_setup.py::TestGetAzureMlTrackingUri::test_workspace_access_error PASSED
tests/unit/shared/test_mlflow_setup.py::TestSetupMlflowCrossPlatform::test_local_fallback_no_ml_client PASSED
tests/unit/shared/test_mlflow_setup.py::TestSetupMlflowCrossPlatform::test_azure_ml_success PASSED
tests/unit/shared/test_mlflow_setup.py::TestSetupMlflowCrossPlatform::test_azure_ml_failure_with_fallback PASSED
tests/unit/shared/test_mlflow_setup.py::TestSetupMlflowCrossPlatform::test_azure_ml_failure_no_fallback PASSED
tests/unit/shared/test_mlflow_setup.py::TestSetupMlflowCrossPlatform::test_mlflow_not_installed PASSED
tests/unit/shared/test_mlflow_setup.py::TestPlatformSpecificBehavior::test_colab_drive_mounted PASSED
tests/unit/shared/test_mlflow_setup.py::TestPlatformSpecificBehavior::test_kaggle_platform PASSED
tests/unit/shared/test_mlflow_setup.py::TestPlatformSpecificBehavior::test_local_platform PASSED
tests/unit/shared/test_mlflow_setup.py::TestCreateMlClientFromConfig::test_success_with_env_vars PASSED
tests/unit/shared/test_mlflow_setup.py::TestCreateMlClientFromConfig::test_azure_ml_disabled PASSED
tests/unit/shared/test_mlflow_setup.py::TestCreateMlClientFromConfig::test_config_missing_azure_ml_section PASSED
tests/unit/shared/test_mlflow_setup.py::TestCreateMlClientFromConfig::test_missing_credentials PASSED
tests/unit/shared/test_mlflow_setup.py::TestCreateMlClientFromConfig::test_import_error PASSED
tests/unit/shared/test_mlflow_setup.py::TestSetupMlflowFromConfig::test_config_file_exists_azure_enabled PASSED
tests/unit/shared/test_mlflow_setup.py::TestSetupMlflowFromConfig::test_config_file_exists_azure_disabled PASSED
tests/unit/shared/test_mlflow_setup.py::TestSetupMlflowFromConfig::test_config_file_missing PASSED
tests/unit/shared/test_mlflow_setup.py::TestAzureMlNamespaceCollision::test_import_with_local_azureml_shadowing SKIPPED
tests/unit/shared/test_mlflow_setup.py::TestAzureMlNamespaceCollision::test_check_azureml_mlflow_available_with_shadowing SKIPPED
tests/unit/shared/test_mlflow_setup.py::TestAzureMlNamespaceCollision::test_local_azureml_functions_still_work_after_import SKIPPED
tests/unit/training/test_checkpoint_loader.py::TestValidateCheckpoint::test_validate_checkpoint_valid PASSED
tests/unit/training/test_checkpoint_loader.py::TestValidateCheckpoint::test_validate_checkpoint_with_safetensors PASSED
tests/unit/training/test_checkpoint_loader.py::TestValidateCheckpoint::test_validate_checkpoint_missing_config PASSED
tests/unit/training/test_checkpoint_loader.py::TestValidateCheckpoint::test_validate_checkpoint_missing_model PASSED
tests/unit/training/test_checkpoint_loader.py::TestValidateCheckpoint::test_validate_checkpoint_nonexistent PASSED
tests/unit/training/test_checkpoint_loader.py::TestResolveCheckpointPath::test_resolve_from_env_var PASSED
tests/unit/training/test_checkpoint_loader.py::TestResolveCheckpointPath::test_resolve_from_config PASSED
tests/unit/training/test_checkpoint_loader.py::TestResolveCheckpointPath::test_resolve_from_cache PASSED
tests/unit/training/test_checkpoint_loader.py::TestResolveCheckpointPath::test_resolve_with_pattern PASSED
tests/unit/training/test_checkpoint_loader.py::TestResolveCheckpointPath::test_resolve_none_when_invalid PASSED
tests/unit/training/test_checkpoint_loader.py::TestResolveCheckpointPath::test_resolve_none_when_not_configured PASSED
tests/unit/training/test_checkpoint_loader.py::TestResolveCheckpointPath::test_resolve_priority_env_over_config PASSED
tests/unit/training/test_cv_utils.py::TestKfoldSplitCreation::test_create_kfold_splits_returns_k_folds PASSED
tests/unit/training/test_cv_utils.py::TestKfoldSplitCreation::test_create_kfold_splits_all_samples_in_one_fold PASSED
tests/unit/training/test_cv_utils.py::TestKfoldSplitCreation::test_create_kfold_splits_stratified PASSED
tests/unit/training/test_cv_utils.py::TestKfoldSplitCreation::test_create_kfold_splits_same_seed_same_splits PASSED
tests/unit/training/test_cv_utils.py::TestKfoldSplitCreation::test_create_kfold_splits_different_seed_different_splits PASSED
tests/unit/training/test_cv_utils.py::TestKfoldSplitCreation::test_create_kfold_splits_smoke_yaml_params PASSED
tests/unit/training/test_cv_utils.py::TestKfoldSplitCreation::test_create_kfold_splits_no_shuffle PASSED
tests/unit/training/test_cv_utils.py::TestKfoldSplitCreation::test_create_kfold_splits_insufficient_samples PASSED
tests/unit/training/test_cv_utils.py::TestSaveLoadFoldSplits::test_save_fold_splits PASSED
tests/unit/training/test_cv_utils.py::TestSaveLoadFoldSplits::test_load_fold_splits PASSED
tests/unit/training/test_cv_utils.py::TestSaveLoadFoldSplits::test_load_fold_splits_file_not_found PASSED
tests/unit/training/test_cv_utils.py::TestSaveLoadFoldSplits::test_save_and_load_roundtrip PASSED
tests/unit/training/test_cv_utils.py::TestGetFoldData::test_get_fold_data PASSED
tests/unit/training/test_cv_utils.py::TestValidateSplits::test_validate_splits [CV] Fold 0: {'PERSON': 1, 'ORG': 1} | Missing: []
[CV] Fold 1: {'PERSON': 1, 'ORG': 1} | Missing: []
PASSED
tests/unit/training/test_data_combiner.py::TestCombineDatasets::test_new_only_strategy PASSED
tests/unit/training/test_data_combiner.py::TestCombineDatasets::test_combined_strategy PASSED
tests/unit/training/test_data_combiner.py::TestCombineDatasets::test_append_strategy PASSED
tests/unit/training/test_data_combiner.py::TestCombineDatasets::test_invalid_strategy PASSED
tests/unit/training/test_data_combiner.py::TestCombineDatasets::test_combined_requires_old_dataset PASSED
tests/unit/training/test_data_combiner.py::TestCombineDatasets::test_append_requires_old_dataset PASSED
tests/unit/training/test_data_combiner.py::TestCombineDatasets::test_new_dataset_not_found PASSED
tests/unit/training/test_data_combiner.py::TestCombineDatasets::test_old_dataset_not_found PASSED
tests/unit/training/test_data_combiner.py::TestCombineDatasets::test_no_validation_in_new_dataset PASSED
tests/unit/training/test_data_combiner.py::TestCombineDatasets::test_create_validation_split PASSED
tests/unit/training/test_train_config_defaults.py::test_core_training_defaults PASSED
tests/unit/training/test_train_config_defaults.py::test_metric_defaults PASSED
tests/unit/training/test_train_config_defaults.py::test_early_stopping_defaults PASSED
tests/unit/training/test_trainer.py::TestPrepareDataLoaders::test_prepare_data_loaders_basic PASSED
tests/unit/training/test_trainer.py::TestPrepareDataLoaders::test_prepare_data_loaders_with_indices PASSED
tests/unit/training/test_trainer.py::TestPrepareDataLoaders::test_prepare_data_loaders_use_all_data PASSED
tests/unit/training/test_trainer.py::TestPrepareDataLoaders::test_prepare_data_loaders_deberta_batch_size_cap PASSED
tests/unit/training/test_trainer.py::TestPrepareDataLoaders::test_prepare_data_loaders_val_split_fallback PASSED
tests/unit/training/test_trainer.py::TestPrepareDataLoaders::test_prepare_data_loaders_kfold_cv_val_from_train_data PASSED
tests/unit/training/test_trainer.py::TestCreateOptimizerAndScheduler::test_create_optimizer_and_scheduler_defaults PASSED
tests/unit/training/test_trainer.py::TestCreateOptimizerAndScheduler::test_create_optimizer_and_scheduler_custom PASSED
tests/unit/training/test_trainer.py::TestCreateOptimizerAndScheduler::test_create_optimizer_and_scheduler_warmup_capped PASSED
tests/unit/training/test_trainer.py::TestRunTrainingLoop::test_run_training_loop_basic PASSED
tests/unit/training/test_trainer.py::TestRunTrainingLoop::test_run_training_loop_multiple_epochs PASSED
tests/unit/training/test_trainer.py::TestSaveCheckpoint::test_save_checkpoint_success PASSED
tests/unit/training/test_trainer.py::TestSaveCheckpoint::test_save_checkpoint_creates_directory PASSED
tests/unit/training/test_trainer.py::TestTrainModel::test_train_model_basic PASSED
tests/unit/training/test_trainer.py::TestTrainModel::test_train_model_with_fold_splits PASSED
tests/unit/training/test_trainer.py::TestTrainModel::test_train_model_use_all_data PASSED
tests/unit/training/test_trainer.py::TestTrainModel::test_train_model_invalid_fold_idx PASSED
tests/workflows/test_full_workflow_e2e.py::test_full_workflow_e2e FAILED
tests/workflows/test_notebook_01_e2e.py::TestNotebookE2E_Core::test_environment_detection PASSED
tests/workflows/test_notebook_01_e2e.py::TestNotebookE2E_Core::test_path_setup PASSED
tests/workflows/test_notebook_01_e2e.py::TestNotebookE2E_Core::test_config_loading PASSED
tests/workflows/test_notebook_01_e2e.py::TestNotebookE2E_Core::test_dataset_verification PASSED
tests/workflows/test_notebook_01_e2e.py::TestNotebookE2E_Core::test_mlflow_setup PASSED
tests/workflows/test_notebook_01_e2e.py::TestNotebookE2E_Core::test_hpo_sweep_execution_mocked FAILED
tests/workflows/test_notebook_01_e2e.py::TestNotebookE2E_Core::test_benchmarking_execution_mocked PASSED
tests/workflows/test_notebook_01_e2e.py::TestNotebookE2E_Core::test_output_validation PASSED
tests/workflows/test_notebook_01_e2e.py::TestNotebookE2E_Full::test_repository_setup SKIPPED
tests/workflows/test_notebook_01_e2e.py::TestNotebookE2E_Full::test_dependency_check SKIPPED
tests/workflows/test_notebook_02_e2e.py::test_best_config_selection_e2e PASSED

=================================== FAILURES ===================================
______________ TestPruningBehavior.test_pruner_with_study_manager ______________
tests/hpo/integration/test_early_termination.py:274: in test_pruner_with_study_manager
    study, _, _, _, _ = study_manager.create_or_load_study(
src/training/hpo/core/study.py:252: in create_or_load_study
    storage_path, storage_uri, _ = setup_checkpoint_storage(
src/training/hpo/utils/helpers.py:91: in setup_checkpoint_storage
    storage_path = resolve_storage_path(
src/training/hpo/checkpoint/storage.py:71: in resolve_storage_path
    raise ValueError(
E   ValueError: study_key_hash is required for v2 folder structure. Legacy study_name format is no longer supported.
__________ TestPruningIntegration.test_pruning_with_checkpoint_resume __________
tests/hpo/integration/test_early_termination.py:367: in test_pruning_with_checkpoint_resume
    study1, _, _, _, _ = study_manager1.create_or_load_study(
src/training/hpo/core/study.py:252: in create_or_load_study
    storage_path, storage_uri, _ = setup_checkpoint_storage(
src/training/hpo/utils/helpers.py:91: in setup_checkpoint_storage
    storage_path = resolve_storage_path(
src/training/hpo/checkpoint/storage.py:71: in resolve_storage_path
    raise ValueError(
E   ValueError: study_key_hash is required for v2 folder structure. Legacy study_name format is no longer supported.
____________ TestRefitExecutionErrors.test_refit_subprocess_failure ____________
tests/hpo/integration/test_error_handling.py:334: in test_refit_subprocess_failure
    run_refit_training(
src/training/hpo/execution/local/refit.py:447: in run_refit_training
    refit_run_name = build_mlflow_run_name(
src/infrastructure/naming/mlflow/run_names.py:160: in build_mlflow_run_name
    raise ValueError(
E   ValueError: Naming policy not available or formatting failed. Legacy fallback removed. Process type: hpo_refit
------------------------------ Captured log call -------------------------------
INFO     training.hpo.execution.local.refit:refit.py:378 [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'batch_size': 4, 'dropout': 0.2, 'weight_decay': 0.05, 'backbone': 'distilbert'}
INFO     training.hpo.execution.local.refit:refit.py:110 [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
WARNING  infrastructure.paths.resolve:resolve.py:300 Failed to load paths.yaml config: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_refit_subprocess_failure0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.. Using fallback logic.
WARNING  infrastructure.naming.display_policy:display_policy.py:86 [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-120/test_refit_subprocess_failure0/config/naming.yaml, using empty policy
__________ TestCheckpointResume.test_resume_from_existing_checkpoint ___________
tests/hpo/integration/test_hpo_checkpoint_resume.py:164: in test_resume_from_existing_checkpoint
    assert len(study2.trials) == 1
E   AssertionError: assert 0 == 1
E    +  where 0 = len(<MagicMock name='mock.create_study().trials' id='123304070593888'>)
E    +    where <MagicMock name='mock.create_study().trials' id='123304070593888'> = <MagicMock name='mock.create_study()' id='123304074878336'>.trials
------------------------------ Captured log call -------------------------------
WARNING  evaluation.selection.trial_finder.discovery:discovery.py:64 No v2 study folders found in /tmp/pytest-of-codespace/pytest-120/test_resume_from_existing_chec0/outputs/hpo (resolved from /tmp/pytest-of-codespace/pytest-120/test_resume_from_existing_chec0/outputs/hpo)
INFO     training.hpo.core.study:study.py:286 [HPO] Resuming optimization for distilbert from checkpoint...
INFO     training.hpo.core.study:study.py:339 Loaded 0 existing trials (0 completed, 0 marked as failed)
______________ TestCheckpointResume.test_resume_preserves_trials _______________
tests/hpo/integration/test_hpo_checkpoint_resume.py:220: in test_resume_preserves_trials
    assert len(study2.trials) == 3
E   AssertionError: assert 0 == 3
E    +  where 0 = len(<MagicMock name='mock.create_study().trials' id='123304070593888'>)
E    +    where <MagicMock name='mock.create_study().trials' id='123304070593888'> = <MagicMock name='mock.create_study()' id='123304074878336'>.trials
------------------------------ Captured log call -------------------------------
WARNING  evaluation.selection.trial_finder.discovery:discovery.py:64 No v2 study folders found in /tmp/pytest-of-codespace/pytest-120/test_resume_preserves_trials0/outputs/hpo (resolved from /tmp/pytest-of-codespace/pytest-120/test_resume_preserves_trials0/outputs/hpo)
INFO     training.hpo.core.study:study.py:286 [HPO] Resuming optimization for distilbert from checkpoint...
INFO     training.hpo.core.study:study.py:339 Loaded 0 existing trials (0 completed, 0 marked as failed)
_______ TestCheckpointResume.test_resume_marks_running_trials_as_failed ________
tests/hpo/integration/test_hpo_checkpoint_resume.py:281: in test_resume_marks_running_trials_as_failed
    assert len(study2.trials) == 2
E   AssertionError: assert 0 == 2
E    +  where 0 = len(<MagicMock name='mock.create_study().trials' id='123304070593888'>)
E    +    where <MagicMock name='mock.create_study().trials' id='123304070593888'> = <MagicMock name='mock.create_study()' id='123304074878336'>.trials
------------------------------ Captured log call -------------------------------
WARNING  evaluation.selection.trial_finder.discovery:discovery.py:64 No v2 study folders found in /tmp/pytest-of-codespace/pytest-120/test_resume_marks_running_tria0/outputs/hpo (resolved from /tmp/pytest-of-codespace/pytest-120/test_resume_marks_running_tria0/outputs/hpo)
INFO     training.hpo.core.study:study.py:286 [HPO] Resuming optimization for distilbert from checkpoint...
INFO     training.hpo.core.study:study.py:339 Loaded 0 existing trials (0 completed, 0 marked as failed)
_____ TestCheckpointResume.test_resume_with_auto_resume_false_raises_error _____
tests/hpo/integration/test_hpo_checkpoint_resume.py:332: in test_resume_with_auto_resume_false_raises_error
    with pytest.raises(ValueError, match="auto_resume=false"):
E   Failed: DID NOT RAISE <class 'ValueError'>
------------------------------ Captured log call -------------------------------
WARNING  evaluation.selection.trial_finder.discovery:discovery.py:64 No v2 study folders found in /tmp/pytest-of-codespace/pytest-120/test_resume_with_auto_resume_f0/outputs/hpo (resolved from /tmp/pytest-of-codespace/pytest-120/test_resume_with_auto_resume_f0/outputs/hpo)
INFO     training.hpo.core.study:study.py:272 [HPO] auto_resume=false: Creating new study 'hpo_distilbert_no_resume_test' (ignoring existing study)
INFO     training.hpo.core.study:study.py:366 [HPO] Starting optimization for distilbert with checkpointing...
__________ TestCheckpointResume.test_resume_continues_trial_numbering __________
tests/hpo/integration/test_hpo_checkpoint_resume.py:391: in test_resume_continues_trial_numbering
    assert len(study2.trials) == 2
E   AssertionError: assert 0 == 2
E    +  where 0 = len(<MagicMock name='mock.create_study().trials' id='123304070593888'>)
E    +    where <MagicMock name='mock.create_study().trials' id='123304070593888'> = <MagicMock name='mock.create_study()' id='123304074878336'>.trials
------------------------------ Captured log call -------------------------------
WARNING  evaluation.selection.trial_finder.discovery:discovery.py:64 No v2 study folders found in /tmp/pytest-of-codespace/pytest-120/test_resume_continues_trial_nu0/outputs/hpo (resolved from /tmp/pytest-of-codespace/pytest-120/test_resume_continues_trial_nu0/outputs/hpo)
INFO     training.hpo.core.study:study.py:286 [HPO] Resuming optimization for distilbert from checkpoint...
INFO     training.hpo.core.study:study.py:339 Loaded 0 existing trials (0 completed, 0 marked as failed)
_____ TestCheckpointSmokeYaml.test_checkpoint_smoke_yaml_auto_resume_true ______
tests/hpo/integration/test_hpo_checkpoint_resume.py:502: in test_checkpoint_smoke_yaml_auto_resume_true
    assert len(study2.trials) == 1
E   AssertionError: assert 0 == 1
E    +  where 0 = len(<MagicMock name='mock.create_study().trials' id='123304070593888'>)
E    +    where <MagicMock name='mock.create_study().trials' id='123304070593888'> = <MagicMock name='mock.create_study()' id='123304074878336'>.trials
------------------------------ Captured log call -------------------------------
WARNING  evaluation.selection.trial_finder.discovery:discovery.py:64 No v2 study folders found in /tmp/pytest-of-codespace/pytest-120/test_checkpoint_smoke_yaml_aut0/outputs/hpo (resolved from /tmp/pytest-of-codespace/pytest-120/test_checkpoint_smoke_yaml_aut0/outputs/hpo)
INFO     training.hpo.core.study:study.py:286 [HPO] Resuming optimization for distilbert from checkpoint...
INFO     training.hpo.core.study:study.py:339 Loaded 0 existing trials (0 completed, 0 marked as failed)
_________ TestFullHPOWorkflow.test_full_hpo_workflow_with_cv_and_refit _________
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1447: in __enter__
    original, local = self.get_original()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1420: in get_original
    raise AttributeError(
E   AttributeError: <module 'training.hpo.execution.local.sweep' from '/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep/__init__.py'> does not have the attribute 'mlflow'
__________ TestFullHPOWorkflow.test_full_hpo_workflow_no_cv_no_refit ___________
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1447: in __enter__
    original, local = self.get_original()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1420: in get_original
    raise AttributeError(
E   AttributeError: <module 'training.hpo.execution.local.sweep' from '/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep/__init__.py'> does not have the attribute 'mlflow'
__ TestFullHPOWorkflow.test_full_hpo_workflow_creates_correct_path_structure ___
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1447: in __enter__
    original, local = self.get_original()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1420: in get_original
    raise AttributeError(
E   AttributeError: <module 'training.hpo.execution.local.sweep' from '/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep/__init__.py'> does not have the attribute 'mlflow'
_________ TestHPOResumeWorkflow.test_resume_workflow_preserves_trials __________
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1447: in __enter__
    original, local = self.get_original()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1420: in get_original
    raise AttributeError(
E   AttributeError: <module 'training.hpo.execution.local.sweep' from '/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep/__init__.py'> does not have the attribute 'mlflow'
_______ TestHPOResumeWorkflow.test_resume_workflow_with_different_run_id _______
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1447: in __enter__
    original, local = self.get_original()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1420: in get_original
    raise AttributeError(
E   AttributeError: <module 'training.hpo.execution.local.sweep' from '/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep/__init__.py'> does not have the attribute 'mlflow'
______________ TestHPOResumeWorkflow.test_resume_workflow_with_cv ______________
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1447: in __enter__
    original, local = self.get_original()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1420: in get_original
    raise AttributeError(
E   AttributeError: <module 'training.hpo.execution.local.sweep' from '/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep/__init__.py'> does not have the attribute 'mlflow'
_ TestStudyManagerWithRunMode.test_study_manager_passes_run_mode_to_create_study_name _
tests/hpo/integration/test_hpo_run_mode_variants.py:284: in test_study_manager_passes_run_mode_to_create_study_name
    study_manager.create_or_load_study(
src/training/hpo/core/study.py:252: in create_or_load_study
    storage_path, storage_uri, _ = setup_checkpoint_storage(
src/training/hpo/utils/helpers.py:91: in setup_checkpoint_storage
    storage_path = resolve_storage_path(
src/training/hpo/checkpoint/storage.py:71: in resolve_storage_path
    raise ValueError(
E   ValueError: study_key_hash is required for v2 folder structure. Legacy study_name format is no longer supported.
______ TestRefitTrainingSetup.test_refit_uses_best_trial_hyperparameters _______
tests/hpo/integration/test_refit_training.py:65: in test_refit_uses_best_trial_hyperparameters
    metrics, checkpoint_dir, refit_run_id = run_refit_training(
src/training/hpo/execution/local/refit.py:447: in run_refit_training
    refit_run_name = build_mlflow_run_name(
src/infrastructure/naming/mlflow/run_names.py:160: in build_mlflow_run_name
    raise ValueError(
E   ValueError: Naming policy not available or formatting failed. Legacy fallback removed. Process type: hpo_refit
------------------------------ Captured log call -------------------------------
INFO     training.hpo.execution.local.refit:refit.py:378 [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'batch_size': 4, 'dropout': 0.2, 'weight_decay': 0.05, 'backbone': 'distilbert'}
INFO     training.hpo.execution.local.refit:refit.py:110 [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
WARNING  infrastructure.paths.resolve:resolve.py:300 Failed to load paths.yaml config: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_refit_uses_best_trial_hyp0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.. Using fallback logic.
WARNING  infrastructure.naming.display_policy:display_policy.py:86 [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-120/test_refit_uses_best_trial_hyp0/config/naming.yaml, using empty policy
_____________ TestRefitTrainingSetup.test_refit_creates_mlflow_run _____________
tests/hpo/integration/test_refit_training.py:141: in test_refit_creates_mlflow_run
    metrics, checkpoint_dir, refit_run_id = run_refit_training(
src/training/hpo/execution/local/refit.py:447: in run_refit_training
    refit_run_name = build_mlflow_run_name(
src/infrastructure/naming/mlflow/run_names.py:160: in build_mlflow_run_name
    raise ValueError(
E   ValueError: Naming policy not available or formatting failed. Legacy fallback removed. Process type: hpo_refit
------------------------------ Captured log call -------------------------------
INFO     training.hpo.execution.local.refit:refit.py:378 [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
INFO     training.hpo.execution.local.refit:refit.py:110 [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
WARNING  infrastructure.paths.resolve:resolve.py:300 Failed to load paths.yaml config: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_refit_creates_mlflow_run0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.. Using fallback logic.
WARNING  infrastructure.naming.display_policy:display_policy.py:86 [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-120/test_refit_creates_mlflow_run0/config/naming.yaml, using empty policy
________ TestRefitTrainingSetup.test_refit_creates_v2_output_directory _________
tests/hpo/integration/test_refit_training.py:195: in test_refit_creates_v2_output_directory
    metrics, checkpoint_dir, refit_run_id = run_refit_training(
src/training/hpo/execution/local/refit.py:447: in run_refit_training
    refit_run_name = build_mlflow_run_name(
src/infrastructure/naming/mlflow/run_names.py:160: in build_mlflow_run_name
    raise ValueError(
E   ValueError: Naming policy not available or formatting failed. Legacy fallback removed. Process type: hpo_refit
------------------------------ Captured log call -------------------------------
INFO     training.hpo.execution.local.refit:refit.py:378 [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
INFO     training.hpo.execution.local.refit:refit.py:110 [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
WARNING  infrastructure.paths.resolve:resolve.py:300 Failed to load paths.yaml config: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_refit_creates_v2_output_d0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.. Using fallback logic.
WARNING  infrastructure.naming.display_policy:display_policy.py:86 [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-120/test_refit_creates_v2_output_d0/config/naming.yaml, using empty policy
________ TestRefitTrainingExecution.test_refit_reads_metrics_from_file _________
tests/hpo/integration/test_refit_training.py:259: in test_refit_reads_metrics_from_file
    returned_metrics, checkpoint_dir, refit_run_id = run_refit_training(
src/training/hpo/execution/local/refit.py:447: in run_refit_training
    refit_run_name = build_mlflow_run_name(
src/infrastructure/naming/mlflow/run_names.py:160: in build_mlflow_run_name
    raise ValueError(
E   ValueError: Naming policy not available or formatting failed. Legacy fallback removed. Process type: hpo_refit
------------------------------ Captured log call -------------------------------
INFO     training.hpo.execution.local.refit:refit.py:378 [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
INFO     training.hpo.execution.local.refit:refit.py:110 [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
WARNING  infrastructure.paths.resolve:resolve.py:300 Failed to load paths.yaml config: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_refit_reads_metrics_from_0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.. Using fallback logic.
WARNING  infrastructure.naming.display_policy:display_policy.py:86 [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-120/test_refit_reads_metrics_from_0/config/naming.yaml, using empty policy
_________ TestRefitTrainingExecution.test_refit_logs_metrics_to_mlflow _________
tests/hpo/integration/test_refit_training.py:311: in test_refit_logs_metrics_to_mlflow
    run_refit_training(
src/training/hpo/execution/local/refit.py:447: in run_refit_training
    refit_run_name = build_mlflow_run_name(
src/infrastructure/naming/mlflow/run_names.py:160: in build_mlflow_run_name
    raise ValueError(
E   ValueError: Naming policy not available or formatting failed. Legacy fallback removed. Process type: hpo_refit
------------------------------ Captured log call -------------------------------
INFO     training.hpo.execution.local.refit:refit.py:378 [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
INFO     training.hpo.execution.local.refit:refit.py:110 [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
WARNING  infrastructure.paths.resolve:resolve.py:300 Failed to load paths.yaml config: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_refit_logs_metrics_to_mlf0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.. Using fallback logic.
WARNING  infrastructure.naming.display_policy:display_policy.py:86 [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-120/test_refit_logs_metrics_to_mlf0/config/naming.yaml, using empty policy
______ TestRefitTrainingExecution.test_refit_creates_checkpoint_directory ______
tests/hpo/integration/test_refit_training.py:384: in test_refit_creates_checkpoint_directory
    metrics, returned_checkpoint_dir, refit_run_id = run_refit_training(
src/training/hpo/execution/local/refit.py:447: in run_refit_training
    refit_run_name = build_mlflow_run_name(
src/infrastructure/naming/mlflow/run_names.py:160: in build_mlflow_run_name
    raise ValueError(
E   ValueError: Naming policy not available or formatting failed. Legacy fallback removed. Process type: hpo_refit
------------------------------ Captured log call -------------------------------
INFO     training.hpo.execution.local.refit:refit.py:378 [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
INFO     training.hpo.execution.local.refit:refit.py:110 [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
WARNING  infrastructure.paths.resolve:resolve.py:300 Failed to load paths.yaml config: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_refit_creates_checkpoint_0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.. Using fallback logic.
WARNING  infrastructure.naming.display_policy:display_policy.py:86 [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-120/test_refit_creates_checkpoint_0/config/naming.yaml, using empty policy
_________ TestRefitTrainingSmokeYaml.test_refit_enabled_in_smoke_yaml __________
tests/hpo/integration/test_refit_training.py:447: in test_refit_enabled_in_smoke_yaml
    metrics, checkpoint_dir, refit_run_id = run_refit_training(
src/training/hpo/execution/local/refit.py:447: in run_refit_training
    refit_run_name = build_mlflow_run_name(
src/infrastructure/naming/mlflow/run_names.py:160: in build_mlflow_run_name
    raise ValueError(
E   ValueError: Naming policy not available or formatting failed. Legacy fallback removed. Process type: hpo_refit
------------------------------ Captured log call -------------------------------
INFO     training.hpo.execution.local.refit:refit.py:378 [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
INFO     training.hpo.execution.local.refit:refit.py:110 [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
WARNING  infrastructure.paths.resolve:resolve.py:300 Failed to load paths.yaml config: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_refit_enabled_in_smoke_ya0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.. Using fallback logic.
WARNING  infrastructure.naming.display_policy:display_policy.py:86 [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-120/test_refit_enabled_in_smoke_ya0/config/naming.yaml, using empty policy
____________ TestRefitTrainingSmokeYaml.test_refit_uses_full_epochs ____________
tests/hpo/integration/test_refit_training.py:508: in test_refit_uses_full_epochs
    run_refit_training(
src/training/hpo/execution/local/refit.py:447: in run_refit_training
    refit_run_name = build_mlflow_run_name(
src/infrastructure/naming/mlflow/run_names.py:160: in build_mlflow_run_name
    raise ValueError(
E   ValueError: Naming policy not available or formatting failed. Legacy fallback removed. Process type: hpo_refit
------------------------------ Captured log call -------------------------------
INFO     training.hpo.execution.local.refit:refit.py:378 [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
INFO     training.hpo.execution.local.refit:refit.py:110 [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
WARNING  infrastructure.paths.resolve:resolve.py:300 Failed to load paths.yaml config: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_refit_uses_full_epochs0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.. Using fallback logic.
WARNING  infrastructure.naming.display_policy:display_policy.py:86 [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-120/test_refit_uses_full_epochs0/config/naming.yaml, using empty policy
_ TestRefitCheckpointDuplicationPrevention.test_refit_skips_checkpoint_folder_logging _
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1447: in __enter__
    original, local = self.get_original()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1420: in get_original
    raise AttributeError(
E   AttributeError: <module 'infrastructure.tracking.mlflow.trackers.sweep_tracker' from '/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/trackers/sweep_tracker/__init__.py'> does not have the attribute 'upload_checkpoint_archive'
_ TestRefitCheckpointDuplicationPrevention.test_refit_prevents_duplication_only_archive_uploaded _
tests/hpo/integration/test_refit_training.py:687: in test_refit_prevents_duplication_only_archive_uploaded
    run_refit_training(
src/training/hpo/execution/local/refit.py:447: in run_refit_training
    refit_run_name = build_mlflow_run_name(
src/infrastructure/naming/mlflow/run_names.py:160: in build_mlflow_run_name
    raise ValueError(
E   ValueError: Naming policy not available or formatting failed. Legacy fallback removed. Process type: hpo_refit
------------------------------ Captured log call -------------------------------
INFO     training.hpo.execution.local.refit:refit.py:378 [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
INFO     training.hpo.execution.local.refit:refit.py:110 [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
WARNING  infrastructure.paths.resolve:resolve.py:300 Failed to load paths.yaml config: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_refit_prevents_duplicatio0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.. Using fallback logic.
WARNING  infrastructure.naming.display_policy:display_policy.py:86 [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-120/test_refit_prevents_duplicatio0/config/naming.yaml, using empty policy
_ TestDisableAutoOptunaMark.test_disable_auto_optuna_mark_false_enables_marking _
tests/hpo/integration/test_smoke_yaml_options.py:298: in test_disable_auto_optuna_mark_false_enables_marking
    assert len(running_trials) == 0, "RUNNING trials should be marked FAILED when disable_auto_optuna_mark=false"
E   AssertionError: RUNNING trials should be marked FAILED when disable_auto_optuna_mark=false
E   assert 1 == 0
E    +  where 1 = len([FrozenTrial(number=0, state=<TrialState.RUNNING: 0>, values=None, datetime_start=datetime.datetime(2026, 1, 18, 16, 25, 57, 166271), datetime_complete=None, params={}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={}, trial_id=1, value=None)])
__ TestTrialExecutionWithCV.test_trial_execution_with_cv_creates_nested_runs ___
tests/hpo/integration/test_trial_execution.py:203: in test_trial_execution_with_cv_creates_nested_runs
    avg_metric, fold_metrics = run_training_trial_with_cv(
src/training/hpo/execution/local/cv.py:438: in run_training_trial_with_cv
    trial_base_dir = _build_trial_output_dir(
src/training/hpo/execution/local/cv.py:240: in _build_trial_output_dir
    raise RuntimeError(
E   RuntimeError: Cannot create trial in v2 study folder study-abc12345 without trial_key_hash. study_key_hash=NO, trial_key_hash=NO. This is a critical error that must be fixed.
------------------------------ Captured log call -------------------------------
WARNING  infrastructure.tracking.mlflow.hash_utils:hash_utils.py:446 Could not get or compute study_key_hash. study_key_hash=missing, hpo_parent_run_id=provided, configs=missing
WARNING  infrastructure.naming.display_policy:display_policy.py:86 [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-120/test_trial_execution_with_cv_c0/config/naming.yaml, using empty policy
WARNING  training.hpo.execution.local.cv:cv.py:655 Could not build systematic run name and tags: Naming policy not available or formatting failed. Legacy fallback removed. Process type: hpo_trial, using fallback
WARNING  training.hpo.execution.local.cv:cv.py:688 Could not create trial run: expected string or bytes-like object
WARNING  infrastructure.tracking.mlflow.hash_utils:hash_utils.py:446 Could not get or compute study_key_hash. study_key_hash=missing, hpo_parent_run_id=provided, configs=missing
ERROR    training.hpo.execution.local.cv:cv.py:136 In v2 study folder study-abc12345 but trial_key_hash is None. study_key_hash=NO. Will attempt to compute hash.
___ TestTrialExecutionWithCV.test_trial_execution_with_cv_creates_fold_runs ____
tests/hpo/integration/test_trial_execution.py:296: in test_trial_execution_with_cv_creates_fold_runs
    assert mock_client.create_run.called
E   AssertionError: assert False
E    +  where False = <Mock name='mlflow.tracking.MlflowClient().create_run' id='123304076041312'>.called
E    +    where <Mock name='mlflow.tracking.MlflowClient().create_run' id='123304076041312'> = <Mock name='mlflow.tracking.MlflowClient()' id='123304076041456'>.create_run
------------------------------ Captured log call -------------------------------
WARNING  infrastructure.naming.display_policy:display_policy.py:86 [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-120/test_trial_execution_with_cv_c1/config/naming.yaml, using empty policy
WARNING  training.hpo.execution.local.cv:cv.py:655 Could not build systematic run name and tags: Naming policy not available or formatting failed. Legacy fallback removed. Process type: hpo_trial, using fallback
WARNING  training.hpo.execution.local.cv:cv.py:688 Could not create trial run: expected string or bytes-like object
WARNING  infrastructure.paths.resolve:resolve.py:300 Failed to load paths.yaml config: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_trial_execution_with_cv_c1/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.. Using fallback logic.
INFO     training.hpo.execution.local.cv:cv.py:310 [CV] Created trial folder: /tmp/pytest-of-codespace/pytest-120/test_trial_execution_with_cv_c1/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-1b593325 (trial 0)
___ TestTrialExecutionWithCV.test_trial_execution_with_cv_aggregates_metrics ___
tests/hpo/integration/test_trial_execution.py:341: in test_trial_execution_with_cv_aggregates_metrics
    avg_metric, fold_metrics = run_training_trial_with_cv(
src/training/hpo/execution/local/cv.py:438: in run_training_trial_with_cv
    trial_base_dir = _build_trial_output_dir(
src/training/hpo/execution/local/cv.py:240: in _build_trial_output_dir
    raise RuntimeError(
E   RuntimeError: Cannot create trial in v2 study folder study-abc12345 without trial_key_hash. study_key_hash=YES, trial_key_hash=NO. This is a critical error that must be fixed.
------------------------------ Captured log call -------------------------------
WARNING  infrastructure.naming.display_policy:display_policy.py:86 [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-120/test_trial_execution_with_cv_a0/config/naming.yaml, using empty policy
WARNING  training.hpo.execution.local.cv:cv.py:655 Could not build systematic run name and tags: Naming policy not available or formatting failed. Legacy fallback removed. Process type: hpo_trial, using fallback
WARNING  training.hpo.execution.local.cv:cv.py:688 Could not create trial run: expected string or bytes-like object
WARNING  infrastructure.tracking.mlflow.hash_utils:hash_utils.py:517 Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
WARNING  training.hpo.execution.local.cv:cv.py:111 Could not compute trial_key_hash
ERROR    training.hpo.execution.local.cv:cv.py:136 In v2 study folder study-abc12345 but trial_key_hash is None. study_key_hash=YES. Will attempt to compute hash.
WARNING  training.hpo.execution.local.cv:cv.py:185 In v2 study folder study-abc12345 but trial_key_hash is None. Attempting to recompute from trial_params...
WARNING  infrastructure.tracking.mlflow.hash_utils:hash_utils.py:517 Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
______ TestTrialExecutionWithCV.test_trial_execution_with_cv_output_paths ______
tests/hpo/integration/test_trial_execution.py:420: in test_trial_execution_with_cv_output_paths
    avg_metric, fold_metrics = run_training_trial_with_cv(
src/training/hpo/execution/local/cv.py:438: in run_training_trial_with_cv
    trial_base_dir = _build_trial_output_dir(
src/training/hpo/execution/local/cv.py:240: in _build_trial_output_dir
    raise RuntimeError(
E   RuntimeError: Cannot create trial in v2 study folder study-abc12345 without trial_key_hash. study_key_hash=YES, trial_key_hash=NO. This is a critical error that must be fixed.
------------------------------ Captured log call -------------------------------
WARNING  infrastructure.naming.display_policy:display_policy.py:86 [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-120/test_trial_execution_with_cv_o0/config/naming.yaml, using empty policy
WARNING  training.hpo.execution.local.cv:cv.py:655 Could not build systematic run name and tags: Naming policy not available or formatting failed. Legacy fallback removed. Process type: hpo_trial, using fallback
WARNING  training.hpo.execution.local.cv:cv.py:688 Could not create trial run: expected string or bytes-like object
WARNING  infrastructure.tracking.mlflow.hash_utils:hash_utils.py:517 Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
WARNING  training.hpo.execution.local.cv:cv.py:111 Could not compute trial_key_hash
ERROR    training.hpo.execution.local.cv:cv.py:136 In v2 study folder study-abc12345 but trial_key_hash is None. study_key_hash=YES. Will attempt to compute hash.
WARNING  training.hpo.execution.local.cv:cv.py:185 In v2 study folder study-abc12345 but trial_key_hash is None. Attempting to recompute from trial_params...
WARNING  infrastructure.tracking.mlflow.hash_utils:hash_utils.py:517 Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
___ TestTrialExecutionWithCV.test_trial_execution_with_cv_smoke_yaml_params ____
tests/hpo/integration/test_trial_execution.py:490: in test_trial_execution_with_cv_smoke_yaml_params
    avg_metric, fold_metrics = run_training_trial_with_cv(
src/training/hpo/execution/local/cv.py:438: in run_training_trial_with_cv
    trial_base_dir = _build_trial_output_dir(
src/training/hpo/execution/local/cv.py:240: in _build_trial_output_dir
    raise RuntimeError(
E   RuntimeError: Cannot create trial in v2 study folder study-abc12345 without trial_key_hash. study_key_hash=YES, trial_key_hash=NO. This is a critical error that must be fixed.
------------------------------ Captured log call -------------------------------
WARNING  infrastructure.naming.display_policy:display_policy.py:86 [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-120/test_trial_execution_with_cv_s0/config/naming.yaml, using empty policy
WARNING  training.hpo.execution.local.cv:cv.py:655 Could not build systematic run name and tags: Naming policy not available or formatting failed. Legacy fallback removed. Process type: hpo_trial, using fallback
WARNING  training.hpo.execution.local.cv:cv.py:688 Could not create trial run: expected string or bytes-like object
WARNING  infrastructure.tracking.mlflow.hash_utils:hash_utils.py:517 Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
WARNING  training.hpo.execution.local.cv:cv.py:111 Could not compute trial_key_hash
ERROR    training.hpo.execution.local.cv:cv.py:136 In v2 study folder study-abc12345 but trial_key_hash is None. study_key_hash=YES. Will attempt to compute hash.
WARNING  training.hpo.execution.local.cv:cv.py:185 In v2 study folder study-abc12345 but trial_key_hash is None. Attempting to recompute from trial_params...
WARNING  infrastructure.tracking.mlflow.hash_utils:hash_utils.py:517 Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
______ TestSweepTrackerV2HashComputation.test_v2_hash_computation_success ______
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py:110: in test_v2_hash_computation_success
    with patch.object(tracker, "_log_sweep_metadata"):
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1447: in __enter__
    original, local = self.get_original()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1420: in get_original
    raise AttributeError(
E   AttributeError: <infrastructure.tracking.mlflow.trackers.sweep_tracker.sweep_tracker.MLflowSweepTracker object at 0x7024f706ee90> does not have the attribute '_log_sweep_metadata'
_ TestSweepTrackerV2HashComputation.test_v2_hash_computation_missing_train_config _
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py:168: in test_v2_hash_computation_missing_train_config
    with patch.object(tracker, "_log_sweep_metadata"):
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1447: in __enter__
    original, local = self.get_original()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1420: in get_original
    raise AttributeError(
E   AttributeError: <infrastructure.tracking.mlflow.trackers.sweep_tracker.sweep_tracker.MLflowSweepTracker object at 0x7024f78b8af0> does not have the attribute '_log_sweep_metadata'
------------------------------ Captured log call -------------------------------
INFO     common.shared.mlflow_setup:mlflow_setup.py:320 Using local tracking: sqlite:////workspaces/resume-ner-azureml/mlruns/mlflow.db
INFO     common.shared.mlflow_setup:mlflow_setup.py:320 Using local tracking: sqlite:////workspaces/resume-ner-azureml/mlruns/mlflow.db
_ TestSweepTrackerV2HashComputation.test_v2_hash_computation_empty_eval_config _
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py:221: in test_v2_hash_computation_empty_eval_config
    with patch.object(tracker, "_log_sweep_metadata"):
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1447: in __enter__
    original, local = self.get_original()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1420: in get_original
    raise AttributeError(
E   AttributeError: <infrastructure.tracking.mlflow.trackers.sweep_tracker.sweep_tracker.MLflowSweepTracker object at 0x7024f749aef0> does not have the attribute '_log_sweep_metadata'
------------------------------ Captured log call -------------------------------
INFO     common.shared.mlflow_setup:mlflow_setup.py:320 Using local tracking: sqlite:////workspaces/resume-ner-azureml/mlruns/mlflow.db
INFO     common.shared.mlflow_setup:mlflow_setup.py:320 Using local tracking: sqlite:////workspaces/resume-ner-azureml/mlruns/mlflow.db
_ TestSweepTrackerBestTrialSearch.test_best_trial_search_by_study_key_hash_success _
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py:298: in test_best_trial_search_by_study_key_hash_success
    tracker._log_best_trial_id(
E   AttributeError: 'MLflowSweepTracker' object has no attribute '_log_best_trial_id'
_ TestSweepTrackerBestTrialSearch.test_best_trial_search_filters_by_parent_run_id _
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py:375: in test_best_trial_search_filters_by_parent_run_id
    tracker._log_best_trial_id(
E   AttributeError: 'MLflowSweepTracker' object has no attribute '_log_best_trial_id'
_ TestSweepTrackerBestTrialSearch.test_best_trial_search_no_parent_study_key_hash _
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py:424: in test_best_trial_search_no_parent_study_key_hash
    tracker._log_best_trial_id(
E   AttributeError: 'MLflowSweepTracker' object has no attribute '_log_best_trial_id'
___ TestSweepTrackerBestTrialSearch.test_best_trial_search_no_matching_runs ____
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py:474: in test_best_trial_search_no_matching_runs
    tracker._log_best_trial_id(
E   AttributeError: 'MLflowSweepTracker' object has no attribute '_log_best_trial_id'
__ TestSweepTrackerBestTrialSearch.test_best_trial_search_exception_handling ___
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py:514: in test_best_trial_search_exception_handling
    tracker._log_best_trial_id(
E   AttributeError: 'MLflowSweepTracker' object has no attribute '_log_best_trial_id'
___ TestSweepTrackerTrialNumberExtraction.test_extract_trial_number_from_tag ___
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py:536: in test_extract_trial_number_from_tag
    result = tracker._extract_trial_number(mock_run)
E   AttributeError: 'MLflowSweepTracker' object has no attribute '_extract_trial_number'
_ TestSweepTrackerTrialNumberExtraction.test_extract_trial_number_from_run_name _
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py:547: in test_extract_trial_number_from_run_name
    result = tracker._extract_trial_number(mock_run)
E   AttributeError: 'MLflowSweepTracker' object has no attribute '_extract_trial_number'
_ TestSweepTrackerTrialNumberExtraction.test_extract_trial_number_returns_none_when_not_found _
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py:558: in test_extract_trial_number_returns_none_when_not_found
    result = tracker._extract_trial_number(mock_run)
E   AttributeError: 'MLflowSweepTracker' object has no attribute '_extract_trial_number'
_____ TestSearchRootsIntegration.test_search_roots_used_in_local_discovery _____
tests/selection/integration/test_artifact_acquisition_unified_config.py:94: in test_search_roots_used_in_local_discovery
    assert result.success is True
E   AssertionError: assert False is True
E    +  where False = ArtifactResult(request=ArtifactRequest(artifact_kind=<ArtifactKind.CHECKPOINT: 'checkpoint'>, run_id='test_run_123', backbone='distilbert', study_key_hash='study12345678', trial_key_hash='trial87654321', refit_run_id='refit_run_123', experiment_name=None, metadata={'search_roots': ['custom_root1', 'custom_root2']}), success=False, path=None, source=None, status=None, error='Acquisition error: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_search_roots_used_in_loca0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.', metadata={}).success
------------------------------ Captured log call -------------------------------
INFO     evaluation.selection.artifact_unified.acquisition:acquisition.py:105 Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_123..., study_key_hash=study123..., trial_key_hash=trial876...
INFO     evaluation.selection.artifact_unified.acquisition:acquisition.py:153 Selected artifact run: artifact_run_id=refit_run_12..., trial_run_id=test_run_123...
INFO     evaluation.selection.artifact_unified.acquisition:acquisition.py:173 Artifact acquisition config for checkpoint: priority=['local'], local.enabled=True, local.validate=True, drive.enabled=True, mlflow.enabled=True
INFO     evaluation.selection.artifact_unified.acquisition:acquisition.py:242 Acquiring artifact from local: run_id=refit_run_12..., artifact_kind=checkpoint, backbone=distilbert
ERROR    evaluation.selection.artifact_unified.acquisition:acquisition.py:303 Artifact acquisition failed: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_search_roots_used_in_loca0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/acquisition.py", line 249, in acquire_artifact
    acquired_path = _acquire_from_location(
  File "/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/acquisition.py", line 342, in _acquire_from_location
    destination = _build_artifact_destination(
  File "/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/acquisition.py", line 487, in _build_artifact_destination
    base_dir = resolve_output_path(
  File "/workspaces/resume-ner-azureml/src/infrastructure/paths/resolve.py", line 88, in resolve_output_path
    paths_config = load_paths_config(config_dir)
  File "/workspaces/resume-ner-azureml/src/infrastructure/paths/config.py", line 83, in load_paths_config
    raise FileNotFoundError(
FileNotFoundError: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_search_roots_used_in_loca0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.
_ TestArtifactKindsPriorityIntegration.test_artifact_kinds_fallback_to_global_priority _
tests/selection/integration/test_artifact_acquisition_unified_config.py:239: in test_artifact_kinds_fallback_to_global_priority
    assert result.success is True
E   AssertionError: assert False is True
E    +  where False = ArtifactResult(request=ArtifactRequest(artifact_kind=<ArtifactKind.CHECKPOINT: 'checkpoint'>, run_id='test_run_123', backbone='distilbert', study_key_hash='study12345678', trial_key_hash='trial87654321', refit_run_id=None, experiment_name=None, metadata={}), success=False, path=None, source=None, status=None, error='Acquisition error: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_artifact_kinds_fallback_t0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.', metadata={}).success
------------------------------ Captured log call -------------------------------
INFO     evaluation.selection.artifact_unified.acquisition:acquisition.py:105 Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_123..., study_key_hash=study123..., trial_key_hash=trial876...
INFO     evaluation.selection.artifact_unified.acquisition:acquisition.py:153 Selected artifact run: artifact_run_id=refit_run_12..., trial_run_id=test_run_123...
INFO     evaluation.selection.artifact_unified.acquisition:acquisition.py:173 Artifact acquisition config for checkpoint: priority=['local', 'mlflow'], local.enabled=True, local.validate=True, drive.enabled=True, mlflow.enabled=True
INFO     evaluation.selection.artifact_unified.acquisition:acquisition.py:242 Acquiring artifact from local: run_id=refit_run_12..., artifact_kind=checkpoint, backbone=distilbert
ERROR    evaluation.selection.artifact_unified.acquisition:acquisition.py:303 Artifact acquisition failed: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_artifact_kinds_fallback_t0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/acquisition.py", line 249, in acquire_artifact
    acquired_path = _acquire_from_location(
  File "/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/acquisition.py", line 342, in _acquire_from_location
    destination = _build_artifact_destination(
  File "/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/acquisition.py", line 487, in _build_artifact_destination
    base_dir = resolve_output_path(
  File "/workspaces/resume-ner-azureml/src/infrastructure/paths/resolve.py", line 88, in resolve_output_path
    paths_config = load_paths_config(config_dir)
  File "/workspaces/resume-ner-azureml/src/infrastructure/paths/config.py", line 83, in load_paths_config
    raise FileNotFoundError(
FileNotFoundError: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_artifact_kinds_fallback_t0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.
________ TestConfigOptionCombinations.test_config_with_disabled_sources ________
tests/selection/integration/test_artifact_acquisition_unified_config.py:386: in test_config_with_disabled_sources
    assert result.success is True
E   AssertionError: assert False is True
E    +  where False = ArtifactResult(request=ArtifactRequest(artifact_kind=<ArtifactKind.CHECKPOINT: 'checkpoint'>, run_id='test_run_123', backbone='distilbert', study_key_hash='study12345678', trial_key_hash='trial87654321', refit_run_id=None, experiment_name=None, metadata={}), success=False, path=None, source=None, status=None, error='Acquisition error: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_config_with_disabled_sour0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.', metadata={}).success
------------------------------ Captured log call -------------------------------
INFO     evaluation.selection.artifact_unified.acquisition:acquisition.py:105 Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_123..., study_key_hash=study123..., trial_key_hash=trial876...
INFO     evaluation.selection.artifact_unified.acquisition:acquisition.py:153 Selected artifact run: artifact_run_id=refit_run_12..., trial_run_id=test_run_123...
INFO     evaluation.selection.artifact_unified.acquisition:acquisition.py:173 Artifact acquisition config for checkpoint: priority=['local', 'drive', 'mlflow'], local.enabled=True, local.validate=True, drive.enabled=False, mlflow.enabled=False
INFO     evaluation.selection.artifact_unified.acquisition:acquisition.py:242 Acquiring artifact from local: run_id=refit_run_12..., artifact_kind=checkpoint, backbone=distilbert
ERROR    evaluation.selection.artifact_unified.acquisition:acquisition.py:303 Artifact acquisition failed: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_config_with_disabled_sour0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/acquisition.py", line 249, in acquire_artifact
    acquired_path = _acquire_from_location(
  File "/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/acquisition.py", line 342, in _acquire_from_location
    destination = _build_artifact_destination(
  File "/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/acquisition.py", line 487, in _build_artifact_destination
    base_dir = resolve_output_path(
  File "/workspaces/resume-ner-azureml/src/infrastructure/paths/resolve.py", line 88, in resolve_output_path
    paths_config = load_paths_config(config_dir)
  File "/workspaces/resume-ner-azureml/src/infrastructure/paths/config.py", line 83, in load_paths_config
    raise FileNotFoundError(
FileNotFoundError: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_config_with_disabled_sour0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.
_______________ test_cache_dual_file_strategy_creates_all_files ________________
tests/selection/integration/test_best_model_cache.py:44: in test_cache_dual_file_strategy_creates_all_files
    timestamped_file, latest_file, index_file = save_best_model_cache(
src/evaluation/selection/cache.py:250: in save_best_model_cache
    timestamped_file, latest_file, index_file = save_cache_with_dual_strategy(
src/infrastructure/paths/cache.py:195: in save_cache_with_dual_strategy
    cache_dir = resolve_output_path(
src/infrastructure/paths/resolve.py:88: in resolve_output_path
    paths_config = load_paths_config(config_dir)
src/infrastructure/paths/config.py:83: in load_paths_config
    raise FileNotFoundError(
E   FileNotFoundError: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_cache_dual_file_strategy_0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.
______________ test_cache_load_valid_cache_with_mlflow_validation ______________
tests/selection/integration/test_best_model_cache.py:91: in test_cache_load_valid_cache_with_mlflow_validation
    latest_file = get_cache_file_path(
src/infrastructure/paths/cache.py:58: in get_cache_file_path
    paths_config = load_paths_config(config_dir)
src/infrastructure/paths/config.py:83: in load_paths_config
    raise FileNotFoundError(
E   FileNotFoundError: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_cache_load_valid_cache_wi0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.
_______________ test_cache_load_cache_key_mismatch_returns_none ________________
tests/selection/integration/test_best_model_cache.py:135: in test_cache_load_cache_key_mismatch_returns_none
    latest_file = get_cache_file_path(
src/infrastructure/paths/cache.py:58: in get_cache_file_path
    paths_config = load_paths_config(config_dir)
src/infrastructure/paths/config.py:83: in load_paths_config
    raise FileNotFoundError(
E   FileNotFoundError: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_cache_load_cache_key_mism0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.
______________________ test_cache_partial_write_recovery _______________________
tests/selection/integration/test_best_model_cache.py:205: in test_cache_partial_write_recovery
    with patch("evaluation.selection.cache.MlflowClient", return_value=mock_client):
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1447: in __enter__
    original, local = self.get_original()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1420: in get_original
    raise AttributeError(
E   AttributeError: <module 'evaluation.selection.cache' from '/workspaces/resume-ner-azureml/src/evaluation/selection/cache.py'> does not have the attribute 'MlflowClient'
_____________________ test_cache_missing_metrics_handling ______________________
tests/selection/integration/test_best_model_cache.py:248: in test_cache_missing_metrics_handling
    latest_file = get_cache_file_path(
src/infrastructure/paths/cache.py:58: in get_cache_file_path
    paths_config = load_paths_config(config_dir)
src/infrastructure/paths/config.py:83: in load_paths_config
    raise FileNotFoundError(
E   FileNotFoundError: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_cache_missing_metrics_han0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.
__ TestLatencyAggregationIntegration.test_latency_aggregation_latest_strategy __
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:160: in make_managed_session
    yield session
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1530: in _search_runs
    experiment_ids = [int(e) for e in experiment_ids]
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1530: in <listcomp>
    experiment_ids = [int(e) for e in experiment_ids]
E   ValueError: invalid literal for int() with base 10: 'benchmark_experiment_id_123'

The above exception was the direct cause of the following exception:
tests/selection/integration/test_best_model_selection_config_integration.py:87: in test_latency_aggregation_latest_strategy
    result = find_best_model_from_mlflow(
src/evaluation/selection/mlflow_selection.py:554: in find_best_model_from_mlflow
    valid_benchmark_runs = _query_and_filter_benchmark_runs(
src/evaluation/selection/mlflow_selection.py:171: in _query_and_filter_benchmark_runs
    benchmark_runs = query_runs_by_tags(
src/infrastructure/tracking/mlflow/queries.py:66: in query_runs_by_tags
    all_runs = client.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py:3662: in search_runs
    return self._tracking_client.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py:848: in search_runs
    return self.store.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/abstract_store.py:751: in search_runs
    runs, token = self._search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1500: in _search_runs
    with self.ManagedSessionMaker() as session:
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:153: in __exit__
    self.gen.throw(typ, value, traceback)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:177: in make_managed_session
    raise MlflowException(message=e, error_code=INTERNAL_ERROR) from e
E   mlflow.exceptions.MlflowException: invalid literal for int() with base 10: 'benchmark_experiment_id_123'
------------------------------ Captured log call -------------------------------
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:540 Finding best model from MLflow
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:541   Benchmark experiment: test_experiment-benchmark
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:542   HPO experiments: 1
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:543   Objective metric: macro-f1
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:544   Composite weights: F1=0.70, Latency=0.30
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:549   Latency aggregation: latest (from config file, applied when multiple benchmark runs exist with same benchmark_key)
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:169 Querying benchmark runs...
__ TestLatencyAggregationIntegration.test_latency_aggregation_median_strategy __
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:160: in make_managed_session
    yield session
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1530: in _search_runs
    experiment_ids = [int(e) for e in experiment_ids]
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1530: in <listcomp>
    experiment_ids = [int(e) for e in experiment_ids]
E   ValueError: invalid literal for int() with base 10: 'benchmark_experiment_id_123'

The above exception was the direct cause of the following exception:
tests/selection/integration/test_best_model_selection_config_integration.py:129: in test_latency_aggregation_median_strategy
    result = find_best_model_from_mlflow(
src/evaluation/selection/mlflow_selection.py:554: in find_best_model_from_mlflow
    valid_benchmark_runs = _query_and_filter_benchmark_runs(
src/evaluation/selection/mlflow_selection.py:171: in _query_and_filter_benchmark_runs
    benchmark_runs = query_runs_by_tags(
src/infrastructure/tracking/mlflow/queries.py:66: in query_runs_by_tags
    all_runs = client.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py:3662: in search_runs
    return self._tracking_client.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py:848: in search_runs
    return self.store.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/abstract_store.py:751: in search_runs
    runs, token = self._search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1500: in _search_runs
    with self.ManagedSessionMaker() as session:
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:153: in __exit__
    self.gen.throw(typ, value, traceback)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:177: in make_managed_session
    raise MlflowException(message=e, error_code=INTERNAL_ERROR) from e
E   mlflow.exceptions.MlflowException: invalid literal for int() with base 10: 'benchmark_experiment_id_123'
------------------------------ Captured log call -------------------------------
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:540 Finding best model from MLflow
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:541   Benchmark experiment: test_experiment-benchmark
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:542   HPO experiments: 1
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:543   Objective metric: macro-f1
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:544   Composite weights: F1=0.70, Latency=0.30
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:549   Latency aggregation: median (from config file, applied when multiple benchmark runs exist with same benchmark_key)
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:169 Querying benchmark runs...
___ TestLatencyAggregationIntegration.test_latency_aggregation_mean_strategy ___
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:160: in make_managed_session
    yield session
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1530: in _search_runs
    experiment_ids = [int(e) for e in experiment_ids]
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1530: in <listcomp>
    experiment_ids = [int(e) for e in experiment_ids]
E   ValueError: invalid literal for int() with base 10: 'benchmark_experiment_id_123'

The above exception was the direct cause of the following exception:
tests/selection/integration/test_best_model_selection_config_integration.py:171: in test_latency_aggregation_mean_strategy
    result = find_best_model_from_mlflow(
src/evaluation/selection/mlflow_selection.py:554: in find_best_model_from_mlflow
    valid_benchmark_runs = _query_and_filter_benchmark_runs(
src/evaluation/selection/mlflow_selection.py:171: in _query_and_filter_benchmark_runs
    benchmark_runs = query_runs_by_tags(
src/infrastructure/tracking/mlflow/queries.py:66: in query_runs_by_tags
    all_runs = client.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py:3662: in search_runs
    return self._tracking_client.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py:848: in search_runs
    return self.store.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/abstract_store.py:751: in search_runs
    runs, token = self._search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1500: in _search_runs
    with self.ManagedSessionMaker() as session:
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:153: in __exit__
    self.gen.throw(typ, value, traceback)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:177: in make_managed_session
    raise MlflowException(message=e, error_code=INTERNAL_ERROR) from e
E   mlflow.exceptions.MlflowException: invalid literal for int() with base 10: 'benchmark_experiment_id_123'
------------------------------ Captured log call -------------------------------
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:540 Finding best model from MLflow
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:541   Benchmark experiment: test_experiment-benchmark
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:542   HPO experiments: 1
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:543   Objective metric: macro-f1
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:544   Composite weights: F1=0.70, Latency=0.30
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:549   Latency aggregation: mean (from config file, applied when multiple benchmark runs exist with same benchmark_key)
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:169 Querying benchmark runs...
__ TestScoringConfigIntegration.test_scoring_weights_used_in_composite_score ___
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:160: in make_managed_session
    yield session
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1530: in _search_runs
    experiment_ids = [int(e) for e in experiment_ids]
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1530: in <listcomp>
    experiment_ids = [int(e) for e in experiment_ids]
E   ValueError: invalid literal for int() with base 10: 'benchmark_experiment_id_123'

The above exception was the direct cause of the following exception:
tests/selection/integration/test_best_model_selection_config_integration.py:407: in test_scoring_weights_used_in_composite_score
    result = find_best_model_from_mlflow(
src/evaluation/selection/mlflow_selection.py:554: in find_best_model_from_mlflow
    valid_benchmark_runs = _query_and_filter_benchmark_runs(
src/evaluation/selection/mlflow_selection.py:171: in _query_and_filter_benchmark_runs
    benchmark_runs = query_runs_by_tags(
src/infrastructure/tracking/mlflow/queries.py:66: in query_runs_by_tags
    all_runs = client.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py:3662: in search_runs
    return self._tracking_client.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py:848: in search_runs
    return self.store.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/abstract_store.py:751: in search_runs
    runs, token = self._search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1500: in _search_runs
    with self.ManagedSessionMaker() as session:
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:153: in __exit__
    self.gen.throw(typ, value, traceback)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:177: in make_managed_session
    raise MlflowException(message=e, error_code=INTERNAL_ERROR) from e
E   mlflow.exceptions.MlflowException: invalid literal for int() with base 10: 'benchmark_experiment_id_123'
------------------------------ Captured log call -------------------------------
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:540 Finding best model from MLflow
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:541   Benchmark experiment: test_experiment-benchmark
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:542   HPO experiments: 1
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:543   Objective metric: macro-f1
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:544   Composite weights: F1=0.80, Latency=0.20
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:549   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:169 Querying benchmark runs...
__ TestScoringConfigIntegration.test_normalize_weights_controls_normalization __
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:160: in make_managed_session
    yield session
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1530: in _search_runs
    experiment_ids = [int(e) for e in experiment_ids]
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1530: in <listcomp>
    experiment_ids = [int(e) for e in experiment_ids]
E   ValueError: invalid literal for int() with base 10: 'benchmark_experiment_id_123'

The above exception was the direct cause of the following exception:
tests/selection/integration/test_best_model_selection_config_integration.py:447: in test_normalize_weights_controls_normalization
    result = find_best_model_from_mlflow(
src/evaluation/selection/mlflow_selection.py:554: in find_best_model_from_mlflow
    valid_benchmark_runs = _query_and_filter_benchmark_runs(
src/evaluation/selection/mlflow_selection.py:171: in _query_and_filter_benchmark_runs
    benchmark_runs = query_runs_by_tags(
src/infrastructure/tracking/mlflow/queries.py:66: in query_runs_by_tags
    all_runs = client.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py:3662: in search_runs
    return self._tracking_client.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py:848: in search_runs
    return self.store.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/abstract_store.py:751: in search_runs
    runs, token = self._search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1500: in _search_runs
    with self.ManagedSessionMaker() as session:
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:153: in __exit__
    self.gen.throw(typ, value, traceback)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:177: in make_managed_session
    raise MlflowException(message=e, error_code=INTERNAL_ERROR) from e
E   mlflow.exceptions.MlflowException: invalid literal for int() with base 10: 'benchmark_experiment_id_123'
------------------------------ Captured log call -------------------------------
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:540 Finding best model from MLflow
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:541   Benchmark experiment: test_experiment-benchmark
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:542   HPO experiments: 1
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:543   Objective metric: macro-f1
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:544   Composite weights: F1=0.70, Latency=0.30
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:549   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:169 Querying benchmark runs...
_ TestBenchmarkRequiredMetricsIntegration.test_required_metrics_filters_benchmark_runs _
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:160: in make_managed_session
    yield session
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1530: in _search_runs
    experiment_ids = [int(e) for e in experiment_ids]
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1530: in <listcomp>
    experiment_ids = [int(e) for e in experiment_ids]
E   ValueError: invalid literal for int() with base 10: 'benchmark_experiment_id_123'

The above exception was the direct cause of the following exception:
tests/selection/integration/test_best_model_selection_config_integration.py:526: in test_required_metrics_filters_benchmark_runs
    result = find_best_model_from_mlflow(
src/evaluation/selection/mlflow_selection.py:554: in find_best_model_from_mlflow
    valid_benchmark_runs = _query_and_filter_benchmark_runs(
src/evaluation/selection/mlflow_selection.py:171: in _query_and_filter_benchmark_runs
    benchmark_runs = query_runs_by_tags(
src/infrastructure/tracking/mlflow/queries.py:66: in query_runs_by_tags
    all_runs = client.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py:3662: in search_runs
    return self._tracking_client.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py:848: in search_runs
    return self.store.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/abstract_store.py:751: in search_runs
    runs, token = self._search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1500: in _search_runs
    with self.ManagedSessionMaker() as session:
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:153: in __exit__
    self.gen.throw(typ, value, traceback)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:177: in make_managed_session
    raise MlflowException(message=e, error_code=INTERNAL_ERROR) from e
E   mlflow.exceptions.MlflowException: invalid literal for int() with base 10: 'benchmark_experiment_id_123'
------------------------------ Captured log call -------------------------------
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:540 Finding best model from MLflow
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:541   Benchmark experiment: test_experiment-benchmark
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:542   HPO experiments: 1
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:543   Objective metric: macro-f1
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:544   Composite weights: F1=0.70, Latency=0.30
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:549   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:169 Querying benchmark runs...
_______ TestCompleteConfigWorkflow.test_all_config_options_used_together _______
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:160: in make_managed_session
    yield session
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1530: in _search_runs
    experiment_ids = [int(e) for e in experiment_ids]
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1530: in <listcomp>
    experiment_ids = [int(e) for e in experiment_ids]
E   ValueError: invalid literal for int() with base 10: 'benchmark_experiment_id_123'

The above exception was the direct cause of the following exception:
tests/selection/integration/test_best_model_selection_config_integration.py:622: in test_all_config_options_used_together
    result = find_best_model_from_mlflow(
src/evaluation/selection/mlflow_selection.py:554: in find_best_model_from_mlflow
    valid_benchmark_runs = _query_and_filter_benchmark_runs(
src/evaluation/selection/mlflow_selection.py:171: in _query_and_filter_benchmark_runs
    benchmark_runs = query_runs_by_tags(
src/infrastructure/tracking/mlflow/queries.py:66: in query_runs_by_tags
    all_runs = client.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py:3662: in search_runs
    return self._tracking_client.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py:848: in search_runs
    return self.store.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/abstract_store.py:751: in search_runs
    runs, token = self._search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1500: in _search_runs
    with self.ManagedSessionMaker() as session:
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:153: in __exit__
    self.gen.throw(typ, value, traceback)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:177: in make_managed_session
    raise MlflowException(message=e, error_code=INTERNAL_ERROR) from e
E   mlflow.exceptions.MlflowException: invalid literal for int() with base 10: 'benchmark_experiment_id_123'
------------------------------ Captured log call -------------------------------
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:540 Finding best model from MLflow
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:541   Benchmark experiment: test_experiment-benchmark
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:542   HPO experiments: 1
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:543   Objective metric: macro-f1
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:544   Composite weights: F1=0.70, Latency=0.30
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:549   Latency aggregation: median (from config file, applied when multiple benchmark runs exist with same benchmark_key)
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:169 Querying benchmark runs...
__ TestMLflowSelectionConfigUsage.test_find_best_model_uses_objective_metric ___
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:160: in make_managed_session
    yield session
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1530: in _search_runs
    experiment_ids = [int(e) for e in experiment_ids]
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1530: in <listcomp>
    experiment_ids = [int(e) for e in experiment_ids]
E   ValueError: invalid literal for int() with base 10: 'benchmark_experiment_id_123'

The above exception was the direct cause of the following exception:
tests/selection/integration/test_mlflow_selection_config.py:34: in test_find_best_model_uses_objective_metric
    result = find_best_model_from_mlflow(
src/evaluation/selection/mlflow_selection.py:554: in find_best_model_from_mlflow
    valid_benchmark_runs = _query_and_filter_benchmark_runs(
src/evaluation/selection/mlflow_selection.py:171: in _query_and_filter_benchmark_runs
    benchmark_runs = query_runs_by_tags(
src/infrastructure/tracking/mlflow/queries.py:66: in query_runs_by_tags
    all_runs = client.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py:3662: in search_runs
    return self._tracking_client.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py:848: in search_runs
    return self.store.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/abstract_store.py:751: in search_runs
    runs, token = self._search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1500: in _search_runs
    with self.ManagedSessionMaker() as session:
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:153: in __exit__
    self.gen.throw(typ, value, traceback)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:177: in make_managed_session
    raise MlflowException(message=e, error_code=INTERNAL_ERROR) from e
E   mlflow.exceptions.MlflowException: invalid literal for int() with base 10: 'benchmark_experiment_id_123'
------------------------------ Captured log call -------------------------------
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:540 Finding best model from MLflow
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:541   Benchmark experiment: test_experiment-benchmark
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:542   HPO experiments: 2
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:543   Objective metric: macro-f1
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:544   Composite weights: F1=0.70, Latency=0.30
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:549   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:169 Querying benchmark runs...
___ TestMLflowSelectionConfigUsage.test_find_best_model_uses_scoring_weights ___
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:160: in make_managed_session
    yield session
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1530: in _search_runs
    experiment_ids = [int(e) for e in experiment_ids]
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1530: in <listcomp>
    experiment_ids = [int(e) for e in experiment_ids]
E   ValueError: invalid literal for int() with base 10: 'benchmark_experiment_id_123'

The above exception was the direct cause of the following exception:
tests/selection/integration/test_mlflow_selection_config.py:72: in test_find_best_model_uses_scoring_weights
    result = find_best_model_from_mlflow(
src/evaluation/selection/mlflow_selection.py:554: in find_best_model_from_mlflow
    valid_benchmark_runs = _query_and_filter_benchmark_runs(
src/evaluation/selection/mlflow_selection.py:171: in _query_and_filter_benchmark_runs
    benchmark_runs = query_runs_by_tags(
src/infrastructure/tracking/mlflow/queries.py:66: in query_runs_by_tags
    all_runs = client.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py:3662: in search_runs
    return self._tracking_client.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py:848: in search_runs
    return self.store.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/abstract_store.py:751: in search_runs
    runs, token = self._search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1500: in _search_runs
    with self.ManagedSessionMaker() as session:
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:153: in __exit__
    self.gen.throw(typ, value, traceback)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:177: in make_managed_session
    raise MlflowException(message=e, error_code=INTERNAL_ERROR) from e
E   mlflow.exceptions.MlflowException: invalid literal for int() with base 10: 'benchmark_experiment_id_123'
------------------------------ Captured log call -------------------------------
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:540 Finding best model from MLflow
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:541   Benchmark experiment: test_experiment-benchmark
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:542   HPO experiments: 2
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:543   Objective metric: macro-f1
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:544   Composite weights: F1=0.70, Latency=0.30
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:549   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:169 Querying benchmark runs...
_ TestMLflowSelectionConfigUsage.test_find_best_model_uses_benchmark_required_metrics _
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:160: in make_managed_session
    yield session
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1530: in _search_runs
    experiment_ids = [int(e) for e in experiment_ids]
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1530: in <listcomp>
    experiment_ids = [int(e) for e in experiment_ids]
E   ValueError: invalid literal for int() with base 10: 'benchmark_experiment_id_123'

The above exception was the direct cause of the following exception:
tests/selection/integration/test_mlflow_selection_config.py:112: in test_find_best_model_uses_benchmark_required_metrics
    result = find_best_model_from_mlflow(
src/evaluation/selection/mlflow_selection.py:554: in find_best_model_from_mlflow
    valid_benchmark_runs = _query_and_filter_benchmark_runs(
src/evaluation/selection/mlflow_selection.py:171: in _query_and_filter_benchmark_runs
    benchmark_runs = query_runs_by_tags(
src/infrastructure/tracking/mlflow/queries.py:66: in query_runs_by_tags
    all_runs = client.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py:3662: in search_runs
    return self._tracking_client.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py:848: in search_runs
    return self.store.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/abstract_store.py:751: in search_runs
    runs, token = self._search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1500: in _search_runs
    with self.ManagedSessionMaker() as session:
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:153: in __exit__
    self.gen.throw(typ, value, traceback)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:177: in make_managed_session
    raise MlflowException(message=e, error_code=INTERNAL_ERROR) from e
E   mlflow.exceptions.MlflowException: invalid literal for int() with base 10: 'benchmark_experiment_id_123'
------------------------------ Captured log call -------------------------------
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:540 Finding best model from MLflow
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:541   Benchmark experiment: test_experiment-benchmark
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:542   HPO experiments: 2
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:543   Objective metric: macro-f1
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:544   Composite weights: F1=0.70, Latency=0.30
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:549   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:169 Querying benchmark runs...
___ TestMLflowSelectionConfigUsage.test_find_best_model_weight_normalization ___
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:160: in make_managed_session
    yield session
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1530: in _search_runs
    experiment_ids = [int(e) for e in experiment_ids]
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1530: in <listcomp>
    experiment_ids = [int(e) for e in experiment_ids]
E   ValueError: invalid literal for int() with base 10: 'benchmark_experiment_id_123'

The above exception was the direct cause of the following exception:
tests/selection/integration/test_mlflow_selection_config.py:147: in test_find_best_model_weight_normalization
    result = find_best_model_from_mlflow(
src/evaluation/selection/mlflow_selection.py:554: in find_best_model_from_mlflow
    valid_benchmark_runs = _query_and_filter_benchmark_runs(
src/evaluation/selection/mlflow_selection.py:171: in _query_and_filter_benchmark_runs
    benchmark_runs = query_runs_by_tags(
src/infrastructure/tracking/mlflow/queries.py:66: in query_runs_by_tags
    all_runs = client.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py:3662: in search_runs
    return self._tracking_client.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py:848: in search_runs
    return self.store.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/abstract_store.py:751: in search_runs
    runs, token = self._search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1500: in _search_runs
    with self.ManagedSessionMaker() as session:
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:153: in __exit__
    self.gen.throw(typ, value, traceback)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:177: in make_managed_session
    raise MlflowException(message=e, error_code=INTERNAL_ERROR) from e
E   mlflow.exceptions.MlflowException: invalid literal for int() with base 10: 'benchmark_experiment_id_123'
------------------------------ Captured log call -------------------------------
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:540 Finding best model from MLflow
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:541   Benchmark experiment: test_experiment-benchmark
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:542   HPO experiments: 2
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:543   Objective metric: accuracy
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:544   Composite weights: F1=0.50, Latency=0.50
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:549   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:169 Querying benchmark runs...
_ TestMLflowSelectionConfigUsage.test_find_best_model_composite_score_calculation _
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:160: in make_managed_session
    yield session
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1530: in _search_runs
    experiment_ids = [int(e) for e in experiment_ids]
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1530: in <listcomp>
    experiment_ids = [int(e) for e in experiment_ids]
E   ValueError: invalid literal for int() with base 10: 'benchmark_experiment_id_123'

The above exception was the direct cause of the following exception:
tests/selection/integration/test_mlflow_selection_config.py:210: in test_find_best_model_composite_score_calculation
    result = find_best_model_from_mlflow(
src/evaluation/selection/mlflow_selection.py:554: in find_best_model_from_mlflow
    valid_benchmark_runs = _query_and_filter_benchmark_runs(
src/evaluation/selection/mlflow_selection.py:171: in _query_and_filter_benchmark_runs
    benchmark_runs = query_runs_by_tags(
src/infrastructure/tracking/mlflow/queries.py:66: in query_runs_by_tags
    all_runs = client.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py:3662: in search_runs
    return self._tracking_client.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py:848: in search_runs
    return self.store.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/abstract_store.py:751: in search_runs
    runs, token = self._search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1500: in _search_runs
    with self.ManagedSessionMaker() as session:
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:153: in __exit__
    self.gen.throw(typ, value, traceback)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:177: in make_managed_session
    raise MlflowException(message=e, error_code=INTERNAL_ERROR) from e
E   mlflow.exceptions.MlflowException: invalid literal for int() with base 10: 'benchmark_experiment_id_123'
------------------------------ Captured log call -------------------------------
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:540 Finding best model from MLflow
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:541   Benchmark experiment: test_experiment-benchmark
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:542   HPO experiments: 2
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:543   Objective metric: macro-f1
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:544   Composite weights: F1=0.70, Latency=0.30
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:549   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:169 Querying benchmark runs...
_ TestMLflowSelectionConfigUsage.test_find_best_model_all_config_options_together _
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:160: in make_managed_session
    yield session
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1530: in _search_runs
    experiment_ids = [int(e) for e in experiment_ids]
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1530: in <listcomp>
    experiment_ids = [int(e) for e in experiment_ids]
E   ValueError: invalid literal for int() with base 10: 'benchmark_experiment_id_123'

The above exception was the direct cause of the following exception:
tests/selection/integration/test_mlflow_selection_config.py:252: in test_find_best_model_all_config_options_together
    result = find_best_model_from_mlflow(
src/evaluation/selection/mlflow_selection.py:554: in find_best_model_from_mlflow
    valid_benchmark_runs = _query_and_filter_benchmark_runs(
src/evaluation/selection/mlflow_selection.py:171: in _query_and_filter_benchmark_runs
    benchmark_runs = query_runs_by_tags(
src/infrastructure/tracking/mlflow/queries.py:66: in query_runs_by_tags
    all_runs = client.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py:3662: in search_runs
    return self._tracking_client.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py:848: in search_runs
    return self.store.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/abstract_store.py:751: in search_runs
    runs, token = self._search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1500: in _search_runs
    with self.ManagedSessionMaker() as session:
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:153: in __exit__
    self.gen.throw(typ, value, traceback)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:177: in make_managed_session
    raise MlflowException(message=e, error_code=INTERNAL_ERROR) from e
E   mlflow.exceptions.MlflowException: invalid literal for int() with base 10: 'benchmark_experiment_id_123'
------------------------------ Captured log call -------------------------------
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:540 Finding best model from MLflow
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:541   Benchmark experiment: test_experiment-benchmark
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:542   HPO experiments: 2
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:543   Objective metric: macro-f1
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:544   Composite weights: F1=0.70, Latency=0.30
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:549   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:169 Querying benchmark runs...
___ TestMLflowSelectionConfigUsage.test_find_best_model_custom_config_values ___
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:160: in make_managed_session
    yield session
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1530: in _search_runs
    experiment_ids = [int(e) for e in experiment_ids]
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1530: in <listcomp>
    experiment_ids = [int(e) for e in experiment_ids]
E   ValueError: invalid literal for int() with base 10: 'benchmark_experiment_id_123'

The above exception was the direct cause of the following exception:
tests/selection/integration/test_mlflow_selection_config.py:290: in test_find_best_model_custom_config_values
    result = find_best_model_from_mlflow(
src/evaluation/selection/mlflow_selection.py:554: in find_best_model_from_mlflow
    valid_benchmark_runs = _query_and_filter_benchmark_runs(
src/evaluation/selection/mlflow_selection.py:171: in _query_and_filter_benchmark_runs
    benchmark_runs = query_runs_by_tags(
src/infrastructure/tracking/mlflow/queries.py:66: in query_runs_by_tags
    all_runs = client.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py:3662: in search_runs
    return self._tracking_client.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py:848: in search_runs
    return self.store.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/abstract_store.py:751: in search_runs
    runs, token = self._search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1500: in _search_runs
    with self.ManagedSessionMaker() as session:
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:153: in __exit__
    self.gen.throw(typ, value, traceback)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:177: in make_managed_session
    raise MlflowException(message=e, error_code=INTERNAL_ERROR) from e
E   mlflow.exceptions.MlflowException: invalid literal for int() with base 10: 'benchmark_experiment_id_123'
------------------------------ Captured log call -------------------------------
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:540 Finding best model from MLflow
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:541   Benchmark experiment: test_experiment-benchmark
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:542   HPO experiments: 2
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:543   Objective metric: accuracy
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:544   Composite weights: F1=0.50, Latency=0.50
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:549   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:169 Querying benchmark runs...
___ TestSelectionCacheConfig.test_load_cached_best_model_validates_cache_key ___
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1447: in __enter__
    original, local = self.get_original()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1420: in get_original
    raise AttributeError(
E   AttributeError: <module 'evaluation.selection.cache' from '/workspaces/resume-ner-azureml/src/evaluation/selection/cache.py'> does not have the attribute 'MlflowClient'
_______ TestSelectionCacheConfig.test_run_mode_reuse_if_exists_behavior ________
tests/selection/integration/test_selection_cache_config.py:276: in test_run_mode_reuse_if_exists_behavior
    with patch("evaluation.selection.cache.MlflowClient") as mock_client_class:
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1447: in __enter__
    original, local = self.get_original()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1420: in get_original
    raise AttributeError(
E   AttributeError: <module 'evaluation.selection.cache' from '/workspaces/resume-ner-azureml/src/evaluation/selection/cache.py'> does not have the attribute 'MlflowClient'
__ TestSelectionConfigEdgeCases.test_find_best_model_missing_required_metrics __
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:160: in make_managed_session
    yield session
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1530: in _search_runs
    experiment_ids = [int(e) for e in experiment_ids]
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1530: in <listcomp>
    experiment_ids = [int(e) for e in experiment_ids]
E   ValueError: invalid literal for int() with base 10: 'benchmark_experiment_id_123'

The above exception was the direct cause of the following exception:
tests/selection/integration/test_selection_edge_cases.py:199: in test_find_best_model_missing_required_metrics
    result = find_best_model_from_mlflow(
src/evaluation/selection/mlflow_selection.py:554: in find_best_model_from_mlflow
    valid_benchmark_runs = _query_and_filter_benchmark_runs(
src/evaluation/selection/mlflow_selection.py:171: in _query_and_filter_benchmark_runs
    benchmark_runs = query_runs_by_tags(
src/infrastructure/tracking/mlflow/queries.py:66: in query_runs_by_tags
    all_runs = client.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py:3662: in search_runs
    return self._tracking_client.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py:848: in search_runs
    return self.store.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/abstract_store.py:751: in search_runs
    runs, token = self._search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1500: in _search_runs
    with self.ManagedSessionMaker() as session:
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:153: in __exit__
    self.gen.throw(typ, value, traceback)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:177: in make_managed_session
    raise MlflowException(message=e, error_code=INTERNAL_ERROR) from e
E   mlflow.exceptions.MlflowException: invalid literal for int() with base 10: 'benchmark_experiment_id_123'
------------------------------ Captured log call -------------------------------
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:540 Finding best model from MLflow
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:541   Benchmark experiment: test_experiment-benchmark
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:542   HPO experiments: 2
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:543   Objective metric: macro-f1
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:544   Composite weights: F1=0.70, Latency=0.30
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:549   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:169 Querying benchmark runs...
___ TestSelectionConfigEdgeCases.test_find_best_model_empty_required_metrics ___
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:160: in make_managed_session
    yield session
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1530: in _search_runs
    experiment_ids = [int(e) for e in experiment_ids]
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1530: in <listcomp>
    experiment_ids = [int(e) for e in experiment_ids]
E   ValueError: invalid literal for int() with base 10: 'benchmark_experiment_id_123'

The above exception was the direct cause of the following exception:
tests/selection/integration/test_selection_edge_cases.py:229: in test_find_best_model_empty_required_metrics
    result = find_best_model_from_mlflow(
src/evaluation/selection/mlflow_selection.py:554: in find_best_model_from_mlflow
    valid_benchmark_runs = _query_and_filter_benchmark_runs(
src/evaluation/selection/mlflow_selection.py:171: in _query_and_filter_benchmark_runs
    benchmark_runs = query_runs_by_tags(
src/infrastructure/tracking/mlflow/queries.py:66: in query_runs_by_tags
    all_runs = client.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py:3662: in search_runs
    return self._tracking_client.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py:848: in search_runs
    return self.store.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/abstract_store.py:751: in search_runs
    runs, token = self._search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1500: in _search_runs
    with self.ManagedSessionMaker() as session:
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:153: in __exit__
    self.gen.throw(typ, value, traceback)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:177: in make_managed_session
    raise MlflowException(message=e, error_code=INTERNAL_ERROR) from e
E   mlflow.exceptions.MlflowException: invalid literal for int() with base 10: 'benchmark_experiment_id_123'
------------------------------ Captured log call -------------------------------
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:540 Finding best model from MLflow
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:541   Benchmark experiment: test_experiment-benchmark
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:542   HPO experiments: 2
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:543   Objective metric: macro-f1
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:544   Composite weights: F1=0.70, Latency=0.30
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:549   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:169 Querying benchmark runs...
____ TestSelectionWorkflow.test_workflow_loads_config_and_uses_all_options _____
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:160: in make_managed_session
    yield session
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1530: in _search_runs
    experiment_ids = [int(e) for e in experiment_ids]
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1530: in <listcomp>
    experiment_ids = [int(e) for e in experiment_ids]
E   ValueError: invalid literal for int() with base 10: 'benchmark_experiment_id_123'

The above exception was the direct cause of the following exception:
tests/selection/integration/test_selection_workflow.py:71: in test_workflow_loads_config_and_uses_all_options
    result = find_best_model_from_mlflow(
src/evaluation/selection/mlflow_selection.py:554: in find_best_model_from_mlflow
    valid_benchmark_runs = _query_and_filter_benchmark_runs(
src/evaluation/selection/mlflow_selection.py:171: in _query_and_filter_benchmark_runs
    benchmark_runs = query_runs_by_tags(
src/infrastructure/tracking/mlflow/queries.py:66: in query_runs_by_tags
    all_runs = client.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py:3662: in search_runs
    return self._tracking_client.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py:848: in search_runs
    return self.store.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/abstract_store.py:751: in search_runs
    runs, token = self._search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1500: in _search_runs
    with self.ManagedSessionMaker() as session:
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:153: in __exit__
    self.gen.throw(typ, value, traceback)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:177: in make_managed_session
    raise MlflowException(message=e, error_code=INTERNAL_ERROR) from e
E   mlflow.exceptions.MlflowException: invalid literal for int() with base 10: 'benchmark_experiment_id_123'
------------------------------ Captured log call -------------------------------
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:540 Finding best model from MLflow
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:541   Benchmark experiment: test_experiment-benchmark
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:542   HPO experiments: 2
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:543   Objective metric: macro-f1
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:544   Composite weights: F1=0.70, Latency=0.30
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:549   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:169 Querying benchmark runs...
___________ TestSelectionWorkflow.test_workflow_custom_config_values ___________
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:160: in make_managed_session
    yield session
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1530: in _search_runs
    experiment_ids = [int(e) for e in experiment_ids]
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1530: in <listcomp>
    experiment_ids = [int(e) for e in experiment_ids]
E   ValueError: invalid literal for int() with base 10: 'benchmark_experiment_id_123'

The above exception was the direct cause of the following exception:
tests/selection/integration/test_selection_workflow.py:135: in test_workflow_custom_config_values
    result = find_best_model_from_mlflow(
src/evaluation/selection/mlflow_selection.py:554: in find_best_model_from_mlflow
    valid_benchmark_runs = _query_and_filter_benchmark_runs(
src/evaluation/selection/mlflow_selection.py:171: in _query_and_filter_benchmark_runs
    benchmark_runs = query_runs_by_tags(
src/infrastructure/tracking/mlflow/queries.py:66: in query_runs_by_tags
    all_runs = client.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py:3662: in search_runs
    return self._tracking_client.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py:848: in search_runs
    return self.store.search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/abstract_store.py:751: in search_runs
    runs, token = self._search_runs(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:1500: in _search_runs
    with self.ManagedSessionMaker() as session:
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:153: in __exit__
    self.gen.throw(typ, value, traceback)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:177: in make_managed_session
    raise MlflowException(message=e, error_code=INTERNAL_ERROR) from e
E   mlflow.exceptions.MlflowException: invalid literal for int() with base 10: 'benchmark_experiment_id_123'
------------------------------ Captured log call -------------------------------
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:540 Finding best model from MLflow
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:541   Benchmark experiment: test_experiment-benchmark
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:542   HPO experiments: 2
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:543   Objective metric: accuracy
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:544   Composite weights: F1=0.50, Latency=0.50
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:549   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
INFO     evaluation.selection.mlflow_selection:mlflow_selection.py:169 Querying benchmark runs...
________ TestSelectionWorkflow.test_workflow_cache_loading_with_config _________
tests/selection/integration/test_selection_workflow.py:227: in test_workflow_cache_loading_with_config
    with patch("evaluation.selection.cache.MlflowClient") as mock_client_class:
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1447: in __enter__
    original, local = self.get_original()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1420: in get_original
    raise AttributeError(
E   AttributeError: <module 'evaluation.selection.cache' from '/workspaces/resume-ner-azureml/src/evaluation/selection/cache.py'> does not have the attribute 'MlflowClient'
____________ TestColabMounting.test_create_colab_store_default_path ____________
tests/shared/unit/test_drive_backup.py:467: in test_create_colab_store_default_path
    store = create_colab_store(root_dir, config_dir)
src/infrastructure/storage/drive.py:460: in create_colab_store
    paths_config = load_paths_config(config_dir)
src/infrastructure/paths/config.py:83: in load_paths_config
    raise FileNotFoundError(
E   FileNotFoundError: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_create_colab_store_defaul0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.
___ TestBenchmarkTrackingEnabled.test_benchmark_tracking_enabled_creates_run ___
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1967: in _exec_single_context
    self.dialect.do_execute(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py:952: in do_execute
    cursor.execute(statement, parameters)
E   sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:160: in make_managed_session
    yield session
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:586: in create_run
    inputs_list = self._get_run_inputs(session, [run_id])
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:626: in _get_run_inputs
    ).all()
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py:2704: in all
    return self._iter().all()  # type: ignore
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py:2857: in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py:2351: in execute
    return self._execute_internal(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py:2228: in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py:577: in orm_pre_session_exec
    session._autoflush()
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py:3051: in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py:3040: in _autoflush
    self.flush()
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py:4331: in flush
    self._flush(objects)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py:4466: in _flush
    with util.safe_reraise():
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py:224: in __exit__
    raise exc_value.with_traceback(exc_tb)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py:4427: in _flush
    flush_context.execute()
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py:466: in execute
    rec.execute(self)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py:642: in execute
    util.preloaded.orm_persistence.save_obj(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py:93: in save_obj
    _emit_insert_statements(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py:1048: in _emit_insert_statements
    result = connection.execute(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1419: in execute
    return meth(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py:527: in _execute_on_connection
    return connection._execute_clauseelement(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1641: in _execute_clauseelement
    ret = self._execute_context(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1846: in _execute_context
    return self._exec_single_context(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1986: in _exec_single_context
    self._handle_dbapi_exception(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py:2363: in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1967: in _exec_single_context
    self.dialect.do_execute(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py:952: in do_execute
    cursor.execute(statement, parameters)
E   sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
E   (sqlite3.OperationalError) attempt to write a readonly database
E   [SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
E   [parameters: ('74619c8e115c4468a08d999cc9b5b171', 'enchanting-worm-640', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753565863, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/74619c8e115c4468a08d999cc9b5b171/artifacts', '1')]
E   (Background on this error at: https://sqlalche.me/e/20/e3q8)

The above exception was the direct cause of the following exception:
/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/trackers/benchmark_tracker.py:450: in start_benchmark_run
    mlflow.set_tags(tags)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/fluent.py:1404: in set_tags
    run_id = _get_or_start_run().info.run_id
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/fluent.py:3024: in _get_or_start_run
    return start_run()
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/fluent.py:475: in start_run
    active_run_obj = client.create_run(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py:479: in create_run
    return self._tracking_client.create_run(experiment_id, start_time, tags, run_name)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/telemetry/track.py:30: in wrapper
    result = func(*args, **kwargs)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py:183: in create_run
    return self.store.create_run(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:540: in create_run
    with self.ManagedSessionMaker() as session:
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:153: in __exit__
    self.gen.throw(typ, value, traceback)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:171: in make_managed_session
    raise MlflowException(message=e, error_code=TEMPORARILY_UNAVAILABLE) from e
E   mlflow.exceptions.MlflowException: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
E   (sqlite3.OperationalError) attempt to write a readonly database
E   [SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
E   [parameters: ('74619c8e115c4468a08d999cc9b5b171', 'enchanting-worm-640', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753565863, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/74619c8e115c4468a08d999cc9b5b171/artifacts', '1')]
E   (Background on this error at: https://sqlalche.me/e/20/e3q8)

During handling of the above exception, another exception occurred:
/workspaces/resume-ner-azureml/tests/tracking/integration/test_tracking_config_enabled.py:76: in test_benchmark_tracking_enabled_creates_run
    assert handle is not None
E   assert None is not None
------------------------------ Captured log call -------------------------------
WARNING  infrastructure.tracking.mlflow.trackers.base_tracker:base_tracker.py:74 Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753565216, 1768753565216)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
WARNING  infrastructure.tracking.mlflow.trackers.base_tracker:base_tracker.py:75 Continuing without MLflow tracking...
WARNING  infrastructure.tracking.mlflow.trackers.benchmark_tracker:benchmark_tracker.py:386 [START_BENCHMARK_RUN] No valid parent run IDs found. Received: trial=None, refit=None, sweep=None. These may be timestamps or None - will query MLflow if trial_key_hash available.
INFO     infrastructure.tracking.mlflow.trackers.benchmark_tracker:benchmark_tracker.py:399 [START_BENCHMARK_RUN] Building tags: context=None, study_key_hash=None..., trial_key_hash=None..., context.model=None, context.process_type=None
INFO     infrastructure.naming.mlflow.tags_registry:tags_registry.py:221 [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_benchmark_tracking_enable0/config/tags.yaml, using defaults
INFO     infrastructure.tracking.mlflow.trackers.benchmark_tracker:benchmark_tracker.py:436 [START_BENCHMARK_RUN] Built tags, grouping tags present: study_key_hash=False, trial_key_hash=False, code.model=unknown, code.stage=unknown
WARNING  infrastructure.tracking.mlflow.trackers.benchmark_tracker:benchmark_tracker.py:478 MLflow tracking failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('74619c8e115c4468a08d999cc9b5b171', 'enchanting-worm-640', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753565863, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/74619c8e115c4468a08d999cc9b5b171/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
WARNING  infrastructure.tracking.mlflow.trackers.benchmark_tracker:benchmark_tracker.py:479 Continuing benchmarking without MLflow tracking...
____ TestTrainingTrackingEnabled.test_training_tracking_enabled_creates_run ____
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1967: in _exec_single_context
    self.dialect.do_execute(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py:952: in do_execute
    cursor.execute(statement, parameters)
E   sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:160: in make_managed_session
    yield session
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:586: in create_run
    inputs_list = self._get_run_inputs(session, [run_id])
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:626: in _get_run_inputs
    ).all()
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py:2704: in all
    return self._iter().all()  # type: ignore
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py:2857: in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py:2351: in execute
    return self._execute_internal(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py:2228: in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py:577: in orm_pre_session_exec
    session._autoflush()
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py:3051: in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py:3040: in _autoflush
    self.flush()
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py:4331: in flush
    self._flush(objects)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py:4466: in _flush
    with util.safe_reraise():
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py:224: in __exit__
    raise exc_value.with_traceback(exc_tb)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py:4427: in _flush
    flush_context.execute()
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py:466: in execute
    rec.execute(self)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py:642: in execute
    util.preloaded.orm_persistence.save_obj(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py:93: in save_obj
    _emit_insert_statements(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py:1048: in _emit_insert_statements
    result = connection.execute(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1419: in execute
    return meth(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py:527: in _execute_on_connection
    return connection._execute_clauseelement(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1641: in _execute_clauseelement
    ret = self._execute_context(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1846: in _execute_context
    return self._exec_single_context(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1986: in _exec_single_context
    self._handle_dbapi_exception(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py:2363: in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1967: in _exec_single_context
    self.dialect.do_execute(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py:952: in do_execute
    cursor.execute(statement, parameters)
E   sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
E   (sqlite3.OperationalError) attempt to write a readonly database
E   [SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
E   [parameters: ('8b7f6526b9284666ac533096d5b33eb2', 'spiffy-koi-18', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753566778, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/8b7f6526b9284666ac533096d5b33eb2/artifacts', '1')]
E   (Background on this error at: https://sqlalche.me/e/20/e3q8)

The above exception was the direct cause of the following exception:
src/infrastructure/tracking/mlflow/trackers/training_tracker.py:133: in start_training_run
    mlflow.set_tags(tags)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/fluent.py:1404: in set_tags
    run_id = _get_or_start_run().info.run_id
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/fluent.py:3024: in _get_or_start_run
    return start_run()
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/fluent.py:475: in start_run
    active_run_obj = client.create_run(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py:479: in create_run
    return self._tracking_client.create_run(experiment_id, start_time, tags, run_name)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/telemetry/track.py:30: in wrapper
    result = func(*args, **kwargs)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py:183: in create_run
    return self.store.create_run(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:540: in create_run
    with self.ManagedSessionMaker() as session:
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:153: in __exit__
    self.gen.throw(typ, value, traceback)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:171: in make_managed_session
    raise MlflowException(message=e, error_code=TEMPORARILY_UNAVAILABLE) from e
E   mlflow.exceptions.MlflowException: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
E   (sqlite3.OperationalError) attempt to write a readonly database
E   [SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
E   [parameters: ('8b7f6526b9284666ac533096d5b33eb2', 'spiffy-koi-18', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753566778, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/8b7f6526b9284666ac533096d5b33eb2/artifacts', '1')]
E   (Background on this error at: https://sqlalche.me/e/20/e3q8)

During handling of the above exception, another exception occurred:
tests/tracking/integration/test_tracking_config_enabled.py:148: in test_training_tracking_enabled_creates_run
    assert handle is not None
E   assert None is not None
------------------------------ Captured log call -------------------------------
WARNING  infrastructure.tracking.mlflow.trackers.base_tracker:base_tracker.py:74 Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753566767, 1768753566767)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
WARNING  infrastructure.tracking.mlflow.trackers.base_tracker:base_tracker.py:75 Continuing without MLflow tracking...
INFO     infrastructure.naming.mlflow.tags_registry:tags_registry.py:221 [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_training_tracking_enabled0/config/tags.yaml, using defaults
WARNING  infrastructure.tracking.mlflow.trackers.training_tracker:training_tracker.py:197 MLflow tracking failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('8b7f6526b9284666ac533096d5b33eb2', 'spiffy-koi-18', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753566778, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/8b7f6526b9284666ac533096d5b33eb2/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
WARNING  infrastructure.tracking.mlflow.trackers.training_tracker:training_tracker.py:198 Continuing training without MLflow tracking...
__ TestConversionTrackingEnabled.test_conversion_tracking_enabled_creates_run __
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1967: in _exec_single_context
    self.dialect.do_execute(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py:952: in do_execute
    cursor.execute(statement, parameters)
E   sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:160: in make_managed_session
    yield session
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:586: in create_run
    inputs_list = self._get_run_inputs(session, [run_id])
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:626: in _get_run_inputs
    ).all()
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py:2704: in all
    return self._iter().all()  # type: ignore
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py:2857: in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py:2351: in execute
    return self._execute_internal(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py:2228: in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py:577: in orm_pre_session_exec
    session._autoflush()
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py:3051: in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py:3040: in _autoflush
    self.flush()
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py:4331: in flush
    self._flush(objects)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py:4466: in _flush
    with util.safe_reraise():
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py:224: in __exit__
    raise exc_value.with_traceback(exc_tb)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py:4427: in _flush
    flush_context.execute()
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py:466: in execute
    rec.execute(self)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py:642: in execute
    util.preloaded.orm_persistence.save_obj(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py:93: in save_obj
    _emit_insert_statements(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py:1048: in _emit_insert_statements
    result = connection.execute(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1419: in execute
    return meth(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py:527: in _execute_on_connection
    return connection._execute_clauseelement(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1641: in _execute_clauseelement
    ret = self._execute_context(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1846: in _execute_context
    return self._exec_single_context(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1986: in _exec_single_context
    self._handle_dbapi_exception(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py:2363: in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1967: in _exec_single_context
    self.dialect.do_execute(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py:952: in do_execute
    cursor.execute(statement, parameters)
E   sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
E   (sqlite3.OperationalError) attempt to write a readonly database
E   [SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
E   [parameters: ('0c186b9dd7ed475eb5537a95987e5409', 'sincere-snail-576', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753567744, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/0c186b9dd7ed475eb5537a95987e5409/artifacts', '1')]
E   (Background on this error at: https://sqlalche.me/e/20/e3q8)

The above exception was the direct cause of the following exception:
src/infrastructure/tracking/mlflow/trackers/conversion_tracker.py:116: in start_conversion_run
    mlflow.set_tags(tags)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/fluent.py:1404: in set_tags
    run_id = _get_or_start_run().info.run_id
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/fluent.py:3024: in _get_or_start_run
    return start_run()
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/fluent.py:475: in start_run
    active_run_obj = client.create_run(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py:479: in create_run
    return self._tracking_client.create_run(experiment_id, start_time, tags, run_name)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/telemetry/track.py:30: in wrapper
    result = func(*args, **kwargs)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py:183: in create_run
    return self.store.create_run(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:540: in create_run
    with self.ManagedSessionMaker() as session:
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:153: in __exit__
    self.gen.throw(typ, value, traceback)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:171: in make_managed_session
    raise MlflowException(message=e, error_code=TEMPORARILY_UNAVAILABLE) from e
E   mlflow.exceptions.MlflowException: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
E   (sqlite3.OperationalError) attempt to write a readonly database
E   [SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
E   [parameters: ('0c186b9dd7ed475eb5537a95987e5409', 'sincere-snail-576', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753567744, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/0c186b9dd7ed475eb5537a95987e5409/artifacts', '1')]
E   (Background on this error at: https://sqlalche.me/e/20/e3q8)

During handling of the above exception, another exception occurred:
tests/tracking/integration/test_tracking_config_enabled.py:218: in test_conversion_tracking_enabled_creates_run
    assert handle is not None
E   assert None is not None
------------------------------ Captured log call -------------------------------
WARNING  infrastructure.tracking.mlflow.trackers.base_tracker:base_tracker.py:74 Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753567737, 1768753567737)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
WARNING  infrastructure.tracking.mlflow.trackers.base_tracker:base_tracker.py:75 Continuing without MLflow tracking...
INFO     infrastructure.naming.mlflow.tags_registry:tags_registry.py:221 [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_conversion_tracking_enabl0/config/tags.yaml, using defaults
WARNING  infrastructure.tracking.mlflow.trackers.conversion_tracker:conversion_tracker.py:151 MLflow tracking failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('0c186b9dd7ed475eb5537a95987e5409', 'sincere-snail-576', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753567744, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/0c186b9dd7ed475eb5537a95987e5409/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
WARNING  infrastructure.tracking.mlflow.trackers.conversion_tracker:conversion_tracker.py:152 Continuing conversion without MLflow tracking...
__________________________ test_build_output_path_hpo __________________________
src/infrastructure/paths/resolve.py:284: in build_output_path
    paths_config = load_paths_config(config_dir, storage_env=storage_env)
src/infrastructure/paths/config.py:73: in load_paths_config
    if paths_config_path.exists():
/opt/conda/envs/resume-ner-training/lib/python3.10/pathlib.py:1290: in exists
    self.stat()
/opt/conda/envs/resume-ner-training/lib/python3.10/pathlib.py:1097: in stat
    return self._accessor.stat(self, follow_symlinks=follow_symlinks)
E   PermissionError: [Errno 13] Permission denied: '/root/config/paths.yaml'

During handling of the above exception, another exception occurred:
tests/tracking/unit/test_naming_centralized.py:92: in test_build_output_path_hpo
    path = build_output_path(Path("/root"), context)
src/infrastructure/paths/resolve.py:302: in build_output_path
    return _build_output_path_fallback(root_dir, context, base_outputs)
src/infrastructure/paths/resolve.py:174: in _build_output_path_fallback
    raise ValueError(
E   ValueError: HPO v2 requires study_key_hash and trial_key_hash. Got study_key_hash=missing, trial_key_hash=missing
------------------------------ Captured log call -------------------------------
WARNING  infrastructure.paths.resolve:resolve.py:300 Failed to load paths.yaml config: [Errno 13] Permission denied: '/root/config/paths.yaml'. Using fallback logic.
_ TestSweepTrackerConfigInference.test_log_best_trial_id_infers_config_from_output_dir _
tests/tracking/unit/test_sweep_tracker_config_inference.py:87: in test_log_best_trial_id_infers_config_from_output_dir
    tracker._log_best_trial_id(
E   AttributeError: 'MLflowSweepTracker' object has no attribute '_log_best_trial_id'
_ TestSweepTrackerConfigInference.test_log_best_trial_id_finds_config_in_parent_chain _
tests/tracking/unit/test_sweep_tracker_config_inference.py:146: in test_log_best_trial_id_finds_config_in_parent_chain
    tracker._log_best_trial_id(
E   AttributeError: 'MLflowSweepTracker' object has no attribute '_log_best_trial_id'
_ TestSweepTrackerConfigInference.test_log_best_trial_id_falls_back_to_cwd_config_when_not_found _
/workspaces/resume-ner-azureml/tests/tracking/unit/test_sweep_tracker_config_inference.py:199: in test_log_best_trial_id_falls_back_to_cwd_config_when_not_found
    tracker._log_best_trial_id(
E   AttributeError: 'MLflowSweepTracker' object has no attribute '_log_best_trial_id'
___ TestSweepTrackerConfigInference.test_config_dir_not_in_outputs_directory ___
tests/tracking/unit/test_sweep_tracker_config_inference.py:373: in test_config_dir_not_in_outputs_directory
    tracker._log_best_trial_id(
E   AttributeError: 'MLflowSweepTracker' object has no attribute '_log_best_trial_id'
____________________________ test_full_workflow_e2e ____________________________
tests/workflows/test_full_workflow_e2e.py:248: in test_full_workflow_e2e
    assert len(study.trials) > 0
E   AssertionError: assert 0 > 0
E    +  where 0 = len(<MagicMock name='mock.create_study().trials' id='123304070593888'>)
E    +    where <MagicMock name='mock.create_study().trials' id='123304070593888'> = <MagicMock name='mock.create_study()' id='123304074878336'>.trials
------------------------------ Captured log call -------------------------------
INFO     training.hpo.execution.local.sweep_original:sweep_original.py:824 [EARLY HASH] Computed v2 study_key_hash=79a646c77bb15106... for folder creation
INFO     training.hpo.core.study:study.py:366 [HPO] Starting optimization for distilroberta with checkpointing...
INFO     infrastructure.naming.mlflow.config:config.py:362 [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
INFO     infrastructure.naming.mlflow.config:config.py:369 [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
INFO     infrastructure.tracking.mlflow.index.version_counter:version_counter.py:92 [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:7202413b67e3126708d0544fb08f79f5b50640effd450..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
INFO     infrastructure.tracking.mlflow.index.version_counter:version_counter.py:109 [Reserve Version] Loaded 32 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
INFO     infrastructure.tracking.mlflow.index.version_counter:version_counter.py:157 [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0
INFO     infrastructure.tracking.mlflow.index.version_counter:version_counter.py:201 [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)
INFO     infrastructure.tracking.mlflow.index.version_counter:version_counter.py:229 [Reserve Version]  Successfully reserved version 1 for counter_key resume-ner:hpo:7202413b67e3126708d0544fb08f79f5b50... (run_id: pending_2026...)
INFO     training.hpo.execution.local.sweep_original:sweep_original.py:1009 [HPO Setup] k_folds=5, fold_splits_file=None, k_fold config: {'enabled': True, 'n_splits': 5, 'random_seed': 42, 'shuffle': True, 'stratified': True}
INFO     training.hpo.execution.local.sweep_original:sweep_original.py:187 [CV Setup]  Created 5 fold splits and saved to /workspaces/resume-ner-azureml/outputs/hpo/local/distilroberta/study-79a646c7/fold_splits.json
WARNING  infrastructure.tracking.mlflow.trackers.base_tracker:base_tracker.py:74 Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('resume_ner_baseline-hpo-distilroberta', None, 'active', 1768753577472, 1768753577472)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
WARNING  infrastructure.tracking.mlflow.trackers.base_tracker:base_tracker.py:75 Continuing without MLflow tracking...
WARNING  infrastructure.tracking.mlflow.trackers.base_tracker:base_tracker.py:74 Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('resume_ner_baseline-hpo-distilroberta', None, 'active', 1768753577476, 1768753577476)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
WARNING  infrastructure.tracking.mlflow.trackers.base_tracker:base_tracker.py:75 Continuing without MLflow tracking...
WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker_original:sweep_tracker_original.py:1042 Using LOCAL MLflow tracking (not Azure ML)
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker_original:sweep_tracker_original.py:1044 To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
ERROR    infrastructure.tracking.mlflow.trackers.sweep_tracker.sweep_tracker:sweep_tracker.py:179 [START_SWEEP_RUN] MLflow tracking failed: No Experiment with id=908579302991784457 exists
ERROR    infrastructure.tracking.mlflow.trackers.sweep_tracker.sweep_tracker:sweep_tracker.py:180 [START_SWEEP_RUN] Traceback: Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/trackers/sweep_tracker/sweep_tracker.py", line 147, in start_sweep_run
    handle, study_key_hash, study_family_hash = create_mlflow_sweep_run(
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/trackers/sweep_tracker/run_creation.py", line 40, in create_mlflow_sweep_run
    with mlflow.start_run(run_name=run_name) as parent_run:
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/fluent.py", line 475, in start_run
    active_run_obj = client.create_run(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 479, in create_run
    return self._tracking_client.create_run(experiment_id, start_time, tags, run_name)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/telemetry/track.py", line 30, in wrapper
    result = func(*args, **kwargs)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 183, in create_run
    return self.store.create_run(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 541, in create_run
    experiment = self.get_experiment(experiment_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 462, in get_experiment
    return self._get_experiment(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 440, in _get_experiment
    raise MlflowException(
mlflow.exceptions.MlflowException: No Experiment with id=908579302991784457 exists

WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker.sweep_tracker:sweep_tracker.py:182 Continuing HPO without MLflow tracking...
INFO     training.hpo.execution.local.sweep_original:sweep_original.py:1162 Setting Phase 2 tags on parent run None... (data_config=present, train_config=present)
WARNING  training.hpo.execution.local.sweep_original:sweep_original.py:633 _set_phase2_hpo_tags: parent_run_id is empty, skipping
INFO     training.hpo.execution.local.sweep_original:sweep_original.py:1175  Successfully completed Phase 2 tag setting for parent run None...
INFO     training.hpo.tracking.cleanup:cleanup.py:665 [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.
INFO     training.hpo.execution.local.sweep_original:sweep_original.py:1282 [REFIT] Starting refit training for best trial <MagicMock name='mock.create_study().best_trial.number' id='123304023602848'>
WARNING  training.hpo.execution.local.sweep_original:sweep_original.py:1372 [REFIT] Could not determine trial_key_hash from best trial run tags. Refit run may not match trial run.
INFO     training.hpo.execution.local.refit:refit.py:378 [REFIT] Starting refit training for trial <MagicMock name='mock.create_study().best_trial.number' id='123304023602848'> with hyperparameters: <MagicMock name='mock.create_study().best_trial.params' id='123304023545200'>
INFO     training.hpo.execution.local.refit:refit.py:110 [REFIT] Computed trial_id="trial_<MagicMock name='mock.create_study().best_trial.number' id='123304023602848'>_20260118_162617", run_id='20260118_162617', trial_number=<MagicMock name='mock.create_study().best_trial.number' id='123304023602848'>
WARNING  infrastructure.tracking.mlflow.hash_utils:hash_utils.py:517 Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
WARNING  training.hpo.execution.local.sweep_original:sweep_original.py:1471 [REFIT] Refit training failed: Cannot create refit in v2 study folder study-79a646c7 without trial_key_hash. study_key_hash=present, trial_key_hash=missing, refit_params=missing. Hash computation from trial parameters may have failed.. Continuing without refit checkpoint.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep_original.py", line 1411, in run_local_hpo_sweep
    refit_metrics, refit_checkpoint_dir, refit_run_id = run_refit_training(
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 413, in run_refit_training
    refit_output_dir = _build_refit_output_dir(
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 228, in _build_refit_output_dir
    raise RuntimeError(
RuntimeError: Cannot create refit in v2 study folder study-79a646c7 without trial_key_hash. study_key_hash=present, trial_key_hash=missing, refit_params=missing. Hash computation from trial parameters may have failed.
INFO     infrastructure.naming.mlflow.tags_registry:tags_registry.py:221 [Tags Registry] Config file not found at /workspaces/resume-ner-azureml/outputs/hpo/config/tags.yaml, using defaults
WARNING  training.hpo.execution.local.sweep_original:sweep_original.py:1595 [REFIT] No refit_run_id available to mark as FINISHED
_____________ TestNotebookE2E_Core.test_hpo_sweep_execution_mocked _____________
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1447: in __enter__
    original, local = self.get_original()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1420: in get_original
    raise AttributeError(
E   AttributeError: <module 'training.hpo.execution.local.sweep' from '/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep/__init__.py'> does not have the attribute 'mlflow'
=============================== warnings summary ===============================
tests/config/unit/test_fingerprint_placeholder_fallback.py::TestFingerprintPlaceholderFallback::test_placeholder_values_are_short_enough_for_naming
  /workspaces/resume-ner-azureml/tests/config/unit/test_fingerprint_placeholder_fallback.py:128: RuntimeWarning: Fingerprint computation functions not available. Using placeholder fingerprints.
    spec_fp, exec_fp = _compute_fingerprints(

tests/tracking/scripts/test_artifact_upload_manual.py::test_monkey_patch_registration
  /opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/_pytest/python.py:170: PytestReturnNotNoneWarning: Test functions should return None, but tests/tracking/scripts/test_artifact_upload_manual.py::test_monkey_patch_registration returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/tracking/scripts/test_artifact_upload_manual.py::test_artifact_upload_to_child_run
  /opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/_pytest/python.py:170: PytestReturnNotNoneWarning: Test functions should return None, but tests/tracking/scripts/test_artifact_upload_manual.py::test_artifact_upload_to_child_run returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/tracking/scripts/test_artifact_upload_manual.py::test_refit_run_completion
  /opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/_pytest/python.py:170: PytestReturnNotNoneWarning: Test functions should return None, but tests/tracking/scripts/test_artifact_upload_manual.py::test_refit_run_completion returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED tests/hpo/integration/test_early_termination.py::TestPruningBehavior::test_pruner_with_study_manager
FAILED tests/hpo/integration/test_early_termination.py::TestPruningIntegration::test_pruning_with_checkpoint_resume
FAILED tests/hpo/integration/test_error_handling.py::TestRefitExecutionErrors::test_refit_subprocess_failure
FAILED tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointResume::test_resume_from_existing_checkpoint
FAILED tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointResume::test_resume_preserves_trials
FAILED tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointResume::test_resume_marks_running_trials_as_failed
FAILED tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointResume::test_resume_with_auto_resume_false_raises_error
FAILED tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointResume::test_resume_continues_trial_numbering
FAILED tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointSmokeYaml::test_checkpoint_smoke_yaml_auto_resume_true
FAILED tests/hpo/integration/test_hpo_full_workflow.py::TestFullHPOWorkflow::test_full_hpo_workflow_with_cv_and_refit
FAILED tests/hpo/integration/test_hpo_full_workflow.py::TestFullHPOWorkflow::test_full_hpo_workflow_no_cv_no_refit
FAILED tests/hpo/integration/test_hpo_full_workflow.py::TestFullHPOWorkflow::test_full_hpo_workflow_creates_correct_path_structure
FAILED tests/hpo/integration/test_hpo_resume_workflow.py::TestHPOResumeWorkflow::test_resume_workflow_preserves_trials
FAILED tests/hpo/integration/test_hpo_resume_workflow.py::TestHPOResumeWorkflow::test_resume_workflow_with_different_run_id
FAILED tests/hpo/integration/test_hpo_resume_workflow.py::TestHPOResumeWorkflow::test_resume_workflow_with_cv
FAILED tests/hpo/integration/test_hpo_run_mode_variants.py::TestStudyManagerWithRunMode::test_study_manager_passes_run_mode_to_create_study_name
FAILED tests/hpo/integration/test_refit_training.py::TestRefitTrainingSetup::test_refit_uses_best_trial_hyperparameters
FAILED tests/hpo/integration/test_refit_training.py::TestRefitTrainingSetup::test_refit_creates_mlflow_run
FAILED tests/hpo/integration/test_refit_training.py::TestRefitTrainingSetup::test_refit_creates_v2_output_directory
FAILED tests/hpo/integration/test_refit_training.py::TestRefitTrainingExecution::test_refit_reads_metrics_from_file
FAILED tests/hpo/integration/test_refit_training.py::TestRefitTrainingExecution::test_refit_logs_metrics_to_mlflow
FAILED tests/hpo/integration/test_refit_training.py::TestRefitTrainingExecution::test_refit_creates_checkpoint_directory
FAILED tests/hpo/integration/test_refit_training.py::TestRefitTrainingSmokeYaml::test_refit_enabled_in_smoke_yaml
FAILED tests/hpo/integration/test_refit_training.py::TestRefitTrainingSmokeYaml::test_refit_uses_full_epochs
FAILED tests/hpo/integration/test_refit_training.py::TestRefitCheckpointDuplicationPrevention::test_refit_skips_checkpoint_folder_logging
FAILED tests/hpo/integration/test_refit_training.py::TestRefitCheckpointDuplicationPrevention::test_refit_prevents_duplication_only_archive_uploaded
FAILED tests/hpo/integration/test_smoke_yaml_options.py::TestDisableAutoOptunaMark::test_disable_auto_optuna_mark_false_enables_marking
FAILED tests/hpo/integration/test_trial_execution.py::TestTrialExecutionWithCV::test_trial_execution_with_cv_creates_nested_runs
FAILED tests/hpo/integration/test_trial_execution.py::TestTrialExecutionWithCV::test_trial_execution_with_cv_creates_fold_runs
FAILED tests/hpo/integration/test_trial_execution.py::TestTrialExecutionWithCV::test_trial_execution_with_cv_aggregates_metrics
FAILED tests/hpo/integration/test_trial_execution.py::TestTrialExecutionWithCV::test_trial_execution_with_cv_output_paths
FAILED tests/hpo/integration/test_trial_execution.py::TestTrialExecutionWithCV::test_trial_execution_with_cv_smoke_yaml_params
FAILED tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerV2HashComputation::test_v2_hash_computation_success
FAILED tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerV2HashComputation::test_v2_hash_computation_missing_train_config
FAILED tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerV2HashComputation::test_v2_hash_computation_empty_eval_config
FAILED tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerBestTrialSearch::test_best_trial_search_by_study_key_hash_success
FAILED tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerBestTrialSearch::test_best_trial_search_filters_by_parent_run_id
FAILED tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerBestTrialSearch::test_best_trial_search_no_parent_study_key_hash
FAILED tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerBestTrialSearch::test_best_trial_search_no_matching_runs
FAILED tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerBestTrialSearch::test_best_trial_search_exception_handling
FAILED tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerTrialNumberExtraction::test_extract_trial_number_from_tag
FAILED tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerTrialNumberExtraction::test_extract_trial_number_from_run_name
FAILED tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerTrialNumberExtraction::test_extract_trial_number_returns_none_when_not_found
FAILED tests/selection/integration/test_artifact_acquisition_unified_config.py::TestSearchRootsIntegration::test_search_roots_used_in_local_discovery
FAILED tests/selection/integration/test_artifact_acquisition_unified_config.py::TestArtifactKindsPriorityIntegration::test_artifact_kinds_fallback_to_global_priority
FAILED tests/selection/integration/test_artifact_acquisition_unified_config.py::TestConfigOptionCombinations::test_config_with_disabled_sources
FAILED tests/selection/integration/test_best_model_cache.py::test_cache_dual_file_strategy_creates_all_files
FAILED tests/selection/integration/test_best_model_cache.py::test_cache_load_valid_cache_with_mlflow_validation
FAILED tests/selection/integration/test_best_model_cache.py::test_cache_load_cache_key_mismatch_returns_none
FAILED tests/selection/integration/test_best_model_cache.py::test_cache_partial_write_recovery
FAILED tests/selection/integration/test_best_model_cache.py::test_cache_missing_metrics_handling
FAILED tests/selection/integration/test_best_model_selection_config_integration.py::TestLatencyAggregationIntegration::test_latency_aggregation_latest_strategy
FAILED tests/selection/integration/test_best_model_selection_config_integration.py::TestLatencyAggregationIntegration::test_latency_aggregation_median_strategy
FAILED tests/selection/integration/test_best_model_selection_config_integration.py::TestLatencyAggregationIntegration::test_latency_aggregation_mean_strategy
FAILED tests/selection/integration/test_best_model_selection_config_integration.py::TestScoringConfigIntegration::test_scoring_weights_used_in_composite_score
FAILED tests/selection/integration/test_best_model_selection_config_integration.py::TestScoringConfigIntegration::test_normalize_weights_controls_normalization
FAILED tests/selection/integration/test_best_model_selection_config_integration.py::TestBenchmarkRequiredMetricsIntegration::test_required_metrics_filters_benchmark_runs
FAILED tests/selection/integration/test_best_model_selection_config_integration.py::TestCompleteConfigWorkflow::test_all_config_options_used_together
FAILED tests/selection/integration/test_mlflow_selection_config.py::TestMLflowSelectionConfigUsage::test_find_best_model_uses_objective_metric
FAILED tests/selection/integration/test_mlflow_selection_config.py::TestMLflowSelectionConfigUsage::test_find_best_model_uses_scoring_weights
FAILED tests/selection/integration/test_mlflow_selection_config.py::TestMLflowSelectionConfigUsage::test_find_best_model_uses_benchmark_required_metrics
FAILED tests/selection/integration/test_mlflow_selection_config.py::TestMLflowSelectionConfigUsage::test_find_best_model_weight_normalization
FAILED tests/selection/integration/test_mlflow_selection_config.py::TestMLflowSelectionConfigUsage::test_find_best_model_composite_score_calculation
FAILED tests/selection/integration/test_mlflow_selection_config.py::TestMLflowSelectionConfigUsage::test_find_best_model_all_config_options_together
FAILED tests/selection/integration/test_mlflow_selection_config.py::TestMLflowSelectionConfigUsage::test_find_best_model_custom_config_values
FAILED tests/selection/integration/test_selection_cache_config.py::TestSelectionCacheConfig::test_load_cached_best_model_validates_cache_key
FAILED tests/selection/integration/test_selection_cache_config.py::TestSelectionCacheConfig::test_run_mode_reuse_if_exists_behavior
FAILED tests/selection/integration/test_selection_edge_cases.py::TestSelectionConfigEdgeCases::test_find_best_model_missing_required_metrics
FAILED tests/selection/integration/test_selection_edge_cases.py::TestSelectionConfigEdgeCases::test_find_best_model_empty_required_metrics
FAILED tests/selection/integration/test_selection_workflow.py::TestSelectionWorkflow::test_workflow_loads_config_and_uses_all_options
FAILED tests/selection/integration/test_selection_workflow.py::TestSelectionWorkflow::test_workflow_custom_config_values
FAILED tests/selection/integration/test_selection_workflow.py::TestSelectionWorkflow::test_workflow_cache_loading_with_config
FAILED tests/shared/unit/test_drive_backup.py::TestColabMounting::test_create_colab_store_default_path
FAILED tests/tracking/integration/test_tracking_config_enabled.py::TestBenchmarkTrackingEnabled::test_benchmark_tracking_enabled_creates_run
FAILED tests/tracking/integration/test_tracking_config_enabled.py::TestTrainingTrackingEnabled::test_training_tracking_enabled_creates_run
FAILED tests/tracking/integration/test_tracking_config_enabled.py::TestConversionTrackingEnabled::test_conversion_tracking_enabled_creates_run
FAILED tests/tracking/unit/test_naming_centralized.py::test_build_output_path_hpo
FAILED tests/tracking/unit/test_sweep_tracker_config_inference.py::TestSweepTrackerConfigInference::test_log_best_trial_id_infers_config_from_output_dir
FAILED tests/tracking/unit/test_sweep_tracker_config_inference.py::TestSweepTrackerConfigInference::test_log_best_trial_id_finds_config_in_parent_chain
FAILED tests/tracking/unit/test_sweep_tracker_config_inference.py::TestSweepTrackerConfigInference::test_log_best_trial_id_falls_back_to_cwd_config_when_not_found
FAILED tests/tracking/unit/test_sweep_tracker_config_inference.py::TestSweepTrackerConfigInference::test_config_dir_not_in_outputs_directory
FAILED tests/workflows/test_full_workflow_e2e.py::test_full_workflow_e2e - As...
FAILED tests/workflows/test_notebook_01_e2e.py::TestNotebookE2E_Core::test_hpo_sweep_execution_mocked
=========== 83 failed, 1273 passed, 53 skipped, 4 warnings in 32.61s ===========

============================================================
[INFO] Pytest log file: /workspaces/resume-ner-azureml/outputs/pytest_logs/pytest_20260118_162545.log
============================================================

2026-01-18 16:25:47,354 - infrastructure.tracking.mlflow.compatibility - INFO - Applied Azure ML artifact compatibility patch
2026-01-18 16:25:49,709 - evaluation.benchmarking.orchestrator_original - INFO - Skipping benchmarking (test data not available)
2026-01-18 16:25:49,714 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_handles_1/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_handles_1/checkpoint --test-data /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_handles_1/test.json --batch-sizes 1 8 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_handles_1/benchmark.json
2026-01-18 16:25:49,714 - evaluation.benchmarking.utils - ERROR - Benchmarking failed with return code 1
2026-01-18 16:25:49,718 - evaluation.benchmarking.orchestrator_original - INFO - Benchmarking distilbert (trial-25d03eeb)...
2026-01-18 16:25:49,726 - evaluation.benchmarking.orchestrator_original - WARNING - [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=xyz789uvw012...
2026-01-18 16:25:49,727 - evaluation.benchmarking.orchestrator_original - INFO - [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 16:25:49,727 - evaluation.benchmarking.orchestrator_original - INFO - Benchmark completed: /tmp/pytest-of-codespace/pytest-120/test_benchmark_passes_trial_id0/outputs/benchmarking/test/benchmark.json
2026-01-18 16:25:49,728 - evaluation.benchmarking.orchestrator_original - INFO - Benchmarking complete. 1/1 trials benchmarked.
2026-01-18 16:25:49,731 - evaluation.benchmarking.orchestrator_original - INFO - Benchmarking distilbert (trial_1_20251231_161745)...
2026-01-18 16:25:49,733 - evaluation.benchmarking.orchestrator_original - WARNING - [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=xyz789uvw012...
2026-01-18 16:25:49,734 - evaluation.benchmarking.orchestrator_original - INFO - [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 16:25:49,734 - evaluation.benchmarking.orchestrator_original - INFO - Benchmark completed: /tmp/pytest-of-codespace/pytest-120/test_benchmark_passes_trial_id1/outputs/benchmarking/test/benchmark.json
2026-01-18 16:25:49,734 - evaluation.benchmarking.orchestrator_original - INFO - Benchmarking complete. 1/1 trials benchmarked.
2026-01-18 16:25:49,738 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_mlflow_t0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_mlflow_t0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_mlflow_t0/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_mlflow_t0/benchmark.json
2026-01-18 16:25:49,738 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Building run name: trial_id=trial-25d03eeb, root_dir=/tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_mlflow_t0, config_dir=/tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_mlflow_t0/config
2026-01-18 16:25:49,738 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Generated run name: local_distilbert_benchmark_trial-25d03eeb
2026-01-18 16:25:49,742 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_mlflow_t1/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_mlflow_t1/checkpoint --test-data /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_mlflow_t1/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_mlflow_t1/benchmark.json
2026-01-18 16:25:49,742 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Constructed trial_id from trial_key_hash: trial-25d03eeb
2026-01-18 16:25:49,743 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Building run name: trial_id=trial-25d03eeb, root_dir=/tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_mlflow_t1, config_dir=/tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_mlflow_t1/config
2026-01-18 16:25:49,743 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Generated run name: local_distilbert_benchmark_trial-25d03eeb
2026-01-18 16:25:49,747 - evaluation.benchmarking.orchestrator_original - INFO - Benchmarking distilbert (trial_0)...
2026-01-18 16:25:49,748 - evaluation.benchmarking.orchestrator_original - WARNING - [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 16:25:49,749 - evaluation.benchmarking.orchestrator_original - INFO - [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 16:25:49,749 - evaluation.benchmarking.orchestrator_original - INFO - Benchmark completed: /tmp/pytest-of-codespace/pytest-120/test_benchmark_best_trials_use0/outputs/benchmarking/test/benchmark.json
2026-01-18 16:25:49,750 - evaluation.benchmarking.orchestrator_original - INFO - Benchmarking complete. 1/1 trials benchmarked.
2026-01-18 16:25:49,753 - evaluation.benchmarking.orchestrator_original - INFO - Benchmarking distilbert (trial_0)...
2026-01-18 16:25:49,755 - evaluation.benchmarking.orchestrator_original - WARNING - [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 16:25:49,756 - evaluation.benchmarking.orchestrator_original - INFO - [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 16:25:49,756 - evaluation.benchmarking.orchestrator_original - INFO - Benchmark completed: /tmp/pytest-of-codespace/pytest-120/test_benchmark_best_trials_use1/outputs/benchmarking/test/benchmark.json
2026-01-18 16:25:49,756 - evaluation.benchmarking.orchestrator_original - INFO - Benchmarking complete. 1/1 trials benchmarked.
2026-01-18 16:25:49,760 - evaluation.benchmarking.orchestrator_original - INFO - Benchmarking distilbert (trial_0)...
2026-01-18 16:25:49,762 - evaluation.benchmarking.orchestrator_original - WARNING - [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 16:25:49,764 - evaluation.benchmarking.orchestrator_original - INFO - [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 16:25:49,764 - evaluation.benchmarking.orchestrator_original - INFO - Benchmark completed: /tmp/pytest-of-codespace/pytest-120/test_benchmark_best_trials_use2/outputs/benchmarking/test/benchmark.json
2026-01-18 16:25:49,764 - evaluation.benchmarking.orchestrator_original - INFO - Benchmarking complete. 1/1 trials benchmarked.
2026-01-18 16:25:49,769 - evaluation.benchmarking.orchestrator_original - INFO - Benchmarking distilbert (trial_0)...
2026-01-18 16:25:49,770 - evaluation.benchmarking.orchestrator_original - WARNING - [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 16:25:49,771 - evaluation.benchmarking.orchestrator_original - INFO - [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 16:25:49,771 - evaluation.benchmarking.orchestrator_original - INFO - Benchmark completed: /tmp/pytest-of-codespace/pytest-120/test_benchmark_best_trials_use3/outputs/benchmarking/test/benchmark.json
2026-01-18 16:25:49,772 - evaluation.benchmarking.orchestrator_original - INFO - Benchmarking complete. 1/1 trials benchmarked.
2026-01-18 16:25:49,776 - evaluation.benchmarking.orchestrator_original - INFO - Benchmarking distilbert (trial_0)...
2026-01-18 16:25:49,777 - evaluation.benchmarking.orchestrator_original - WARNING - [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 16:25:49,778 - evaluation.benchmarking.orchestrator_original - INFO - [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 16:25:49,779 - evaluation.benchmarking.orchestrator_original - INFO - Benchmark completed: /tmp/pytest-of-codespace/pytest-120/test_benchmark_best_trials_use4/outputs/benchmarking/test/custom_benchmark.json
2026-01-18 16:25:49,779 - evaluation.benchmarking.orchestrator_original - INFO - Benchmarking complete. 1/1 trials benchmarked.
2026-01-18 16:25:49,783 - evaluation.benchmarking.orchestrator_original - INFO - Benchmarking distilbert (trial_0)...
2026-01-18 16:25:49,784 - evaluation.benchmarking.orchestrator_original - WARNING - [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 16:25:49,785 - evaluation.benchmarking.orchestrator_original - INFO - [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 16:25:49,785 - evaluation.benchmarking.orchestrator_original - INFO - Benchmark completed: /tmp/pytest-of-codespace/pytest-120/test_benchmark_best_trials_use5/outputs/benchmarking/test/benchmark.json
2026-01-18 16:25:49,785 - evaluation.benchmarking.orchestrator_original - INFO - Benchmarking complete. 1/1 trials benchmarked.
2026-01-18 16:25:49,789 - evaluation.benchmarking.orchestrator_original - INFO - Benchmarking distilbert (trial_0)...
2026-01-18 16:25:49,791 - evaluation.benchmarking.orchestrator_original - WARNING - [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 16:25:49,792 - evaluation.benchmarking.orchestrator_original - INFO - [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 16:25:49,792 - evaluation.benchmarking.orchestrator_original - INFO - Benchmark completed: /tmp/pytest-of-codespace/pytest-120/test_benchmark_best_trials_use6/outputs/benchmarking/test/custom_benchmark.json
2026-01-18 16:25:49,792 - evaluation.benchmarking.orchestrator_original - INFO - Benchmarking complete. 1/1 trials benchmarked.
2026-01-18 16:25:49,796 - evaluation.benchmarking.orchestrator_original - INFO - Benchmarking distilbert (trial_0)...
2026-01-18 16:25:49,797 - evaluation.benchmarking.orchestrator_original - WARNING - [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 16:25:49,798 - evaluation.benchmarking.orchestrator_original - INFO - [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 16:25:49,798 - evaluation.benchmarking.orchestrator_original - INFO - Benchmark completed: /tmp/pytest-of-codespace/pytest-120/test_benchmark_best_trials_all0/outputs/benchmarking/test/custom_benchmark.json
2026-01-18 16:25:49,798 - evaluation.benchmarking.orchestrator_original - INFO - Benchmarking complete. 1/1 trials benchmarked.
2026-01-18 16:25:49,801 - evaluation.benchmarking.orchestrator_original - INFO - Benchmarking distilbert (trial_0)...
2026-01-18 16:25:49,803 - evaluation.benchmarking.orchestrator_original - WARNING - [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 16:25:49,803 - evaluation.benchmarking.orchestrator_original - INFO - [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 16:25:49,804 - evaluation.benchmarking.orchestrator_original - ERROR - Benchmark failed for distilbert
2026-01-18 16:25:49,804 - evaluation.benchmarking.orchestrator_original - INFO - Benchmarking complete. 0/1 trials benchmarked.
2026-01-18 16:25:49,806 - evaluation.benchmarking.filter - INFO - Run mode is 'force_new' - skipping idempotency check, will benchmark all champions
2026-01-18 16:25:49,809 - evaluation.benchmarking.filter - INFO - Skipping distilbert - benchmark already exists (trial_key_hash=trial_hash_456...)
2026-01-18 16:25:49,820 - evaluation.benchmarking.existence_checker - WARNING - Found 1 finished benchmark run(s) by hash (fallback). Consider setting benchmark_key tag for more reliable idempotency. trial_key_hash=trial_hash_456...
2026-01-18 16:25:49,825 - evaluation.benchmarking.existence_checker - WARNING - Found 1 finished benchmark run(s) by hash (fallback). Consider setting benchmark_key tag for more reliable idempotency. trial_key_hash=trial_hash_456...
2026-01-18 16:25:49,828 - evaluation.benchmarking.filter - INFO - Skipping distilbert - benchmark already exists (trial_key_hash=trial_hash_456...)
2026-01-18 16:25:49,832 - evaluation.benchmarking.filter - INFO - Run mode is 'force_new' - skipping idempotency check, will benchmark all champions
2026-01-18 16:25:49,837 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_bat0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_bat0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_bat0/test.json --batch-sizes 1 8 16 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_bat0/benchmark.json
2026-01-18 16:25:49,840 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_ite0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_ite0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_ite0/test.json --batch-sizes 1 8 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_ite0/benchmark.json
2026-01-18 16:25:49,842 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_war0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_war0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_war0/test.json --batch-sizes 1 8 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_war0/benchmark.json
2026-01-18 16:25:49,845 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_max0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_max0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_max0/test.json --batch-sizes 1 8 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_max0/benchmark.json
2026-01-18 16:25:49,848 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_dev0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_dev0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_dev0/test.json --batch-sizes 1 8 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_dev0/benchmark.json --device cuda
2026-01-18 16:25:49,850 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_skips_de0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_skips_de0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_skips_de0/test.json --batch-sizes 1 8 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_skips_de0/benchmark.json
2026-01-18 16:25:49,853 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_out0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_out0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_out0/test.json --batch-sizes 1 8 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_uses_out0/benchmark.json
2026-01-18 16:25:49,855 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_all_conf0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_all_conf0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_all_conf0/test.json --batch-sizes 2 4 32 --iterations 200 --warmup 20 --max-length 256 --output /tmp/pytest-of-codespace/pytest-120/test_run_benchmarking_all_conf0/custom_benchmark.json --device cuda
2026-01-18 16:25:49,862 - evaluation.benchmarking.orchestrator_original - INFO - Benchmarking distilbert (trial_0)...
2026-01-18 16:25:49,864 - evaluation.benchmarking.orchestrator_original - WARNING - [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 16:25:49,865 - evaluation.benchmarking.orchestrator_original - INFO - [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 16:25:49,865 - evaluation.benchmarking.orchestrator_original - INFO - Benchmark completed: /tmp/pytest-of-codespace/pytest-120/test_workflow_loads_config_and0/outputs/benchmarking/test/benchmark.json
2026-01-18 16:25:49,865 - evaluation.benchmarking.orchestrator_original - INFO - Benchmarking complete. 1/1 trials benchmarked.
2026-01-18 16:25:49,869 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-120/test_workflow_custom_config_va0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-120/test_workflow_custom_config_va0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-120/test_workflow_custom_config_va0/test.json --batch-sizes 2 4 32 --iterations 200 --warmup 20 --max-length 256 --output /tmp/pytest-of-codespace/pytest-120/test_workflow_custom_config_va0/custom_benchmark.json --device cuda
2026-01-18 16:25:49,874 - evaluation.benchmarking.orchestrator_original - INFO - Benchmarking distilbert (trial_0)...
2026-01-18 16:25:49,876 - evaluation.benchmarking.orchestrator_original - WARNING - [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 16:25:49,877 - evaluation.benchmarking.orchestrator_original - INFO - [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 16:25:49,877 - evaluation.benchmarking.orchestrator_original - INFO - Benchmark completed: /tmp/pytest-of-codespace/pytest-120/test_workflow_defaults_when_co0/outputs/benchmarking/test/benchmark.json
2026-01-18 16:25:49,877 - evaluation.benchmarking.orchestrator_original - INFO - Benchmarking complete. 1/1 trials benchmarked.
2026-01-18 16:25:49,923 - evaluation.benchmarking.filter - INFO - Run mode is 'force_new' - skipping idempotency check, will benchmark all champions
2026-01-18 16:25:49,925 - evaluation.benchmarking.filter - INFO - Skipping distilbert - benchmark already exists (trial_key_hash=trial_hash_456...)
2026-01-18 16:25:49,925 - evaluation.benchmarking.filter - INFO - Skipping deberta - benchmark already exists (trial_key_hash=trial_hash_789...)
2026-01-18 16:25:49,936 - evaluation.benchmarking.filter - INFO - Skipping distilbert - benchmark already exists (trial_key_hash=trial_hash_456...)
2026-01-18 16:25:49,938 - evaluation.benchmarking.filter - INFO - Skipping distilbert - benchmark already exists (trial_key_hash=trial_hash_456...)
2026-01-18 16:25:49,942 - evaluation.benchmarking.filter - WARNING - No run_id found for champion distilbert, skipping idempotency check
2026-01-18 16:25:49,945 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_failure_mlflow_client_una0/config/tags.yaml, using defaults
2026-01-18 16:25:49,955 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-120/test_trial_id_from_parameter_o0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-120/test_trial_id_from_parameter_o0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-120/test_trial_id_from_parameter_o0/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-120/test_trial_id_from_parameter_o0/benchmark.json
2026-01-18 16:25:49,955 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Building run name: trial_id=1_20251231_161745, root_dir=/tmp/pytest-of-codespace/pytest-120/test_trial_id_from_parameter_o0, config_dir=/tmp/pytest-of-codespace/pytest-120/test_trial_id_from_parameter_o0/config
2026-01-18 16:25:49,956 - evaluation.benchmarking.utils - WARNING - Could not log benchmark results to MLflow: Naming policy not available or formatting failed. Legacy fallback removed. Process type: <Mock name='create_naming_context().process_type' id='123304183202432'>
2026-01-18 16:25:49,959 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-120/test_trial_id_from_parameter_n0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-120/test_trial_id_from_parameter_n0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-120/test_trial_id_from_parameter_n0/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-120/test_trial_id_from_parameter_n0/benchmark.json
2026-01-18 16:25:49,959 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Building run name: trial_id=trial-25d03eeb, root_dir=/tmp/pytest-of-codespace/pytest-120/test_trial_id_from_parameter_n0, config_dir=/tmp/pytest-of-codespace/pytest-120/test_trial_id_from_parameter_n0/config
2026-01-18 16:25:49,960 - evaluation.benchmarking.utils - WARNING - Could not log benchmark results to MLflow: Naming policy not available or formatting failed. Legacy fallback removed. Process type: <Mock name='create_naming_context().process_type' id='123304183352960'>
2026-01-18 16:25:49,963 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-120/test_trial_id_extraction_from_0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-120/test_trial_id_extraction_from_0/hpo/local/distilbert/study-abc123/trial_1_20251231_161745/checkpoint --test-data /tmp/pytest-of-codespace/pytest-120/test_trial_id_extraction_from_0/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-120/test_trial_id_extraction_from_0/benchmark.json
2026-01-18 16:25:49,964 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Extracted trial_id from checkpoint path: trial_1_20251231_161745 (at level 1)
2026-01-18 16:25:49,964 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Building run name: trial_id=trial_1_20251231_161745, root_dir=/tmp/pytest-of-codespace/pytest-120/test_trial_id_extraction_from_0, config_dir=/tmp/pytest-of-codespace/pytest-120/test_trial_id_extraction_from_0/config
2026-01-18 16:25:49,964 - evaluation.benchmarking.utils - WARNING - Could not log benchmark results to MLflow: Naming policy not available or formatting failed. Legacy fallback removed. Process type: <Mock name='create_naming_context().process_type' id='123304183329840'>
2026-01-18 16:25:49,968 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-120/test_trial_id_extraction_from_1/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-120/test_trial_id_extraction_from_1/hpo/local/distilbert/study-abc123/trial-25d03eeb/checkpoint --test-data /tmp/pytest-of-codespace/pytest-120/test_trial_id_extraction_from_1/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-120/test_trial_id_extraction_from_1/benchmark.json
2026-01-18 16:25:49,968 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Extracted trial_id from checkpoint path: trial-25d03eeb (at level 1)
2026-01-18 16:25:49,968 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Building run name: trial_id=trial-25d03eeb, root_dir=/tmp/pytest-of-codespace/pytest-120/test_trial_id_extraction_from_1, config_dir=/tmp/pytest-of-codespace/pytest-120/test_trial_id_extraction_from_1/config
2026-01-18 16:25:49,968 - evaluation.benchmarking.utils - WARNING - Could not log benchmark results to MLflow: Naming policy not available or formatting failed. Legacy fallback removed. Process type: <Mock name='create_naming_context().process_type' id='123304183386496'>
2026-01-18 16:25:49,972 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-120/test_trial_id_extraction_from_2/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-120/test_trial_id_extraction_from_2/hpo/local/distilbert/study-abc123/trial-25d03eeb/refit/checkpoint --test-data /tmp/pytest-of-codespace/pytest-120/test_trial_id_extraction_from_2/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-120/test_trial_id_extraction_from_2/benchmark.json
2026-01-18 16:25:49,972 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Extracted trial_id from checkpoint path: trial-25d03eeb (at level 2)
2026-01-18 16:25:49,973 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Building run name: trial_id=trial-25d03eeb, root_dir=/tmp/pytest-of-codespace/pytest-120/test_trial_id_extraction_from_2, config_dir=/tmp/pytest-of-codespace/pytest-120/test_trial_id_extraction_from_2/config
2026-01-18 16:25:49,973 - evaluation.benchmarking.utils - WARNING - Could not log benchmark results to MLflow: Naming policy not available or formatting failed. Legacy fallback removed. Process type: <Mock name='create_naming_context().process_type' id='123304266146064'>
2026-01-18 16:25:49,976 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-120/test_trial_id_fallback_to_tria0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-120/test_trial_id_fallback_to_tria0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-120/test_trial_id_fallback_to_tria0/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-120/test_trial_id_fallback_to_tria0/benchmark.json
2026-01-18 16:25:49,977 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Constructed trial_id from trial_key_hash: trial-25d03eeb
2026-01-18 16:25:49,977 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Building run name: trial_id=trial-25d03eeb, root_dir=/tmp/pytest-of-codespace/pytest-120/test_trial_id_fallback_to_tria0, config_dir=/tmp/pytest-of-codespace/pytest-120/test_trial_id_fallback_to_tria0/config
2026-01-18 16:25:49,977 - evaluation.benchmarking.utils - WARNING - Could not log benchmark results to MLflow: Naming policy not available or formatting failed. Legacy fallback removed. Process type: <Mock name='create_naming_context().process_type' id='123304181559360'>
2026-01-18 16:25:49,981 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-120/test_trial_id_parameter_overri0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-120/test_trial_id_parameter_overri0/hpo/local/distilbert/study-abc123/trial-25d03eeb/checkpoint --test-data /tmp/pytest-of-codespace/pytest-120/test_trial_id_parameter_overri0/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-120/test_trial_id_parameter_overri0/benchmark.json
2026-01-18 16:25:49,982 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Building run name: trial_id=trial-custom123, root_dir=/tmp/pytest-of-codespace/pytest-120/test_trial_id_parameter_overri0, config_dir=/tmp/pytest-of-codespace/pytest-120/test_trial_id_parameter_overri0/config
2026-01-18 16:25:49,982 - evaluation.benchmarking.utils - WARNING - Could not log benchmark results to MLflow: Naming policy not available or formatting failed. Legacy fallback removed. Process type: <Mock name='create_naming_context().process_type' id='123304182914624'>
2026-01-18 16:25:49,989 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_hpo_trial_workflow0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 16:25:49,990 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 16:25:49,990 - infrastructure.tracking.mlflow.index.version_counter - INFO - [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:e0fbae020bc656b7e6425344765f3df149e5f6831d2b7..., root_dir=/tmp/pytest-of-codespace/pytest-120/test_hpo_trial_workflow0, config_dir=/tmp/pytest-of-codespace/pytest-120/test_hpo_trial_workflow0/config, counter_path=/tmp/pytest-of-codespace/pytest-120/test_hpo_trial_workflow0/outputs/cache/run_name_counter.json
2026-01-18 16:25:49,990 - infrastructure.tracking.mlflow.index.version_counter - INFO - [Reserve Version] Loaded 0 existing allocations from /tmp/pytest-of-codespace/pytest-120/test_hpo_trial_workflow0/outputs/cache/run_name_counter.json
2026-01-18 16:25:49,990 - infrastructure.tracking.mlflow.index.version_counter - INFO - [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0
2026-01-18 16:25:49,990 - infrastructure.tracking.mlflow.index.version_counter - INFO - [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)
2026-01-18 16:25:49,991 - infrastructure.tracking.mlflow.index.version_counter - INFO - [Reserve Version]  Successfully reserved version 1 for counter_key resume-ner:hpo:e0fbae020bc656b7e6425344765f3df149e... (run_id: pending_2026...)
2026-01-18 16:25:49,998 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_final_training_workflow0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 16:25:49,998 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=final_training
2026-01-18 16:25:50,006 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_benchmarking_workflow0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 16:25:50,006 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=benchmarking
2026-01-18 16:25:50,013 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_paths_match_naming_patter0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 16:25:50,013 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 16:25:50,014 - infrastructure.tracking.mlflow.index.version_counter - INFO - [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:fa924e9d2ac5675b719f2281bdcff80badd52b78fb7ef..., root_dir=/tmp/pytest-of-codespace/pytest-120/test_paths_match_naming_patter0, config_dir=/tmp/pytest-of-codespace/pytest-120/test_paths_match_naming_patter0/config, counter_path=/tmp/pytest-of-codespace/pytest-120/test_paths_match_naming_patter0/outputs/cache/run_name_counter.json
2026-01-18 16:25:50,014 - infrastructure.tracking.mlflow.index.version_counter - INFO - [Reserve Version] Loaded 0 existing allocations from /tmp/pytest-of-codespace/pytest-120/test_paths_match_naming_patter0/outputs/cache/run_name_counter.json
2026-01-18 16:25:50,014 - infrastructure.tracking.mlflow.index.version_counter - INFO - [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0
2026-01-18 16:25:50,014 - infrastructure.tracking.mlflow.index.version_counter - INFO - [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)
2026-01-18 16:25:50,014 - infrastructure.tracking.mlflow.index.version_counter - INFO - [Reserve Version]  Successfully reserved version 1 for counter_key resume-ner:hpo:fa924e9d2ac5675b719f2281bdcff80badd... (run_id: pending_2026...)
2026-01-18 16:25:50,024 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_naming_patterns_from_nami0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 16:25:50,024 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 16:25:50,024 - infrastructure.tracking.mlflow.index.version_counter - INFO - [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:e0fbae020bc656b7e6425344765f3df149e5f6831d2b7..., root_dir=/tmp/pytest-of-codespace/pytest-120/test_naming_patterns_from_nami0, config_dir=/tmp/pytest-of-codespace/pytest-120/test_naming_patterns_from_nami0/config, counter_path=/tmp/pytest-of-codespace/pytest-120/test_naming_patterns_from_nami0/outputs/cache/run_name_counter.json
2026-01-18 16:25:50,024 - infrastructure.tracking.mlflow.index.version_counter - INFO - [Reserve Version] Loaded 0 existing allocations from /tmp/pytest-of-codespace/pytest-120/test_naming_patterns_from_nami0/outputs/cache/run_name_counter.json
2026-01-18 16:25:50,024 - infrastructure.tracking.mlflow.index.version_counter - INFO - [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0
2026-01-18 16:25:50,025 - infrastructure.tracking.mlflow.index.version_counter - INFO - [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)
2026-01-18 16:25:50,025 - infrastructure.tracking.mlflow.index.version_counter - INFO - [Reserve Version]  Successfully reserved version 1 for counter_key resume-ner:hpo:e0fbae020bc656b7e6425344765f3df149e... (run_id: pending_2026...)
2026-01-18 16:25:50,038 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_naming_conventions_consis0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 16:25:50,038 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 16:25:50,038 - infrastructure.tracking.mlflow.index.version_counter - INFO - [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:e0fbae020bc656b7e6425344765f3df149e5f6831d2b7..., root_dir=/tmp/pytest-of-codespace/pytest-120/test_naming_conventions_consis0, config_dir=/tmp/pytest-of-codespace/pytest-120/test_naming_conventions_consis0/config, counter_path=/tmp/pytest-of-codespace/pytest-120/test_naming_conventions_consis0/outputs/cache/run_name_counter.json
2026-01-18 16:25:50,039 - infrastructure.tracking.mlflow.index.version_counter - INFO - [Reserve Version] Loaded 0 existing allocations from /tmp/pytest-of-codespace/pytest-120/test_naming_conventions_consis0/outputs/cache/run_name_counter.json
2026-01-18 16:25:50,039 - infrastructure.tracking.mlflow.index.version_counter - INFO - [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0
2026-01-18 16:25:50,039 - infrastructure.tracking.mlflow.index.version_counter - INFO - [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)
2026-01-18 16:25:50,039 - infrastructure.tracking.mlflow.index.version_counter - INFO - [Reserve Version]  Successfully reserved version 1 for counter_key resume-ner:hpo:e0fbae020bc656b7e6425344765f3df149e... (run_id: pending_2026...)
2026-01-18 16:25:50,265 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_naming_run_name_auto_incr0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 16:25:50,266 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 16:25:50,269 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_naming_run_name_auto_incr1/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 16:25:50,269 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 16:25:50,272 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_naming_run_name_auto_incr2/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 16:25:50,272 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=benchmarking
2026-01-18 16:25:50,274 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_naming_run_name_auto_incr3/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 16:25:50,275 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 16:25:50,653 - deployment.conversion.orchestration - INFO - Output directory: /tmp/pytest-of-codespace/pytest-120/test_opset_version_passed_to_s0/outputs/conversion/test
2026-01-18 16:25:50,657 - deployment.conversion.orchestration - INFO - Created MLflow run: test_run_name (test_run_id...)
2026-01-18 16:25:50,657 - deployment.conversion.orchestration - INFO - Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /path/to/checkpoint --config-dir /tmp/pytest-of-codespace/pytest-120/test_opset_version_passed_to_s0/config --backbone distilbert-base-uncased --output-dir /tmp/pytest-of-codespace/pytest-120/test_opset_version_passed_to_s0/outputs/conversion/test --opset-version 19 --run-smoke-test
2026-01-18 16:25:50,658 - infrastructure.tracking.mlflow.lifecycle - INFO - Run test_run_id... already terminated with status <Mock name='create_mlflow_client().get_run().info.status' id='123304180903712'> (expected FINISHED)
2026-01-18 16:25:50,658 - deployment.conversion.orchestration - INFO - Conversion completed. ONNX model: /tmp/pytest-of-codespace/pytest-120/test_opset_version_passed_to_s0/outputs/conversion/test/model.onnx
2026-01-18 16:25:50,664 - deployment.conversion.orchestration - INFO - Output directory: /tmp/pytest-of-codespace/pytest-120/test_quantization_int8_adds_fl0/outputs/conversion/test
2026-01-18 16:25:50,667 - deployment.conversion.orchestration - INFO - Created MLflow run: test_run_name (test_run_id...)
2026-01-18 16:25:50,667 - deployment.conversion.orchestration - INFO - Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /path/to/checkpoint --config-dir /tmp/pytest-of-codespace/pytest-120/test_quantization_int8_adds_fl0/config --backbone distilbert-base-uncased --output-dir /tmp/pytest-of-codespace/pytest-120/test_quantization_int8_adds_fl0/outputs/conversion/test --opset-version 18 --quantize-int8 --run-smoke-test
2026-01-18 16:25:50,668 - infrastructure.tracking.mlflow.lifecycle - INFO - Run test_run_id... already terminated with status <Mock name='create_mlflow_client().get_run().info.status' id='123304183178112'> (expected FINISHED)
2026-01-18 16:25:50,668 - deployment.conversion.orchestration - INFO - Conversion completed. ONNX model: /tmp/pytest-of-codespace/pytest-120/test_quantization_int8_adds_fl0/outputs/conversion/test/model_int8.onnx
2026-01-18 16:25:50,677 - deployment.conversion.orchestration - INFO - Output directory: /tmp/pytest-of-codespace/pytest-120/test_quantization_none_no_flag0/outputs/conversion/test
2026-01-18 16:25:50,680 - deployment.conversion.orchestration - INFO - Created MLflow run: test_run_name (test_run_id...)
2026-01-18 16:25:50,680 - deployment.conversion.orchestration - INFO - Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /path/to/checkpoint --config-dir /tmp/pytest-of-codespace/pytest-120/test_quantization_none_no_flag0/config --backbone distilbert-base-uncased --output-dir /tmp/pytest-of-codespace/pytest-120/test_quantization_none_no_flag0/outputs/conversion/test --opset-version 18 --run-smoke-test
2026-01-18 16:25:50,681 - infrastructure.tracking.mlflow.lifecycle - INFO - Run test_run_id... already terminated with status <Mock name='create_mlflow_client().get_run().info.status' id='123304181236256'> (expected FINISHED)
2026-01-18 16:25:50,681 - deployment.conversion.orchestration - INFO - Conversion completed. ONNX model: /tmp/pytest-of-codespace/pytest-120/test_quantization_none_no_flag0/outputs/conversion/test/model.onnx
2026-01-18 16:25:50,687 - deployment.conversion.orchestration - INFO - Output directory: /tmp/pytest-of-codespace/pytest-120/test_run_smoke_test_true_adds_0/outputs/conversion/test
2026-01-18 16:25:50,691 - deployment.conversion.orchestration - INFO - Created MLflow run: test_run_name (test_run_id...)
2026-01-18 16:25:50,691 - deployment.conversion.orchestration - INFO - Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /path/to/checkpoint --config-dir /tmp/pytest-of-codespace/pytest-120/test_run_smoke_test_true_adds_0/config --backbone distilbert-base-uncased --output-dir /tmp/pytest-of-codespace/pytest-120/test_run_smoke_test_true_adds_0/outputs/conversion/test --opset-version 18 --run-smoke-test
2026-01-18 16:25:50,692 - infrastructure.tracking.mlflow.lifecycle - INFO - Run test_run_id... already terminated with status <Mock name='create_mlflow_client().get_run().info.status' id='123304162646992'> (expected FINISHED)
2026-01-18 16:25:50,692 - deployment.conversion.orchestration - INFO - Conversion completed. ONNX model: /tmp/pytest-of-codespace/pytest-120/test_run_smoke_test_true_adds_0/outputs/conversion/test/model.onnx
2026-01-18 16:25:50,698 - deployment.conversion.orchestration - INFO - Output directory: /tmp/pytest-of-codespace/pytest-120/test_run_smoke_test_false_no_f0/outputs/conversion/test
2026-01-18 16:25:50,701 - deployment.conversion.orchestration - INFO - Created MLflow run: test_run_name (test_run_id...)
2026-01-18 16:25:50,702 - deployment.conversion.orchestration - INFO - Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /path/to/checkpoint --config-dir /tmp/pytest-of-codespace/pytest-120/test_run_smoke_test_false_no_f0/config --backbone distilbert-base-uncased --output-dir /tmp/pytest-of-codespace/pytest-120/test_run_smoke_test_false_no_f0/outputs/conversion/test --opset-version 18
2026-01-18 16:25:50,702 - infrastructure.tracking.mlflow.lifecycle - INFO - Run test_run_id... already terminated with status <Mock name='create_mlflow_client().get_run().info.status' id='123304181216160'> (expected FINISHED)
2026-01-18 16:25:50,703 - deployment.conversion.orchestration - INFO - Conversion completed. ONNX model: /tmp/pytest-of-codespace/pytest-120/test_run_smoke_test_false_no_f0/outputs/conversion/test/model.onnx
2026-01-18 16:25:50,708 - deployment.conversion.orchestration - INFO - Output directory: /tmp/pytest-of-codespace/pytest-120/test_filename_pattern_used_in_0/outputs/conversion/test
2026-01-18 16:25:50,711 - deployment.conversion.orchestration - INFO - Created MLflow run: test_run_name (test_run_id...)
2026-01-18 16:25:50,711 - deployment.conversion.orchestration - INFO - Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /path/to/checkpoint --config-dir /tmp/pytest-of-codespace/pytest-120/test_filename_pattern_used_in_0/config --backbone distilbert-base-uncased --output-dir /tmp/pytest-of-codespace/pytest-120/test_filename_pattern_used_in_0/outputs/conversion/test --opset-version 18 --run-smoke-test
2026-01-18 16:25:50,712 - infrastructure.tracking.mlflow.lifecycle - INFO - Run test_run_id... already terminated with status <Mock name='create_mlflow_client().get_run().info.status' id='123304162672256'> (expected FINISHED)
2026-01-18 16:25:50,712 - deployment.conversion.orchestration - INFO - Conversion completed. ONNX model: /tmp/pytest-of-codespace/pytest-120/test_filename_pattern_used_in_0/outputs/conversion/test/custom_fp32_model.onnx
2026-01-18 16:25:50,798 - evaluation.selection.trial_finder.mlflow_queries - INFO - Found 3 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 16:25:50,799 - evaluation.selection.trial_finder.champion_selection - INFO - Grouped runs for distilbert: 0 v1 group(s), 1 v2 group(s)
2026-01-18 16:25:50,799 - evaluation.selection.trial_finder.champion_selection - INFO - Found 1 eligible group(s) for distilbert (0 skipped due to min_trials requirement)
2026-01-18 16:25:50,799 - evaluation.selection.artifact_unified.selectors - INFO - Found refit run refit-run-12... for trial run2... (selected latest from 1 refit run(s))
2026-01-18 16:25:50,800 - evaluation.selection.trial_finder.champion_selection - INFO - Found refit run refit-run-12... for champion trial run2... (using SSOT selector: refit_preferred)
2026-01-18 16:25:50,802 - evaluation.selection.trial_finder.mlflow_queries - INFO - No runs found with stage='hpo_trial' for distilbert, trying legacy stage='hpo'
2026-01-18 16:25:50,802 - evaluation.selection.trial_finder.mlflow_queries - INFO - Found 0 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 16:25:50,802 - evaluation.selection.trial_finder.champion_selection - INFO - Grouped runs for distilbert: 0 v1 group(s), 0 v2 group(s)
2026-01-18 16:25:50,802 - evaluation.selection.trial_finder.mlflow_queries - WARNING - No v2 groups found and v1 is no longer supported. Returning empty groups.
2026-01-18 16:25:50,802 - evaluation.selection.trial_finder.champion_selection - WARNING - No valid groups found for distilbert. No trial runs found in HPO experiment 'test_hpo_experiment'. This may indicate:
  - HPO was not run for this backbone
  - Runs exist but don't have required tags (stage='hpo_trial' or 'hpo', backbone tag)
  - Runs exist but were filtered out (missing metrics, artifacts, or grouping tags)
Skipping champion selection for distilbert.
2026-01-18 16:25:50,805 - evaluation.selection.trial_finder.mlflow_queries - INFO - Found 2 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 16:25:50,806 - evaluation.selection.trial_finder.champion_selection - INFO - Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)
2026-01-18 16:25:50,806 - evaluation.selection.trial_finder.mlflow_queries - WARNING - No v2 groups found and v1 is no longer supported. Returning empty groups.
2026-01-18 16:25:50,806 - evaluation.selection.trial_finder.champion_selection - WARNING - No valid groups found for distilbert. Found 1 v1 group(s) and 0 v2 group(s), but none matched selection criteria (schema_version preference: auto). This may indicate:
  - Groups don't meet min_trials requirement
  - Schema version mismatch (prefer_schema_version=auto)
  - Groups filtered out by other criteria
Skipping champion selection for distilbert.
2026-01-18 16:25:50,809 - evaluation.selection.trial_finder.mlflow_queries - INFO - Found 3 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 16:25:50,809 - evaluation.selection.trial_finder.champion_selection - INFO - Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)
2026-01-18 16:25:50,809 - evaluation.selection.trial_finder.mlflow_queries - WARNING - No v2 groups found and v1 is no longer supported. Returning empty groups.
2026-01-18 16:25:50,810 - evaluation.selection.trial_finder.champion_selection - WARNING - No valid groups found for distilbert. Found 1 v1 group(s) and 0 v2 group(s), but none matched selection criteria (schema_version preference: auto). This may indicate:
  - Groups don't meet min_trials requirement
  - Schema version mismatch (prefer_schema_version=auto)
  - Groups filtered out by other criteria
Skipping champion selection for distilbert.
2026-01-18 16:25:50,813 - evaluation.selection.trial_finder.mlflow_queries - INFO - Found 3 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 16:25:50,814 - evaluation.selection.trial_finder.champion_selection - INFO - Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)
2026-01-18 16:25:50,814 - evaluation.selection.trial_finder.mlflow_queries - WARNING - No v2 groups found and v1 is no longer supported. Returning empty groups.
2026-01-18 16:25:50,815 - evaluation.selection.trial_finder.champion_selection - WARNING - No valid groups found for distilbert. Found 1 v1 group(s) and 0 v2 group(s), but none matched selection criteria (schema_version preference: auto). This may indicate:
  - Groups don't meet min_trials requirement
  - Schema version mismatch (prefer_schema_version=auto)
  - Groups filtered out by other criteria
Skipping champion selection for distilbert.
2026-01-18 16:25:50,818 - evaluation.selection.trial_finder.mlflow_queries - INFO - Found 3 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 16:25:50,819 - evaluation.selection.trial_finder.champion_selection - WARNING - Artifact filter: 1 run(s) have code.artifact.available='false' (explicitly marked as unavailable)
2026-01-18 16:25:50,819 - evaluation.selection.trial_finder.champion_selection - WARNING - Artifact filter: 1 run(s) excluded (1 explicitly false, 0 missing/legacy allowed)
2026-01-18 16:25:50,819 - evaluation.selection.trial_finder.champion_selection - WARNING - Artifact filter removed 1 runs for distilbert (2 remaining). Check that runs have 'code.artifact.available' tag set to 'true'.
2026-01-18 16:25:50,819 - evaluation.selection.trial_finder.champion_selection - INFO - Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)
2026-01-18 16:25:50,819 - evaluation.selection.trial_finder.mlflow_queries - WARNING - No v2 groups found and v1 is no longer supported. Returning empty groups.
2026-01-18 16:25:50,820 - evaluation.selection.trial_finder.champion_selection - WARNING - No valid groups found for distilbert. Found 1 v1 group(s) and 0 v2 group(s), but none matched selection criteria (schema_version preference: auto). This may indicate:
  - Groups don't meet min_trials requirement
  - Schema version mismatch (prefer_schema_version=auto)
  - Groups filtered out by other criteria
Skipping champion selection for distilbert.
2026-01-18 16:25:50,822 - evaluation.selection.trial_finder.mlflow_queries - INFO - Found 3 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 16:25:50,823 - evaluation.selection.trial_finder.champion_selection - INFO - Grouped runs for distilbert: 0 v1 group(s), 1 v2 group(s)
2026-01-18 16:25:50,823 - evaluation.selection.trial_finder.champion_selection - INFO - Found 1 eligible group(s) for distilbert (0 skipped due to min_trials requirement)
2026-01-18 16:25:50,823 - evaluation.selection.artifact_unified.selectors - INFO - Found refit run refit-run-12... for trial run2... (selected latest from 1 refit run(s))
2026-01-18 16:25:50,824 - evaluation.selection.trial_finder.champion_selection - INFO - Found refit run refit-run-12... for champion trial run2... (using SSOT selector: refit_preferred)
2026-01-18 16:25:50,827 - evaluation.selection.trial_finder.mlflow_queries - INFO - Found 5 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 16:25:50,827 - evaluation.selection.trial_finder.champion_selection - INFO - Grouped runs for distilbert: 1 v1 group(s), 1 v2 group(s)
2026-01-18 16:25:50,827 - evaluation.selection.trial_finder.champion_selection - INFO - Found both v1 and v2 runs for distilbert. Using 2.0 groups only (never mixing versions).
2026-01-18 16:25:50,827 - evaluation.selection.trial_finder.champion_selection - INFO - Found 1 eligible group(s) for distilbert (0 skipped due to min_trials requirement)
2026-01-18 16:25:50,828 - evaluation.selection.artifact_unified.selectors - INFO - Found refit run refit-run-12... for trial run3... (selected latest from 1 refit run(s))
2026-01-18 16:25:50,828 - evaluation.selection.trial_finder.champion_selection - INFO - Found refit run refit-run-12... for champion trial run3... (using SSOT selector: refit_preferred)
2026-01-18 16:25:50,831 - evaluation.selection.trial_finder.mlflow_queries - INFO - Found 3 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 16:25:50,831 - evaluation.selection.trial_finder.champion_selection - INFO - Grouped runs for distilbert: 0 v1 group(s), 1 v2 group(s)
2026-01-18 16:25:50,831 - evaluation.selection.trial_finder.champion_selection - INFO - Found 1 eligible group(s) for distilbert (0 skipped due to min_trials requirement)
2026-01-18 16:25:50,832 - evaluation.selection.artifact_unified.selectors - INFO - Found refit run refit-run-12... for trial run3... (selected latest from 1 refit run(s))
2026-01-18 16:25:50,832 - evaluation.selection.trial_finder.champion_selection - INFO - Found refit run refit-run-12... for champion trial run3... (using SSOT selector: refit_preferred)
2026-01-18 16:25:50,835 - evaluation.selection.trial_finder.mlflow_queries - INFO - Found 6 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 16:25:50,836 - evaluation.selection.trial_finder.champion_selection - INFO - Grouped runs for distilbert: 0 v1 group(s), 2 v2 group(s)
2026-01-18 16:25:50,837 - evaluation.selection.trial_finder.champion_selection - INFO - Found 2 eligible group(s) for distilbert (0 skipped due to min_trials requirement)
2026-01-18 16:25:50,837 - evaluation.selection.artifact_unified.selectors - INFO - Found refit run refit-run-12... for trial run6... (selected latest from 1 refit run(s))
2026-01-18 16:25:50,837 - evaluation.selection.trial_finder.champion_selection - INFO - Found refit run refit-run-12... for champion trial run6... (using SSOT selector: refit_preferred)
2026-01-18 16:25:50,840 - evaluation.selection.trial_finder.mlflow_queries - INFO - Found 5 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 16:25:50,840 - evaluation.selection.trial_finder.champion_selection - INFO - Grouped runs for distilbert: 0 v1 group(s), 1 v2 group(s)
2026-01-18 16:25:50,841 - evaluation.selection.trial_finder.champion_selection - INFO - Found 1 eligible group(s) for distilbert (0 skipped due to min_trials requirement)
2026-01-18 16:25:50,841 - evaluation.selection.artifact_unified.selectors - INFO - Found refit run refit-run-12... for trial run3... (selected latest from 1 refit run(s))
2026-01-18 16:25:50,841 - evaluation.selection.trial_finder.champion_selection - INFO - Found refit run refit-run-12... for champion trial run3... (using SSOT selector: refit_preferred)
2026-01-18 16:25:50,844 - training.execution.executor - INFO - Final training config loaded from final_training.yaml
2026-01-18 16:25:50,844 - training.execution.executor - INFO - Output directory: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_re0/outputs/v1
2026-01-18 16:25:50,845 - training.execution.executor - INFO - Found existing completed final training run
2026-01-18 16:25:50,845 - training.execution.executor - INFO -   Output directory: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_re0/outputs/v1
2026-01-18 16:25:50,845 - training.execution.executor - INFO -   Checkpoint: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_re0/outputs/v1/checkpoint
2026-01-18 16:25:50,845 - training.execution.executor - INFO -   Reusing existing checkpoint (run.mode: reuse_if_exists)
2026-01-18 16:25:50,848 - training.execution.executor - INFO - Final training config loaded from final_training.yaml
2026-01-18 16:25:50,848 - training.execution.executor - INFO - Output directory: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_fo0/outputs/v1
2026-01-18 16:25:50,851 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_fo0/config/tags.yaml, using defaults
2026-01-18 16:25:50,863 - training.execution.mlflow_setup - INFO -  View run test_run_name at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/640045798020977743/runs/4e8515fdfbd7415484a0ae2c159266bf
2026-01-18 16:25:50,863 - training.execution.mlflow_setup - INFO - Created MLflow run: test_run_name (4e8515fdfbd7...)
2026-01-18 16:25:50,863 - training.execution.executor - INFO - Created MLflow run: test_run_name (4e8515fdfbd7...)
2026-01-18 16:25:50,866 - training.execution.executor - INFO - Running final training...
2026-01-18 16:25:50,867 - infrastructure.tracking.mlflow.lifecycle - INFO - Run 4e8515fdfbd7... already terminated with status <Mock name='mock.get_run().info.status' id='123304183349888'> (expected FINISHED)
2026-01-18 16:25:50,867 - training.execution.executor - INFO - Saved metadata to: None
2026-01-18 16:25:50,867 - training.execution.executor - INFO - Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_fo0/outputs/v1/checkpoint
2026-01-18 16:25:50,867 - training.execution.executor - INFO - MLflow run: 4e8515fdfbd7...
2026-01-18 16:25:50,870 - training.execution.executor - INFO - Final training config loaded from final_training.yaml
2026-01-18 16:25:50,871 - training.execution.executor - INFO - Output directory: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_re1/outputs/v1
2026-01-18 16:25:50,871 - training.execution.executor - WARNING - Could not search for existing runs: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_re1/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.
2026-01-18 16:25:50,876 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_re1/config/tags.yaml, using defaults
2026-01-18 16:25:50,888 - training.execution.mlflow_setup - INFO -  View run test_run_name at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/640045798020977743/runs/4887db959d2d47059c04709512f7268c
2026-01-18 16:25:50,889 - training.execution.mlflow_setup - INFO - Created MLflow run: test_run_name (4887db959d2d...)
2026-01-18 16:25:50,889 - training.execution.executor - INFO - Created MLflow run: test_run_name (4887db959d2d...)
2026-01-18 16:25:50,891 - training.execution.executor - INFO - Running final training...
2026-01-18 16:25:50,892 - infrastructure.tracking.mlflow.lifecycle - INFO - Run 4887db959d2d... already terminated with status <Mock name='mock.get_run().info.status' id='123304183277184'> (expected FINISHED)
2026-01-18 16:25:50,892 - training.execution.executor - INFO - Saved metadata to: None
2026-01-18 16:25:50,892 - training.execution.executor - INFO - Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_re1/outputs/v1/checkpoint
2026-01-18 16:25:50,892 - training.execution.executor - INFO - MLflow run: 4887db959d2d...
2026-01-18 16:25:50,895 - training.execution.executor - INFO - Final training config loaded from final_training.yaml
2026-01-18 16:25:50,895 - training.execution.executor - INFO - Output directory: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_mi0/outputs/v1
2026-01-18 16:25:50,898 - training.execution.executor - INFO - Final training config loaded from final_training.yaml
2026-01-18 16:25:50,899 - training.execution.executor - INFO - Output directory: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_lo0/outputs/v1
2026-01-18 16:25:50,901 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_lo0/config/tags.yaml, using defaults
2026-01-18 16:25:50,912 - training.execution.mlflow_setup - INFO -  View run test_run_name at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/640045798020977743/runs/9dd5bd306dba4f08936de341740583db
2026-01-18 16:25:50,912 - training.execution.mlflow_setup - INFO - Created MLflow run: test_run_name (9dd5bd306dba...)
2026-01-18 16:25:50,913 - training.execution.executor - INFO - Created MLflow run: test_run_name (9dd5bd306dba...)
2026-01-18 16:25:50,915 - training.execution.executor - INFO - Running final training...
2026-01-18 16:25:50,916 - infrastructure.tracking.mlflow.lifecycle - INFO - Run 9dd5bd306dba... already terminated with status <Mock name='mock.get_run().info.status' id='123304183274832'> (expected FINISHED)
2026-01-18 16:25:50,916 - training.execution.executor - INFO - Saved metadata to: None
2026-01-18 16:25:50,916 - training.execution.executor - INFO - Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_lo0/outputs/v1/checkpoint
2026-01-18 16:25:50,916 - training.execution.executor - INFO - MLflow run: 9dd5bd306dba...
2026-01-18 16:25:50,919 - training.execution.executor - INFO - Final training config loaded from final_training.yaml
2026-01-18 16:25:50,919 - training.execution.executor - INFO - Output directory: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_tr0/outputs/v1
2026-01-18 16:25:50,922 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_tr0/config/tags.yaml, using defaults
2026-01-18 16:25:50,931 - training.execution.mlflow_setup - INFO -  View run test_run_name at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/640045798020977743/runs/8a0786b0aea047fca7155fe23dfe91e0
2026-01-18 16:25:50,931 - training.execution.mlflow_setup - INFO - Created MLflow run: test_run_name (8a0786b0aea0...)
2026-01-18 16:25:50,931 - training.execution.executor - INFO - Created MLflow run: test_run_name (8a0786b0aea0...)
2026-01-18 16:25:50,933 - training.execution.executor - INFO - Running final training...
2026-01-18 16:25:50,934 - infrastructure.tracking.mlflow.lifecycle - INFO - Run 8a0786b0aea0... already has status <Mock name='mock.get_run().info.status' id='123304183273104'>, skipping termination (expected RUNNING)
2026-01-18 16:25:50,937 - training.execution.executor - INFO - Final training config loaded from final_training.yaml
2026-01-18 16:25:50,937 - training.execution.executor - INFO - Output directory: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_ml0/outputs/v1
2026-01-18 16:25:50,939 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_ml0/config/tags.yaml, using defaults
2026-01-18 16:25:50,948 - training.execution.mlflow_setup - INFO -  View run test_run_name at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/640045798020977743/runs/61bc9f38fb1b4a148ecccb9748845c9a
2026-01-18 16:25:50,948 - training.execution.mlflow_setup - INFO - Created MLflow run: test_run_name (61bc9f38fb1b...)
2026-01-18 16:25:50,948 - training.execution.executor - INFO - Created MLflow run: test_run_name (61bc9f38fb1b...)
2026-01-18 16:25:50,951 - training.execution.executor - INFO - Running final training...
2026-01-18 16:25:50,951 - infrastructure.tracking.mlflow.lifecycle - INFO - Run 61bc9f38fb1b... already terminated with status <Mock name='mock.get_run().info.status' id='123304183282128'> (expected FINISHED)
2026-01-18 16:25:50,951 - training.execution.executor - INFO - Saved metadata to: None
2026-01-18 16:25:50,952 - training.execution.executor - INFO - Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_ml0/outputs/v1/checkpoint
2026-01-18 16:25:50,952 - training.execution.executor - INFO - MLflow run: 61bc9f38fb1b...
2026-01-18 16:25:50,955 - training.execution.executor - INFO - Final training config loaded from final_training.yaml
2026-01-18 16:25:50,955 - training.execution.executor - INFO - Output directory: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_so0/outputs/v1
2026-01-18 16:25:50,957 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_so0/config/tags.yaml, using defaults
2026-01-18 16:25:50,966 - training.execution.mlflow_setup - INFO -  View run test_run_name at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/640045798020977743/runs/a6c39d411f31484b9acd07f9b5f0162e
2026-01-18 16:25:50,966 - training.execution.mlflow_setup - INFO - Created MLflow run: test_run_name (a6c39d411f31...)
2026-01-18 16:25:50,966 - training.execution.executor - INFO - Created MLflow run: test_run_name (a6c39d411f31...)
2026-01-18 16:25:50,968 - training.execution.executor - INFO - Running final training...
2026-01-18 16:25:50,969 - infrastructure.tracking.mlflow.lifecycle - INFO - Run a6c39d411f31... already terminated with status <Mock name='mock.get_run().info.status' id='123304183175856'> (expected FINISHED)
2026-01-18 16:25:50,969 - training.execution.executor - INFO - Saved metadata to: None
2026-01-18 16:25:50,969 - training.execution.executor - INFO - Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_so0/outputs/v1/checkpoint
2026-01-18 16:25:50,970 - training.execution.executor - INFO - MLflow run: a6c39d411f31...
2026-01-18 16:25:50,973 - training.execution.executor - INFO - Final training config loaded from final_training.yaml
2026-01-18 16:25:50,973 - training.execution.executor - INFO - Output directory: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_so1/outputs/v1
2026-01-18 16:25:50,976 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_so1/config/tags.yaml, using defaults
2026-01-18 16:25:50,985 - training.execution.mlflow_setup - INFO -  View run test_run_name at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/640045798020977743/runs/1fa8bf30980a4f46aee1b5202b85b616
2026-01-18 16:25:50,985 - training.execution.mlflow_setup - INFO - Created MLflow run: test_run_name (1fa8bf30980a...)
2026-01-18 16:25:50,985 - training.execution.executor - INFO - Created MLflow run: test_run_name (1fa8bf30980a...)
2026-01-18 16:25:50,988 - training.execution.executor - INFO - Running final training...
2026-01-18 16:25:50,988 - infrastructure.tracking.mlflow.lifecycle - INFO - Run 1fa8bf30980a... already terminated with status <Mock name='mock.get_run().info.status' id='123304183272960'> (expected FINISHED)
2026-01-18 16:25:50,988 - training.execution.executor - INFO - Saved metadata to: None
2026-01-18 16:25:50,988 - training.execution.executor - INFO - Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_so1/outputs/v1/checkpoint
2026-01-18 16:25:50,989 - training.execution.executor - INFO - MLflow run: 1fa8bf30980a...
2026-01-18 16:25:50,992 - training.execution.executor - INFO - Final training config loaded from final_training.yaml
2026-01-18 16:25:50,992 - training.execution.executor - INFO - Output directory: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_hy0/outputs/v1
2026-01-18 16:25:50,994 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_hy0/config/tags.yaml, using defaults
2026-01-18 16:25:51,003 - training.execution.mlflow_setup - INFO -  View run test_run_name at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/640045798020977743/runs/971f69509d254cad87f42dc3a9d014e5
2026-01-18 16:25:51,003 - training.execution.mlflow_setup - INFO - Created MLflow run: test_run_name (971f69509d25...)
2026-01-18 16:25:51,004 - training.execution.executor - INFO - Created MLflow run: test_run_name (971f69509d25...)
2026-01-18 16:25:51,006 - training.execution.executor - INFO - Running final training...
2026-01-18 16:25:51,007 - infrastructure.tracking.mlflow.lifecycle - INFO - Run 971f69509d25... already terminated with status <Mock name='mock.get_run().info.status' id='123304183166480'> (expected FINISHED)
2026-01-18 16:25:51,007 - training.execution.executor - INFO - Saved metadata to: None
2026-01-18 16:25:51,007 - training.execution.executor - INFO - Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_hy0/outputs/v1/checkpoint
2026-01-18 16:25:51,007 - training.execution.executor - INFO - MLflow run: 971f69509d25...
2026-01-18 16:25:51,010 - training.execution.executor - INFO - Final training config loaded from final_training.yaml
2026-01-18 16:25:51,010 - training.execution.executor - INFO - Output directory: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_ml1/outputs/v1
2026-01-18 16:25:51,013 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_ml1/config/tags.yaml, using defaults
2026-01-18 16:25:51,022 - training.execution.mlflow_setup - INFO -  View run default_run_name at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/170515528070219461/runs/2066149438eb44b2996c5d1d2498a40d
2026-01-18 16:25:51,022 - training.execution.mlflow_setup - INFO - Created MLflow run: default_run_name (2066149438eb...)
2026-01-18 16:25:51,022 - training.execution.executor - INFO - Created MLflow run: default_run_name (2066149438eb...)
2026-01-18 16:25:51,024 - training.execution.executor - INFO - Running final training...
2026-01-18 16:25:51,025 - infrastructure.tracking.mlflow.lifecycle - INFO - Run 2066149438eb... already terminated with status <Mock name='mock.get_run().info.status' id='123304183270896'> (expected FINISHED)
2026-01-18 16:25:51,025 - training.execution.executor - INFO - Saved metadata to: None
2026-01-18 16:25:51,025 - training.execution.executor - INFO - Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-120/test_execute_final_training_ml1/outputs/v1/checkpoint
2026-01-18 16:25:51,025 - training.execution.executor - INFO - MLflow run: 2066149438eb...
2026-01-18 16:25:51,028 - training.execution.executor - INFO - Final training config loaded from final_training.yaml
2026-01-18 16:25:51,029 - training.execution.executor - INFO - Output directory: /tmp/pytest-of-codespace/pytest-120/test_logging_eval_interval_loa0/outputs/v1
2026-01-18 16:25:51,031 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_logging_eval_interval_loa0/config/tags.yaml, using defaults
2026-01-18 16:25:51,040 - training.execution.mlflow_setup - INFO -  View run test_run_name at: file:///tmp/mlflow/#/experiments/640045798020977743/runs/fc2eb2b090e14495b65e1701fa107924
2026-01-18 16:25:51,040 - training.execution.mlflow_setup - INFO - Created MLflow run: test_run_name (fc2eb2b090e1...)
2026-01-18 16:25:51,040 - training.execution.executor - INFO - Created MLflow run: test_run_name (fc2eb2b090e1...)
2026-01-18 16:25:51,043 - training.execution.executor - INFO - Running final training...
2026-01-18 16:25:51,043 - infrastructure.tracking.mlflow.lifecycle - INFO - Run fc2eb2b090e1... already terminated with status <Mock name='mock.get_run().info.status' id='123304183166624'> (expected FINISHED)
2026-01-18 16:25:51,044 - training.execution.executor - INFO - Saved metadata to: None
2026-01-18 16:25:51,044 - training.execution.executor - INFO - Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-120/test_logging_eval_interval_loa0/outputs/v1/checkpoint
2026-01-18 16:25:51,044 - training.execution.executor - INFO - MLflow run: fc2eb2b090e1...
2026-01-18 16:25:51,047 - training.execution.executor - INFO - Final training config loaded from final_training.yaml
2026-01-18 16:25:51,047 - training.execution.executor - INFO - Output directory: /tmp/pytest-of-codespace/pytest-120/test_logging_save_interval_loa0/outputs/v1
2026-01-18 16:25:51,049 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_logging_save_interval_loa0/config/tags.yaml, using defaults
2026-01-18 16:25:51,063 - training.execution.mlflow_setup - INFO -  View run test_run_name at: file:///tmp/mlflow/#/experiments/640045798020977743/runs/82e68a4be85640649f5e4c6932315072
2026-01-18 16:25:51,064 - training.execution.mlflow_setup - INFO - Created MLflow run: test_run_name (82e68a4be856...)
2026-01-18 16:25:51,064 - training.execution.executor - INFO - Created MLflow run: test_run_name (82e68a4be856...)
2026-01-18 16:25:51,067 - training.execution.executor - INFO - Running final training...
2026-01-18 16:25:51,067 - infrastructure.tracking.mlflow.lifecycle - INFO - Run 82e68a4be856... already terminated with status <Mock name='mock.get_run().info.status' id='123304183166912'> (expected FINISHED)
2026-01-18 16:25:51,067 - training.execution.executor - INFO - Saved metadata to: None
2026-01-18 16:25:51,068 - training.execution.executor - INFO - Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-120/test_logging_save_interval_loa0/outputs/v1/checkpoint
2026-01-18 16:25:51,068 - training.execution.executor - INFO - MLflow run: 82e68a4be856...
2026-01-18 16:25:51,070 - training.execution.executor - INFO - Final training config loaded from final_training.yaml
2026-01-18 16:25:51,071 - training.execution.executor - INFO - Output directory: /tmp/pytest-of-codespace/pytest-120/test_logging_intervals_both_lo0/outputs/v1
2026-01-18 16:25:51,073 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_logging_intervals_both_lo0/config/tags.yaml, using defaults
2026-01-18 16:25:51,082 - training.execution.mlflow_setup - INFO -  View run test_run_name at: file:///tmp/mlflow/#/experiments/640045798020977743/runs/43a201ece3564efc8b815a1abaa3ea9e
2026-01-18 16:25:51,082 - training.execution.mlflow_setup - INFO - Created MLflow run: test_run_name (43a201ece356...)
2026-01-18 16:25:51,082 - training.execution.executor - INFO - Created MLflow run: test_run_name (43a201ece356...)
2026-01-18 16:25:51,085 - training.execution.executor - INFO - Running final training...
2026-01-18 16:25:51,085 - infrastructure.tracking.mlflow.lifecycle - INFO - Run 43a201ece356... already terminated with status <Mock name='mock.get_run().info.status' id='123304183162400'> (expected FINISHED)
2026-01-18 16:25:51,085 - training.execution.executor - INFO - Saved metadata to: None
2026-01-18 16:25:51,086 - training.execution.executor - INFO - Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-120/test_logging_intervals_both_lo0/outputs/v1/checkpoint
2026-01-18 16:25:51,086 - training.execution.executor - INFO - MLflow run: 43a201ece356...
[I 2026-01-18 16:25:51,522] A new study created in RDB with name: test_best
[I 2026-01-18 16:25:51,576] Trial 0 finished with value: 0.7 and parameters: {}. Best is trial 0 with value: 0.7.
[I 2026-01-18 16:25:51,611] Trial 1 finished with value: 0.85 and parameters: {}. Best is trial 1 with value: 0.85.
[I 2026-01-18 16:25:51,629] Trial 2 finished with value: 0.75 and parameters: {}. Best is trial 1 with value: 0.85.
[I 2026-01-18 16:25:51,722] A new study created in RDB with name: test_extract
[I 2026-01-18 16:25:51,760] Trial 0 finished with value: 0.8 and parameters: {'learning_rate': 2.2148191704015396e-05, 'batch_size': 4}. Best is trial 0 with value: 0.8.
[I 2026-01-18 16:25:51,855] A new study created in RDB with name: test_cv
[I 2026-01-18 16:25:51,889] Trial 0 finished with value: 0.8 and parameters: {}. Best is trial 0 with value: 0.8.
[I 2026-01-18 16:25:51,983] A new study created in RDB with name: test_minimize
[I 2026-01-18 16:25:52,007] Trial 0 finished with value: 0.1 and parameters: {}. Best is trial 0 with value: 0.1.
[I 2026-01-18 16:25:52,026] Trial 1 finished with value: 0.2 and parameters: {}. Best is trial 0 with value: 0.1.
[I 2026-01-18 16:25:52,041] Trial 2 finished with value: 0.15 and parameters: {}. Best is trial 0 with value: 0.1.
[I 2026-01-18 16:25:52,138] A new study created in RDB with name: test_refit
[I 2026-01-18 16:25:52,171] Trial 0 finished with value: 0.7 and parameters: {'learning_rate': 2.7948831704518953e-05}. Best is trial 0 with value: 0.7.
[I 2026-01-18 16:25:52,198] Trial 1 finished with value: 0.85 and parameters: {'learning_rate': 1.8390503155236624e-05}. Best is trial 1 with value: 0.85.
[I 2026-01-18 16:25:52,292] A new study created in RDB with name: test_empty
[I 2026-01-18 16:25:52,386] A new study created in RDB with name: test_params
[I 2026-01-18 16:25:52,448] Trial 0 finished with value: 0.8 and parameters: {'learning_rate': 3.760767930006252e-05, 'batch_size': 6, 'dropout': 0.24323882946595562, 'weight_decay': 0.007960139707821117}. Best is trial 0 with value: 0.8.
[I 2026-01-18 16:25:52,547] A new study created in RDB with name: test_delay
[I 2026-01-18 16:25:52,581] Trial 0 finished with value: 0.5 and parameters: {}. Best is trial 0 with value: 0.5.
[I 2026-01-18 16:25:52,604] Trial 1 finished with value: 0.3 and parameters: {}. Best is trial 0 with value: 0.5.
[I 2026-01-18 16:25:52,698] A new study created in RDB with name: test_prune
[I 2026-01-18 16:25:52,732] Trial 0 finished with value: 0.8 and parameters: {}. Best is trial 0 with value: 0.8.
[I 2026-01-18 16:25:52,755] Trial 1 finished with value: 0.75 and parameters: {}. Best is trial 0 with value: 0.8.
[I 2026-01-18 16:25:52,774] Trial 2 pruned. 
[I 2026-01-18 16:25:53,175] A new study created in RDB with name: test_interval
[I 2026-01-18 16:25:53,216] Trial 0 finished with value: 0.3 and parameters: {}. Best is trial 0 with value: 0.3.
[I 2026-01-18 16:25:53,318] A new study created in RDB with name: test_best
[I 2026-01-18 16:25:53,351] Trial 0 finished with value: 0.9 and parameters: {}. Best is trial 0 with value: 0.9.
[I 2026-01-18 16:25:53,373] Trial 1 finished with value: 0.7 and parameters: {}. Best is trial 0 with value: 0.9.
[I 2026-01-18 16:25:53,396] Trial 2 finished with value: 0.8 and parameters: {}. Best is trial 0 with value: 0.9.
[I 2026-01-18 16:25:53,499] A new study created in RDB with name: test_no_prune
[I 2026-01-18 16:25:53,526] Trial 0 finished with value: 0.5 and parameters: {}. Best is trial 0 with value: 0.5.
[I 2026-01-18 16:25:53,546] Trial 1 finished with value: 0.5 and parameters: {}. Best is trial 0 with value: 0.5.
[I 2026-01-18 16:25:53,563] Trial 2 finished with value: 0.5 and parameters: {}. Best is trial 0 with value: 0.5.
2026-01-18 16:25:53,585 - training.hpo.trial.metrics - ERROR - metrics.json not found at expected location: /tmp/pytest-of-codespace/pytest-120/test_missing_metrics_file0/outputs/hpo/local/distilbert/study-abc12345/trial-def67890/metrics.json. Trial output dir: /tmp/pytest-of-codespace/pytest-120/test_missing_metrics_file0/outputs/hpo/local/distilbert/study-abc12345/trial-def67890, Root dir: /tmp/pytest-of-codespace/pytest-120/test_missing_metrics_file0
2026-01-18 16:25:53,596 - training.hpo.execution.local.cv - WARNING - Could not build systematic run name and tags: Naming policy not available or formatting failed. Legacy fallback removed. Process type: hpo_trial, using fallback
2026-01-18 16:25:53,597 - training.hpo.execution.local.cv - WARNING - Could not create trial run: Could not find experiment with ID exp_123
2026-01-18 16:25:53,597 - training.hpo.execution.local.cv - INFO - [CV] Created trial folder: /workspaces/resume-ner-azureml/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-10fa119c (trial 0)
2026-01-18 16:25:53,604 - infrastructure.tracking.mlflow.hash_utils - WARNING - Could not get or compute study_key_hash. study_key_hash=missing, hpo_parent_run_id=provided, configs=missing
2026-01-18 16:25:53,604 - training.hpo.execution.local.cv - WARNING - Could not build systematic run name and tags: HPO trial run name built without study_key_hash; check study identity propagation., using fallback
2026-01-18 16:25:53,605 - training.hpo.execution.local.cv - WARNING - Could not create trial run: Could not find experiment with ID exp_123
2026-01-18 16:25:53,605 - infrastructure.tracking.mlflow.hash_utils - WARNING - Could not get or compute study_key_hash. study_key_hash=missing, hpo_parent_run_id=provided, configs=missing
2026-01-18 16:25:53,606 - training.hpo.execution.local.cv - ERROR - In v2 study folder study-abc12345 but trial_key_hash is None. study_key_hash=NO. Will attempt to compute hash.
2026-01-18 16:25:53,610 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'batch_size': 4, 'dropout': 0.2, 'weight_decay': 0.05, 'backbone': 'distilbert'}
2026-01-18 16:25:53,610 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 16:25:53,635 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial 0 with hyperparameters: {'backbone': 'distilbert'}
2026-01-18 16:25:53,635 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 16:25:53,636 - training.hpo.execution.local.refit - WARNING - Could not construct v2 refit folder, falling back to legacy: 'NoneType' object has no attribute 'mkdir'
[I 2026-01-18 16:25:53,727] A new study created in RDB with name: empty_study
[I 2026-01-18 16:25:53,839] A new study created in RDB with name: hpo_distilbert_test
2026-01-18 16:25:53,860 - evaluation.selection.trial_finder.discovery - WARNING - No v2 study folders found in /tmp/pytest-of-codespace/pytest-120/test_resume_from_existing_chec0/outputs/hpo (resolved from /tmp/pytest-of-codespace/pytest-120/test_resume_from_existing_chec0/outputs/hpo)
2026-01-18 16:25:53,861 - training.hpo.core.study - INFO - [HPO] Resuming optimization for distilbert from checkpoint...
2026-01-18 16:25:53,865 - training.hpo.core.study - INFO - Loaded 0 existing trials (0 completed, 0 marked as failed)
[I 2026-01-18 16:25:53,956] A new study created in RDB with name: hpo_distilbert_resume_test
2026-01-18 16:25:54,011 - evaluation.selection.trial_finder.discovery - WARNING - No v2 study folders found in /tmp/pytest-of-codespace/pytest-120/test_resume_preserves_trials0/outputs/hpo (resolved from /tmp/pytest-of-codespace/pytest-120/test_resume_preserves_trials0/outputs/hpo)
2026-01-18 16:25:54,011 - training.hpo.core.study - INFO - [HPO] Resuming optimization for distilbert from checkpoint...
2026-01-18 16:25:54,012 - training.hpo.core.study - INFO - Loaded 0 existing trials (0 completed, 0 marked as failed)
[I 2026-01-18 16:25:54,113] A new study created in RDB with name: hpo_distilbert_interrupted_test
2026-01-18 16:25:54,145 - evaluation.selection.trial_finder.discovery - WARNING - No v2 study folders found in /tmp/pytest-of-codespace/pytest-120/test_resume_marks_running_tria0/outputs/hpo (resolved from /tmp/pytest-of-codespace/pytest-120/test_resume_marks_running_tria0/outputs/hpo)
2026-01-18 16:25:54,145 - training.hpo.core.study - INFO - [HPO] Resuming optimization for distilbert from checkpoint...
2026-01-18 16:25:54,146 - training.hpo.core.study - INFO - Loaded 0 existing trials (0 completed, 0 marked as failed)
[I 2026-01-18 16:25:54,240] A new study created in RDB with name: hpo_distilbert_no_resume_test
2026-01-18 16:25:54,262 - evaluation.selection.trial_finder.discovery - WARNING - No v2 study folders found in /tmp/pytest-of-codespace/pytest-120/test_resume_with_auto_resume_f0/outputs/hpo (resolved from /tmp/pytest-of-codespace/pytest-120/test_resume_with_auto_resume_f0/outputs/hpo)
2026-01-18 16:25:54,263 - training.hpo.core.study - INFO - [HPO] auto_resume=false: Creating new study 'hpo_distilbert_no_resume_test' (ignoring existing study)
2026-01-18 16:25:54,263 - training.hpo.core.study - INFO - [HPO] Starting optimization for distilbert with checkpointing...
[I 2026-01-18 16:25:54,354] A new study created in RDB with name: hpo_distilbert_numbering_test
2026-01-18 16:25:54,393 - evaluation.selection.trial_finder.discovery - WARNING - No v2 study folders found in /tmp/pytest-of-codespace/pytest-120/test_resume_continues_trial_nu0/outputs/hpo (resolved from /tmp/pytest-of-codespace/pytest-120/test_resume_continues_trial_nu0/outputs/hpo)
2026-01-18 16:25:54,393 - training.hpo.core.study - INFO - [HPO] Resuming optimization for distilbert from checkpoint...
2026-01-18 16:25:54,394 - training.hpo.core.study - INFO - Loaded 0 existing trials (0 completed, 0 marked as failed)
[I 2026-01-18 16:25:54,509] A new study created in RDB with name: hpo_distilbert_smoke_resume
2026-01-18 16:25:54,530 - evaluation.selection.trial_finder.discovery - WARNING - No v2 study folders found in /tmp/pytest-of-codespace/pytest-120/test_checkpoint_smoke_yaml_aut0/outputs/hpo (resolved from /tmp/pytest-of-codespace/pytest-120/test_checkpoint_smoke_yaml_aut0/outputs/hpo)
2026-01-18 16:25:54,530 - training.hpo.core.study - INFO - [HPO] Resuming optimization for distilbert from checkpoint...
2026-01-18 16:25:54,530 - training.hpo.core.study - INFO - Loaded 0 existing trials (0 completed, 0 marked as failed)
[I 2026-01-18 16:25:54,627] A new study created in RDB with name: hpo_distilbert_file_test
[I 2026-01-18 16:25:54,719] A new study created in RDB with name: hpo_distilbert_persist_test
[I 2026-01-18 16:25:54,890] A new study created in RDB with name: hpo_distilbert_move_test
2026-01-18 16:25:55,923 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 16:25:55,923 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 16:25:55,923 - infrastructure.tracking.mlflow.index.version_counter - INFO - [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:7cc3f049175243f692c605769296b4e812adcb6c65bbf..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-18 16:25:55,924 - infrastructure.tracking.mlflow.index.version_counter - INFO - [Reserve Version] Loaded 30 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-18 16:25:55,925 - infrastructure.tracking.mlflow.index.version_counter - INFO - [Reserve Version] Found 12 allocations for counter_key (after deduplication): committed=[], reserved=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], expired=[], max_committed_version=0
2026-01-18 16:25:55,925 - infrastructure.tracking.mlflow.index.version_counter - INFO - [Reserve Version] Allocation details: [(1, 'reserved', 'pending_2026'), (2, 'reserved', 'pending_2026'), (3, 'reserved', 'pending_2026'), (4, 'reserved', 'pending_2026'), (5, 'reserved', 'pending_2026'), (6, 'reserved', 'pending_2026'), (7, 'reserved', 'pending_2026'), (8, 'reserved', 'pending_2026'), (9, 'reserved', 'pending_2026'), (10, 'reserved', 'pending_2026'), (11, 'reserved', 'pending_2026'), (12, 'reserved', 'pending_2026')]
2026-01-18 16:25:55,925 - infrastructure.tracking.mlflow.index.version_counter - INFO - [Reserve Version] Reserving next version: 13 (incremented from max_committed=0, skipped 12 reserved/expired versions)
2026-01-18 16:25:55,926 - infrastructure.tracking.mlflow.index.version_counter - INFO - [Reserve Version]  Successfully reserved version 13 for counter_key resume-ner:hpo:7cc3f049175243f692c605769296b4e812a... (run_id: pending_2026...)
2026-01-18 16:25:55,930 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_setup_hpo_mlflow_run_comp0/config, raw_auto_inc_config={}
2026-01-18 16:25:55,930 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 16:25:55,935 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 16:25:55,935 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 16:25:55,936 - infrastructure.tracking.mlflow.index.version_counter - INFO - [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:9aece94eee717121a4a3d5b5d8008953a198f5d1de41f..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-18 16:25:55,936 - infrastructure.tracking.mlflow.index.version_counter - INFO - [Reserve Version] Loaded 31 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-18 16:25:55,936 - infrastructure.tracking.mlflow.index.version_counter - INFO - [Reserve Version] Found 11 allocations for counter_key (after deduplication): committed=[], reserved=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], expired=[], max_committed_version=0
2026-01-18 16:25:55,936 - infrastructure.tracking.mlflow.index.version_counter - INFO - [Reserve Version] Allocation details: [(1, 'reserved', 'pending_2026'), (2, 'reserved', 'pending_2026'), (3, 'reserved', 'pending_2026'), (4, 'reserved', 'pending_2026'), (5, 'reserved', 'pending_2026'), (6, 'reserved', 'pending_2026'), (7, 'reserved', 'pending_2026'), (8, 'reserved', 'pending_2026'), (9, 'reserved', 'pending_2026'), (10, 'reserved', 'pending_2026'), (11, 'reserved', 'pending_2026')]
2026-01-18 16:25:55,936 - infrastructure.tracking.mlflow.index.version_counter - INFO - [Reserve Version] Reserving next version: 12 (incremented from max_committed=0, skipped 11 reserved/expired versions)
2026-01-18 16:25:55,937 - infrastructure.tracking.mlflow.index.version_counter - INFO - [Reserve Version]  Successfully reserved version 12 for counter_key resume-ner:hpo:9aece94eee717121a4a3d5b5d8008953a19... (run_id: pending_2026...)
2026-01-18 16:25:55,950 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_setup_hpo_mlflow_run_trus0/project1/config, raw_auto_inc_config={}
2026-01-18 16:25:55,950 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 16:25:55,954 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_setup_hpo_mlflow_run_infe0/project/config, raw_auto_inc_config={}
2026-01-18 16:25:55,954 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 16:25:56,044 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'batch_size': 4, 'dropout': 0.2, 'weight_decay': 0.05, 'backbone': 'distilbert'}
2026-01-18 16:25:56,044 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 16:25:56,070 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-18 16:25:56,070 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 16:25:56,095 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-18 16:25:56,096 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 16:25:56,120 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-18 16:25:56,120 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 16:25:56,148 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-18 16:25:56,148 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 16:25:56,174 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-18 16:25:56,174 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 16:25:56,200 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-18 16:25:56,200 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 16:25:56,225 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-18 16:25:56,225 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 16:25:56,338 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-18 16:25:56,338 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
[I 2026-01-18 16:25:56,449] A new study created in RDB with name: test_timeout
[I 2026-01-18 16:25:56,577] Trial 0 finished with value: 0.75 and parameters: {}. Best is trial 0 with value: 0.75.
[I 2026-01-18 16:25:56,695] Trial 1 finished with value: 0.75 and parameters: {}. Best is trial 0 with value: 0.75.
[I 2026-01-18 16:25:56,812] Trial 2 finished with value: 0.75 and parameters: {}. Best is trial 0 with value: 0.75.
[I 2026-01-18 16:25:56,933] Trial 3 finished with value: 0.75 and parameters: {}. Best is trial 0 with value: 0.75.
[I 2026-01-18 16:25:57,053] Trial 4 finished with value: 0.75 and parameters: {}. Best is trial 0 with value: 0.75.
[I 2026-01-18 16:25:57,161] A new study created in RDB with name: test_mark
[I 2026-01-18 16:25:57,307] A new study created in RDB with name: test_skip_mark
2026-01-18 16:25:57,367 - training.hpo.execution.local.trial - INFO - [TRIAL] Training completed. Objective metric 'macro-f1': 0.75
2026-01-18 16:25:57,373 - training.hpo.execution.local.trial - INFO - [TRIAL] Training completed. Objective metric 'macro-f1': 0.8
2026-01-18 16:25:57,379 - training.hpo.execution.local.trial - INFO - [TRIAL] Training completed. Objective metric 'macro-f1': 0.75
2026-01-18 16:25:57,388 - infrastructure.tracking.mlflow.hash_utils - WARNING - Could not get or compute study_key_hash. study_key_hash=missing, hpo_parent_run_id=provided, configs=missing
2026-01-18 16:25:57,389 - training.hpo.execution.local.cv - WARNING - Could not build systematic run name and tags: Naming policy not available or formatting failed. Legacy fallback removed. Process type: hpo_trial, using fallback
2026-01-18 16:25:57,389 - training.hpo.execution.local.cv - WARNING - Could not create trial run: expected string or bytes-like object
2026-01-18 16:25:57,390 - infrastructure.tracking.mlflow.hash_utils - WARNING - Could not get or compute study_key_hash. study_key_hash=missing, hpo_parent_run_id=provided, configs=missing
2026-01-18 16:25:57,390 - training.hpo.execution.local.cv - ERROR - In v2 study folder study-abc12345 but trial_key_hash is None. study_key_hash=NO. Will attempt to compute hash.
2026-01-18 16:25:57,416 - training.hpo.execution.local.cv - WARNING - Could not build systematic run name and tags: Naming policy not available or formatting failed. Legacy fallback removed. Process type: hpo_trial, using fallback
2026-01-18 16:25:57,417 - training.hpo.execution.local.cv - WARNING - Could not create trial run: expected string or bytes-like object
2026-01-18 16:25:57,417 - training.hpo.execution.local.cv - INFO - [CV] Created trial folder: /tmp/pytest-of-codespace/pytest-120/test_trial_execution_with_cv_c1/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-1b593325 (trial 0)
2026-01-18 16:25:57,430 - training.hpo.execution.local.cv - WARNING - Could not build systematic run name and tags: Naming policy not available or formatting failed. Legacy fallback removed. Process type: hpo_trial, using fallback
2026-01-18 16:25:57,430 - training.hpo.execution.local.cv - WARNING - Could not create trial run: expected string or bytes-like object
2026-01-18 16:25:57,430 - infrastructure.tracking.mlflow.hash_utils - WARNING - Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
2026-01-18 16:25:57,430 - training.hpo.execution.local.cv - WARNING - Could not compute trial_key_hash
2026-01-18 16:25:57,430 - training.hpo.execution.local.cv - ERROR - In v2 study folder study-abc12345 but trial_key_hash is None. study_key_hash=YES. Will attempt to compute hash.
2026-01-18 16:25:57,430 - training.hpo.execution.local.cv - WARNING - In v2 study folder study-abc12345 but trial_key_hash is None. Attempting to recompute from trial_params...
2026-01-18 16:25:57,431 - infrastructure.tracking.mlflow.hash_utils - WARNING - Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
2026-01-18 16:25:57,452 - training.hpo.execution.local.cv - WARNING - Could not build systematic run name and tags: Naming policy not available or formatting failed. Legacy fallback removed. Process type: hpo_trial, using fallback
2026-01-18 16:25:57,452 - training.hpo.execution.local.cv - WARNING - Could not create trial run: expected string or bytes-like object
2026-01-18 16:25:57,452 - infrastructure.tracking.mlflow.hash_utils - WARNING - Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
2026-01-18 16:25:57,452 - training.hpo.execution.local.cv - WARNING - Could not compute trial_key_hash
2026-01-18 16:25:57,452 - training.hpo.execution.local.cv - ERROR - In v2 study folder study-abc12345 but trial_key_hash is None. study_key_hash=YES. Will attempt to compute hash.
2026-01-18 16:25:57,452 - training.hpo.execution.local.cv - WARNING - In v2 study folder study-abc12345 but trial_key_hash is None. Attempting to recompute from trial_params...
2026-01-18 16:25:57,452 - infrastructure.tracking.mlflow.hash_utils - WARNING - Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
2026-01-18 16:25:57,473 - training.hpo.execution.local.cv - WARNING - Could not build systematic run name and tags: Naming policy not available or formatting failed. Legacy fallback removed. Process type: hpo_trial, using fallback
2026-01-18 16:25:57,473 - training.hpo.execution.local.cv - WARNING - Could not create trial run: expected string or bytes-like object
2026-01-18 16:25:57,473 - infrastructure.tracking.mlflow.hash_utils - WARNING - Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
2026-01-18 16:25:57,473 - training.hpo.execution.local.cv - WARNING - Could not compute trial_key_hash
2026-01-18 16:25:57,474 - training.hpo.execution.local.cv - ERROR - In v2 study folder study-abc12345 but trial_key_hash is None. study_key_hash=YES. Will attempt to compute hash.
2026-01-18 16:25:57,474 - training.hpo.execution.local.cv - WARNING - In v2 study folder study-abc12345 but trial_key_hash is None. Attempting to recompute from trial_params...
2026-01-18 16:25:57,474 - infrastructure.tracking.mlflow.hash_utils - WARNING - Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
2026-01-18 16:25:57,565 - infrastructure.config.selection - WARNING - top_k_for_stable_score (5) > min_trials_per_group (3). Clamping top_k to 3.
2026-01-18 16:25:57,594 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_auto_generated_study_name0/config, raw_auto_inc_config={}
2026-01-18 16:25:57,594 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 16:25:57,597 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_custom_study_name_include0/config, raw_auto_inc_config={}
2026-01-18 16:25:57,598 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 16:25:57,601 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_auto_generated_with_varia0/config, raw_auto_inc_config={}
2026-01-18 16:25:57,601 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 16:25:57,615 - infrastructure.paths.repo - WARNING - Could not find repository root. Falling back to current working directory: /tmp/pytest-of-codespace/pytest-120/test_raises_value_error_when_n0/random/structure
2026/01/18 16:25:58 INFO mlflow.store.db.utils: Creating initial MLflow database tables...
2026/01/18 16:25:58 INFO mlflow.store.db.utils: Updating database tables
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Running upgrade  -> 451aebb31d03, add metric step
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Running upgrade 451aebb31d03 -> 90e64c465722, migrate user column to tags
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Running upgrade 90e64c465722 -> 181f10493468, allow nulls for metric values
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Running upgrade 181f10493468 -> df50e92ffc5e, Add Experiment Tags Table
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Running upgrade df50e92ffc5e -> 7ac759974ad8, Update run tags with larger limit
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Running upgrade 7ac759974ad8 -> 89d4b8295536, create latest metrics table
2026-01-18 16:25:58 INFO  [89d4b8295536_create_latest_metrics_table_py] Migration complete!
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Running upgrade 89d4b8295536 -> 2b4d017a5e9b, add model registry tables to db
2026-01-18 16:25:58 INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Adding registered_models and model_versions tables to database.
2026-01-18 16:25:58 INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Migration complete!
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Running upgrade 2b4d017a5e9b -> cfd24bdc0731, Update run status constraint with killed
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Running upgrade cfd24bdc0731 -> 0a8213491aaa, drop_duplicate_killed_constraint
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Running upgrade 0a8213491aaa -> 728d730b5ebd, add registered model tags table
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Running upgrade 728d730b5ebd -> 27a6a02d2cf1, add model version tags table
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Running upgrade 27a6a02d2cf1 -> 84291f40a231, add run_link to model_version
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Running upgrade 84291f40a231 -> a8c4a736bde6, allow nulls for run_id
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Running upgrade a8c4a736bde6 -> 39d1c3be5f05, add_is_nan_constraint_for_metrics_tables_if_necessary
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Running upgrade 39d1c3be5f05 -> c48cb773bb87, reset_default_value_for_is_nan_in_metrics_table_for_mysql
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Running upgrade c48cb773bb87 -> bd07f7e963c5, create index on run_uuid
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Running upgrade bd07f7e963c5 -> 0c779009ac13, add deleted_time field to runs table
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Running upgrade 0c779009ac13 -> cc1f77228345, change param value length to 500
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Running upgrade cc1f77228345 -> 97727af70f4d, Add creation_time and last_update_time to experiments table
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Running upgrade 97727af70f4d -> 3500859a5d39, Add Model Aliases table
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Running upgrade 3500859a5d39 -> 7f2a7d5fae7d, add datasets inputs input_tags tables
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Running upgrade 7f2a7d5fae7d -> 2d6e25af4d3e, increase max param val length from 500 to 8000
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Running upgrade 2d6e25af4d3e -> acf3f17fdcc7, add storage location field to model versions
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Running upgrade acf3f17fdcc7 -> 867495a8f9d4, add trace tables
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Running upgrade 867495a8f9d4 -> 5b0e9adcef9c, add cascade deletion to trace tables foreign keys
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Running upgrade 5b0e9adcef9c -> 4465047574b1, increase max dataset schema size
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Running upgrade 4465047574b1 -> f5a4f2784254, increase run tag value limit to 8000
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Running upgrade f5a4f2784254 -> 0584bdc529eb, add cascading deletion to datasets from experiments
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Running upgrade 0584bdc529eb -> 400f98739977, add logged model tables
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Running upgrade 400f98739977 -> 6953534de441, add step to inputs table
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Running upgrade 6953534de441 -> bda7b8c39065, increase_model_version_tag_value_limit
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Running upgrade bda7b8c39065 -> cbc13b556ace, add V3 trace schema columns
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Running upgrade cbc13b556ace -> 770bee3ae1dd, add assessments table
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Running upgrade 770bee3ae1dd -> a1b2c3d4e5f6, add spans table
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Running upgrade a1b2c3d4e5f6 -> de4033877273, create entity_associations table
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Running upgrade de4033877273 -> 1a0cddfcaa16, Add webhooks and webhook_events tables
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Running upgrade 1a0cddfcaa16 -> 534353b11cbc, add scorer tables
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Running upgrade 534353b11cbc -> 71994744cf8e, add evaluation datasets
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Running upgrade 71994744cf8e -> 3da73c924c2f, add outputs to dataset record
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Running upgrade 3da73c924c2f -> bf29a5ff90ea, add jobs table
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2026-01-18 16:25:58 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
2026/01/18 16:25:58 INFO mlflow.tracking.fluent: Experiment with name 'test' does not exist. Creating a new experiment.
2026-01-18 16:25:58,745 - common.shared.mlflow_setup - INFO - Using local tracking: sqlite:////workspaces/resume-ner-azureml/mlruns/mlflow.db
2026-01-18 16:25:58 INFO  [common.shared.mlflow_setup] Using local tracking: sqlite:////workspaces/resume-ner-azureml/mlruns/mlflow.db
2026-01-18 16:25:58,749 - common.shared.mlflow_setup - INFO - Using local tracking: sqlite:////workspaces/resume-ner-azureml/mlruns/mlflow.db
2026-01-18 16:25:58 INFO  [common.shared.mlflow_setup] Using local tracking: sqlite:////workspaces/resume-ner-azureml/mlruns/mlflow.db
2026-01-18 16:25:58,807 - common.shared.mlflow_setup - INFO - Using local tracking: sqlite:////workspaces/resume-ner-azureml/mlruns/mlflow.db
2026-01-18 16:25:58 INFO  [common.shared.mlflow_setup] Using local tracking: sqlite:////workspaces/resume-ner-azureml/mlruns/mlflow.db
2026-01-18 16:25:58,809 - common.shared.mlflow_setup - INFO - Using local tracking: sqlite:////workspaces/resume-ner-azureml/mlruns/mlflow.db
2026-01-18 16:25:58 INFO  [common.shared.mlflow_setup] Using local tracking: sqlite:////workspaces/resume-ner-azureml/mlruns/mlflow.db
2026-01-18 16:25:58,871 - common.shared.mlflow_setup - INFO - Using local tracking: sqlite:////workspaces/resume-ner-azureml/mlruns/mlflow.db
2026-01-18 16:25:58 INFO  [common.shared.mlflow_setup] Using local tracking: sqlite:////workspaces/resume-ner-azureml/mlruns/mlflow.db
2026-01-18 16:25:58,874 - common.shared.mlflow_setup - INFO - Using local tracking: sqlite:////workspaces/resume-ner-azureml/mlruns/mlflow.db
2026-01-18 16:25:58 INFO  [common.shared.mlflow_setup] Using local tracking: sqlite:////workspaces/resume-ner-azureml/mlruns/mlflow.db
2026-01-18 16:25:59,755 - infrastructure.shared.backup - INFO - Backed up HPO checkpoint database to Drive: study.db
2026-01-18 16:25:59 INFO  [infrastructure.shared.backup] Backed up HPO checkpoint database to Drive: study.db
2026-01-18 16:25:59,755 - infrastructure.shared.backup - INFO - Backed up entire study folder to Drive: study-c3659fea
2026-01-18 16:25:59 INFO  [infrastructure.shared.backup] Backed up entire study folder to Drive: study-c3659fea
2026-01-18 16:25:59,758 - infrastructure.shared.backup - INFO - Backed up HPO checkpoint database to Drive: study.db
2026-01-18 16:25:59 INFO  [infrastructure.shared.backup] Backed up HPO checkpoint database to Drive: study.db
2026-01-18 16:25:59,759 - infrastructure.shared.backup - INFO - Backed up entire study folder to Drive: study-c3659fea
2026-01-18 16:25:59 INFO  [infrastructure.shared.backup] Backed up entire study folder to Drive: study-c3659fea
2026-01-18 16:25:59,762 - infrastructure.shared.backup - INFO - Backed up HPO checkpoint database to Drive: study.db
2026-01-18 16:25:59 INFO  [infrastructure.shared.backup] Backed up HPO checkpoint database to Drive: study.db
2026-01-18 16:25:59,762 - infrastructure.shared.backup - INFO - Backed up entire study folder to Drive: study-c3659fea
2026-01-18 16:25:59 INFO  [infrastructure.shared.backup] Backed up entire study folder to Drive: study-c3659fea
2026-01-18 16:25:59,764 - evaluation.selection.trial_finder.discovery - WARNING - No v2 study folders found in /tmp/pytest-of-codespace/pytest-120/test_backup_warns_when_study_f0/outputs/hpo/colab/distilbert (resolved from /tmp/pytest-of-codespace/pytest-120/test_backup_warns_when_study_f0/outputs/hpo/colab/distilbert)
2026-01-18 16:25:59 WARNI [evaluation.selection.trial_finder.discovery] No v2 study folders found in /tmp/pytest-of-codespace/pytest-120/test_backup_warns_when_study_f0/outputs/hpo/colab/distilbert (resolved from /tmp/pytest-of-codespace/pytest-120/test_backup_warns_when_study_f0/outputs/hpo/colab/distilbert)
2026-01-18 16:25:59,764 - infrastructure.shared.backup - WARNING - Study folder not found in /tmp/pytest-of-codespace/pytest-120/test_backup_warns_when_study_f0/outputs/hpo/colab/distilbert (resolved from /tmp/pytest-of-codespace/pytest-120/test_backup_warns_when_study_f0/outputs/hpo/colab/distilbert)
2026-01-18 16:25:59 WARNI [infrastructure.shared.backup] Study folder not found in /tmp/pytest-of-codespace/pytest-120/test_backup_warns_when_study_f0/outputs/hpo/colab/distilbert (resolved from /tmp/pytest-of-codespace/pytest-120/test_backup_warns_when_study_f0/outputs/hpo/colab/distilbert)
2026-01-18 16:25:59,767 - infrastructure.shared.backup - WARNING - study.db not found: /tmp/pytest-of-codespace/pytest-120/test_backup_checks_file_existe0/outputs/hpo/colab/distilbert/study-c3659fea/study.db
2026-01-18 16:25:59 WARNI [infrastructure.shared.backup] study.db not found: /tmp/pytest-of-codespace/pytest-120/test_backup_checks_file_existe0/outputs/hpo/colab/distilbert/study-c3659fea/study.db
2026-01-18 16:25:59,767 - infrastructure.shared.backup - INFO - Backed up entire study folder to Drive: study-c3659fea
2026-01-18 16:25:59 INFO  [infrastructure.shared.backup] Backed up entire study folder to Drive: study-c3659fea
2026-01-18 16:25:59,794 - infrastructure.shared.backup - WARNING - Incremental backup error for study.db (trial 0): Backup failed
2026-01-18 16:25:59 WARNI [infrastructure.shared.backup] Incremental backup error for study.db (trial 0): Backup failed
2026-01-18 16:25:59,801 - infrastructure.shared.backup - INFO - Immediate backup successful: checkpoint.tar.gz
2026-01-18 16:25:59 INFO  [infrastructure.shared.backup] Immediate backup successful: checkpoint.tar.gz
2026-01-18 16:25:59,804 - infrastructure.shared.backup - INFO - Immediate backup successful: checkpoint_dir
2026-01-18 16:25:59 INFO  [infrastructure.shared.backup] Immediate backup successful: checkpoint_dir
2026-01-18 16:25:59,817 - infrastructure.shared.backup - WARNING - Immediate backup failed: checkpoint.tar.gz
2026-01-18 16:25:59 WARNI [infrastructure.shared.backup] Immediate backup failed: checkpoint.tar.gz
2026-01-18 16:25:59,819 - infrastructure.shared.backup - WARNING - Immediate backup error for checkpoint.tar.gz: Backup error
2026-01-18 16:25:59 WARNI [infrastructure.shared.backup] Immediate backup error for checkpoint.tar.gz: Backup error
2026-01-18 16:25:59,843 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:25:59 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:25:59,851 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:25:59 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:25:59,861 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:25:59 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:25:59,863 - evaluation.selection.artifact_unified.acquisition - INFO - Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_id_..., study_key_hash=N/A..., trial_key_hash=N/A...
2026-01-18 16:25:59 INFO  [evaluation.selection.artifact_unified.acquisition] Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_id_..., study_key_hash=N/A..., trial_key_hash=N/A...
2026-01-18 16:25:59,870 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:25:59 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:25:59,876 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:25:59 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:25:59,882 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:25:59 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:25:59,885 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:25:59 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:25:59,892 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:25:59 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:25:59,895 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:25:59 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:25:59,901 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:25:59 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:25:59,904 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:25:59 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:25:59,910 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:25:59 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:25:59,913 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:25:59 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:25:59,919 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:25:59 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:25:59,925 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:25:59 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:25:59,931 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:25:59 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:25:59,937 - evaluation.selection.artifact_unified.acquisition - INFO - Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_123..., study_key_hash=study123..., trial_key_hash=trial876...
2026-01-18 16:25:59 INFO  [evaluation.selection.artifact_unified.acquisition] Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_123..., study_key_hash=study123..., trial_key_hash=trial876...
2026-01-18 16:25:59,937 - evaluation.selection.artifact_unified.acquisition - INFO - Selected artifact run: artifact_run_id=refit_run_12..., trial_run_id=test_run_123...
2026-01-18 16:25:59 INFO  [evaluation.selection.artifact_unified.acquisition] Selected artifact run: artifact_run_id=refit_run_12..., trial_run_id=test_run_123...
2026-01-18 16:25:59,938 - evaluation.selection.artifact_unified.acquisition - INFO - Artifact acquisition config for checkpoint: priority=['local'], local.enabled=True, local.validate=True, drive.enabled=True, mlflow.enabled=True
2026-01-18 16:25:59 INFO  [evaluation.selection.artifact_unified.acquisition] Artifact acquisition config for checkpoint: priority=['local'], local.enabled=True, local.validate=True, drive.enabled=True, mlflow.enabled=True
2026-01-18 16:25:59,938 - evaluation.selection.artifact_unified.acquisition - INFO - Acquiring artifact from local: run_id=refit_run_12..., artifact_kind=checkpoint, backbone=distilbert
2026-01-18 16:25:59 INFO  [evaluation.selection.artifact_unified.acquisition] Acquiring artifact from local: run_id=refit_run_12..., artifact_kind=checkpoint, backbone=distilbert
2026-01-18 16:25:59,938 - evaluation.selection.artifact_unified.acquisition - ERROR - Artifact acquisition failed: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_search_roots_used_in_loca0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/acquisition.py", line 249, in acquire_artifact
    acquired_path = _acquire_from_location(
  File "/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/acquisition.py", line 342, in _acquire_from_location
    destination = _build_artifact_destination(
  File "/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/acquisition.py", line 487, in _build_artifact_destination
    base_dir = resolve_output_path(
  File "/workspaces/resume-ner-azureml/src/infrastructure/paths/resolve.py", line 88, in resolve_output_path
    paths_config = load_paths_config(config_dir)
  File "/workspaces/resume-ner-azureml/src/infrastructure/paths/config.py", line 83, in load_paths_config
    raise FileNotFoundError(
FileNotFoundError: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_search_roots_used_in_loca0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.
2026-01-18 16:25:59 ERROR [evaluation.selection.artifact_unified.acquisition] Artifact acquisition failed: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_search_roots_used_in_loca0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/acquisition.py", line 249, in acquire_artifact
    acquired_path = _acquire_from_location(
  File "/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/acquisition.py", line 342, in _acquire_from_location
    destination = _build_artifact_destination(
  File "/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/acquisition.py", line 487, in _build_artifact_destination
    base_dir = resolve_output_path(
  File "/workspaces/resume-ner-azureml/src/infrastructure/paths/resolve.py", line 88, in resolve_output_path
    paths_config = load_paths_config(config_dir)
  File "/workspaces/resume-ner-azureml/src/infrastructure/paths/config.py", line 83, in load_paths_config
    raise FileNotFoundError(
FileNotFoundError: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_search_roots_used_in_loca0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.
2026-01-18 16:25:59,976 - evaluation.selection.artifact_unified.acquisition - INFO - Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_123..., study_key_hash=study123..., trial_key_hash=trial876...
2026-01-18 16:25:59 INFO  [evaluation.selection.artifact_unified.acquisition] Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_123..., study_key_hash=study123..., trial_key_hash=trial876...
2026-01-18 16:25:59,976 - evaluation.selection.artifact_unified.acquisition - INFO - Selected artifact run: artifact_run_id=refit_run_12..., trial_run_id=test_run_123...
2026-01-18 16:25:59 INFO  [evaluation.selection.artifact_unified.acquisition] Selected artifact run: artifact_run_id=refit_run_12..., trial_run_id=test_run_123...
2026-01-18 16:25:59,976 - evaluation.selection.artifact_unified.acquisition - INFO - Artifact acquisition config for checkpoint: priority=['mlflow', 'local'], local.enabled=True, local.validate=True, drive.enabled=True, mlflow.enabled=True
2026-01-18 16:25:59 INFO  [evaluation.selection.artifact_unified.acquisition] Artifact acquisition config for checkpoint: priority=['mlflow', 'local'], local.enabled=True, local.validate=True, drive.enabled=True, mlflow.enabled=True
2026-01-18 16:25:59,976 - evaluation.selection.artifact_unified.acquisition - INFO - Acquiring artifact from mlflow: run_id=refit_run_12..., artifact_kind=checkpoint, backbone=distilbert
2026-01-18 16:25:59 INFO  [evaluation.selection.artifact_unified.acquisition] Acquiring artifact from mlflow: run_id=refit_run_12..., artifact_kind=checkpoint, backbone=distilbert
2026-01-18 16:25:59,976 - evaluation.selection.artifact_unified.acquisition - ERROR - Artifact acquisition failed: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_artifact_kinds_priority_o0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/acquisition.py", line 249, in acquire_artifact
    acquired_path = _acquire_from_location(
  File "/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/acquisition.py", line 342, in _acquire_from_location
    destination = _build_artifact_destination(
  File "/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/acquisition.py", line 487, in _build_artifact_destination
    base_dir = resolve_output_path(
  File "/workspaces/resume-ner-azureml/src/infrastructure/paths/resolve.py", line 88, in resolve_output_path
    paths_config = load_paths_config(config_dir)
  File "/workspaces/resume-ner-azureml/src/infrastructure/paths/config.py", line 83, in load_paths_config
    raise FileNotFoundError(
FileNotFoundError: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_artifact_kinds_priority_o0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.
2026-01-18 16:25:59 ERROR [evaluation.selection.artifact_unified.acquisition] Artifact acquisition failed: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_artifact_kinds_priority_o0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/acquisition.py", line 249, in acquire_artifact
    acquired_path = _acquire_from_location(
  File "/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/acquisition.py", line 342, in _acquire_from_location
    destination = _build_artifact_destination(
  File "/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/acquisition.py", line 487, in _build_artifact_destination
    base_dir = resolve_output_path(
  File "/workspaces/resume-ner-azureml/src/infrastructure/paths/resolve.py", line 88, in resolve_output_path
    paths_config = load_paths_config(config_dir)
  File "/workspaces/resume-ner-azureml/src/infrastructure/paths/config.py", line 83, in load_paths_config
    raise FileNotFoundError(
FileNotFoundError: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_artifact_kinds_priority_o0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.
2026-01-18 16:25:59,980 - evaluation.selection.artifact_unified.acquisition - INFO - Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_123..., study_key_hash=study123..., trial_key_hash=trial876...
2026-01-18 16:25:59 INFO  [evaluation.selection.artifact_unified.acquisition] Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_123..., study_key_hash=study123..., trial_key_hash=trial876...
2026-01-18 16:25:59,980 - evaluation.selection.artifact_unified.acquisition - INFO - Selected artifact run: artifact_run_id=refit_run_12..., trial_run_id=test_run_123...
2026-01-18 16:25:59 INFO  [evaluation.selection.artifact_unified.acquisition] Selected artifact run: artifact_run_id=refit_run_12..., trial_run_id=test_run_123...
2026-01-18 16:25:59,980 - evaluation.selection.artifact_unified.acquisition - INFO - Artifact acquisition config for checkpoint: priority=['local', 'mlflow'], local.enabled=True, local.validate=True, drive.enabled=True, mlflow.enabled=True
2026-01-18 16:25:59 INFO  [evaluation.selection.artifact_unified.acquisition] Artifact acquisition config for checkpoint: priority=['local', 'mlflow'], local.enabled=True, local.validate=True, drive.enabled=True, mlflow.enabled=True
2026-01-18 16:25:59,980 - evaluation.selection.artifact_unified.acquisition - INFO - Acquiring artifact from local: run_id=refit_run_12..., artifact_kind=checkpoint, backbone=distilbert
2026-01-18 16:25:59 INFO  [evaluation.selection.artifact_unified.acquisition] Acquiring artifact from local: run_id=refit_run_12..., artifact_kind=checkpoint, backbone=distilbert
2026-01-18 16:25:59,981 - evaluation.selection.artifact_unified.acquisition - ERROR - Artifact acquisition failed: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_artifact_kinds_fallback_t0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/acquisition.py", line 249, in acquire_artifact
    acquired_path = _acquire_from_location(
  File "/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/acquisition.py", line 342, in _acquire_from_location
    destination = _build_artifact_destination(
  File "/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/acquisition.py", line 487, in _build_artifact_destination
    base_dir = resolve_output_path(
  File "/workspaces/resume-ner-azureml/src/infrastructure/paths/resolve.py", line 88, in resolve_output_path
    paths_config = load_paths_config(config_dir)
  File "/workspaces/resume-ner-azureml/src/infrastructure/paths/config.py", line 83, in load_paths_config
    raise FileNotFoundError(
FileNotFoundError: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_artifact_kinds_fallback_t0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.
2026-01-18 16:25:59 ERROR [evaluation.selection.artifact_unified.acquisition] Artifact acquisition failed: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_artifact_kinds_fallback_t0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/acquisition.py", line 249, in acquire_artifact
    acquired_path = _acquire_from_location(
  File "/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/acquisition.py", line 342, in _acquire_from_location
    destination = _build_artifact_destination(
  File "/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/acquisition.py", line 487, in _build_artifact_destination
    base_dir = resolve_output_path(
  File "/workspaces/resume-ner-azureml/src/infrastructure/paths/resolve.py", line 88, in resolve_output_path
    paths_config = load_paths_config(config_dir)
  File "/workspaces/resume-ner-azureml/src/infrastructure/paths/config.py", line 83, in load_paths_config
    raise FileNotFoundError(
FileNotFoundError: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_artifact_kinds_fallback_t0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.
2026-01-18 16:25:59,990 - evaluation.selection.artifact_unified.acquisition - INFO - Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_123..., study_key_hash=study123..., trial_key_hash=trial876...
2026-01-18 16:25:59 INFO  [evaluation.selection.artifact_unified.acquisition] Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_123..., study_key_hash=study123..., trial_key_hash=trial876...
2026-01-18 16:25:59,991 - evaluation.selection.artifact_unified.acquisition - INFO - Selected artifact run: artifact_run_id=refit_run_12..., trial_run_id=test_run_123...
2026-01-18 16:25:59 INFO  [evaluation.selection.artifact_unified.acquisition] Selected artifact run: artifact_run_id=refit_run_12..., trial_run_id=test_run_123...
2026-01-18 16:25:59,991 - evaluation.selection.artifact_unified.acquisition - INFO - Artifact acquisition config for checkpoint: priority=['local', 'drive', 'mlflow'], local.enabled=True, local.validate=True, drive.enabled=False, mlflow.enabled=False
2026-01-18 16:25:59 INFO  [evaluation.selection.artifact_unified.acquisition] Artifact acquisition config for checkpoint: priority=['local', 'drive', 'mlflow'], local.enabled=True, local.validate=True, drive.enabled=False, mlflow.enabled=False
2026-01-18 16:25:59,991 - evaluation.selection.artifact_unified.acquisition - INFO - Acquiring artifact from local: run_id=refit_run_12..., artifact_kind=checkpoint, backbone=distilbert
2026-01-18 16:25:59 INFO  [evaluation.selection.artifact_unified.acquisition] Acquiring artifact from local: run_id=refit_run_12..., artifact_kind=checkpoint, backbone=distilbert
2026-01-18 16:25:59,991 - evaluation.selection.artifact_unified.acquisition - ERROR - Artifact acquisition failed: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_config_with_disabled_sour0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/acquisition.py", line 249, in acquire_artifact
    acquired_path = _acquire_from_location(
  File "/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/acquisition.py", line 342, in _acquire_from_location
    destination = _build_artifact_destination(
  File "/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/acquisition.py", line 487, in _build_artifact_destination
    base_dir = resolve_output_path(
  File "/workspaces/resume-ner-azureml/src/infrastructure/paths/resolve.py", line 88, in resolve_output_path
    paths_config = load_paths_config(config_dir)
  File "/workspaces/resume-ner-azureml/src/infrastructure/paths/config.py", line 83, in load_paths_config
    raise FileNotFoundError(
FileNotFoundError: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_config_with_disabled_sour0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.
2026-01-18 16:25:59 ERROR [evaluation.selection.artifact_unified.acquisition] Artifact acquisition failed: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_config_with_disabled_sour0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/acquisition.py", line 249, in acquire_artifact
    acquired_path = _acquire_from_location(
  File "/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/acquisition.py", line 342, in _acquire_from_location
    destination = _build_artifact_destination(
  File "/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/acquisition.py", line 487, in _build_artifact_destination
    base_dir = resolve_output_path(
  File "/workspaces/resume-ner-azureml/src/infrastructure/paths/resolve.py", line 88, in resolve_output_path
    paths_config = load_paths_config(config_dir)
  File "/workspaces/resume-ner-azureml/src/infrastructure/paths/config.py", line 83, in load_paths_config
    raise FileNotFoundError(
FileNotFoundError: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_config_with_disabled_sour0/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.
2026-01-18 16:26:00,001 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:26:00 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:26:00,008 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:26:00 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:26:00,014 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:26:00 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:26:00,020 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:26:00 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:26:00,026 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:26:00 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:26:00,032 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:26:00 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 16:26:00,137 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-18 16:26:00 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-18 16:26:00,137 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-18 16:26:00 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-18 16:26:00,137 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 1
2026-01-18 16:26:00 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 1
2026-01-18 16:26:00,137 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-18 16:26:00 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-18 16:26:00,137 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30
2026-01-18 16:26:00 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.70, Latency=0.30
2026-01-18 16:26:00,137 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from config file, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 16:26:00 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from config file, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 16:26:00,137 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-18 16:26:00 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-18 16:26:00,317 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-18 16:26:00 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-18 16:26:00,317 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-18 16:26:00 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-18 16:26:00,317 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 1
2026-01-18 16:26:00 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 1
2026-01-18 16:26:00,317 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-18 16:26:00 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-18 16:26:00,317 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30
2026-01-18 16:26:00 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.70, Latency=0.30
2026-01-18 16:26:00,317 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: median (from config file, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 16:26:00 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: median (from config file, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 16:26:00,317 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-18 16:26:00 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-18 16:26:00,809 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-18 16:26:00 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-18 16:26:00,809 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-18 16:26:00 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-18 16:26:00,809 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 1
2026-01-18 16:26:00 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 1
2026-01-18 16:26:00,809 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-18 16:26:00 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-18 16:26:00,809 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30
2026-01-18 16:26:00 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.70, Latency=0.30
2026-01-18 16:26:00,810 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: mean (from config file, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 16:26:00 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: mean (from config file, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 16:26:00,810 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-18 16:26:00 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-18 16:26:00,969 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-18 16:26:00 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-18 16:26:00,969 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-18 16:26:00 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-18 16:26:00,969 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 1
2026-01-18 16:26:00 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 1
2026-01-18 16:26:00,969 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-18 16:26:00 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-18 16:26:00,969 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.80, Latency=0.20
2026-01-18 16:26:00 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.80, Latency=0.20
2026-01-18 16:26:00,969 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 16:26:00 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 16:26:00,969 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-18 16:26:00 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-18 16:26:01,123 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-18 16:26:01 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-18 16:26:01,123 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-18 16:26:01 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-18 16:26:01,123 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 1
2026-01-18 16:26:01 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 1
2026-01-18 16:26:01,123 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-18 16:26:01 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-18 16:26:01,124 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30
2026-01-18 16:26:01 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.70, Latency=0.30
2026-01-18 16:26:01,124 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 16:26:01 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 16:26:01,124 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-18 16:26:01 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-18 16:26:01,277 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-18 16:26:01 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-18 16:26:01,277 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-18 16:26:01 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-18 16:26:01,278 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 1
2026-01-18 16:26:01 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 1
2026-01-18 16:26:01,278 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-18 16:26:01 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-18 16:26:01,278 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30
2026-01-18 16:26:01 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.70, Latency=0.30
2026-01-18 16:26:01,278 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 16:26:01 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 16:26:01,278 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-18 16:26:01 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-18 16:26:01,841 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-18 16:26:01 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-18 16:26:01,841 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-18 16:26:01 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-18 16:26:01,841 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 1
2026-01-18 16:26:01 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 1
2026-01-18 16:26:01,841 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-18 16:26:01 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-18 16:26:01,841 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30
2026-01-18 16:26:01 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.70, Latency=0.30
2026-01-18 16:26:01,841 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: median (from config file, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 16:26:01 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: median (from config file, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 16:26:01,841 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-18 16:26:01 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-18 16:26:02,015 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-18 16:26:02 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-18 16:26:02,015 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-18 16:26:02 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-18 16:26:02,015 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 2
2026-01-18 16:26:02 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 2
2026-01-18 16:26:02,015 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-18 16:26:02 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-18 16:26:02,015 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30
2026-01-18 16:26:02 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.70, Latency=0.30
2026-01-18 16:26:02,015 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 16:26:02 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 16:26:02,015 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-18 16:26:02 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-18 16:26:02,165 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-18 16:26:02 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-18 16:26:02,165 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-18 16:26:02 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-18 16:26:02,165 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 2
2026-01-18 16:26:02 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 2
2026-01-18 16:26:02,165 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-18 16:26:02 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-18 16:26:02,165 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30
2026-01-18 16:26:02 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.70, Latency=0.30
2026-01-18 16:26:02,165 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 16:26:02 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 16:26:02,165 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-18 16:26:02 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-18 16:26:02,318 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-18 16:26:02 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-18 16:26:02,318 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-18 16:26:02 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-18 16:26:02,318 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 2
2026-01-18 16:26:02 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 2
2026-01-18 16:26:02,318 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-18 16:26:02 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-18 16:26:02,318 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30
2026-01-18 16:26:02 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.70, Latency=0.30
2026-01-18 16:26:02,318 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 16:26:02 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 16:26:02,318 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-18 16:26:02 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-18 16:26:02,809 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-18 16:26:02 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-18 16:26:02,809 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-18 16:26:02 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-18 16:26:02,809 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 2
2026-01-18 16:26:02 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 2
2026-01-18 16:26:02,809 - evaluation.selection.mlflow_selection - INFO -   Objective metric: accuracy
2026-01-18 16:26:02 INFO  [evaluation.selection.mlflow_selection]   Objective metric: accuracy
2026-01-18 16:26:02,809 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.50, Latency=0.50
2026-01-18 16:26:02 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.50, Latency=0.50
2026-01-18 16:26:02,809 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 16:26:02 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 16:26:02,809 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-18 16:26:02 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-18 16:26:02,959 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-18 16:26:02 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-18 16:26:02,959 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-18 16:26:02 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-18 16:26:02,959 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 2
2026-01-18 16:26:02 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 2
2026-01-18 16:26:02,959 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-18 16:26:02 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-18 16:26:02,959 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30
2026-01-18 16:26:02 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.70, Latency=0.30
2026-01-18 16:26:02,959 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 16:26:02 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 16:26:02,959 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-18 16:26:02 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-18 16:26:03,114 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-18 16:26:03 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-18 16:26:03,114 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-18 16:26:03 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-18 16:26:03,114 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 2
2026-01-18 16:26:03 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 2
2026-01-18 16:26:03,114 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-18 16:26:03 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-18 16:26:03,114 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30
2026-01-18 16:26:03 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.70, Latency=0.30
2026-01-18 16:26:03,114 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 16:26:03 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 16:26:03,115 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-18 16:26:03 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-18 16:26:03,268 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-18 16:26:03 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-18 16:26:03,269 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-18 16:26:03 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-18 16:26:03,269 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 2
2026-01-18 16:26:03 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 2
2026-01-18 16:26:03,269 - evaluation.selection.mlflow_selection - INFO -   Objective metric: accuracy
2026-01-18 16:26:03 INFO  [evaluation.selection.mlflow_selection]   Objective metric: accuracy
2026-01-18 16:26:03,269 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.50, Latency=0.50
2026-01-18 16:26:03 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.50, Latency=0.50
2026-01-18 16:26:03,269 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 16:26:03 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 16:26:03,269 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-18 16:26:03 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-18 16:26:03,829 - evaluation.selection.cache - INFO - Saved best model selection cache: latest.json
2026-01-18 16:26:03 INFO  [evaluation.selection.cache] Saved best model selection cache: latest.json
2026-01-18 16:26:03,898 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-18 16:26:03 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-18 16:26:03,898 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-18 16:26:03 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-18 16:26:03,898 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 2
2026-01-18 16:26:03 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 2
2026-01-18 16:26:03,898 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-18 16:26:03 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-18 16:26:03,898 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30
2026-01-18 16:26:03 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.70, Latency=0.30
2026-01-18 16:26:03,898 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 16:26:03 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 16:26:03,898 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-18 16:26:03 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-18 16:26:04,051 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-18 16:26:04 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-18 16:26:04,051 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-18 16:26:04 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-18 16:26:04,051 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 2
2026-01-18 16:26:04 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 2
2026-01-18 16:26:04,051 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-18 16:26:04 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-18 16:26:04,051 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30
2026-01-18 16:26:04 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.70, Latency=0.30
2026-01-18 16:26:04,051 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 16:26:04 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 16:26:04,051 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-18 16:26:04 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-18 16:26:04,210 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-18 16:26:04 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-18 16:26:04,210 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-18 16:26:04 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-18 16:26:04,210 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 2
2026-01-18 16:26:04 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 2
2026-01-18 16:26:04,210 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-18 16:26:04 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-18 16:26:04,210 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30
2026-01-18 16:26:04 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.70, Latency=0.30
2026-01-18 16:26:04,210 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 16:26:04 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 16:26:04,210 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-18 16:26:04 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-18 16:26:04,686 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-18 16:26:04 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-18 16:26:04,686 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-18 16:26:04 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-18 16:26:04,686 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 2
2026-01-18 16:26:04 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 2
2026-01-18 16:26:04,686 - evaluation.selection.mlflow_selection - INFO -   Objective metric: accuracy
2026-01-18 16:26:04 INFO  [evaluation.selection.mlflow_selection]   Objective metric: accuracy
2026-01-18 16:26:04,686 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.50, Latency=0.50
2026-01-18 16:26:04 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.50, Latency=0.50
2026-01-18 16:26:04,686 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 16:26:04 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 16:26:04,686 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-18 16:26:04 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-18 16:26:05 WARNI [infrastructure.paths.resolve] Failed to load paths.yaml config: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_end_to_end_final_training0/project/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.. Using fallback logic.
2026-01-18 16:26:05 WARNI [infrastructure.paths.resolve] Failed to load paths.yaml config: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_end_to_end_final_training0/project/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.. Using fallback logic.
2026-01-18 16:26:05 WARNI [infrastructure.paths.resolve] Failed to load paths.yaml config: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_end_to_end_conversion0/project/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.. Using fallback logic.
2026-01-18 16:26:05 WARNI [infrastructure.paths.resolve] Failed to load paths.yaml config: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_cross_platform_same_spec_0/project/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.. Using fallback logic.
2026-01-18 16:26:05 WARNI [infrastructure.paths.resolve] Failed to load paths.yaml config: Paths configuration file not found: /tmp/pytest-of-codespace/pytest-120/test_cross_platform_same_spec_0/project/config/paths.yaml. Please ensure config/paths.yaml exists in the repository root.. Using fallback logic.
2026/01/18 16:26:05 INFO mlflow.tracking.fluent: Experiment with name 'test-experiment' does not exist. Creating a new experiment.
2026/01/18 16:26:05 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753565216, 1768753565216)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 350, in create_experiment
    eid = session.query(SqlExperiment).filter_by(name=name).first().experiment_id
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2759, in first
    return self.limit(1)._iter().first()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1233, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753565216, 1768753565216)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:05,236 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753565216, 1768753565216)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:05 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753565216, 1768753565216)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:05,236 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Continuing without MLflow tracking...
2026-01-18 16:26:05 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Continuing without MLflow tracking...
2026-01-18 16:26:05,238 - infrastructure.tracking.mlflow.trackers.benchmark_tracker - WARNING - [START_BENCHMARK_RUN] No valid parent run IDs found. Received: trial=None, refit=None, sweep=None. These may be timestamps or None - will query MLflow if trial_key_hash available.
2026-01-18 16:26:05 WARNI [infrastructure.tracking.mlflow.trackers.benchmark_tracker] [START_BENCHMARK_RUN] No valid parent run IDs found. Received: trial=None, refit=None, sweep=None. These may be timestamps or None - will query MLflow if trial_key_hash available.
2026-01-18 16:26:05,238 - infrastructure.tracking.mlflow.trackers.benchmark_tracker - INFO - [START_BENCHMARK_RUN] Building tags: context=None, study_key_hash=None..., trial_key_hash=None..., context.model=None, context.process_type=None
2026-01-18 16:26:05 INFO  [infrastructure.tracking.mlflow.trackers.benchmark_tracker] [START_BENCHMARK_RUN] Building tags: context=None, study_key_hash=None..., trial_key_hash=None..., context.model=None, context.process_type=None
2026-01-18 16:26:05,238 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_benchmark_tracking_enable0/config/tags.yaml, using defaults
2026-01-18 16:26:05 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_benchmark_tracking_enable0/config/tags.yaml, using defaults
2026-01-18 16:26:05,239 - infrastructure.tracking.mlflow.trackers.benchmark_tracker - INFO - [START_BENCHMARK_RUN] Built tags, grouping tags present: study_key_hash=False, trial_key_hash=False, code.model=unknown, code.stage=unknown
2026-01-18 16:26:05 INFO  [infrastructure.tracking.mlflow.trackers.benchmark_tracker] [START_BENCHMARK_RUN] Built tags, grouping tags present: study_key_hash=False, trial_key_hash=False, code.model=unknown, code.stage=unknown
2026/01/18 16:26:05 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('74619c8e115c4468a08d999cc9b5b171', 'enchanting-worm-640', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753565863, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/74619c8e115c4468a08d999cc9b5b171/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 586, in create_run
    inputs_list = self._get_run_inputs(session, [run_id])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 626, in _get_run_inputs
    ).all()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2704, in all
    return self._iter().all()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1048, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('74619c8e115c4468a08d999cc9b5b171', 'enchanting-worm-640', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753565863, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/74619c8e115c4468a08d999cc9b5b171/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:05,870 - infrastructure.tracking.mlflow.trackers.benchmark_tracker - WARNING - MLflow tracking failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('74619c8e115c4468a08d999cc9b5b171', 'enchanting-worm-640', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753565863, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/74619c8e115c4468a08d999cc9b5b171/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:05 WARNI [infrastructure.tracking.mlflow.trackers.benchmark_tracker] MLflow tracking failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('74619c8e115c4468a08d999cc9b5b171', 'enchanting-worm-640', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753565863, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/74619c8e115c4468a08d999cc9b5b171/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:05,870 - infrastructure.tracking.mlflow.trackers.benchmark_tracker - WARNING - Continuing benchmarking without MLflow tracking...
2026-01-18 16:26:05 WARNI [infrastructure.tracking.mlflow.trackers.benchmark_tracker] Continuing benchmarking without MLflow tracking...
2026/01/18 16:26:06 INFO mlflow.tracking.fluent: Experiment with name 'test-experiment' does not exist. Creating a new experiment.
2026/01/18 16:26:06 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753566760, 1768753566760)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 350, in create_experiment
    eid = session.query(SqlExperiment).filter_by(name=name).first().experiment_id
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2759, in first
    return self.limit(1)._iter().first()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1233, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753566760, 1768753566760)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:06,761 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753566760, 1768753566760)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:06 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753566760, 1768753566760)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:06,761 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Continuing without MLflow tracking...
2026-01-18 16:26:06 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Continuing without MLflow tracking...
2026-01-18 16:26:06,762 - infrastructure.tracking.mlflow.trackers.benchmark_tracker - INFO - [Benchmark Tracker] MLflow tracking disabled for benchmark stage (tracking.benchmark.enabled=false)
2026-01-18 16:26:06 INFO  [infrastructure.tracking.mlflow.trackers.benchmark_tracker] [Benchmark Tracker] MLflow tracking disabled for benchmark stage (tracking.benchmark.enabled=false)
2026/01/18 16:26:06 INFO mlflow.tracking.fluent: Experiment with name 'test-experiment' does not exist. Creating a new experiment.
2026/01/18 16:26:06 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753566767, 1768753566767)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 350, in create_experiment
    eid = session.query(SqlExperiment).filter_by(name=name).first().experiment_id
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2759, in first
    return self.limit(1)._iter().first()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1233, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753566767, 1768753566767)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:06,769 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753566767, 1768753566767)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:06 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753566767, 1768753566767)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:06,769 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Continuing without MLflow tracking...
2026-01-18 16:26:06 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Continuing without MLflow tracking...
2026-01-18 16:26:06,771 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_training_tracking_enabled0/config/tags.yaml, using defaults
2026-01-18 16:26:06 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_training_tracking_enabled0/config/tags.yaml, using defaults
2026/01/18 16:26:06 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('8b7f6526b9284666ac533096d5b33eb2', 'spiffy-koi-18', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753566778, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/8b7f6526b9284666ac533096d5b33eb2/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 586, in create_run
    inputs_list = self._get_run_inputs(session, [run_id])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 626, in _get_run_inputs
    ).all()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2704, in all
    return self._iter().all()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1048, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('8b7f6526b9284666ac533096d5b33eb2', 'spiffy-koi-18', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753566778, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/8b7f6526b9284666ac533096d5b33eb2/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:06,784 - infrastructure.tracking.mlflow.trackers.training_tracker - WARNING - MLflow tracking failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('8b7f6526b9284666ac533096d5b33eb2', 'spiffy-koi-18', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753566778, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/8b7f6526b9284666ac533096d5b33eb2/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:06 WARNI [infrastructure.tracking.mlflow.trackers.training_tracker] MLflow tracking failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('8b7f6526b9284666ac533096d5b33eb2', 'spiffy-koi-18', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753566778, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/8b7f6526b9284666ac533096d5b33eb2/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:06,784 - infrastructure.tracking.mlflow.trackers.training_tracker - WARNING - Continuing training without MLflow tracking...
2026-01-18 16:26:06 WARNI [infrastructure.tracking.mlflow.trackers.training_tracker] Continuing training without MLflow tracking...
2026/01/18 16:26:07 INFO mlflow.tracking.fluent: Experiment with name 'test-experiment' does not exist. Creating a new experiment.
2026/01/18 16:26:07 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753567729, 1768753567729)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 350, in create_experiment
    eid = session.query(SqlExperiment).filter_by(name=name).first().experiment_id
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2759, in first
    return self.limit(1)._iter().first()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1233, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753567729, 1768753567729)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:07,730 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753567729, 1768753567729)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:07 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753567729, 1768753567729)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:07,730 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Continuing without MLflow tracking...
2026-01-18 16:26:07 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Continuing without MLflow tracking...
2026-01-18 16:26:07,732 - infrastructure.tracking.mlflow.trackers.training_tracker - INFO - [Training Tracker] MLflow tracking disabled for training stage (tracking.training.enabled=false)
2026-01-18 16:26:07 INFO  [infrastructure.tracking.mlflow.trackers.training_tracker] [Training Tracker] MLflow tracking disabled for training stage (tracking.training.enabled=false)
2026/01/18 16:26:07 INFO mlflow.tracking.fluent: Experiment with name 'test-experiment' does not exist. Creating a new experiment.
2026/01/18 16:26:07 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753567737, 1768753567737)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 350, in create_experiment
    eid = session.query(SqlExperiment).filter_by(name=name).first().experiment_id
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2759, in first
    return self.limit(1)._iter().first()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1233, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753567737, 1768753567737)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:07,738 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753567737, 1768753567737)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:07 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753567737, 1768753567737)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:07,738 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Continuing without MLflow tracking...
2026-01-18 16:26:07 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Continuing without MLflow tracking...
2026-01-18 16:26:07,740 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_conversion_tracking_enabl0/config/tags.yaml, using defaults
2026-01-18 16:26:07 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_conversion_tracking_enabl0/config/tags.yaml, using defaults
2026/01/18 16:26:07 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('0c186b9dd7ed475eb5537a95987e5409', 'sincere-snail-576', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753567744, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/0c186b9dd7ed475eb5537a95987e5409/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 586, in create_run
    inputs_list = self._get_run_inputs(session, [run_id])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 626, in _get_run_inputs
    ).all()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2704, in all
    return self._iter().all()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1048, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('0c186b9dd7ed475eb5537a95987e5409', 'sincere-snail-576', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753567744, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/0c186b9dd7ed475eb5537a95987e5409/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:07,749 - infrastructure.tracking.mlflow.trackers.conversion_tracker - WARNING - MLflow tracking failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('0c186b9dd7ed475eb5537a95987e5409', 'sincere-snail-576', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753567744, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/0c186b9dd7ed475eb5537a95987e5409/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:07 WARNI [infrastructure.tracking.mlflow.trackers.conversion_tracker] MLflow tracking failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('0c186b9dd7ed475eb5537a95987e5409', 'sincere-snail-576', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753567744, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/0c186b9dd7ed475eb5537a95987e5409/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:07,749 - infrastructure.tracking.mlflow.trackers.conversion_tracker - WARNING - Continuing conversion without MLflow tracking...
2026-01-18 16:26:07 WARNI [infrastructure.tracking.mlflow.trackers.conversion_tracker] Continuing conversion without MLflow tracking...
2026/01/18 16:26:08 INFO mlflow.tracking.fluent: Experiment with name 'test-experiment' does not exist. Creating a new experiment.
2026/01/18 16:26:08 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753568283, 1768753568283)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 350, in create_experiment
    eid = session.query(SqlExperiment).filter_by(name=name).first().experiment_id
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2759, in first
    return self.limit(1)._iter().first()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1233, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753568283, 1768753568283)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:08,285 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753568283, 1768753568283)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:08 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753568283, 1768753568283)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:08,285 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Continuing without MLflow tracking...
2026-01-18 16:26:08 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Continuing without MLflow tracking...
2026-01-18 16:26:08,286 - infrastructure.tracking.mlflow.trackers.conversion_tracker - INFO - [Conversion Tracker] MLflow tracking disabled for conversion stage (tracking.conversion.enabled=false)
2026-01-18 16:26:08 INFO  [infrastructure.tracking.mlflow.trackers.conversion_tracker] [Conversion Tracker] MLflow tracking disabled for conversion stage (tracking.conversion.enabled=false)
2026/01/18 16:26:08 INFO mlflow.tracking.fluent: Experiment with name 'test-experiment' does not exist. Creating a new experiment.
2026/01/18 16:26:08 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753568291, 1768753568291)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 350, in create_experiment
    eid = session.query(SqlExperiment).filter_by(name=name).first().experiment_id
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2759, in first
    return self.limit(1)._iter().first()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1233, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753568291, 1768753568291)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:08,293 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753568291, 1768753568291)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:08 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753568291, 1768753568291)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:08,293 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Continuing without MLflow tracking...
2026-01-18 16:26:08 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Continuing without MLflow tracking...
2026-01-18 16:26:08,294 - infrastructure.tracking.mlflow.trackers.benchmark_tracker - WARNING - [START_BENCHMARK_RUN] No valid parent run IDs found. Received: trial=None, refit=None, sweep=None. These may be timestamps or None - will query MLflow if trial_key_hash available.
2026-01-18 16:26:08 WARNI [infrastructure.tracking.mlflow.trackers.benchmark_tracker] [START_BENCHMARK_RUN] No valid parent run IDs found. Received: trial=None, refit=None, sweep=None. These may be timestamps or None - will query MLflow if trial_key_hash available.
2026-01-18 16:26:08,295 - infrastructure.tracking.mlflow.trackers.benchmark_tracker - INFO - [START_BENCHMARK_RUN] Building tags: context=None, study_key_hash=None..., trial_key_hash=None..., context.model=None, context.process_type=None
2026-01-18 16:26:08 INFO  [infrastructure.tracking.mlflow.trackers.benchmark_tracker] [START_BENCHMARK_RUN] Building tags: context=None, study_key_hash=None..., trial_key_hash=None..., context.model=None, context.process_type=None
2026-01-18 16:26:08,295 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_benchmark_log_artifacts_d0/config/tags.yaml, using defaults
2026-01-18 16:26:08 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_benchmark_log_artifacts_d0/config/tags.yaml, using defaults
2026-01-18 16:26:08,295 - infrastructure.tracking.mlflow.trackers.benchmark_tracker - INFO - [START_BENCHMARK_RUN] Built tags, grouping tags present: study_key_hash=False, trial_key_hash=False, code.model=unknown, code.stage=unknown
2026-01-18 16:26:08 INFO  [infrastructure.tracking.mlflow.trackers.benchmark_tracker] [START_BENCHMARK_RUN] Built tags, grouping tags present: study_key_hash=False, trial_key_hash=False, code.model=unknown, code.stage=unknown
2026/01/18 16:26:08 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('f9c48d8eec72415885b66cd9c84cae79', 'serious-pug-998', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753568299, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/f9c48d8eec72415885b66cd9c84cae79/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 586, in create_run
    inputs_list = self._get_run_inputs(session, [run_id])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 626, in _get_run_inputs
    ).all()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2704, in all
    return self._iter().all()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1048, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('f9c48d8eec72415885b66cd9c84cae79', 'serious-pug-998', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753568299, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/f9c48d8eec72415885b66cd9c84cae79/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:08,303 - infrastructure.tracking.mlflow.trackers.benchmark_tracker - WARNING - MLflow tracking failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('f9c48d8eec72415885b66cd9c84cae79', 'serious-pug-998', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753568299, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/f9c48d8eec72415885b66cd9c84cae79/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:08 WARNI [infrastructure.tracking.mlflow.trackers.benchmark_tracker] MLflow tracking failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('f9c48d8eec72415885b66cd9c84cae79', 'serious-pug-998', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753568299, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/f9c48d8eec72415885b66cd9c84cae79/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:08,304 - infrastructure.tracking.mlflow.trackers.benchmark_tracker - WARNING - Continuing benchmarking without MLflow tracking...
2026-01-18 16:26:08 WARNI [infrastructure.tracking.mlflow.trackers.benchmark_tracker] Continuing benchmarking without MLflow tracking...
2026/01/18 16:26:08 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('b04cd809cf5e4f56a97c09057e7f690b', 'upbeat-stag-434', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753568307, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/b04cd809cf5e4f56a97c09057e7f690b/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 586, in create_run
    inputs_list = self._get_run_inputs(session, [run_id])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 626, in _get_run_inputs
    ).all()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2704, in all
    return self._iter().all()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1048, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('b04cd809cf5e4f56a97c09057e7f690b', 'upbeat-stag-434', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753568307, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/b04cd809cf5e4f56a97c09057e7f690b/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:08,312 - infrastructure.tracking.mlflow.trackers.benchmark_tracker - WARNING - Could not log benchmark results to MLflow: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('b04cd809cf5e4f56a97c09057e7f690b', 'upbeat-stag-434', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753568307, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/b04cd809cf5e4f56a97c09057e7f690b/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:08 WARNI [infrastructure.tracking.mlflow.trackers.benchmark_tracker] Could not log benchmark results to MLflow: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('b04cd809cf5e4f56a97c09057e7f690b', 'upbeat-stag-434', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753568307, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/b04cd809cf5e4f56a97c09057e7f690b/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026/01/18 16:26:08 INFO mlflow.tracking.fluent: Experiment with name 'test-experiment' does not exist. Creating a new experiment.
2026/01/18 16:26:08 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753568317, 1768753568317)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 350, in create_experiment
    eid = session.query(SqlExperiment).filter_by(name=name).first().experiment_id
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2759, in first
    return self.limit(1)._iter().first()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1233, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753568317, 1768753568317)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:08,319 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753568317, 1768753568317)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:08 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753568317, 1768753568317)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:08,319 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Continuing without MLflow tracking...
2026-01-18 16:26:08 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Continuing without MLflow tracking...
2026-01-18 16:26:08,321 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_training_log_checkpoint_d0/config/tags.yaml, using defaults
2026-01-18 16:26:08 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_training_log_checkpoint_d0/config/tags.yaml, using defaults
2026/01/18 16:26:08 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('dd37bdabf77948859ce2376b7b1f6f03', 'whimsical-swan-265', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753568325, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/dd37bdabf77948859ce2376b7b1f6f03/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 586, in create_run
    inputs_list = self._get_run_inputs(session, [run_id])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 626, in _get_run_inputs
    ).all()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2704, in all
    return self._iter().all()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1048, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('dd37bdabf77948859ce2376b7b1f6f03', 'whimsical-swan-265', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753568325, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/dd37bdabf77948859ce2376b7b1f6f03/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:08,329 - infrastructure.tracking.mlflow.trackers.training_tracker - WARNING - MLflow tracking failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('dd37bdabf77948859ce2376b7b1f6f03', 'whimsical-swan-265', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753568325, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/dd37bdabf77948859ce2376b7b1f6f03/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:08 WARNI [infrastructure.tracking.mlflow.trackers.training_tracker] MLflow tracking failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('dd37bdabf77948859ce2376b7b1f6f03', 'whimsical-swan-265', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753568325, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/dd37bdabf77948859ce2376b7b1f6f03/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:08,329 - infrastructure.tracking.mlflow.trackers.training_tracker - WARNING - Continuing training without MLflow tracking...
2026-01-18 16:26:08 WARNI [infrastructure.tracking.mlflow.trackers.training_tracker] Continuing training without MLflow tracking...
2026/01/18 16:26:08 INFO mlflow.tracking.fluent: Experiment with name 'test-experiment' does not exist. Creating a new experiment.
2026/01/18 16:26:08 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753568335, 1768753568335)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 350, in create_experiment
    eid = session.query(SqlExperiment).filter_by(name=name).first().experiment_id
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2759, in first
    return self.limit(1)._iter().first()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1233, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753568335, 1768753568335)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:08,337 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753568335, 1768753568335)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:08 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753568335, 1768753568335)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:08,337 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Continuing without MLflow tracking...
2026-01-18 16:26:08 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Continuing without MLflow tracking...
2026-01-18 16:26:08,338 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_training_log_metrics_json0/config/tags.yaml, using defaults
2026-01-18 16:26:08 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_training_log_metrics_json0/config/tags.yaml, using defaults
2026/01/18 16:26:08 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('c4bd32232c4a45cbba83fe20b579cc3b', 'unruly-grouse-361', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753568342, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/c4bd32232c4a45cbba83fe20b579cc3b/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 586, in create_run
    inputs_list = self._get_run_inputs(session, [run_id])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 626, in _get_run_inputs
    ).all()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2704, in all
    return self._iter().all()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1048, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('c4bd32232c4a45cbba83fe20b579cc3b', 'unruly-grouse-361', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753568342, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/c4bd32232c4a45cbba83fe20b579cc3b/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:08,347 - infrastructure.tracking.mlflow.trackers.training_tracker - WARNING - MLflow tracking failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('c4bd32232c4a45cbba83fe20b579cc3b', 'unruly-grouse-361', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753568342, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/c4bd32232c4a45cbba83fe20b579cc3b/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:08 WARNI [infrastructure.tracking.mlflow.trackers.training_tracker] MLflow tracking failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('c4bd32232c4a45cbba83fe20b579cc3b', 'unruly-grouse-361', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753568342, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/c4bd32232c4a45cbba83fe20b579cc3b/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:08,347 - infrastructure.tracking.mlflow.trackers.training_tracker - WARNING - Continuing training without MLflow tracking...
2026-01-18 16:26:08 WARNI [infrastructure.tracking.mlflow.trackers.training_tracker] Continuing training without MLflow tracking...
2026-01-18 16:26:08,347 - infrastructure.tracking.mlflow.artifacts.uploader - INFO - Creating checkpoint archive from /tmp/pytest-of-codespace/pytest-120/test_training_log_metrics_json0/outputs/final_training/checkpoint...
2026-01-18 16:26:08 INFO  [infrastructure.tracking.mlflow.artifacts.uploader] Creating checkpoint archive from /tmp/pytest-of-codespace/pytest-120/test_training_log_metrics_json0/outputs/final_training/checkpoint...
2026-01-18 16:26:08,348 - infrastructure.tracking.mlflow.artifacts.manager - INFO - Created checkpoint archive: /tmp/checkpoint_ss_7gr0t.tar.gz (0 files, 0.0MB)
2026-01-18 16:26:08 INFO  [infrastructure.tracking.mlflow.artifacts.manager] Created checkpoint archive: /tmp/checkpoint_ss_7gr0t.tar.gz (0 files, 0.0MB)
2026-01-18 16:26:08,348 - infrastructure.tracking.mlflow._artifacts_file - INFO - Uploading checkpoint archive (0.0MB)...
2026-01-18 16:26:08 INFO  [infrastructure.tracking.mlflow._artifacts_file] Uploading checkpoint archive (0.0MB)...
2026-01-18 16:26:08,349 - infrastructure.tracking.mlflow._artifacts_file - INFO - Successfully uploaded checkpoint archive: checkpoint_ss_7gr0t.tar.gz
2026-01-18 16:26:08 INFO  [infrastructure.tracking.mlflow._artifacts_file] Successfully uploaded checkpoint archive: checkpoint_ss_7gr0t.tar.gz
2026-01-18 16:26:08,349 - infrastructure.tracking.mlflow.artifacts.uploader - INFO - Successfully uploaded checkpoint archive: 0 files (0.0MB) for trial 0
2026-01-18 16:26:08 INFO  [infrastructure.tracking.mlflow.artifacts.uploader] Successfully uploaded checkpoint archive: 0 files (0.0MB) for trial 0
2026/01/18 16:26:08 INFO mlflow.tracking.fluent: Experiment with name 'test-experiment' does not exist. Creating a new experiment.
2026/01/18 16:26:08 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753568355, 1768753568355)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 350, in create_experiment
    eid = session.query(SqlExperiment).filter_by(name=name).first().experiment_id
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2759, in first
    return self.limit(1)._iter().first()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1233, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753568355, 1768753568355)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:08,356 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753568355, 1768753568355)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:08 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753568355, 1768753568355)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:08,356 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Continuing without MLflow tracking...
2026-01-18 16:26:08 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Continuing without MLflow tracking...
2026-01-18 16:26:08,358 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_conversion_log_onnx_model0/config/tags.yaml, using defaults
2026-01-18 16:26:08 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_conversion_log_onnx_model0/config/tags.yaml, using defaults
2026/01/18 16:26:08 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('7ec495a41e534ccb82c01b1fc8164448', 'orderly-dolphin-829', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753568362, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/7ec495a41e534ccb82c01b1fc8164448/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 586, in create_run
    inputs_list = self._get_run_inputs(session, [run_id])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 626, in _get_run_inputs
    ).all()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2704, in all
    return self._iter().all()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1048, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('7ec495a41e534ccb82c01b1fc8164448', 'orderly-dolphin-829', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753568362, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/7ec495a41e534ccb82c01b1fc8164448/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:08,368 - infrastructure.tracking.mlflow.trackers.conversion_tracker - WARNING - MLflow tracking failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('7ec495a41e534ccb82c01b1fc8164448', 'orderly-dolphin-829', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753568362, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/7ec495a41e534ccb82c01b1fc8164448/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:08 WARNI [infrastructure.tracking.mlflow.trackers.conversion_tracker] MLflow tracking failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('7ec495a41e534ccb82c01b1fc8164448', 'orderly-dolphin-829', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753568362, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/7ec495a41e534ccb82c01b1fc8164448/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:08,368 - infrastructure.tracking.mlflow.trackers.conversion_tracker - WARNING - Continuing conversion without MLflow tracking...
2026-01-18 16:26:08 WARNI [infrastructure.tracking.mlflow.trackers.conversion_tracker] Continuing conversion without MLflow tracking...
2026/01/18 16:26:08 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('2179e1e0f055499abee716d00d537ec4', 'resilient-kite-124', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753568372, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/2179e1e0f055499abee716d00d537ec4/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 586, in create_run
    inputs_list = self._get_run_inputs(session, [run_id])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 626, in _get_run_inputs
    ).all()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2704, in all
    return self._iter().all()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1048, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('2179e1e0f055499abee716d00d537ec4', 'resilient-kite-124', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753568372, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/2179e1e0f055499abee716d00d537ec4/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:08,377 - infrastructure.tracking.mlflow.trackers.conversion_tracker - WARNING - Could not log conversion results to MLflow: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('2179e1e0f055499abee716d00d537ec4', 'resilient-kite-124', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753568372, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/2179e1e0f055499abee716d00d537ec4/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:08 WARNI [infrastructure.tracking.mlflow.trackers.conversion_tracker] Could not log conversion results to MLflow: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('2179e1e0f055499abee716d00d537ec4', 'resilient-kite-124', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753568372, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/2179e1e0f055499abee716d00d537ec4/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026/01/18 16:26:08 INFO mlflow.tracking.fluent: Experiment with name 'test-experiment' does not exist. Creating a new experiment.
2026/01/18 16:26:08 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753568382, 1768753568382)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 350, in create_experiment
    eid = session.query(SqlExperiment).filter_by(name=name).first().experiment_id
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2759, in first
    return self.limit(1)._iter().first()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1233, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753568382, 1768753568382)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:08,383 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753568382, 1768753568382)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:08 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768753568382, 1768753568382)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:08,384 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Continuing without MLflow tracking...
2026-01-18 16:26:08 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Continuing without MLflow tracking...
2026-01-18 16:26:08,386 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_conversion_log_conversion0/config/tags.yaml, using defaults
2026-01-18 16:26:08 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_conversion_log_conversion0/config/tags.yaml, using defaults
2026/01/18 16:26:08 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('fba466b114db487589387a05180fec2a', 'agreeable-mink-105', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753568390, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/fba466b114db487589387a05180fec2a/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 586, in create_run
    inputs_list = self._get_run_inputs(session, [run_id])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 626, in _get_run_inputs
    ).all()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2704, in all
    return self._iter().all()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1048, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('fba466b114db487589387a05180fec2a', 'agreeable-mink-105', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753568390, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/fba466b114db487589387a05180fec2a/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:08,395 - infrastructure.tracking.mlflow.trackers.conversion_tracker - WARNING - MLflow tracking failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('fba466b114db487589387a05180fec2a', 'agreeable-mink-105', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753568390, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/fba466b114db487589387a05180fec2a/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:08 WARNI [infrastructure.tracking.mlflow.trackers.conversion_tracker] MLflow tracking failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('fba466b114db487589387a05180fec2a', 'agreeable-mink-105', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753568390, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/fba466b114db487589387a05180fec2a/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:08,395 - infrastructure.tracking.mlflow.trackers.conversion_tracker - WARNING - Continuing conversion without MLflow tracking...
2026-01-18 16:26:08 WARNI [infrastructure.tracking.mlflow.trackers.conversion_tracker] Continuing conversion without MLflow tracking...
2026/01/18 16:26:08 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('86400ff8ced342f2a66feb3920fcf800', 'crawling-bee-433', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753568399, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/86400ff8ced342f2a66feb3920fcf800/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 586, in create_run
    inputs_list = self._get_run_inputs(session, [run_id])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 626, in _get_run_inputs
    ).all()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2704, in all
    return self._iter().all()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1048, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('86400ff8ced342f2a66feb3920fcf800', 'crawling-bee-433', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753568399, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/86400ff8ced342f2a66feb3920fcf800/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:08,403 - infrastructure.tracking.mlflow.trackers.conversion_tracker - WARNING - Could not log conversion results to MLflow: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('86400ff8ced342f2a66feb3920fcf800', 'crawling-bee-433', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753568399, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/86400ff8ced342f2a66feb3920fcf800/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:08 WARNI [infrastructure.tracking.mlflow.trackers.conversion_tracker] Could not log conversion results to MLflow: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('86400ff8ced342f2a66feb3920fcf800', 'crawling-bee-433', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753568399, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/86400ff8ced342f2a66feb3920fcf800/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:08 INFO  [tracking.mlflow.artifacts] Uploading checkpoint archive (0.0MB)...
2026-01-18 16:26:08 INFO  [tracking.mlflow.artifacts] Successfully uploaded checkpoint archive: tmpa3qgorfi.tar.gz
2026-01-18 16:26:08 INFO  [tracking.mlflow.artifacts] Uploading checkpoint archive (0.0MB)...
2026-01-18 16:26:08 INFO  [tracking.mlflow.artifacts] Successfully uploaded checkpoint archive: tmpp8b_crxd.tar.gz
2026-01-18 16:26:08,426 - infrastructure.tracking.mlflow.lifecycle - INFO - Successfully terminated run refit-run-id... with status FINISHED
2026-01-18 16:26:08 INFO  [infrastructure.tracking.mlflow.lifecycle] Successfully terminated run refit-run-id... with status FINISHED
2026-01-18 16:26:08,428 - infrastructure.tracking.mlflow.lifecycle - INFO - Successfully terminated run refit-run-id... with status FAILED
2026-01-18 16:26:08 INFO  [infrastructure.tracking.mlflow.lifecycle] Successfully terminated run refit-run-id... with status FAILED
2026-01-18 16:26:08,431 - infrastructure.tracking.mlflow.lifecycle - INFO - Run refit-run-id... already has status FINISHED, skipping termination (expected RUNNING)
2026-01-18 16:26:08 INFO  [infrastructure.tracking.mlflow.lifecycle] Run refit-run-id... already has status FINISHED, skipping termination (expected RUNNING)
2026-01-18 16:26:08,461 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_naming_run_name_config0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 16:26:08 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_naming_run_name_config0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 16:26:08,462 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 16:26:08 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 16:26:08 WARNI [tracking.mlflow.artifacts] Failed to upload artifact tmppu9g0y9x: Upload failed
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/artifacts.py", line 90, in log_artifact_safe
    retry_with_backoff(
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/utils.py", line 64, in retry_with_backoff
    return func()
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/artifacts.py", line 85, in upload_func
    mlflow.log_artifact(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: Upload failed
2026-01-18 16:26:08 INFO  [tracking.mlflow.artifacts] Uploading 2 files from /tmp/tmpwjw37lx1...
2026-01-18 16:26:08 INFO  [tracking.mlflow.artifacts] Successfully uploaded 2 files from tmpwjw37lx1 (run_id=active)
2026-01-18 16:26:08 INFO  [tracking.mlflow.artifacts] Uploading checkpoint archive (0.0MB)...
2026-01-18 16:26:08 INFO  [tracking.mlflow.artifacts] Successfully uploaded checkpoint archive: tmpn4zefzmn.tar.gz
2026-01-18 16:26:08,491 - infrastructure.tracking.mlflow.lifecycle - INFO - Successfully terminated run test-run-id-... with status FINISHED
2026-01-18 16:26:08 INFO  [infrastructure.tracking.mlflow.lifecycle] Successfully terminated run test-run-id-... with status FINISHED
2026-01-18 16:26:08,493 - infrastructure.tracking.mlflow.lifecycle - INFO - Run test-run-id-... already has status FINISHED, skipping termination (expected RUNNING)
2026-01-18 16:26:08 INFO  [infrastructure.tracking.mlflow.lifecycle] Run test-run-id-... already has status FINISHED, skipping termination (expected RUNNING)
2026-01-18 16:26:08,496 - infrastructure.tracking.mlflow.lifecycle - INFO - Successfully terminated run test-run-id-... with status FINISHED
2026-01-18 16:26:08 INFO  [infrastructure.tracking.mlflow.lifecycle] Successfully terminated run test-run-id-... with status FINISHED
2026-01-18 16:26:08,498 - infrastructure.tracking.mlflow.lifecycle - INFO - Successfully terminated run test-run-id-... with status FINISHED
2026-01-18 16:26:08 INFO  [infrastructure.tracking.mlflow.lifecycle] Successfully terminated run test-run-id-... with status FINISHED
2026-01-18 16:26:08,501 - infrastructure.tracking.mlflow.lifecycle - INFO - Successfully terminated run test-run-id-... with status FINISHED
2026-01-18 16:26:08 INFO  [infrastructure.tracking.mlflow.lifecycle] Successfully terminated run test-run-id-... with status FINISHED
2026-01-18 16:26:08,507 - infrastructure.tracking.mlflow.runs - INFO - Created new experiment: new-experiment (exp-456)
2026-01-18 16:26:08 INFO  [infrastructure.tracking.mlflow.runs] Created new experiment: new-experiment (exp-456)
2026-01-18 16:26:08 WARNI [infrastructure.paths.resolve] Failed to load paths.yaml config: [Errno 13] Permission denied: '/root/config/paths.yaml'. Using fallback logic.
2026-01-18 16:26:08 WARNI [infrastructure.paths.resolve] Failed to load paths.yaml config: [Errno 13] Permission denied: '/root/config/paths.yaml'. Using fallback logic.
2026-01-18 16:26:08 WARNI [infrastructure.paths.resolve] Failed to load paths.yaml config: [Errno 13] Permission denied: '/root/config/paths.yaml'. Using fallback logic.
2026-01-18 16:26:08 WARNI [infrastructure.paths.resolve] Failed to load paths.yaml config: [Errno 13] Permission denied: '/root/config/paths.yaml'. Using fallback logic.
2026-01-18 16:26:08 WARNI [infrastructure.paths.resolve] Failed to load paths.yaml config: [Errno 13] Permission denied: '/root/config/paths.yaml'. Using fallback logic.
2026-01-18 16:26:08 WARNI [infrastructure.paths.resolve] Failed to load paths.yaml config: [Errno 13] Permission denied: '/root/config/paths.yaml'. Using fallback logic.
2026-01-18 16:26:08,584 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_hpo_trial_run_name0/config, raw_auto_inc_config={}
2026-01-18 16:26:08 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_hpo_trial_run_name0/config, raw_auto_inc_config={}
2026-01-18 16:26:08,584 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 16:26:08 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 16:26:08,588 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_hpo_trial_fold_run_name0/config, raw_auto_inc_config={}
2026-01-18 16:26:08 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_hpo_trial_fold_run_name0/config, raw_auto_inc_config={}
2026-01-18 16:26:08,588 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo_trial_fold
2026-01-18 16:26:08 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo_trial_fold
2026-01-18 16:26:08 WARNI [infrastructure.naming.display_policy] [Naming Policy] No pattern for process_type: hpo_sweep
2026-01-18 16:26:08,592 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_hpo_refit_run_name0/config, raw_auto_inc_config={}
2026-01-18 16:26:08 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_hpo_refit_run_name0/config, raw_auto_inc_config={}
2026-01-18 16:26:08,592 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 16:26:08 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 16:26:08,595 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_hpo_sweep_run_name0/config, raw_auto_inc_config={}
2026-01-18 16:26:08 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_hpo_sweep_run_name0/config, raw_auto_inc_config={}
2026-01-18 16:26:08,595 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 16:26:08 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 16:26:08 WARNI [infrastructure.naming.display_policy] [Naming Policy] Missing key in pattern substitution: 'study_hash'
2026-01-18 16:26:08,599 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_run_name_max_length0/config, raw_auto_inc_config={}
2026-01-18 16:26:08 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_run_name_max_length0/config, raw_auto_inc_config={}
2026-01-18 16:26:08,599 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 16:26:08 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 16:26:08 WARNI [infrastructure.naming.display_policy] [Naming Policy] Missing key in pattern substitution: 'study_hash'
2026-01-18 16:26:08,602 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_run_name_forbidden_chars_0/config, raw_auto_inc_config={}
2026-01-18 16:26:08 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-120/test_run_name_forbidden_chars_0/config, raw_auto_inc_config={}
2026-01-18 16:26:08,602 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 16:26:08 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 16:26:08,606 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 16:26:08 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 16:26:08,606 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=final_training
2026-01-18 16:26:08 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=final_training
2026-01-18 16:26:08 ERROR [infrastructure.naming.display_policy] [Naming Policy] Run name exceeds max length (256): aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa... (length: 300)
2026-01-18 16:26:08 ERROR [infrastructure.naming.display_policy] [Naming Policy] Run name contains forbidden characters ['/']: local/distilbert/hpo_trial...
2026-01-18 16:26:08 WARNI [infrastructure.naming.display_policy] [Naming Policy] Run name exceeds recommended length (150): aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa... (length: 200)
2026-01-18 16:26:08,730 - infrastructure.tracking.mlflow.trackers.sweep_tracker.sweep_tracker - INFO - [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=parent-run-i...
2026-01-18 16:26:08 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker.sweep_tracker] [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=parent-run-i...
2026-01-18 16:26:08,730 - infrastructure.tracking.mlflow.trackers.sweep_tracker_original - INFO - [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=parent-run-i...
2026-01-18 16:26:08 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker_original] [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=parent-run-i...
2026-01-18 16:26:08,731 - infrastructure.tracking.mlflow.trackers.sweep_tracker_original - INFO - [LOG_FINAL_METRICS] Found 0 completed trials out of 1 total
2026-01-18 16:26:08 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker_original] [LOG_FINAL_METRICS] Found 0 completed trials out of 1 total
2026-01-18 16:26:08,731 - infrastructure.tracking.mlflow.trackers.sweep_tracker_original - INFO - [LOG_FINAL_METRICS] Logging metrics: n_trials=1, n_completed_trials=0
2026-01-18 16:26:08 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker_original] [LOG_FINAL_METRICS] Logging metrics: n_trials=1, n_completed_trials=0
2026-01-18 16:26:08,731 - infrastructure.tracking.mlflow.trackers.sweep_tracker_original - INFO - [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=0.5
2026-01-18 16:26:08 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker_original] [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=0.5
2026-01-18 16:26:08,731 - infrastructure.tracking.mlflow.trackers.sweep_tracker_original - INFO - [LOG_FINAL_METRICS] Logging best hyperparameters: {'learning_rate': 0.0001}
2026-01-18 16:26:08 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker_original] [LOG_FINAL_METRICS] Logging best hyperparameters: {'learning_rate': 0.0001}
2026-01-18 16:26:08,732 - infrastructure.tracking.mlflow.trackers.sweep_tracker_original - INFO - [LOG_FINAL_METRICS] Finding and logging best trial run ID...
2026-01-18 16:26:08 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker_original] [LOG_FINAL_METRICS] Finding and logging best trial run ID...
2026-01-18 16:26:08,732 - infrastructure.tracking.mlflow.trackers.sweep_tracker_original - INFO - Found best trial 0 (study_key_hash + trial_number + parentRunId): child-run-id...
2026-01-18 16:26:08 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker_original] Found best trial 0 (study_key_hash + trial_number + parentRunId): child-run-id...
2026-01-18 16:26:08,733 - infrastructure.tracking.mlflow.trackers.sweep_tracker_original - INFO - [LOG_FINAL_METRICS] Completed best trial ID logging
2026-01-18 16:26:08 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker_original] [LOG_FINAL_METRICS] Completed best trial ID logging
2026-01-18 16:26:08,733 - infrastructure.tracking.mlflow.trackers.sweep_tracker_original - INFO - [LOG_FINAL_METRICS] Checkpoint logging enabled: None (upload_checkpoint=True)
2026-01-18 16:26:08 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker_original] [LOG_FINAL_METRICS] Checkpoint logging enabled: None (upload_checkpoint=True)
2026-01-18 16:26:08,733 - infrastructure.tracking.mlflow.trackers.sweep_tracker_original - INFO - [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
2026-01-18 16:26:08 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker_original] [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
2026-01-18 16:26:08,733 - infrastructure.tracking.mlflow.trackers.sweep_tracker_original - INFO - [LOG_FINAL_METRICS] Completed log_final_metrics successfully
2026-01-18 16:26:08 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker_original] [LOG_FINAL_METRICS] Completed log_final_metrics successfully
2026-01-18 16:26:08,743 - infrastructure.tracking.mlflow.trackers.sweep_tracker.sweep_tracker - INFO - [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=parent-run-i...
2026-01-18 16:26:08 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker.sweep_tracker] [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=parent-run-i...
2026-01-18 16:26:08,743 - infrastructure.tracking.mlflow.trackers.sweep_tracker_original - INFO - [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=parent-run-i...
2026-01-18 16:26:08 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker_original] [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=parent-run-i...
2026-01-18 16:26:08,744 - infrastructure.tracking.mlflow.trackers.sweep_tracker_original - INFO - [LOG_FINAL_METRICS] Found 0 completed trials out of 1 total
2026-01-18 16:26:08 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker_original] [LOG_FINAL_METRICS] Found 0 completed trials out of 1 total
2026-01-18 16:26:08,744 - infrastructure.tracking.mlflow.trackers.sweep_tracker_original - INFO - [LOG_FINAL_METRICS] Logging metrics: n_trials=1, n_completed_trials=0
2026-01-18 16:26:08 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker_original] [LOG_FINAL_METRICS] Logging metrics: n_trials=1, n_completed_trials=0
2026-01-18 16:26:08,744 - infrastructure.tracking.mlflow.trackers.sweep_tracker_original - INFO - [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=0.5
2026-01-18 16:26:08 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker_original] [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=0.5
2026-01-18 16:26:08,744 - infrastructure.tracking.mlflow.trackers.sweep_tracker_original - INFO - [LOG_FINAL_METRICS] Logging best hyperparameters: {'learning_rate': 0.0001}
2026-01-18 16:26:08 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker_original] [LOG_FINAL_METRICS] Logging best hyperparameters: {'learning_rate': 0.0001}
2026-01-18 16:26:08,745 - infrastructure.tracking.mlflow.trackers.sweep_tracker_original - INFO - [LOG_FINAL_METRICS] Finding and logging best trial run ID...
2026-01-18 16:26:08 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker_original] [LOG_FINAL_METRICS] Finding and logging best trial run ID...
2026-01-18 16:26:08,745 - infrastructure.tracking.mlflow.trackers.sweep_tracker_original - INFO - Found best trial 0 (study_key_hash + trial_number + parentRunId): child-run-id...
2026-01-18 16:26:08 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker_original] Found best trial 0 (study_key_hash + trial_number + parentRunId): child-run-id...
2026/01/18 16:26:08 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('4308a93bc2b74e2099dfeff140279f0b', 'smiling-pug-343', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753568750, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/4308a93bc2b74e2099dfeff140279f0b/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 586, in create_run
    inputs_list = self._get_run_inputs(session, [run_id])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 626, in _get_run_inputs
    ).all()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2704, in all
    return self._iter().all()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1048, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('4308a93bc2b74e2099dfeff140279f0b', 'smiling-pug-343', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753568750, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/4308a93bc2b74e2099dfeff140279f0b/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:08,755 - infrastructure.tracking.mlflow.trackers.sweep_tracker_original - WARNING - Could not find MLflow run ID for best trial 0. Search failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('4308a93bc2b74e2099dfeff140279f0b', 'smiling-pug-343', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753568750, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/4308a93bc2b74e2099dfeff140279f0b/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8). This may be a timing issue - trial runs may not be created/committed yet.
2026-01-18 16:26:08 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker_original] Could not find MLflow run ID for best trial 0. Search failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('4308a93bc2b74e2099dfeff140279f0b', 'smiling-pug-343', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768753568750, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/4308a93bc2b74e2099dfeff140279f0b/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8). This may be a timing issue - trial runs may not be created/committed yet.
2026-01-18 16:26:08,755 - infrastructure.tracking.mlflow.trackers.sweep_tracker_original - INFO - [LOG_FINAL_METRICS] Completed best trial ID logging
2026-01-18 16:26:08 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker_original] [LOG_FINAL_METRICS] Completed best trial ID logging
2026-01-18 16:26:08,755 - infrastructure.tracking.mlflow.trackers.sweep_tracker_original - INFO - [LOG_FINAL_METRICS] Checkpoint logging enabled: None (upload_checkpoint=True)
2026-01-18 16:26:08 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker_original] [LOG_FINAL_METRICS] Checkpoint logging enabled: None (upload_checkpoint=True)
2026-01-18 16:26:08,755 - infrastructure.tracking.mlflow.trackers.sweep_tracker_original - INFO - [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
2026-01-18 16:26:08 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker_original] [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
2026-01-18 16:26:08,755 - infrastructure.tracking.mlflow.trackers.sweep_tracker_original - INFO - [LOG_FINAL_METRICS] Completed log_final_metrics successfully
2026-01-18 16:26:08 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker_original] [LOG_FINAL_METRICS] Completed log_final_metrics successfully
2026-01-18 16:26:08,785 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_load_tags_registry_fallba0/config/tags.yaml, using defaults
2026-01-18 16:26:08 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_load_tags_registry_fallba0/config/tags.yaml, using defaults
2026-01-18 16:26:08,807 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_build_mlflow_tags_minimal0/config/tags.yaml, using defaults
2026-01-18 16:26:08 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_build_mlflow_tags_minimal0/config/tags.yaml, using defaults
2026-01-18 16:26:08,809 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_build_mlflow_tags_hpo_pro0/config/tags.yaml, using defaults
2026-01-18 16:26:08 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_build_mlflow_tags_hpo_pro0/config/tags.yaml, using defaults
2026-01-18 16:26:08,812 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_build_mlflow_tags_hpo_ref0/config/tags.yaml, using defaults
2026-01-18 16:26:08 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_build_mlflow_tags_hpo_ref0/config/tags.yaml, using defaults
2026-01-18 16:26:08,814 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_build_mlflow_tags_benchma0/config/tags.yaml, using defaults
2026-01-18 16:26:08 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_build_mlflow_tags_benchma0/config/tags.yaml, using defaults
2026-01-18 16:26:08,817 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_build_mlflow_tags_final_t0/config/tags.yaml, using defaults
2026-01-18 16:26:08 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_build_mlflow_tags_final_t0/config/tags.yaml, using defaults
2026-01-18 16:26:08,819 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_build_mlflow_tags_convers0/config/tags.yaml, using defaults
2026-01-18 16:26:08 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_build_mlflow_tags_convers0/config/tags.yaml, using defaults
2026-01-18 16:26:08,822 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_build_mlflow_tags_optiona0/config/tags.yaml, using defaults
2026-01-18 16:26:08 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_build_mlflow_tags_optiona0/config/tags.yaml, using defaults
2026-01-18 16:26:08,855 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_get_tag_key_falls_back_to0/config/tags.yaml, using defaults
2026-01-18 16:26:08 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_get_tag_key_falls_back_to0/config/tags.yaml, using defaults
2026-01-18 16:26:08,858 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_get_tag_key_raises_when_m0/config/tags.yaml, using defaults
2026-01-18 16:26:08 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_get_tag_key_raises_when_m0/config/tags.yaml, using defaults
2026-01-18 16:26:08,862 - infrastructure.naming.mlflow.tags_registry - WARNING - [Tags Registry] Failed to load config from /tmp/pytest-of-codespace/pytest-120/test_get_tag_key_handles_regis0/config/tags.yaml: mapping values are not allowed here
  in "/tmp/pytest-of-codespace/pytest-120/test_get_tag_key_handles_regis0/config/tags.yaml", line 1, column 14. Using defaults.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/naming/mlflow/tags_registry.py", line 212, in load_tags_registry
    loaded_data = load_yaml(config_path)
  File "/workspaces/resume-ner-azureml/src/common/shared/yaml_utils.py", line 50, in load_yaml
    return yaml.safe_load(handle)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/__init__.py", line 125, in safe_load
    return load(stream, SafeLoader)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/composer.py", line 36, in get_single_node
    document = self.compose_document()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/composer.py", line 55, in compose_document
    node = self.compose_node(None, None)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/composer.py", line 84, in compose_node
    node = self.compose_mapping_node(anchor)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/composer.py", line 127, in compose_mapping_node
    while not self.check_event(MappingEndEvent):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/parser.py", line 98, in check_event
    self.current_event = self.state()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/parser.py", line 428, in parse_block_mapping_key
    if self.check_token(KeyToken):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/scanner.py", line 116, in check_token
    self.fetch_more_tokens()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/scanner.py", line 223, in fetch_more_tokens
    return self.fetch_value()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/scanner.py", line 577, in fetch_value
    raise ScannerError(None, None,
yaml.scanner.ScannerError: mapping values are not allowed here
  in "/tmp/pytest-of-codespace/pytest-120/test_get_tag_key_handles_regis0/config/tags.yaml", line 1, column 14
2026-01-18 16:26:08 WARNI [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Failed to load config from /tmp/pytest-of-codespace/pytest-120/test_get_tag_key_handles_regis0/config/tags.yaml: mapping values are not allowed here
  in "/tmp/pytest-of-codespace/pytest-120/test_get_tag_key_handles_regis0/config/tags.yaml", line 1, column 14. Using defaults.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/naming/mlflow/tags_registry.py", line 212, in load_tags_registry
    loaded_data = load_yaml(config_path)
  File "/workspaces/resume-ner-azureml/src/common/shared/yaml_utils.py", line 50, in load_yaml
    return yaml.safe_load(handle)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/__init__.py", line 125, in safe_load
    return load(stream, SafeLoader)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/composer.py", line 36, in get_single_node
    document = self.compose_document()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/composer.py", line 55, in compose_document
    node = self.compose_node(None, None)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/composer.py", line 84, in compose_node
    node = self.compose_mapping_node(anchor)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/composer.py", line 127, in compose_mapping_node
    while not self.check_event(MappingEndEvent):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/parser.py", line 98, in check_event
    self.current_event = self.state()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/parser.py", line 428, in parse_block_mapping_key
    if self.check_token(KeyToken):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/scanner.py", line 116, in check_token
    self.fetch_more_tokens()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/scanner.py", line 223, in fetch_more_tokens
    return self.fetch_value()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/scanner.py", line 577, in fetch_value
    raise ScannerError(None, None,
yaml.scanner.ScannerError: mapping values are not allowed here
  in "/tmp/pytest-of-codespace/pytest-120/test_get_tag_key_handles_regis0/config/tags.yaml", line 1, column 14
2026-01-18 16:26:08,882 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_load_tags_registry_missin0/config/tags.yaml, using defaults
2026-01-18 16:26:08 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-120/test_load_tags_registry_missin0/config/tags.yaml, using defaults
2026-01-18 16:26:15,832 - common.shared.mlflow_setup - INFO - Using local tracking: sqlite:///./mlruns/mlflow.db
2026-01-18 16:26:15 INFO  [common.shared.mlflow_setup] Using local tracking: sqlite:///./mlruns/mlflow.db
2026-01-18 16:26:15,836 - common.shared.mlflow_setup - INFO - Using Azure ML workspace tracking
2026-01-18 16:26:15 INFO  [common.shared.mlflow_setup] Using Azure ML workspace tracking
2026-01-18 16:26:15,839 - common.shared.mlflow_setup - WARNING - Azure ML tracking failed: Azure ML unavailable
2026-01-18 16:26:15 WARNI [common.shared.mlflow_setup] Azure ML tracking failed: Azure ML unavailable
2026-01-18 16:26:15,839 - common.shared.mlflow_setup - INFO - Falling back to local tracking...
2026-01-18 16:26:15 INFO  [common.shared.mlflow_setup] Falling back to local tracking...
2026-01-18 16:26:15,839 - common.shared.mlflow_setup - INFO - Using local tracking: sqlite:///./mlruns/mlflow.db
2026-01-18 16:26:15 INFO  [common.shared.mlflow_setup] Using local tracking: sqlite:///./mlruns/mlflow.db
2026-01-18 16:26:15,846 - common.shared.mlflow_setup - INFO - Using local tracking: sqlite:////content/drive/MyDrive/resume-ner-mlflow/mlflow.db
2026-01-18 16:26:15 INFO  [common.shared.mlflow_setup] Using local tracking: sqlite:////content/drive/MyDrive/resume-ner-mlflow/mlflow.db
2026-01-18 16:26:15,851 - common.shared.mlflow_setup - INFO - Using local tracking: sqlite:////kaggle/working/mlflow.db
2026-01-18 16:26:15 INFO  [common.shared.mlflow_setup] Using local tracking: sqlite:////kaggle/working/mlflow.db
2026-01-18 16:26:15,855 - common.shared.mlflow_setup - INFO - Using local tracking: sqlite:////workspaces/resume-ner-azureml/mlruns/mlflow.db
2026-01-18 16:26:15 INFO  [common.shared.mlflow_setup] Using local tracking: sqlite:////workspaces/resume-ner-azureml/mlruns/mlflow.db
2026-01-18 16:26:15,859 - common.shared.mlflow_setup - WARNING - [DEBUG] Initial env check - subscription_id: True, resource_group: True, client_id: False, client_secret: False, tenant_id: False
2026-01-18 16:26:15 WARNI [common.shared.mlflow_setup] [DEBUG] Initial env check - subscription_id: True, resource_group: True, client_id: False, client_secret: False, tenant_id: False
2026-01-18 16:26:15,859 - common.shared.mlflow_setup - INFO - Attempting to load credentials from config.env at: config.env
2026-01-18 16:26:15 INFO  [common.shared.mlflow_setup] Attempting to load credentials from config.env at: config.env
2026-01-18 16:26:15,859 - common.shared.mlflow_setup - WARNING - config.env not found at config.env. Looking for: /workspaces/resume-ner-azureml/config.env
2026-01-18 16:26:15 WARNI [common.shared.mlflow_setup] config.env not found at config.env. Looking for: /workspaces/resume-ner-azureml/config.env
2026-01-18 16:26:15,859 - common.shared.mlflow_setup - WARNING - [DEBUG] Platform detected: local
2026-01-18 16:26:15 WARNI [common.shared.mlflow_setup] [DEBUG] Platform detected: local
2026-01-18 16:26:15,859 - common.shared.mlflow_setup - WARNING - [DEBUG] Service Principal check - client_id present: False, client_secret present: False, tenant_id present: False, has_service_principal: False
2026-01-18 16:26:15 WARNI [common.shared.mlflow_setup] [DEBUG] Service Principal check - client_id present: False, client_secret present: False, tenant_id present: False, has_service_principal: False
2026-01-18 16:26:15,860 - common.shared.mlflow_setup - INFO - Using DefaultAzureCredential (trying multiple auth methods)
2026-01-18 16:26:15 INFO  [common.shared.mlflow_setup] Using DefaultAzureCredential (trying multiple auth methods)
2026-01-18 16:26:15,860 - common.shared.mlflow_setup - INFO - Successfully connected to Azure ML workspace: test-ws
2026-01-18 16:26:15 INFO  [common.shared.mlflow_setup] Successfully connected to Azure ML workspace: test-ws
2026-01-18 16:26:15,868 - common.shared.mlflow_setup - WARNING - azure-ai-ml and azure-identity are required for Azure ML tracking. Install with: pip install azure-ai-ml azure-identity. Falling back to local tracking.
2026-01-18 16:26:15 WARNI [common.shared.mlflow_setup] azure-ai-ml and azure-identity are required for Azure ML tracking. Install with: pip install azure-ai-ml azure-identity. Falling back to local tracking.
2026-01-18 16:26:15,870 - common.shared.mlflow_setup - INFO - Azure ML enabled in config, attempting to connect...
2026-01-18 16:26:15 INFO  [common.shared.mlflow_setup] Azure ML enabled in config, attempting to connect...
2026-01-18 16:26:15,871 - common.shared.mlflow_setup - INFO - Azure ML disabled in config, using local tracking
2026-01-18 16:26:15 INFO  [common.shared.mlflow_setup] Azure ML disabled in config, using local tracking
2026-01-18 16:26:15,873 - common.shared.mlflow_setup - WARNING - MLflow config not found, using local tracking
2026-01-18 16:26:15 WARNI [common.shared.mlflow_setup] MLflow config not found, using local tracking
  [Training]  No active MLflow run and no MLFLOW_RUN_ID - cannot log artifacts
  [Training]  No active MLflow run and no MLFLOW_RUN_ID - cannot log artifacts
  [Training]  No active MLflow run and no MLFLOW_RUN_ID - cannot log artifacts
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2026/01/18 16:26:17 INFO mlflow.tracking.fluent: Experiment with name 'resume_ner_baseline-training' does not exist. Creating a new experiment.
2026-01-18 16:26:17,459 - training.hpo.execution.local.sweep_original - INFO - [EARLY HASH] Computed v2 study_key_hash=79a646c77bb15106... for folder creation
2026-01-18 16:26:17 INFO  [training.hpo.execution.local.sweep_original] [EARLY HASH] Computed v2 study_key_hash=79a646c77bb15106... for folder creation
2026-01-18 16:26:17,460 - training.hpo.core.study - INFO - [HPO] Starting optimization for distilroberta with checkpointing...
2026-01-18 16:26:17 INFO  [training.hpo.core.study] [HPO] Starting optimization for distilroberta with checkpointing...
2026-01-18 16:26:17,460 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 16:26:17 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 16:26:17,460 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 16:26:17 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 16:26:17,460 - infrastructure.tracking.mlflow.index.version_counter - INFO - [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:7202413b67e3126708d0544fb08f79f5b50640effd450..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-18 16:26:17 INFO  [infrastructure.tracking.mlflow.index.version_counter] [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:7202413b67e3126708d0544fb08f79f5b50640effd450..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-18 16:26:17,462 - infrastructure.tracking.mlflow.index.version_counter - INFO - [Reserve Version] Loaded 32 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-18 16:26:17 INFO  [infrastructure.tracking.mlflow.index.version_counter] [Reserve Version] Loaded 32 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-18 16:26:17,462 - infrastructure.tracking.mlflow.index.version_counter - INFO - [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0
2026-01-18 16:26:17 INFO  [infrastructure.tracking.mlflow.index.version_counter] [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0
2026-01-18 16:26:17,462 - infrastructure.tracking.mlflow.index.version_counter - INFO - [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)
2026-01-18 16:26:17 INFO  [infrastructure.tracking.mlflow.index.version_counter] [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)
2026-01-18 16:26:17,463 - infrastructure.tracking.mlflow.index.version_counter - INFO - [Reserve Version]  Successfully reserved version 1 for counter_key resume-ner:hpo:7202413b67e3126708d0544fb08f79f5b50... (run_id: pending_2026...)
2026-01-18 16:26:17 INFO  [infrastructure.tracking.mlflow.index.version_counter] [Reserve Version]  Successfully reserved version 1 for counter_key resume-ner:hpo:7202413b67e3126708d0544fb08f79f5b50... (run_id: pending_2026...)
2026-01-18 16:26:17,463 - training.hpo.execution.local.sweep_original - INFO - [HPO Setup] k_folds=5, fold_splits_file=None, k_fold config: {'enabled': True, 'n_splits': 5, 'random_seed': 42, 'shuffle': True, 'stratified': True}
2026-01-18 16:26:17 INFO  [training.hpo.execution.local.sweep_original] [HPO Setup] k_folds=5, fold_splits_file=None, k_fold config: {'enabled': True, 'n_splits': 5, 'random_seed': 42, 'shuffle': True, 'stratified': True}
2026-01-18 16:26:17,464 - training.hpo.execution.local.sweep_original - INFO - [CV Setup]  Created 5 fold splits and saved to /workspaces/resume-ner-azureml/outputs/hpo/local/distilroberta/study-79a646c7/fold_splits.json
2026-01-18 16:26:17 INFO  [training.hpo.execution.local.sweep_original] [CV Setup]  Created 5 fold splits and saved to /workspaces/resume-ner-azureml/outputs/hpo/local/distilroberta/study-79a646c7/fold_splits.json
2026/01/18 16:26:17 INFO mlflow.tracking.fluent: Experiment with name 'resume_ner_baseline-hpo-distilroberta' does not exist. Creating a new experiment.
2026/01/18 16:26:17 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('resume_ner_baseline-hpo-distilroberta', None, 'active', 1768753577472, 1768753577472)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 350, in create_experiment
    eid = session.query(SqlExperiment).filter_by(name=name).first().experiment_id
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2759, in first
    return self.limit(1)._iter().first()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1233, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('resume_ner_baseline-hpo-distilroberta', None, 'active', 1768753577472, 1768753577472)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:17,474 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('resume_ner_baseline-hpo-distilroberta', None, 'active', 1768753577472, 1768753577472)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:17 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('resume_ner_baseline-hpo-distilroberta', None, 'active', 1768753577472, 1768753577472)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:17,474 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Continuing without MLflow tracking...
2026-01-18 16:26:17 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Continuing without MLflow tracking...
2026/01/18 16:26:17 INFO mlflow.tracking.fluent: Experiment with name 'resume_ner_baseline-hpo-distilroberta' does not exist. Creating a new experiment.
2026/01/18 16:26:17 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('resume_ner_baseline-hpo-distilroberta', None, 'active', 1768753577476, 1768753577476)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 350, in create_experiment
    eid = session.query(SqlExperiment).filter_by(name=name).first().experiment_id
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2759, in first
    return self.limit(1)._iter().first()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1233, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('resume_ner_baseline-hpo-distilroberta', None, 'active', 1768753577476, 1768753577476)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:17,478 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('resume_ner_baseline-hpo-distilroberta', None, 'active', 1768753577476, 1768753577476)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:17 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('resume_ner_baseline-hpo-distilroberta', None, 'active', 1768753577476, 1768753577476)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:17,478 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Continuing without MLflow tracking...
2026-01-18 16:26:17 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Continuing without MLflow tracking...
2026-01-18 16:26:17,478 - infrastructure.tracking.mlflow.trackers.sweep_tracker_original - WARNING - Using LOCAL MLflow tracking (not Azure ML)
2026-01-18 16:26:17 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker_original] Using LOCAL MLflow tracking (not Azure ML)
2026-01-18 16:26:17,478 - infrastructure.tracking.mlflow.trackers.sweep_tracker_original - INFO - To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
2026-01-18 16:26:17 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker_original] To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
2026-01-18 16:26:17,484 - infrastructure.tracking.mlflow.trackers.sweep_tracker.sweep_tracker - ERROR - [START_SWEEP_RUN] MLflow tracking failed: No Experiment with id=908579302991784457 exists
2026-01-18 16:26:17 ERROR [infrastructure.tracking.mlflow.trackers.sweep_tracker.sweep_tracker] [START_SWEEP_RUN] MLflow tracking failed: No Experiment with id=908579302991784457 exists
2026-01-18 16:26:17,485 - infrastructure.tracking.mlflow.trackers.sweep_tracker.sweep_tracker - ERROR - [START_SWEEP_RUN] Traceback: Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/trackers/sweep_tracker/sweep_tracker.py", line 147, in start_sweep_run
    handle, study_key_hash, study_family_hash = create_mlflow_sweep_run(
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/trackers/sweep_tracker/run_creation.py", line 40, in create_mlflow_sweep_run
    with mlflow.start_run(run_name=run_name) as parent_run:
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/fluent.py", line 475, in start_run
    active_run_obj = client.create_run(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 479, in create_run
    return self._tracking_client.create_run(experiment_id, start_time, tags, run_name)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/telemetry/track.py", line 30, in wrapper
    result = func(*args, **kwargs)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 183, in create_run
    return self.store.create_run(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 541, in create_run
    experiment = self.get_experiment(experiment_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 462, in get_experiment
    return self._get_experiment(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 440, in _get_experiment
    raise MlflowException(
mlflow.exceptions.MlflowException: No Experiment with id=908579302991784457 exists

2026-01-18 16:26:17 ERROR [infrastructure.tracking.mlflow.trackers.sweep_tracker.sweep_tracker] [START_SWEEP_RUN] Traceback: Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/trackers/sweep_tracker/sweep_tracker.py", line 147, in start_sweep_run
    handle, study_key_hash, study_family_hash = create_mlflow_sweep_run(
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/trackers/sweep_tracker/run_creation.py", line 40, in create_mlflow_sweep_run
    with mlflow.start_run(run_name=run_name) as parent_run:
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/fluent.py", line 475, in start_run
    active_run_obj = client.create_run(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 479, in create_run
    return self._tracking_client.create_run(experiment_id, start_time, tags, run_name)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/telemetry/track.py", line 30, in wrapper
    result = func(*args, **kwargs)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 183, in create_run
    return self.store.create_run(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 541, in create_run
    experiment = self.get_experiment(experiment_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 462, in get_experiment
    return self._get_experiment(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 440, in _get_experiment
    raise MlflowException(
mlflow.exceptions.MlflowException: No Experiment with id=908579302991784457 exists

2026-01-18 16:26:17,485 - infrastructure.tracking.mlflow.trackers.sweep_tracker.sweep_tracker - WARNING - Continuing HPO without MLflow tracking...
2026-01-18 16:26:17 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker.sweep_tracker] Continuing HPO without MLflow tracking...
2026-01-18 16:26:17,485 - training.hpo.execution.local.sweep_original - INFO - Setting Phase 2 tags on parent run None... (data_config=present, train_config=present)
2026-01-18 16:26:17 INFO  [training.hpo.execution.local.sweep_original] Setting Phase 2 tags on parent run None... (data_config=present, train_config=present)
2026-01-18 16:26:17,485 - training.hpo.execution.local.sweep_original - WARNING - _set_phase2_hpo_tags: parent_run_id is empty, skipping
2026-01-18 16:26:17 WARNI [training.hpo.execution.local.sweep_original] _set_phase2_hpo_tags: parent_run_id is empty, skipping
2026-01-18 16:26:17,485 - training.hpo.execution.local.sweep_original - INFO -  Successfully completed Phase 2 tag setting for parent run None...
2026-01-18 16:26:17 INFO  [training.hpo.execution.local.sweep_original]  Successfully completed Phase 2 tag setting for parent run None...
2026-01-18 16:26:17,485 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.
2026-01-18 16:26:17 INFO  [training.hpo.tracking.cleanup] [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.
2026-01-18 16:26:17,486 - training.hpo.execution.local.sweep_original - INFO - [REFIT] Starting refit training for best trial <MagicMock name='mock.create_study().best_trial.number' id='123304023602848'>
2026-01-18 16:26:17 INFO  [training.hpo.execution.local.sweep_original] [REFIT] Starting refit training for best trial <MagicMock name='mock.create_study().best_trial.number' id='123304023602848'>
2026-01-18 16:26:17,487 - training.hpo.execution.local.sweep_original - WARNING - [REFIT] Could not determine trial_key_hash from best trial run tags. Refit run may not match trial run.
2026-01-18 16:26:17 WARNI [training.hpo.execution.local.sweep_original] [REFIT] Could not determine trial_key_hash from best trial run tags. Refit run may not match trial run.
2026-01-18 16:26:17,488 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial <MagicMock name='mock.create_study().best_trial.number' id='123304023602848'> with hyperparameters: <MagicMock name='mock.create_study().best_trial.params' id='123304023545200'>
2026-01-18 16:26:17 INFO  [training.hpo.execution.local.refit] [REFIT] Starting refit training for trial <MagicMock name='mock.create_study().best_trial.number' id='123304023602848'> with hyperparameters: <MagicMock name='mock.create_study().best_trial.params' id='123304023545200'>
2026-01-18 16:26:17,489 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id="trial_<MagicMock name='mock.create_study().best_trial.number' id='123304023602848'>_20260118_162617", run_id='20260118_162617', trial_number=<MagicMock name='mock.create_study().best_trial.number' id='123304023602848'>
2026-01-18 16:26:17 INFO  [training.hpo.execution.local.refit] [REFIT] Computed trial_id="trial_<MagicMock name='mock.create_study().best_trial.number' id='123304023602848'>_20260118_162617", run_id='20260118_162617', trial_number=<MagicMock name='mock.create_study().best_trial.number' id='123304023602848'>
2026-01-18 16:26:17,489 - infrastructure.tracking.mlflow.hash_utils - WARNING - Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
2026-01-18 16:26:17 WARNI [infrastructure.tracking.mlflow.hash_utils] Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
2026-01-18 16:26:17,489 - training.hpo.execution.local.sweep_original - WARNING - [REFIT] Refit training failed: Cannot create refit in v2 study folder study-79a646c7 without trial_key_hash. study_key_hash=present, trial_key_hash=missing, refit_params=missing. Hash computation from trial parameters may have failed.. Continuing without refit checkpoint.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep_original.py", line 1411, in run_local_hpo_sweep
    refit_metrics, refit_checkpoint_dir, refit_run_id = run_refit_training(
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 413, in run_refit_training
    refit_output_dir = _build_refit_output_dir(
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 228, in _build_refit_output_dir
    raise RuntimeError(
RuntimeError: Cannot create refit in v2 study folder study-79a646c7 without trial_key_hash. study_key_hash=present, trial_key_hash=missing, refit_params=missing. Hash computation from trial parameters may have failed.
2026-01-18 16:26:17 WARNI [training.hpo.execution.local.sweep_original] [REFIT] Refit training failed: Cannot create refit in v2 study folder study-79a646c7 without trial_key_hash. study_key_hash=present, trial_key_hash=missing, refit_params=missing. Hash computation from trial parameters may have failed.. Continuing without refit checkpoint.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep_original.py", line 1411, in run_local_hpo_sweep
    refit_metrics, refit_checkpoint_dir, refit_run_id = run_refit_training(
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 413, in run_refit_training
    refit_output_dir = _build_refit_output_dir(
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 228, in _build_refit_output_dir
    raise RuntimeError(
RuntimeError: Cannot create refit in v2 study folder study-79a646c7 without trial_key_hash. study_key_hash=present, trial_key_hash=missing, refit_params=missing. Hash computation from trial parameters may have failed.
2026-01-18 16:26:17,490 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /workspaces/resume-ner-azureml/outputs/hpo/config/tags.yaml, using defaults
2026-01-18 16:26:17 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /workspaces/resume-ner-azureml/outputs/hpo/config/tags.yaml, using defaults
2026-01-18 16:26:17,490 - training.hpo.execution.local.sweep_original - WARNING - [REFIT] No refit_run_id available to mark as FINISHED
2026-01-18 16:26:17 WARNI [training.hpo.execution.local.sweep_original] [REFIT] No refit_run_id available to mark as FINISHED
2026/01/18 16:26:17 INFO mlflow.tracking.fluent: Experiment with name 'test-experiment' does not exist. Creating a new experiment.
2026/01/18 16:26:17 INFO mlflow.tracking.fluent: Experiment with name 'test-benchmark-experiment' does not exist. Creating a new experiment.
2026/01/18 16:26:17 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-benchmark-experiment', None, 'active', 1768753577640, 1768753577640)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 350, in create_experiment
    eid = session.query(SqlExperiment).filter_by(name=name).first().experiment_id
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2759, in first
    return self.limit(1)._iter().first()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1233, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-benchmark-experiment', None, 'active', 1768753577640, 1768753577640)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:17,641 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-benchmark-experiment', None, 'active', 1768753577640, 1768753577640)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:17 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-benchmark-experiment', None, 'active', 1768753577640, 1768753577640)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 16:26:17,641 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Continuing without MLflow tracking...
2026-01-18 16:26:17 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Continuing without MLflow tracking...
2026-01-18 16:26:17,660 - evaluation.benchmarking.orchestrator_original - INFO - Benchmarking distilbert (bbbbbbbbbbbbbbbb)...
2026-01-18 16:26:17 INFO  [evaluation.benchmarking.orchestrator_original] Benchmarking distilbert (bbbbbbbbbbbbbbbb)...
2026-01-18 16:26:17,660 - evaluation.benchmarking.orchestrator_original - INFO - [BENCHMARK] Final run IDs: trial=None..., refit=trial-run-12..., sweep=None...
2026-01-18 16:26:17 INFO  [evaluation.benchmarking.orchestrator_original] [BENCHMARK] Final run IDs: trial=None..., refit=trial-run-12..., sweep=None...
2026-01-18 16:26:17,660 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /workspaces/resume-ner-azureml/src/evaluation/benchmarking/cli.py --checkpoint /workspaces/resume-ner-azureml/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint --test-data /tmp/pytest-of-codespace/pytest-120/test_benchmarking_execution_mo0/dataset_tiny/seed0/test.json --batch-sizes 1 --iterations 10 --warmup 10 --max-length 512 --output /workspaces/resume-ner-azureml/outputs/benchmarking/local/distilbert/study-df9d920c/trial-bbbbbbbb/bench-fcfe0227/benchmark.json
2026-01-18 16:26:17 INFO  [evaluation.benchmarking.utils] Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /workspaces/resume-ner-azureml/src/evaluation/benchmarking/cli.py --checkpoint /workspaces/resume-ner-azureml/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint --test-data /tmp/pytest-of-codespace/pytest-120/test_benchmarking_execution_mo0/dataset_tiny/seed0/test.json --batch-sizes 1 --iterations 10 --warmup 10 --max-length 512 --output /workspaces/resume-ner-azureml/outputs/benchmarking/local/distilbert/study-df9d920c/trial-bbbbbbbb/bench-fcfe0227/benchmark.json
2026-01-18 16:26:17,660 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Building run name: trial_id=bbbbbbbbbbbbbbbb, root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config
2026-01-18 16:26:17 INFO  [evaluation.benchmarking.utils] [Benchmark Run Name] Building run name: trial_id=bbbbbbbbbbbbbbbb, root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config
2026-01-18 16:26:17,661 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 16:26:17 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 16:26:17,661 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=benchmarking
2026-01-18 16:26:17 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=benchmarking
2026-01-18 16:26:17,661 - infrastructure.tracking.mlflow.index.version_counter - INFO - [Reserve Version] Starting reservation: counter_key=resume-ner:benchmarking:038f1ba4748276ba5c84467fe877faf9da74..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-18 16:26:17 INFO  [infrastructure.tracking.mlflow.index.version_counter] [Reserve Version] Starting reservation: counter_key=resume-ner:benchmarking:038f1ba4748276ba5c84467fe877faf9da74..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-18 16:26:17,661 - infrastructure.tracking.mlflow.index.version_counter - INFO - [Reserve Version] Loaded 33 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-18 16:26:17 INFO  [infrastructure.tracking.mlflow.index.version_counter] [Reserve Version] Loaded 33 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-18 16:26:17,661 - infrastructure.tracking.mlflow.index.version_counter - INFO - [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0
2026-01-18 16:26:17 INFO  [infrastructure.tracking.mlflow.index.version_counter] [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0
2026-01-18 16:26:17,661 - infrastructure.tracking.mlflow.index.version_counter - INFO - [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)
2026-01-18 16:26:17 INFO  [infrastructure.tracking.mlflow.index.version_counter] [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)
2026-01-18 16:26:17,662 - infrastructure.tracking.mlflow.index.version_counter - INFO - [Reserve Version]  Successfully reserved version 1 for counter_key resume-ner:benchmarking:038f1ba4748276ba5c84467fe8... (run_id: pending_2026...)
2026-01-18 16:26:17 INFO  [infrastructure.tracking.mlflow.index.version_counter] [Reserve Version]  Successfully reserved version 1 for counter_key resume-ner:benchmarking:038f1ba4748276ba5c84467fe8... (run_id: pending_2026...)
2026-01-18 16:26:17,662 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Generated run name: local_distilbert_benchmark_study-df9d920c_trial-bbbbbbbb_bench-fcfe0227_1
2026-01-18 16:26:17 INFO  [evaluation.benchmarking.utils] [Benchmark Run Name] Generated run name: local_distilbert_benchmark_study-df9d920c_trial-bbbbbbbb_bench-fcfe0227_1
2026-01-18 16:26:17,662 - infrastructure.tracking.mlflow.trackers.benchmark_tracker - WARNING - [START_BENCHMARK_RUN] No valid parent run IDs found. Received: trial=None, refit=trial-run-123, sweep=None. These may be timestamps or None - will query MLflow if trial_key_hash available.
2026-01-18 16:26:17 WARNI [infrastructure.tracking.mlflow.trackers.benchmark_tracker] [START_BENCHMARK_RUN] No valid parent run IDs found. Received: trial=None, refit=trial-run-123, sweep=None. These may be timestamps or None - will query MLflow if trial_key_hash available.
2026-01-18 16:26:17,671 - infrastructure.tracking.mlflow.trackers.benchmark_tracker - WARNING - MLflow tracking failed: No Experiment with id=781293509508769937 exists
2026-01-18 16:26:17 WARNI [infrastructure.tracking.mlflow.trackers.benchmark_tracker] MLflow tracking failed: No Experiment with id=781293509508769937 exists
2026-01-18 16:26:17,671 - infrastructure.tracking.mlflow.trackers.benchmark_tracker - WARNING - Continuing benchmarking without MLflow tracking...
2026-01-18 16:26:17 WARNI [infrastructure.tracking.mlflow.trackers.benchmark_tracker] Continuing benchmarking without MLflow tracking...
2026-01-18 16:26:17,677 - infrastructure.tracking.mlflow.trackers.benchmark_tracker - WARNING - Could not log benchmark results to MLflow: No Experiment with id=781293509508769937 exists
2026-01-18 16:26:17 WARNI [infrastructure.tracking.mlflow.trackers.benchmark_tracker] Could not log benchmark results to MLflow: No Experiment with id=781293509508769937 exists
2026-01-18 16:26:17,677 - evaluation.benchmarking.orchestrator_original - INFO - Benchmark completed: /workspaces/resume-ner-azureml/outputs/benchmarking/local/distilbert/study-df9d920c/trial-bbbbbbbb/bench-fcfe0227/benchmark.json
2026-01-18 16:26:17 INFO  [evaluation.benchmarking.orchestrator_original] Benchmark completed: /workspaces/resume-ner-azureml/outputs/benchmarking/local/distilbert/study-df9d920c/trial-bbbbbbbb/bench-fcfe0227/benchmark.json
2026-01-18 16:26:17,677 - evaluation.benchmarking.orchestrator_original - INFO - Benchmarking complete. 1/1 trials benchmarked.
2026-01-18 16:26:17 INFO  [evaluation.benchmarking.orchestrator_original] Benchmarking complete. 1/1 trials benchmarked.
2026-01-18 16:26:17,749 - deployment.conversion.orchestration - INFO - Output directory: /workspaces/resume-ner-azureml/outputs/conversion/local/local/spec-aaaaaaaa_exec-bbbbbbbb/v1/conv-cd2379f5
2026-01-18 16:26:17 INFO  [deployment.conversion.orchestration] Output directory: /workspaces/resume-ner-azureml/outputs/conversion/local/local/spec-aaaaaaaa_exec-bbbbbbbb/v1/conv-cd2379f5
2026/01/18 16:26:17 INFO mlflow.tracking.fluent: Experiment with name 'resume_ner_baseline-conversion' does not exist. Creating a new experiment.
2026-01-18 16:26:17,752 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 16:26:17 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 16:26:17,752 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=conversion
2026-01-18 16:26:17 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=conversion
2026-01-18 16:26:17,755 - deployment.conversion.orchestration - INFO - Created MLflow run: local_local_conversion_spec-aaaaaaaa_exec-bbbbbbbb_v1_conv-cd2379f5 (run-123...)
2026-01-18 16:26:17 INFO  [deployment.conversion.orchestration] Created MLflow run: local_local_conversion_spec-aaaaaaaa_exec-bbbbbbbb_v1_conv-cd2379f5 (run-123...)
2026-01-18 16:26:17,755 - deployment.conversion.orchestration - INFO - Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /tmp/pytest-of-codespace/pytest-120/test_best_config_selection_e2e0/final_training/local/distilbert/spec-aaaaaaaa_exec-bbbbbbbb/v1/checkpoint --config-dir /workspaces/resume-ner-azureml/config --backbone local --output-dir /workspaces/resume-ner-azureml/outputs/conversion/local/local/spec-aaaaaaaa_exec-bbbbbbbb/v1/conv-cd2379f5 --opset-version 18 --run-smoke-test
2026-01-18 16:26:17 INFO  [deployment.conversion.orchestration] Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /tmp/pytest-of-codespace/pytest-120/test_best_config_selection_e2e0/final_training/local/distilbert/spec-aaaaaaaa_exec-bbbbbbbb/v1/checkpoint --config-dir /workspaces/resume-ner-azureml/config --backbone local --output-dir /workspaces/resume-ner-azureml/outputs/conversion/local/local/spec-aaaaaaaa_exec-bbbbbbbb/v1/conv-cd2379f5 --opset-version 18 --run-smoke-test
2026-01-18 16:26:17,756 - infrastructure.tracking.mlflow.lifecycle - INFO - Run run-123... already terminated with status <Mock name='mock.get_run().info.status' id='123304024103312'> (expected FINISHED)
2026-01-18 16:26:17 INFO  [infrastructure.tracking.mlflow.lifecycle] Run run-123... already terminated with status <Mock name='mock.get_run().info.status' id='123304024103312'> (expected FINISHED)
2026-01-18 16:26:17,756 - deployment.conversion.orchestration - INFO - Conversion completed. ONNX model: /workspaces/resume-ner-azureml/outputs/conversion/local/local/spec-aaaaaaaa_exec-bbbbbbbb/v1/conv-cd2379f5/model_fp32.onnx
2026-01-18 16:26:17 INFO  [deployment.conversion.orchestration] Conversion completed. ONNX model: /workspaces/resume-ner-azureml/outputs/conversion/local/local/spec-aaaaaaaa_exec-bbbbbbbb/v1/conv-cd2379f5/model_fp32.onnx

ERROR conda.cli.main_run:execute(127): `conda run pytest tests/ --tb=short -q` failed. (See above for error)

