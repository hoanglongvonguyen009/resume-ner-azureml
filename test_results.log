
============================================================
[INFO] Pytest log file: /workspaces/resume-ner-azureml/outputs/pytest_logs/pytest_20260118_003101.log
============================================================

============================= test session starts ==============================
platform linux -- Python 3.10.19, pytest-9.0.2, pluggy-1.6.0 -- /opt/conda/envs/resume-ner-training/bin/python
cachedir: .pytest_cache
rootdir: /workspaces/resume-ner-azureml
configfile: pytest.ini
plugins: xdist-3.8.0, cov-7.0.0, asyncio-1.3.0, mock-3.15.1, anyio-4.12.1
asyncio: mode=auto, debug=False, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collecting ... 
----------------------------- live log collection ------------------------------
2026-01-18 00:31:05 INFO     infrastructure.tracking.mlflow.compatibility: Applied Azure ML artifact compatibility patch
collected 1449 items

tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_missing_benchmark_yaml_uses_defaults PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_empty_batch_sizes_handled_gracefully PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_non_integer_batch_sizes_type_check PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_negative_iterations_handled PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_zero_iterations_handled PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_non_integer_iterations_type_check PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_negative_warmup_iterations_handled PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_non_integer_warmup_type_check PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_negative_max_length_handled PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_zero_max_length_handled PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_non_integer_max_length_type_check PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_invalid_device_value_handled PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_test_data_nonexistent_path_handled PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_test_data_resolution_fallback_logic PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_output_filename_with_path_separators PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_missing_benchmarking_section_uses_defaults PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_missing_output_section_uses_defaults PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_benchmark_best_trials_handles_missing_test_data 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: Skipping benchmarking (test data not available)
PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_run_benchmarking_handles_missing_benchmark_script Warning: Benchmark script not found: /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_handles_0/src/evaluation/benchmarking/cli.py
PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_run_benchmarking_handles_subprocess_failure 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:10 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_handles_1/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_handles_1/checkpoint --test-data /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_handles_1/test.json --batch-sizes 1 8 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_handles_1/benchmark.json
2026-01-18 00:31:10 ERROR    evaluation.benchmarking.utils: Benchmarking failed with return code 1
PASSED
tests/benchmarking/integration/test_benchmark_mlflow_tracking.py::TestBenchmarkMlflowTrackingWithTrialId::test_benchmark_passes_trial_id_to_run_benchmarking 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: Benchmarking distilbert (trial-25d03eeb)...
2026-01-18 00:31:10 WARNING  evaluation.benchmarking.orchestrator: [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=xyz789uvw012...
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: Benchmark completed: /tmp/pytest-of-codespace/pytest-38/test_benchmark_passes_trial_id0/outputs/benchmarking/test/benchmark.json
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: Benchmarking complete. 1/1 trials benchmarked.
PASSED
tests/benchmarking/integration/test_benchmark_mlflow_tracking.py::TestBenchmarkMlflowTrackingWithTrialId::test_benchmark_passes_trial_id_old_format 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: Benchmarking distilbert (trial_1_20251231_161745)...
2026-01-18 00:31:10 WARNING  evaluation.benchmarking.orchestrator: [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=xyz789uvw012...
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: Benchmark completed: /tmp/pytest-of-codespace/pytest-38/test_benchmark_passes_trial_id1/outputs/benchmarking/test/benchmark.json
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: Benchmarking complete. 1/1 trials benchmarked.
PASSED
tests/benchmarking/integration/test_benchmark_mlflow_tracking.py::TestBenchmarkMlflowTrackingWithTrialId::test_run_benchmarking_mlflow_tracking_with_trial_id 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:10 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_mlflow_t0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_mlflow_t0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_mlflow_t0/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_mlflow_t0/benchmark.json
2026-01-18 00:31:10 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Building run name: trial_id=trial-25d03eeb, root_dir=/tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_mlflow_t0, config_dir=/tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_mlflow_t0/config
2026-01-18 00:31:10 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_mlflow_t0/config/naming.yaml, using empty policy
2026-01-18 00:31:10 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Generated run name: <Mock name='create_naming_context().storage_env' id='140077280139472'>_<Mock name='create_naming_context().model' id='140077280138224'>_<Mock name='create_naming_context().process_type' id='140077280137360'>_legacy
PASSED
tests/benchmarking/integration/test_benchmark_mlflow_tracking.py::TestBenchmarkMlflowTrackingWithTrialId::test_run_benchmarking_mlflow_tracking_fallback_to_trial_key_hash 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:10 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_mlflow_t1/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_mlflow_t1/checkpoint --test-data /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_mlflow_t1/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_mlflow_t1/benchmark.json
2026-01-18 00:31:10 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Constructed trial_id from trial_key_hash: trial-25d03eeb
2026-01-18 00:31:10 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Building run name: trial_id=trial-25d03eeb, root_dir=/tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_mlflow_t1, config_dir=/tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_mlflow_t1/config
2026-01-18 00:31:10 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_mlflow_t1/config/naming.yaml, using empty policy
2026-01-18 00:31:10 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Generated run name: <Mock name='create_naming_context().storage_env' id='140077279578144'>_<Mock name='create_naming_context().model' id='140077279576608'>_<Mock name='create_naming_context().process_type' id='140077279576752'>_legacy
PASSED
tests/benchmarking/integration/test_benchmark_orchestrator.py::TestBenchmarkOrchestratorConfigUsage::test_benchmark_best_trials_uses_config_batch_sizes 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: Benchmarking distilbert (trial_0)...
2026-01-18 00:31:10 WARNING  evaluation.benchmarking.orchestrator: [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: Benchmark completed: /tmp/pytest-of-codespace/pytest-38/test_benchmark_best_trials_use0/outputs/benchmarking/test/benchmark.json
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: Benchmarking complete. 1/1 trials benchmarked.
PASSED
tests/benchmarking/integration/test_benchmark_orchestrator.py::TestBenchmarkOrchestratorConfigUsage::test_benchmark_best_trials_uses_config_iterations 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: Benchmarking distilbert (trial_0)...
2026-01-18 00:31:10 WARNING  evaluation.benchmarking.orchestrator: [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: Benchmark completed: /tmp/pytest-of-codespace/pytest-38/test_benchmark_best_trials_use1/outputs/benchmarking/test/benchmark.json
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: Benchmarking complete. 1/1 trials benchmarked.
PASSED
tests/benchmarking/integration/test_benchmark_orchestrator.py::TestBenchmarkOrchestratorConfigUsage::test_benchmark_best_trials_uses_config_warmup 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: Benchmarking distilbert (trial_0)...
2026-01-18 00:31:10 WARNING  evaluation.benchmarking.orchestrator: [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: Benchmark completed: /tmp/pytest-of-codespace/pytest-38/test_benchmark_best_trials_use2/outputs/benchmarking/test/benchmark.json
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: Benchmarking complete. 1/1 trials benchmarked.
PASSED
tests/benchmarking/integration/test_benchmark_orchestrator.py::TestBenchmarkOrchestratorConfigUsage::test_benchmark_best_trials_uses_config_max_length 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: Benchmarking distilbert (trial_0)...
2026-01-18 00:31:10 WARNING  evaluation.benchmarking.orchestrator: [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: Benchmark completed: /tmp/pytest-of-codespace/pytest-38/test_benchmark_best_trials_use3/outputs/benchmarking/test/benchmark.json
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: Benchmarking complete. 1/1 trials benchmarked.
PASSED
tests/benchmarking/integration/test_benchmark_orchestrator.py::TestBenchmarkOrchestratorConfigUsage::test_benchmark_best_trials_uses_config_device 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: Benchmarking distilbert (trial_0)...
2026-01-18 00:31:10 WARNING  evaluation.benchmarking.orchestrator: [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: Benchmark completed: /tmp/pytest-of-codespace/pytest-38/test_benchmark_best_trials_use4/outputs/benchmarking/test/custom_benchmark.json
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: Benchmarking complete. 1/1 trials benchmarked.
PASSED
tests/benchmarking/integration/test_benchmark_orchestrator.py::TestBenchmarkOrchestratorConfigUsage::test_benchmark_best_trials_uses_output_filename 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: Benchmarking distilbert (trial_0)...
2026-01-18 00:31:10 WARNING  evaluation.benchmarking.orchestrator: [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: Benchmark completed: /tmp/pytest-of-codespace/pytest-38/test_benchmark_best_trials_use5/outputs/benchmarking/test/benchmark.json
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: Benchmarking complete. 1/1 trials benchmarked.
PASSED
tests/benchmarking/integration/test_benchmark_orchestrator.py::TestBenchmarkOrchestratorConfigUsage::test_benchmark_best_trials_uses_custom_output_filename 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: Benchmarking distilbert (trial_0)...
2026-01-18 00:31:10 WARNING  evaluation.benchmarking.orchestrator: [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: Benchmark completed: /tmp/pytest-of-codespace/pytest-38/test_benchmark_best_trials_use6/outputs/benchmarking/test/custom_benchmark.json
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: Benchmarking complete. 1/1 trials benchmarked.
PASSED
tests/benchmarking/integration/test_benchmark_orchestrator.py::TestBenchmarkOrchestratorConfigUsage::test_benchmark_best_trials_all_config_options_together 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: Benchmarking distilbert (trial_0)...
2026-01-18 00:31:10 WARNING  evaluation.benchmarking.orchestrator: [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: Benchmark completed: /tmp/pytest-of-codespace/pytest-38/test_benchmark_best_trials_all0/outputs/benchmarking/test/custom_benchmark.json
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: Benchmarking complete. 1/1 trials benchmarked.
PASSED
tests/benchmarking/integration/test_benchmark_orchestrator.py::TestBenchmarkOrchestratorConfigUsage::test_benchmark_best_trials_defaults_when_config_missing 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:10 WARNING  infrastructure.paths.resolve: Pattern 'benchmarking_v2' not found in paths.yaml. Using fallback logic.
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: Benchmarking distilbert (trial_0)...
2026-01-18 00:31:10 WARNING  evaluation.benchmarking.orchestrator: [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
Warning: Benchmark script not found: /tmp/pytest-of-codespace/pytest-38/test_benchmark_best_trials_def0/outputs/src/evaluation/benchmarking/cli.py
2026-01-18 00:31:10 ERROR    evaluation.benchmarking.orchestrator: Benchmark failed for distilbert
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: Benchmarking complete. 0/1 trials benchmarked.
PASSED
tests/benchmarking/integration/test_benchmark_run_mode_idempotency.py::TestRunModeIntegration::test_force_new_returns_all_champions 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: Run mode is 'force_new' - skipping idempotency check, will benchmark all champions
PASSED
tests/benchmarking/integration/test_benchmark_run_mode_idempotency.py::TestRunModeIntegration::test_reuse_if_exists_filters_existing_benchmarks 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: Skipping distilbert - benchmark already exists (trial_key_hash=trial_hash_456...)
PASSED
tests/benchmarking/integration/test_benchmark_run_mode_idempotency.py::TestRunModeIntegration::test_get_benchmark_run_mode_from_config PASSED
tests/benchmarking/integration/test_benchmark_run_mode_idempotency.py::TestBenchmarkKeyIdempotencyIntegration::test_benchmark_key_primary_check_succeeds PASSED
tests/benchmarking/integration/test_benchmark_run_mode_idempotency.py::TestBenchmarkKeyIdempotencyIntegration::test_benchmark_key_primary_check_fails_then_fallback 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:10 WARNING  evaluation.benchmarking.orchestrator: Found 1 finished benchmark run(s) by hash (fallback). Consider setting benchmark_key tag for more reliable idempotency. trial_key_hash=trial_hash_456...
PASSED
tests/benchmarking/integration/test_benchmark_run_mode_idempotency.py::TestBenchmarkKeyIdempotencyIntegration::test_benchmark_key_changes_with_config_creates_new_benchmark PASSED
tests/benchmarking/integration/test_benchmark_run_mode_idempotency.py::TestBenchmarkKeyIdempotencyIntegration::test_benchmark_key_same_config_reuses_benchmark PASSED
tests/benchmarking/integration/test_benchmark_run_mode_idempotency.py::TestBackwardCompatibilityFallback::test_fallback_to_hash_when_benchmark_key_tag_missing 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:10 WARNING  evaluation.benchmarking.orchestrator: Found 1 finished benchmark run(s) by hash (fallback). Consider setting benchmark_key tag for more reliable idempotency. trial_key_hash=trial_hash_456...
PASSED
tests/benchmarking/integration/test_benchmark_run_mode_idempotency.py::TestBackwardCompatibilityFallback::test_fallback_requires_both_hashes PASSED
tests/benchmarking/integration/test_benchmark_run_mode_idempotency.py::TestCompleteWorkflowScenarios::test_scenario_reuse_if_exists_with_existing_benchmark 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: Skipping distilbert - benchmark already exists (trial_key_hash=trial_hash_456...)
PASSED
tests/benchmarking/integration/test_benchmark_run_mode_idempotency.py::TestCompleteWorkflowScenarios::test_scenario_reuse_if_exists_without_existing_benchmark PASSED
tests/benchmarking/integration/test_benchmark_run_mode_idempotency.py::TestCompleteWorkflowScenarios::test_scenario_force_new_always_creates 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: Run mode is 'force_new' - skipping idempotency check, will benchmark all champions
PASSED
tests/benchmarking/integration/test_benchmark_run_mode_idempotency.py::TestCompleteWorkflowScenarios::test_scenario_config_change_creates_new_benchmark PASSED
tests/benchmarking/integration/test_benchmark_utils.py::TestBenchmarkUtilsConfigUsage::test_run_benchmarking_uses_batch_sizes 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:10 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_bat0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_bat0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_bat0/test.json --batch-sizes 1 8 16 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_bat0/benchmark.json
PASSED
tests/benchmarking/integration/test_benchmark_utils.py::TestBenchmarkUtilsConfigUsage::test_run_benchmarking_uses_iterations 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:10 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_ite0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_ite0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_ite0/test.json --batch-sizes 1 8 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_ite0/benchmark.json
PASSED
tests/benchmarking/integration/test_benchmark_utils.py::TestBenchmarkUtilsConfigUsage::test_run_benchmarking_uses_warmup_iterations 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:10 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_war0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_war0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_war0/test.json --batch-sizes 1 8 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_war0/benchmark.json
PASSED
tests/benchmarking/integration/test_benchmark_utils.py::TestBenchmarkUtilsConfigUsage::test_run_benchmarking_uses_max_length 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:10 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_max0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_max0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_max0/test.json --batch-sizes 1 8 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_max0/benchmark.json
PASSED
tests/benchmarking/integration/test_benchmark_utils.py::TestBenchmarkUtilsConfigUsage::test_run_benchmarking_uses_device_when_provided 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:10 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_dev0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_dev0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_dev0/test.json --batch-sizes 1 8 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_dev0/benchmark.json --device cuda
PASSED
tests/benchmarking/integration/test_benchmark_utils.py::TestBenchmarkUtilsConfigUsage::test_run_benchmarking_skips_device_when_null 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:10 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_skips_de0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_skips_de0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_skips_de0/test.json --batch-sizes 1 8 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_skips_de0/benchmark.json
PASSED
tests/benchmarking/integration/test_benchmark_utils.py::TestBenchmarkUtilsConfigUsage::test_run_benchmarking_uses_output_path 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:10 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_out0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_out0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_out0/test.json --batch-sizes 1 8 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_out0/benchmark.json
PASSED
tests/benchmarking/integration/test_benchmark_utils.py::TestBenchmarkUtilsConfigUsage::test_run_benchmarking_all_config_options_together 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:10 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_all_conf0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_all_conf0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_all_conf0/test.json --batch-sizes 2 4 32 --iterations 200 --warmup 20 --max-length 256 --output /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_all_conf0/custom_benchmark.json --device cuda
PASSED
tests/benchmarking/integration/test_benchmark_workflow.py::TestBenchmarkWorkflow::test_workflow_loads_config_and_uses_all_options 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: Benchmarking distilbert (trial_0)...
2026-01-18 00:31:10 WARNING  evaluation.benchmarking.orchestrator: [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: Benchmark completed: /tmp/pytest-of-codespace/pytest-38/test_workflow_loads_config_and0/outputs/benchmarking/test/benchmark.json
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: Benchmarking complete. 1/1 trials benchmarked.
PASSED
tests/benchmarking/integration/test_benchmark_workflow.py::TestBenchmarkWorkflow::test_workflow_custom_config_values 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:10 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-38/test_workflow_custom_config_va0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-38/test_workflow_custom_config_va0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-38/test_workflow_custom_config_va0/test.json --batch-sizes 2 4 32 --iterations 200 --warmup 20 --max-length 256 --output /tmp/pytest-of-codespace/pytest-38/test_workflow_custom_config_va0/custom_benchmark.json --device cuda
PASSED
tests/benchmarking/integration/test_benchmark_workflow.py::TestBenchmarkWorkflow::test_workflow_defaults_when_config_missing 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: Benchmarking distilbert (trial_0)...
2026-01-18 00:31:10 WARNING  evaluation.benchmarking.orchestrator: [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: Benchmark completed: /tmp/pytest-of-codespace/pytest-38/test_workflow_defaults_when_co0/outputs/benchmarking/test/benchmark.json
2026-01-18 00:31:10 INFO     evaluation.benchmarking.orchestrator: Benchmarking complete. 1/1 trials benchmarked.
PASSED
tests/benchmarking/unit/test_benchmark_config.py::TestBenchmarkConfigLoading::test_load_experiment_config_includes_benchmark PASSED
tests/benchmarking/unit/test_benchmark_config.py::TestBenchmarkConfigLoading::test_load_experiment_config_defaults_to_benchmark_yaml PASSED
tests/benchmarking/unit/test_benchmark_config.py::TestBenchmarkConfigLoading::test_load_all_configs_loads_benchmark_if_exists PASSED
tests/benchmarking/unit/test_benchmark_config.py::TestBenchmarkConfigLoading::test_load_all_configs_skips_benchmark_if_not_exists PASSED
tests/benchmarking/unit/test_benchmark_config.py::TestBenchmarkConfigLoading::test_load_benchmark_config_structure PASSED
tests/benchmarking/unit/test_benchmark_config.py::TestBenchmarkConfigLoading::test_load_benchmark_config_with_custom_values PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_batch_sizes_extraction PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_batch_sizes_default PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_batch_sizes_custom_values PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_iterations_extraction PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_iterations_default PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_iterations_custom_value PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_warmup_iterations_extraction PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_warmup_iterations_default PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_warmup_iterations_custom_value PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_max_length_extraction PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_max_length_default PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_max_length_custom_value PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_device_extraction_null PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_device_extraction_cuda PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_device_extraction_cpu PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_device_default PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_test_data_extraction_null PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_test_data_extraction_relative_path PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_test_data_extraction_absolute_path PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_test_data_default PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_output_filename_extraction PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_output_filename_default PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_output_filename_custom_value PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_all_options_together PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestRunModeConfig::test_run_mode_extraction_reuse_if_exists PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestRunModeConfig::test_run_mode_extraction_force_new PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestRunModeConfig::test_run_mode_default_when_missing PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestRunModeConfig::test_run_mode_default_when_run_section_missing PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestRunModeConfig::test_get_benchmark_run_mode_uses_config PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestRunModeConfig::test_get_benchmark_run_mode_default PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestRunModeBehavior::test_force_new_skips_filtering 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     evaluation.benchmarking.orchestrator: Run mode is 'force_new' - skipping idempotency check, will benchmark all champions
PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestRunModeBehavior::test_reuse_if_exists_filters_existing 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     evaluation.benchmarking.orchestrator: Skipping distilbert - benchmark already exists (trial_key_hash=trial_hash_456...)
2026-01-18 00:31:11 INFO     evaluation.benchmarking.orchestrator: Skipping deberta - benchmark already exists (trial_key_hash=trial_hash_789...)
PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestBenchmarkKeyIdempotency::test_build_benchmark_key_includes_config_hash PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestBenchmarkKeyIdempotency::test_benchmark_key_changes_with_config_change PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestBenchmarkKeyIdempotency::test_benchmark_key_same_with_same_config PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestBenchmarkKeyIdempotency::test_benchmark_key_used_as_primary_check PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestBenchmarkKeyFallback::test_fallback_to_hash_when_benchmark_key_not_found PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestBenchmarkKeyFallback::test_fallback_requires_both_hashes PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestIdempotencySuccessCases::test_success_benchmark_exists_by_benchmark_key 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     evaluation.benchmarking.orchestrator: Skipping distilbert - benchmark already exists (trial_key_hash=trial_hash_456...)
PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestIdempotencySuccessCases::test_success_benchmark_exists_by_hash_fallback 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     evaluation.benchmarking.orchestrator: Skipping distilbert - benchmark already exists (trial_key_hash=trial_hash_456...)
PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestIdempotencySuccessCases::test_success_benchmark_not_exists_creates_new PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestIdempotencyFailureCases::test_failure_missing_champion_run_id 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 WARNING  evaluation.benchmarking.orchestrator: No run_id found for champion distilbert, skipping idempotency check
PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestIdempotencyFailureCases::test_failure_mlflow_client_unavailable 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_failure_mlflow_client_una0/config/tags.yaml, using defaults
PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestIdempotencyFailureCases::test_failure_mlflow_check_raises_exception PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestConfigDocumentationCoverage::test_run_mode_documentation_covered PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestConfigDocumentationCoverage::test_idempotency_documentation_covered PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestConfigDocumentationCoverage::test_independence_from_hpo_config PASSED
tests/benchmarking/unit/test_benchmark_trial_id_extraction.py::TestTrialIdExtraction::test_trial_id_from_parameter_old_format 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-38/test_trial_id_from_parameter_o0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-38/test_trial_id_from_parameter_o0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-38/test_trial_id_from_parameter_o0/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-38/test_trial_id_from_parameter_o0/benchmark.json
2026-01-18 00:31:11 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Building run name: trial_id=1_20251231_161745, root_dir=/tmp/pytest-of-codespace/pytest-38/test_trial_id_from_parameter_o0, config_dir=/tmp/pytest-of-codespace/pytest-38/test_trial_id_from_parameter_o0/config
2026-01-18 00:31:11 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-38/test_trial_id_from_parameter_o0/config/naming.yaml, using empty policy
2026-01-18 00:31:11 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Generated run name: <Mock name='create_naming_context().storage_env' id='140077279584144'>_<Mock name='create_naming_context().model' id='140077279582080'>_<Mock name='create_naming_context().process_type' id='140077279577808'>_legacy
PASSED
tests/benchmarking/unit/test_benchmark_trial_id_extraction.py::TestTrialIdExtraction::test_trial_id_from_parameter_new_format 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-38/test_trial_id_from_parameter_n0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-38/test_trial_id_from_parameter_n0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-38/test_trial_id_from_parameter_n0/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-38/test_trial_id_from_parameter_n0/benchmark.json
2026-01-18 00:31:11 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Building run name: trial_id=trial-25d03eeb, root_dir=/tmp/pytest-of-codespace/pytest-38/test_trial_id_from_parameter_n0, config_dir=/tmp/pytest-of-codespace/pytest-38/test_trial_id_from_parameter_n0/config
2026-01-18 00:31:11 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-38/test_trial_id_from_parameter_n0/config/naming.yaml, using empty policy
2026-01-18 00:31:11 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Generated run name: <Mock name='create_naming_context().storage_env' id='140077280119488'>_<Mock name='create_naming_context().model' id='140077280121120'>_<Mock name='create_naming_context().process_type' id='140077280120208'>_legacy
PASSED
tests/benchmarking/unit/test_benchmark_trial_id_extraction.py::TestTrialIdExtraction::test_trial_id_extraction_from_path_old_format 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-38/test_trial_id_extraction_from_0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-38/test_trial_id_extraction_from_0/hpo/local/distilbert/study-abc123/trial_1_20251231_161745/checkpoint --test-data /tmp/pytest-of-codespace/pytest-38/test_trial_id_extraction_from_0/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-38/test_trial_id_extraction_from_0/benchmark.json
2026-01-18 00:31:11 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Extracted trial_id from checkpoint path: trial_1_20251231_161745 (at level 1)
2026-01-18 00:31:11 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Building run name: trial_id=trial_1_20251231_161745, root_dir=/tmp/pytest-of-codespace/pytest-38/test_trial_id_extraction_from_0, config_dir=/tmp/pytest-of-codespace/pytest-38/test_trial_id_extraction_from_0/config
2026-01-18 00:31:11 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-38/test_trial_id_extraction_from_0/config/naming.yaml, using empty policy
2026-01-18 00:31:11 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Generated run name: <Mock name='create_naming_context().storage_env' id='140077280128128'>_<Mock name='create_naming_context().model' id='140077280133024'>_<Mock name='create_naming_context().process_type' id='140077280132928'>_legacy
PASSED
tests/benchmarking/unit/test_benchmark_trial_id_extraction.py::TestTrialIdExtraction::test_trial_id_extraction_from_path_new_format 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-38/test_trial_id_extraction_from_1/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-38/test_trial_id_extraction_from_1/hpo/local/distilbert/study-abc123/trial-25d03eeb/checkpoint --test-data /tmp/pytest-of-codespace/pytest-38/test_trial_id_extraction_from_1/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-38/test_trial_id_extraction_from_1/benchmark.json
2026-01-18 00:31:11 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Extracted trial_id from checkpoint path: trial-25d03eeb (at level 1)
2026-01-18 00:31:11 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Building run name: trial_id=trial-25d03eeb, root_dir=/tmp/pytest-of-codespace/pytest-38/test_trial_id_extraction_from_1, config_dir=/tmp/pytest-of-codespace/pytest-38/test_trial_id_extraction_from_1/config
2026-01-18 00:31:11 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-38/test_trial_id_extraction_from_1/config/naming.yaml, using empty policy
2026-01-18 00:31:11 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Generated run name: <Mock name='create_naming_context().storage_env' id='140077279557712'>_<Mock name='create_naming_context().model' id='140077279558432'>_<Mock name='create_naming_context().process_type' id='140077279557520'>_legacy
PASSED
tests/benchmarking/unit/test_benchmark_trial_id_extraction.py::TestTrialIdExtraction::test_trial_id_extraction_from_refit_checkpoint_path 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-38/test_trial_id_extraction_from_2/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-38/test_trial_id_extraction_from_2/hpo/local/distilbert/study-abc123/trial-25d03eeb/refit/checkpoint --test-data /tmp/pytest-of-codespace/pytest-38/test_trial_id_extraction_from_2/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-38/test_trial_id_extraction_from_2/benchmark.json
2026-01-18 00:31:11 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Extracted trial_id from checkpoint path: trial-25d03eeb (at level 2)
2026-01-18 00:31:11 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Building run name: trial_id=trial-25d03eeb, root_dir=/tmp/pytest-of-codespace/pytest-38/test_trial_id_extraction_from_2, config_dir=/tmp/pytest-of-codespace/pytest-38/test_trial_id_extraction_from_2/config
2026-01-18 00:31:11 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-38/test_trial_id_extraction_from_2/config/naming.yaml, using empty policy
2026-01-18 00:31:11 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Generated run name: <Mock name='create_naming_context().storage_env' id='140077280339200'>_<Mock name='create_naming_context().model' id='140077280339248'>_<Mock name='create_naming_context().process_type' id='140077280339344'>_legacy
PASSED
tests/benchmarking/unit/test_benchmark_trial_id_extraction.py::TestTrialIdExtraction::test_trial_id_fallback_to_trial_key_hash 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-38/test_trial_id_fallback_to_tria0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-38/test_trial_id_fallback_to_tria0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-38/test_trial_id_fallback_to_tria0/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-38/test_trial_id_fallback_to_tria0/benchmark.json
2026-01-18 00:31:11 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Constructed trial_id from trial_key_hash: trial-25d03eeb
2026-01-18 00:31:11 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Building run name: trial_id=trial-25d03eeb, root_dir=/tmp/pytest-of-codespace/pytest-38/test_trial_id_fallback_to_tria0, config_dir=/tmp/pytest-of-codespace/pytest-38/test_trial_id_fallback_to_tria0/config
2026-01-18 00:31:11 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-38/test_trial_id_fallback_to_tria0/config/naming.yaml, using empty policy
2026-01-18 00:31:11 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Generated run name: <Mock name='create_naming_context().storage_env' id='140077280106368'>_<Mock name='create_naming_context().model' id='140077280106512'>_<Mock name='create_naming_context().process_type' id='140077280105840'>_legacy
PASSED
tests/benchmarking/unit/test_benchmark_trial_id_extraction.py::TestTrialIdExtraction::test_trial_id_parameter_overrides_path_extraction 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-38/test_trial_id_parameter_overri0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-38/test_trial_id_parameter_overri0/hpo/local/distilbert/study-abc123/trial-25d03eeb/checkpoint --test-data /tmp/pytest-of-codespace/pytest-38/test_trial_id_parameter_overri0/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-38/test_trial_id_parameter_overri0/benchmark.json
2026-01-18 00:31:11 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Building run name: trial_id=trial-custom123, root_dir=/tmp/pytest-of-codespace/pytest-38/test_trial_id_parameter_overri0, config_dir=/tmp/pytest-of-codespace/pytest-38/test_trial_id_parameter_overri0/config
2026-01-18 00:31:11 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-38/test_trial_id_parameter_overri0/config/naming.yaml, using empty policy
2026-01-18 00:31:11 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Generated run name: <Mock name='create_naming_context().storage_env' id='140077280296816'>_<Mock name='create_naming_context().model' id='140077280293168'>_<Mock name='create_naming_context().process_type' id='140077280285008'>_legacy
PASSED
tests/config/integration/test_config_integration.py::TestEndToEndScenarios::test_hpo_trial_workflow 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-38/test_hpo_trial_workflow0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 00:31:11 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 00:31:11 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:e0fbae020bc656b7e6425344765f3df149e5f6831d2b7..., root_dir=/tmp/pytest-of-codespace/pytest-38/test_hpo_trial_workflow0, config_dir=/tmp/pytest-of-codespace/pytest-38/test_hpo_trial_workflow0/config, counter_path=/tmp/pytest-of-codespace/pytest-38/test_hpo_trial_workflow0/outputs/cache/run_name_counter.json
2026-01-18 00:31:11 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Loaded 0 existing allocations from /tmp/pytest-of-codespace/pytest-38/test_hpo_trial_workflow0/outputs/cache/run_name_counter.json
2026-01-18 00:31:11 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0
2026-01-18 00:31:11 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)
2026-01-18 00:31:11 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version]  Successfully reserved version 1 for counter_key resume-ner:hpo:e0fbae020bc656b7e6425344765f3df149e... (run_id: pending_2026...)
PASSED
tests/config/integration/test_config_integration.py::TestEndToEndScenarios::test_final_training_workflow 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-38/test_final_training_workflow0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 00:31:11 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=final_training
PASSED
tests/config/integration/test_config_integration.py::TestEndToEndScenarios::test_benchmarking_workflow 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 WARNING  infrastructure.naming.display_policy: [Naming Policy] No pattern for process_type: benchmarking
2026-01-18 00:31:11 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-38/test_benchmarking_workflow0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 00:31:11 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=benchmarking
2026-01-18 00:31:11 WARNING  infrastructure.naming.mlflow.run_names: Could not reserve version for run name: Benchmarking requires trial_id for run_key, using base name without version
PASSED
tests/config/integration/test_config_integration.py::TestConfigurationConsistency::test_paths_match_naming_patterns 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-38/test_paths_match_naming_patter0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 00:31:11 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 00:31:11 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:fa924e9d2ac5675b719f2281bdcff80badd52b78fb7ef..., root_dir=/tmp/pytest-of-codespace/pytest-38/test_paths_match_naming_patter0, config_dir=/tmp/pytest-of-codespace/pytest-38/test_paths_match_naming_patter0/config, counter_path=/tmp/pytest-of-codespace/pytest-38/test_paths_match_naming_patter0/outputs/cache/run_name_counter.json
2026-01-18 00:31:11 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Loaded 0 existing allocations from /tmp/pytest-of-codespace/pytest-38/test_paths_match_naming_patter0/outputs/cache/run_name_counter.json
2026-01-18 00:31:11 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0
2026-01-18 00:31:11 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)
2026-01-18 00:31:11 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version]  Successfully reserved version 1 for counter_key resume-ner:hpo:fa924e9d2ac5675b719f2281bdcff80badd... (run_id: pending_2026...)
PASSED
tests/config/integration/test_config_integration.py::TestConfigurationConsistency::test_tag_keys_from_tags_yaml PASSED
tests/config/integration/test_config_integration.py::TestConfigurationConsistency::test_naming_patterns_from_naming_yaml 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-38/test_naming_patterns_from_nami0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 00:31:11 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 00:31:11 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:e0fbae020bc656b7e6425344765f3df149e5f6831d2b7..., root_dir=/tmp/pytest-of-codespace/pytest-38/test_naming_patterns_from_nami0, config_dir=/tmp/pytest-of-codespace/pytest-38/test_naming_patterns_from_nami0/config, counter_path=/tmp/pytest-of-codespace/pytest-38/test_naming_patterns_from_nami0/outputs/cache/run_name_counter.json
2026-01-18 00:31:11 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Loaded 0 existing allocations from /tmp/pytest-of-codespace/pytest-38/test_naming_patterns_from_nami0/outputs/cache/run_name_counter.json
2026-01-18 00:31:11 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0
2026-01-18 00:31:11 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)
2026-01-18 00:31:11 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version]  Successfully reserved version 1 for counter_key resume-ner:hpo:e0fbae020bc656b7e6425344765f3df149e... (run_id: pending_2026...)
PASSED
tests/config/integration/test_config_integration.py::TestCrossProcessConsistency::test_same_model_environment_consistent_paths PASSED
tests/config/integration/test_config_integration.py::TestCrossProcessConsistency::test_tag_keys_consistent_across_processes PASSED
tests/config/integration/test_config_integration.py::TestCrossProcessConsistency::test_naming_conventions_consistent 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-38/test_naming_conventions_consis0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 00:31:11 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 00:31:11 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:e0fbae020bc656b7e6425344765f3df149e5f6831d2b7..., root_dir=/tmp/pytest-of-codespace/pytest-38/test_naming_conventions_consis0, config_dir=/tmp/pytest-of-codespace/pytest-38/test_naming_conventions_consis0/config, counter_path=/tmp/pytest-of-codespace/pytest-38/test_naming_conventions_consis0/outputs/cache/run_name_counter.json
2026-01-18 00:31:11 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Loaded 0 existing allocations from /tmp/pytest-of-codespace/pytest-38/test_naming_conventions_consis0/outputs/cache/run_name_counter.json
2026-01-18 00:31:11 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0
2026-01-18 00:31:11 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)
2026-01-18 00:31:11 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version]  Successfully reserved version 1 for counter_key resume-ner:hpo:e0fbae020bc656b7e6425344765f3df149e... (run_id: pending_2026...)
PASSED
tests/config/unit/test_config_loader.py::TestConfigHashComputation::test_compute_config_hash_length PASSED
tests/config/unit/test_config_loader.py::TestConfigHashComputation::test_compute_config_hash_deterministic PASSED
tests/config/unit/test_config_loader.py::TestConfigHashComputation::test_compute_config_hash_different_configs PASSED
tests/config/unit/test_config_loader.py::TestConfigHashComputation::test_compute_config_hash_no_randomness PASSED
tests/config/unit/test_config_loader.py::TestConfigHashComputation::test_compute_config_hash_order_independent PASSED
tests/config/unit/test_config_loader.py::TestConfigHashComputation::test_compute_config_hashes_all_domains PASSED
tests/config/unit/test_config_loader.py::TestConfigHashComputation::test_compute_config_hashes_deterministic PASSED
tests/config/unit/test_config_loader.py::TestConfigLoading::test_load_experiment_config PASSED
tests/config/unit/test_config_loader.py::TestConfigLoading::test_load_experiment_config_defaults PASSED
tests/config/unit/test_config_loader.py::TestConfigLoading::test_load_all_configs PASSED
tests/config/unit/test_config_loader.py::TestConfigLoading::test_load_all_configs_with_benchmark PASSED
tests/config/unit/test_config_loader.py::TestConfigMetadata::test_create_config_metadata PASSED
tests/config/unit/test_config_loader.py::TestConfigImmutability::test_snapshot_configs PASSED
tests/config/unit/test_config_loader.py::TestConfigImmutability::test_validate_config_immutability_unchanged PASSED
tests/config/unit/test_config_loader.py::TestConfigImmutability::test_validate_config_immutability_changed PASSED
tests/config/unit/test_config_loader.py::TestConfigImmutability::test_validate_config_immutability_multiple_changes PASSED
tests/config/unit/test_data_config.py::TestDataConfigLoading::test_load_complete_data_config PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_name_option PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_version_option PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_description_option PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_local_path_option PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_seed_option PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_splitting_train_test_ratio PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_splitting_stratified_true PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_splitting_stratified_false PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_splitting_random_seed PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_schema_format PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_schema_annotation_format PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_schema_entity_types PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_schema_stats_median_sentence_length PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_schema_stats_mean_sentence_length PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_schema_stats_p95_sentence_length PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_schema_stats_suggested_sequence_length PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_schema_stats_entity_density PASSED
tests/config/unit/test_data_config.py::TestDataConfigIntegration::test_data_config_via_experiment_config PASSED
tests/config/unit/test_data_config.py::TestDataConfigIntegration::test_build_label_list_from_data_config PASSED
tests/config/unit/test_data_config.py::TestDataConfigValidation::test_data_config_missing_optional_sections PASSED
tests/config/unit/test_data_config.py::TestDataConfigValidation::test_data_config_partial_splitting PASSED
tests/config/unit/test_data_config.py::TestDataConfigValidation::test_data_config_partial_schema PASSED
tests/config/unit/test_data_config.py::TestDataConfigValidation::test_data_config_partial_stats PASSED
tests/config/unit/test_data_config.py::TestDataConfigValidation::test_data_config_numeric_types PASSED
tests/config/unit/test_data_config.py::TestDataConfigValidation::test_data_config_boolean_types PASSED
tests/config/unit/test_data_config.py::TestDataConfigValidation::test_data_config_entity_types_list PASSED
tests/config/unit/test_data_config.py::TestDataConfigRealFiles::test_load_real_resume_tiny_config PASSED
tests/config/unit/test_data_config.py::TestDataConfigRealFiles::test_load_real_resume_v1_config PASSED
tests/config/unit/test_data_config.py::TestDataConfigRealFiles::test_all_real_data_configs_have_required_sections SKIPPED
tests/config/unit/test_experiment_config.py::TestExperimentConfigLoading::test_load_complete_experiment_config PASSED
tests/config/unit/test_experiment_config.py::TestExperimentConfigLoading::test_load_experiment_config_with_defaults PASSED
tests/config/unit/test_experiment_config.py::TestExperimentConfigOptions::test_experiment_name_option PASSED
tests/config/unit/test_experiment_config.py::TestExperimentConfigOptions::test_experiment_name_fallback PASSED
tests/config/unit/test_experiment_config.py::TestExperimentConfigOptions::test_data_config_option PASSED
tests/config/unit/test_experiment_config.py::TestExperimentConfigOptions::test_model_config_option PASSED
tests/config/unit/test_experiment_config.py::TestExperimentConfigOptions::test_train_config_option PASSED
tests/config/unit/test_experiment_config.py::TestExperimentConfigOptions::test_hpo_config_option PASSED
tests/config/unit/test_experiment_config.py::TestExperimentConfigOptions::test_env_config_option PASSED
tests/config/unit/test_experiment_config.py::TestExperimentConfigOptions::test_benchmark_config_option PASSED
tests/config/unit/test_experiment_config.py::TestExperimentConfigOptions::test_benchmark_config_default PASSED
tests/config/unit/test_experiment_config.py::TestStagesConfiguration::test_stages_smoke_aml_experiment PASSED
tests/config/unit/test_experiment_config.py::TestStagesConfiguration::test_stages_smoke_hpo_config PASSED
tests/config/unit/test_experiment_config.py::TestStagesConfiguration::test_stages_smoke_both_options PASSED
tests/config/unit/test_experiment_config.py::TestStagesConfiguration::test_stages_hpo_aml_experiment PASSED
tests/config/unit/test_experiment_config.py::TestStagesConfiguration::test_stages_hpo_hpo_config PASSED
tests/config/unit/test_experiment_config.py::TestStagesConfiguration::test_stages_hpo_both_options PASSED
tests/config/unit/test_experiment_config.py::TestStagesConfiguration::test_stages_training_aml_experiment PASSED
tests/config/unit/test_experiment_config.py::TestStagesConfiguration::test_stages_training_backbones_single PASSED
tests/config/unit/test_experiment_config.py::TestStagesConfiguration::test_stages_training_backbones_multiple PASSED
tests/config/unit/test_experiment_config.py::TestStagesConfiguration::test_stages_training_both_options PASSED
tests/config/unit/test_experiment_config.py::TestStagesConfiguration::test_stages_all_stages_together PASSED
tests/config/unit/test_experiment_config.py::TestNamingConfiguration::test_naming_include_backbone_in_experiment_true PASSED
tests/config/unit/test_experiment_config.py::TestNamingConfiguration::test_naming_include_backbone_in_experiment_false PASSED
tests/config/unit/test_experiment_config.py::TestNamingConfiguration::test_naming_missing_defaults_to_empty PASSED
tests/config/unit/test_experiment_config.py::TestExperimentConfigIntegration::test_load_all_configs_with_experiment_config PASSED
tests/config/unit/test_experiment_config.py::TestExperimentConfigIntegration::test_experiment_config_stages_preserved PASSED
tests/config/unit/test_experiment_config.py::TestExperimentConfigIntegration::test_experiment_config_naming_preserved PASSED
tests/config/unit/test_experiment_config.py::TestExperimentConfigRealFile::test_load_real_resume_ner_baseline_config SKIPPED
tests/config/unit/test_experiment_config.py::TestExperimentConfigRealFile::test_real_resume_ner_baseline_has_all_sections SKIPPED
tests/config/unit/test_fingerprint_placeholder_fallback.py::TestFingerprintPlaceholderFallback::test_compute_fingerprints_returns_placeholders_on_import_error PASSED
tests/config/unit/test_fingerprint_placeholder_fallback.py::TestFingerprintPlaceholderFallback::test_compute_fingerprints_import_error_handling PASSED
tests/config/unit/test_fingerprint_placeholder_fallback.py::TestFingerprintPlaceholderFallback::test_placeholder_values_are_short_enough_for_naming PASSED
tests/config/unit/test_fingerprints.py::test_compute_spec_fp_deterministic PASSED
tests/config/unit/test_fingerprints.py::test_compute_spec_fp_without_seed PASSED
tests/config/unit/test_fingerprints.py::test_compute_spec_fp_different_seeds PASSED
tests/config/unit/test_fingerprints.py::test_compute_exec_fp_basic PASSED
tests/config/unit/test_fingerprints.py::test_compute_exec_fp_auto_detect PASSED
tests/config/unit/test_fingerprints.py::test_compute_exec_fp_different_precision PASSED
tests/config/unit/test_fingerprints.py::test_compute_conv_fp PASSED
tests/config/unit/test_fingerprints.py::test_compute_conv_fp_different_parents PASSED
tests/config/unit/test_fingerprints.py::test_compute_bench_fp PASSED
tests/config/unit/test_fingerprints.py::test_compute_hardware_fp PASSED
tests/config/unit/test_fingerprints.py::test_compute_hardware_fp_empty PASSED
tests/config/unit/test_mlflow_yaml.py::TestAzureMLConfiguration::test_azure_ml_enabled PASSED
tests/config/unit/test_mlflow_yaml.py::TestAzureMLConfiguration::test_azure_ml_disabled PASSED
tests/config/unit/test_mlflow_yaml.py::TestAzureMLConfiguration::test_azure_ml_workspace_name PASSED
tests/config/unit/test_mlflow_yaml.py::TestTrackingConfiguration::test_tracking_benchmark_enabled PASSED
tests/config/unit/test_mlflow_yaml.py::TestTrackingConfiguration::test_tracking_benchmark_disabled PASSED
tests/config/unit/test_mlflow_yaml.py::TestTrackingConfiguration::test_tracking_benchmark_log_artifacts PASSED
tests/config/unit/test_mlflow_yaml.py::TestTrackingConfiguration::test_tracking_training_enabled PASSED
tests/config/unit/test_mlflow_yaml.py::TestTrackingConfiguration::test_tracking_training_disabled PASSED
tests/config/unit/test_mlflow_yaml.py::TestTrackingConfiguration::test_tracking_training_log_checkpoint PASSED
tests/config/unit/test_mlflow_yaml.py::TestTrackingConfiguration::test_tracking_training_log_metrics_json PASSED
tests/config/unit/test_mlflow_yaml.py::TestTrackingConfiguration::test_tracking_conversion_enabled PASSED
tests/config/unit/test_mlflow_yaml.py::TestTrackingConfiguration::test_tracking_conversion_disabled PASSED
tests/config/unit/test_mlflow_yaml.py::TestTrackingConfiguration::test_tracking_conversion_log_onnx_model PASSED
tests/config/unit/test_mlflow_yaml.py::TestTrackingConfiguration::test_tracking_conversion_log_conversion_log PASSED
tests/config/unit/test_mlflow_yaml.py::TestNamingConfiguration::test_naming_project_name PASSED
tests/config/unit/test_mlflow_yaml.py::TestNamingConfiguration::test_naming_tags_max_length PASSED
tests/config/unit/test_mlflow_yaml.py::TestNamingConfiguration::test_naming_tags_sanitize PASSED
tests/config/unit/test_mlflow_yaml.py::TestNamingConfiguration::test_naming_run_name_max_length PASSED
tests/config/unit/test_mlflow_yaml.py::TestNamingConfiguration::test_naming_run_name_shorten_fingerprints PASSED
tests/config/unit/test_mlflow_yaml.py::TestNamingConfiguration::test_naming_run_name_auto_increment_enabled 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     orchestration.jobs.tracking.config.loader: [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-38/test_naming_run_name_auto_incr0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 00:31:11 INFO     orchestration.jobs.tracking.config.loader: [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
PASSED
tests/config/unit/test_mlflow_yaml.py::TestNamingConfiguration::test_naming_run_name_auto_increment_processes_hpo 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     orchestration.jobs.tracking.config.loader: [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-38/test_naming_run_name_auto_incr1/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 00:31:11 INFO     orchestration.jobs.tracking.config.loader: [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
PASSED
tests/config/unit/test_mlflow_yaml.py::TestNamingConfiguration::test_naming_run_name_auto_increment_processes_benchmarking 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     orchestration.jobs.tracking.config.loader: [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-38/test_naming_run_name_auto_incr2/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 00:31:11 INFO     orchestration.jobs.tracking.config.loader: [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=benchmarking
PASSED
tests/config/unit/test_mlflow_yaml.py::TestNamingConfiguration::test_naming_run_name_auto_increment_format 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     orchestration.jobs.tracking.config.loader: [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-38/test_naming_run_name_auto_incr3/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 00:31:11 INFO     orchestration.jobs.tracking.config.loader: [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
PASSED
tests/config/unit/test_mlflow_yaml.py::TestIndexConfiguration::test_index_enabled PASSED
tests/config/unit/test_mlflow_yaml.py::TestIndexConfiguration::test_index_disabled PASSED
tests/config/unit/test_mlflow_yaml.py::TestIndexConfiguration::test_index_max_entries PASSED
tests/config/unit/test_mlflow_yaml.py::TestIndexConfiguration::test_index_file_name PASSED
tests/config/unit/test_mlflow_yaml.py::TestRunFinderConfiguration::test_run_finder_strict_mode_default_true PASSED
tests/config/unit/test_mlflow_yaml.py::TestRunFinderConfiguration::test_run_finder_strict_mode_default_false PASSED
tests/config/unit/test_model_config.py::TestModelConfigLoading::test_load_distilbert_config PASSED
tests/config/unit/test_model_config.py::TestModelConfigLoading::test_load_distilroberta_config PASSED
tests/config/unit/test_model_config.py::TestModelConfigLoading::test_load_deberta_config PASSED
tests/config/unit/test_model_config.py::TestModelConfigOptions::test_backbone_option PASSED
tests/config/unit/test_model_config.py::TestModelConfigOptions::test_tokenizer_option PASSED
tests/config/unit/test_model_config.py::TestModelConfigOptions::test_preprocessing_sequence_length PASSED
tests/config/unit/test_model_config.py::TestModelConfigOptions::test_preprocessing_max_length PASSED
tests/config/unit/test_model_config.py::TestModelConfigOptions::test_preprocessing_tokenization PASSED
tests/config/unit/test_model_config.py::TestModelConfigOptions::test_preprocessing_replace_rare_with_unk PASSED
tests/config/unit/test_model_config.py::TestModelConfigOptions::test_preprocessing_unk_frequency_threshold PASSED
tests/config/unit/test_model_config.py::TestModelConfigOptions::test_preprocessing_keep_stopwords PASSED
tests/config/unit/test_model_config.py::TestModelConfigOptions::test_decoding_use_crf PASSED
tests/config/unit/test_model_config.py::TestModelConfigOptions::test_decoding_crf_learning_rate PASSED
tests/config/unit/test_model_config.py::TestModelConfigOptions::test_loss_use_class_weights PASSED
tests/config/unit/test_model_config.py::TestModelConfigOptions::test_loss_class_weight_smoothing PASSED
tests/config/unit/test_model_config.py::TestModelConfigOptions::test_loss_ignore_index PASSED
tests/config/unit/test_model_config.py::TestModelConfigIntegration::test_model_config_via_experiment_config PASSED
tests/config/unit/test_model_config.py::TestModelConfigIntegration::test_model_config_in_training_config_building PASSED
tests/config/unit/test_model_config.py::TestModelConfigValidation::test_model_config_missing_sections PASSED
tests/config/unit/test_model_config.py::TestModelConfigValidation::test_model_config_partial_preprocessing PASSED
tests/config/unit/test_model_config.py::TestModelConfigValidation::test_model_config_partial_decoding PASSED
tests/config/unit/test_model_config.py::TestModelConfigValidation::test_model_config_partial_loss PASSED
tests/config/unit/test_model_config.py::TestModelConfigValidation::test_model_config_numeric_types PASSED
tests/config/unit/test_model_config.py::TestModelConfigValidation::test_model_config_boolean_types PASSED
tests/config/unit/test_model_config.py::TestModelConfigRealFiles::test_load_real_distilbert_config PASSED
tests/config/unit/test_model_config.py::TestModelConfigRealFiles::test_load_real_distilroberta_config PASSED
tests/config/unit/test_model_config.py::TestModelConfigRealFiles::test_load_real_deberta_config PASSED
tests/config/unit/test_model_config.py::TestModelConfigRealFiles::test_all_real_model_configs_have_required_sections SKIPPED
tests/config/unit/test_naming_yaml.py::TestSchemaVersion::test_schema_version_is_loaded PASSED
tests/config/unit/test_naming_yaml.py::TestSeparatorsExplicit::test_separator_field PASSED
tests/config/unit/test_naming_yaml.py::TestSeparatorsExplicit::test_separator_component PASSED
tests/config/unit/test_naming_yaml.py::TestSeparatorsExplicit::test_separator_version PASSED
tests/config/unit/test_naming_yaml.py::TestVersionFormatExplicit::test_version_format PASSED
tests/config/unit/test_naming_yaml.py::TestVersionFormatExplicit::test_version_separator PASSED
tests/config/unit/test_naming_yaml.py::TestNormalizeExplicit::test_normalize_env_replace PASSED
tests/config/unit/test_naming_yaml.py::TestNormalizeExplicit::test_normalize_env_lowercase PASSED
tests/config/unit/test_naming_yaml.py::TestNormalizeExplicit::test_normalize_model_replace PASSED
tests/config/unit/test_naming_yaml.py::TestNormalizeExplicit::test_normalize_model_lowercase PASSED
tests/config/unit/test_naming_yaml.py::TestValidateExplicit::test_validate_max_length PASSED
tests/config/unit/test_naming_yaml.py::TestValidateExplicit::test_validate_forbidden_chars PASSED
tests/config/unit/test_naming_yaml.py::TestValidateExplicit::test_validate_warn_length PASSED
tests/config/unit/test_naming_yaml.py::TestRunNamesComponentOptions::test_hpo_trial_component_options PASSED
tests/config/unit/test_naming_yaml.py::TestRunNamesComponentOptions::test_hpo_trial_fold_component_options PASSED
tests/config/unit/test_naming_yaml.py::TestRunNamesComponentOptions::test_hpo_refit_component_options PASSED
tests/config/unit/test_naming_yaml.py::TestRunNamesComponentOptions::test_hpo_sweep_semantic_suffix_options PASSED
tests/config/unit/test_naming_yaml.py::TestRunNamesComponentOptions::test_final_training_component_options PASSED
tests/config/unit/test_naming_yaml.py::TestRunNamesComponentOptions::test_benchmarking_component_options PASSED
tests/config/unit/test_naming_yaml.py::TestRunNamesComponentOptions::test_conversion_component_options PASSED
tests/config/unit/test_paths.py::TestLoadPathsConfig::test_load_paths_config_with_file PASSED
tests/config/unit/test_paths.py::TestLoadPathsConfig::test_load_paths_config_without_file PASSED
tests/config/unit/test_paths.py::TestResolveOutputPath::test_resolve_simple_path PASSED
tests/config/unit/test_paths.py::TestResolveOutputPath::test_resolve_cache_subdirectory PASSED
tests/config/unit/test_paths.py::TestResolveOutputPath::test_resolve_path_with_pattern PASSED
tests/config/unit/test_paths.py::TestGetCacheFilePath::test_get_latest_cache_file PASSED
tests/config/unit/test_paths.py::TestGetCacheFilePath::test_get_index_cache_file PASSED
tests/config/unit/test_paths.py::TestGetTimestampedCacheFilename::test_generate_best_config_filename PASSED
tests/config/unit/test_paths.py::TestGetTimestampedCacheFilename::test_generate_final_training_filename PASSED
tests/config/unit/test_paths.py::TestGetCacheStrategyConfig::test_get_strategy_config PASSED
tests/config/unit/test_paths.py::TestSaveCacheWithDualStrategy::test_save_cache_creates_all_files PASSED
tests/config/unit/test_paths.py::TestLoadCacheFile::test_load_latest_cache PASSED
tests/config/unit/test_paths.py::TestLoadCacheFile::test_load_specific_timestamp PASSED
tests/config/unit/test_paths.py::TestLoadCacheFile::test_load_returns_none_when_not_found PASSED
tests/config/unit/test_paths.py::TestPathBuildingV2::test_path_building_v2_hpo PASSED
tests/config/unit/test_paths.py::TestPathBuildingV2::test_path_building_v2_normalized PASSED
tests/config/unit/test_paths.py::TestPathBuildingV2::test_path_building_v2_all_storage_envs PASSED
tests/config/unit/test_paths.py::TestPathBuildingV2::test_path_building_v2_study8_trial8_format PASSED
tests/config/unit/test_paths.py::TestResolveProjectPaths::test_trusts_provided_config_dir PASSED
tests/config/unit/test_paths.py::TestResolveProjectPaths::test_infers_from_output_dir_when_config_dir_none PASSED
tests/config/unit/test_paths.py::TestResolveProjectPaths::test_infers_from_start_path_as_fallback PASSED
tests/config/unit/test_paths.py::TestResolveProjectPaths::test_handles_config_dir_with_different_name PASSED
tests/config/unit/test_paths.py::TestResolveProjectPaths::test_returns_none_when_inference_fails PASSED
tests/config/unit/test_paths.py::TestResolveProjectPaths::test_prioritizes_config_dir_over_output_dir PASSED
tests/config/unit/test_paths.py::TestResolveProjectPaths::test_handles_none_inputs PASSED
tests/config/unit/test_paths.py::TestResolveProjectPaths::test_derives_config_dir_from_root_dir PASSED
tests/config/unit/test_paths.py::TestResolveProjectPaths::test_handles_output_dir_without_outputs_parent PASSED
tests/config/unit/test_paths_yaml.py::TestBaseDirectories::test_base_outputs PASSED
tests/config/unit/test_paths_yaml.py::TestBaseDirectories::test_base_notebooks PASSED
tests/config/unit/test_paths_yaml.py::TestBaseDirectories::test_base_config PASSED
tests/config/unit/test_paths_yaml.py::TestBaseDirectories::test_base_src PASSED
tests/config/unit/test_paths_yaml.py::TestBaseDirectories::test_base_tests PASSED
tests/config/unit/test_paths_yaml.py::TestBaseDirectories::test_base_mlruns PASSED
tests/config/unit/test_paths_yaml.py::TestOutputsSubdirectories::test_outputs_hpo_tests PASSED
tests/config/unit/test_paths_yaml.py::TestOutputsSubdirectories::test_outputs_dry_run PASSED
tests/config/unit/test_paths_yaml.py::TestOutputsSubdirectories::test_outputs_e2e_test PASSED
tests/config/unit/test_paths_yaml.py::TestOutputsSubdirectories::test_outputs_pytest_logs PASSED
tests/config/unit/test_paths_yaml.py::TestFilesConfiguration::test_files_metrics PASSED
tests/config/unit/test_paths_yaml.py::TestFilesConfiguration::test_files_benchmark PASSED
tests/config/unit/test_paths_yaml.py::TestFilesConfiguration::test_files_checkpoint_dir PASSED
tests/config/unit/test_paths_yaml.py::TestFilesConfiguration::test_files_cache_best_config_latest PASSED
tests/config/unit/test_paths_yaml.py::TestFilesConfiguration::test_files_cache_best_config_index PASSED
tests/config/unit/test_paths_yaml.py::TestFilesConfiguration::test_files_cache_final_training_latest PASSED
tests/config/unit/test_paths_yaml.py::TestFilesConfiguration::test_files_cache_final_training_index PASSED
tests/config/unit/test_paths_yaml.py::TestFilesConfiguration::test_files_cache_best_model_selection_latest PASSED
tests/config/unit/test_paths_yaml.py::TestFilesConfiguration::test_files_cache_best_model_selection_index PASSED
tests/config/unit/test_paths_yaml.py::TestFilesConfiguration::test_files_cache_conversion_cache PASSED
tests/config/unit/test_paths_yaml.py::TestCacheStrategiesConfiguration::test_cache_strategies_best_configurations_timestamped_enabled PASSED
tests/config/unit/test_paths_yaml.py::TestCacheStrategiesConfiguration::test_cache_strategies_best_configurations_latest_include_timestamped_ref PASSED
tests/config/unit/test_paths_yaml.py::TestCacheStrategiesConfiguration::test_cache_strategies_best_configurations_index_max_entries PASSED
tests/config/unit/test_paths_yaml.py::TestCacheStrategiesConfiguration::test_cache_strategies_final_training_all_options PASSED
tests/config/unit/test_paths_yaml.py::TestCacheStrategiesConfiguration::test_cache_strategies_best_model_selection_all_options PASSED
tests/config/unit/test_paths_yaml.py::TestDriveConfiguration::test_drive_mount_point PASSED
tests/config/unit/test_paths_yaml.py::TestDriveConfiguration::test_drive_backup_base_dir PASSED
tests/config/unit/test_paths_yaml.py::TestDriveConfiguration::test_drive_auto_restore_on_startup PASSED
tests/config/unit/test_paths_yaml.py::TestDriveConfiguration::test_drive_auto_restore_on_startup_true PASSED
tests/config/unit/test_paths_yaml.py::TestNormalizePathsConfiguration::test_normalize_paths_replace PASSED
tests/config/unit/test_paths_yaml.py::TestNormalizePathsConfiguration::test_normalize_paths_lowercase PASSED
tests/config/unit/test_paths_yaml.py::TestNormalizePathsConfiguration::test_normalize_paths_lowercase_true PASSED
tests/config/unit/test_paths_yaml.py::TestNormalizePathsConfiguration::test_normalize_paths_max_component_length PASSED
tests/config/unit/test_paths_yaml.py::TestNormalizePathsConfiguration::test_normalize_paths_max_path_length PASSED
tests/config/unit/test_paths_yaml.py::TestPatternsConfiguration::test_patterns_best_config_file PASSED
tests/config/unit/test_paths_yaml.py::TestPatternsConfiguration::test_patterns_final_training_cache_file PASSED
tests/config/unit/test_paths_yaml.py::TestPatternsConfiguration::test_patterns_best_model_selection_cache_file PASSED
tests/config/unit/test_run_mode.py::TestGetRunMode::test_get_run_mode_explicit_force_new PASSED
tests/config/unit/test_run_mode.py::TestGetRunMode::test_get_run_mode_explicit_reuse_if_exists PASSED
tests/config/unit/test_run_mode.py::TestGetRunMode::test_get_run_mode_explicit_resume_if_incomplete PASSED
tests/config/unit/test_run_mode.py::TestGetRunMode::test_get_run_mode_default_when_missing PASSED
tests/config/unit/test_run_mode.py::TestGetRunMode::test_get_run_mode_default_when_run_section_missing PASSED
tests/config/unit/test_run_mode.py::TestGetRunMode::test_get_run_mode_custom_default PASSED
tests/config/unit/test_run_mode.py::TestGetRunMode::test_get_run_mode_nested_config PASSED
tests/config/unit/test_run_mode.py::TestGetRunMode::test_get_run_mode_combined_config PASSED
tests/config/unit/test_run_mode.py::TestIsForceNew::test_is_force_new_true PASSED
tests/config/unit/test_run_mode.py::TestIsForceNew::test_is_force_new_false PASSED
tests/config/unit/test_run_mode.py::TestIsForceNew::test_is_force_new_default PASSED
tests/config/unit/test_run_mode.py::TestIsReuseIfExists::test_is_reuse_if_exists_true PASSED
tests/config/unit/test_run_mode.py::TestIsReuseIfExists::test_is_reuse_if_exists_false PASSED
tests/config/unit/test_run_mode.py::TestIsReuseIfExists::test_is_reuse_if_exists_default PASSED
tests/config/unit/test_run_mode.py::TestIsResumeIfIncomplete::test_is_resume_if_incomplete_true PASSED
tests/config/unit/test_run_mode.py::TestIsResumeIfIncomplete::test_is_resume_if_incomplete_false PASSED
tests/config/unit/test_run_mode.py::TestIsResumeIfIncomplete::test_is_resume_if_incomplete_default PASSED
tests/config/unit/test_run_mode.py::TestRunModeIntegration::test_hpo_config_with_run_mode PASSED
tests/config/unit/test_run_mode.py::TestRunModeIntegration::test_combined_hpo_checkpoint_config PASSED
tests/config/unit/test_run_mode.py::TestRunModeIntegration::test_final_training_config_with_run_mode PASSED
tests/config/unit/test_variants.py::TestComputeNextVariant::test_compute_next_variant_hpo_no_existing PASSED
tests/config/unit/test_variants.py::TestComputeNextVariant::test_compute_next_variant_hpo_with_existing PASSED
tests/config/unit/test_variants.py::TestComputeNextVariant::test_compute_next_variant_hpo_custom_base_name PASSED
tests/config/unit/test_variants.py::TestComputeNextVariant::test_compute_next_variant_final_training_no_existing PASSED
tests/config/unit/test_variants.py::TestComputeNextVariant::test_compute_next_variant_final_training_with_existing PASSED
tests/config/unit/test_variants.py::TestFindExistingVariants::test_find_existing_variants_hpo_none PASSED
tests/config/unit/test_variants.py::TestFindExistingVariants::test_find_existing_variants_hpo_implicit_variant_1 PASSED
tests/config/unit/test_variants.py::TestFindExistingVariants::test_find_existing_variants_hpo_explicit_variants PASSED
tests/config/unit/test_variants.py::TestFindExistingVariants::test_find_existing_variants_hpo_mixed_patterns PASSED
tests/config/unit/test_variants.py::TestFindExistingVariants::test_find_existing_variants_final_training PASSED
tests/config/unit/test_variants.py::TestFindExistingVariants::test_find_existing_variants_invalid_process_type PASSED
tests/config/unit/test_variants.py::TestVariantsIntegration::test_hpo_variant_sequence PASSED
tests/config/unit/test_variants.py::TestVariantsIntegration::test_hpo_variant_with_custom_study_name PASSED
tests/conversion/integration/test_conversion_config.py::TestConversionConfig::test_opset_version_passed_to_subprocess 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     deployment.conversion.orchestration: Output directory: /tmp/pytest-of-codespace/pytest-38/test_opset_version_passed_to_s0/outputs/conversion/test
2026-01-18 00:31:11 INFO     deployment.conversion.orchestration: Created MLflow run: test_run_name (test_run_id...)
2026-01-18 00:31:11 INFO     deployment.conversion.orchestration: Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /path/to/checkpoint --config-dir /tmp/pytest-of-codespace/pytest-38/test_opset_version_passed_to_s0/config --backbone distilbert-base-uncased --output-dir /tmp/pytest-of-codespace/pytest-38/test_opset_version_passed_to_s0/outputs/conversion/test --opset-version 19 --run-smoke-test
2026-01-18 00:31:11 WARNING  infrastructure.tracking.mlflow.lifecycle: Could not ensure run test_run_id... is terminated: Run 'test_run_id' not found
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/lifecycle.py", line 130, in ensure_run_terminated
    run = client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'test_run_id' not found
2026-01-18 00:31:11 INFO     deployment.conversion.orchestration: Conversion completed. ONNX model: /tmp/pytest-of-codespace/pytest-38/test_opset_version_passed_to_s0/outputs/conversion/test/model.onnx
PASSED
tests/conversion/integration/test_conversion_config.py::TestConversionConfig::test_quantization_int8_adds_flag 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     deployment.conversion.orchestration: Output directory: /tmp/pytest-of-codespace/pytest-38/test_quantization_int8_adds_fl0/outputs/conversion/test
2026-01-18 00:31:11 INFO     deployment.conversion.orchestration: Created MLflow run: test_run_name (test_run_id...)
2026-01-18 00:31:11 INFO     deployment.conversion.orchestration: Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /path/to/checkpoint --config-dir /tmp/pytest-of-codespace/pytest-38/test_quantization_int8_adds_fl0/config --backbone distilbert-base-uncased --output-dir /tmp/pytest-of-codespace/pytest-38/test_quantization_int8_adds_fl0/outputs/conversion/test --opset-version 18 --quantize-int8 --run-smoke-test
2026-01-18 00:31:11 WARNING  infrastructure.tracking.mlflow.lifecycle: Could not ensure run test_run_id... is terminated: Run 'test_run_id' not found
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/lifecycle.py", line 130, in ensure_run_terminated
    run = client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'test_run_id' not found
2026-01-18 00:31:11 INFO     deployment.conversion.orchestration: Conversion completed. ONNX model: /tmp/pytest-of-codespace/pytest-38/test_quantization_int8_adds_fl0/outputs/conversion/test/model_int8.onnx
PASSED
tests/conversion/integration/test_conversion_config.py::TestConversionConfig::test_quantization_none_no_flag 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     deployment.conversion.orchestration: Output directory: /tmp/pytest-of-codespace/pytest-38/test_quantization_none_no_flag0/outputs/conversion/test
2026-01-18 00:31:11 INFO     deployment.conversion.orchestration: Created MLflow run: test_run_name (test_run_id...)
2026-01-18 00:31:11 INFO     deployment.conversion.orchestration: Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /path/to/checkpoint --config-dir /tmp/pytest-of-codespace/pytest-38/test_quantization_none_no_flag0/config --backbone distilbert-base-uncased --output-dir /tmp/pytest-of-codespace/pytest-38/test_quantization_none_no_flag0/outputs/conversion/test --opset-version 18 --run-smoke-test
2026-01-18 00:31:11 WARNING  infrastructure.tracking.mlflow.lifecycle: Could not ensure run test_run_id... is terminated: Run 'test_run_id' not found
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/lifecycle.py", line 130, in ensure_run_terminated
    run = client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'test_run_id' not found
2026-01-18 00:31:11 INFO     deployment.conversion.orchestration: Conversion completed. ONNX model: /tmp/pytest-of-codespace/pytest-38/test_quantization_none_no_flag0/outputs/conversion/test/model.onnx
PASSED
tests/conversion/integration/test_conversion_config.py::TestConversionConfig::test_run_smoke_test_true_adds_flag 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     deployment.conversion.orchestration: Output directory: /tmp/pytest-of-codespace/pytest-38/test_run_smoke_test_true_adds_0/outputs/conversion/test
2026-01-18 00:31:11 INFO     deployment.conversion.orchestration: Created MLflow run: test_run_name (test_run_id...)
2026-01-18 00:31:11 INFO     deployment.conversion.orchestration: Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /path/to/checkpoint --config-dir /tmp/pytest-of-codespace/pytest-38/test_run_smoke_test_true_adds_0/config --backbone distilbert-base-uncased --output-dir /tmp/pytest-of-codespace/pytest-38/test_run_smoke_test_true_adds_0/outputs/conversion/test --opset-version 18 --run-smoke-test
2026-01-18 00:31:11 WARNING  infrastructure.tracking.mlflow.lifecycle: Could not ensure run test_run_id... is terminated: Run 'test_run_id' not found
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/lifecycle.py", line 130, in ensure_run_terminated
    run = client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'test_run_id' not found
2026-01-18 00:31:11 INFO     deployment.conversion.orchestration: Conversion completed. ONNX model: /tmp/pytest-of-codespace/pytest-38/test_run_smoke_test_true_adds_0/outputs/conversion/test/model.onnx
PASSED
tests/conversion/integration/test_conversion_config.py::TestConversionConfig::test_run_smoke_test_false_no_flag 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     deployment.conversion.orchestration: Output directory: /tmp/pytest-of-codespace/pytest-38/test_run_smoke_test_false_no_f0/outputs/conversion/test
2026-01-18 00:31:11 INFO     deployment.conversion.orchestration: Created MLflow run: test_run_name (test_run_id...)
2026-01-18 00:31:11 INFO     deployment.conversion.orchestration: Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /path/to/checkpoint --config-dir /tmp/pytest-of-codespace/pytest-38/test_run_smoke_test_false_no_f0/config --backbone distilbert-base-uncased --output-dir /tmp/pytest-of-codespace/pytest-38/test_run_smoke_test_false_no_f0/outputs/conversion/test --opset-version 18
2026-01-18 00:31:11 WARNING  infrastructure.tracking.mlflow.lifecycle: Could not ensure run test_run_id... is terminated: Run 'test_run_id' not found
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/lifecycle.py", line 130, in ensure_run_terminated
    run = client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'test_run_id' not found
2026-01-18 00:31:11 INFO     deployment.conversion.orchestration: Conversion completed. ONNX model: /tmp/pytest-of-codespace/pytest-38/test_run_smoke_test_false_no_f0/outputs/conversion/test/model.onnx
PASSED
tests/conversion/integration/test_conversion_config.py::TestConversionConfig::test_filename_pattern_used_in_find_onnx_model 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     deployment.conversion.orchestration: Output directory: /tmp/pytest-of-codespace/pytest-38/test_filename_pattern_used_in_0/outputs/conversion/test
2026-01-18 00:31:11 INFO     deployment.conversion.orchestration: Created MLflow run: test_run_name (test_run_id...)
2026-01-18 00:31:11 INFO     deployment.conversion.orchestration: Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /path/to/checkpoint --config-dir /tmp/pytest-of-codespace/pytest-38/test_filename_pattern_used_in_0/config --backbone distilbert-base-uncased --output-dir /tmp/pytest-of-codespace/pytest-38/test_filename_pattern_used_in_0/outputs/conversion/test --opset-version 18 --run-smoke-test
2026-01-18 00:31:11 WARNING  infrastructure.tracking.mlflow.lifecycle: Could not ensure run test_run_id... is terminated: Run 'test_run_id' not found
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/lifecycle.py", line 130, in ensure_run_terminated
    run = client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'test_run_id' not found
2026-01-18 00:31:11 INFO     deployment.conversion.orchestration: Conversion completed. ONNX model: /tmp/pytest-of-codespace/pytest-38/test_filename_pattern_used_in_0/outputs/conversion/test/custom_fp32_model.onnx
PASSED
tests/conversion/unit/test_conversion_config.py::TestConversionConfigLoading::test_load_yaml_loads_conversion_config PASSED
tests/conversion/unit/test_conversion_config.py::TestConversionConfigLoading::test_conversion_config_structure PASSED
tests/conversion/unit/test_conversion_config.py::TestConversionConfigLoading::test_conversion_config_default_values PASSED
tests/conversion/unit/test_conversion_config.py::TestConversionConfigLoading::test_conversion_config_custom_values PASSED
tests/conversion/unit/test_conversion_config.py::TestConversionConfigLoading::test_conversion_config_missing_sections PASSED
tests/conversion/unit/test_conversion_config.py::TestConversionConfigLoading::test_conversion_config_types PASSED
tests/conversion/unit/test_conversion_config.py::TestConversionConfigLoading::test_load_actual_conversion_yaml PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_target_format_extraction PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_target_format_default PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_onnx_opset_version_extraction PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_onnx_opset_version_custom PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_onnx_opset_version_default PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_onnx_quantization_extraction PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_onnx_quantization_int8 PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_onnx_quantization_dynamic PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_onnx_quantization_default PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_onnx_run_smoke_test_extraction PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_onnx_run_smoke_test_false PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_onnx_run_smoke_test_default PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_output_filename_pattern_extraction PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_output_filename_pattern_custom PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_output_filename_pattern_default PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_all_options_together PASSED
tests/evaluation/selection/integration/test_benchmarking_checkpoint_resolution.py::TestCheckpointResolutionFromLocalDisk::test_resolve_checkpoint_from_champion_checkpoint_path PASSED
tests/evaluation/selection/integration/test_benchmarking_checkpoint_resolution.py::TestCheckpointResolutionFromLocalDisk::test_resolve_checkpoint_from_trial_dir PASSED
tests/evaluation/selection/integration/test_benchmarking_checkpoint_resolution.py::TestCheckpointResolutionFromLocalDisk::test_resolve_checkpoint_from_legacy_trial_structure PASSED
tests/evaluation/selection/integration/test_benchmarking_checkpoint_resolution.py::TestCheckpointResolutionFromMLflow::test_resolve_from_mlflow_refit_run PASSED
tests/evaluation/selection/integration/test_benchmarking_checkpoint_resolution.py::TestCheckpointResolutionFromMLflow::test_resolve_from_mlflow_parent_run PASSED
tests/evaluation/selection/integration/test_benchmarking_checkpoint_resolution.py::TestBenchmarkingWorkflowIntegration::test_benchmarking_with_local_checkpoint PASSED
tests/evaluation/selection/integration/test_benchmarking_checkpoint_resolution.py::TestBenchmarkingWorkflowIntegration::test_benchmarking_with_mlflow_checkpoint PASSED
tests/evaluation/selection/integration/test_checkpoint_acquisition_for_benchmarking.py::TestRefitRunDiscovery::test_find_refit_run_by_hash_match PASSED
tests/evaluation/selection/integration/test_checkpoint_acquisition_for_benchmarking.py::TestRefitRunDiscovery::test_find_refit_run_by_study_hash_only PASSED
tests/evaluation/selection/integration/test_checkpoint_acquisition_for_benchmarking.py::TestRefitRunDiscovery::test_find_refit_run_via_parent_relationship PASSED
tests/evaluation/selection/integration/test_checkpoint_acquisition_for_benchmarking.py::TestRefitRunDiscovery::test_find_any_refit_run_as_last_resort PASSED
tests/evaluation/selection/integration/test_checkpoint_acquisition_for_benchmarking.py::TestCheckpointArtifactDiscovery::test_checkpoint_in_refit_run PASSED
tests/evaluation/selection/integration/test_checkpoint_acquisition_for_benchmarking.py::TestCheckpointArtifactDiscovery::test_checkpoint_in_parent_hpo_run PASSED
tests/evaluation/selection/integration/test_checkpoint_acquisition_for_benchmarking.py::TestCheckpointArtifactDiscovery::test_no_checkpoint_in_trial_run PASSED
tests/evaluation/selection/integration/test_checkpoint_acquisition_for_benchmarking.py::TestCheckpointArtifactDiscovery::test_checkpoint_artifact_paths PASSED
tests/evaluation/selection/integration/test_checkpoint_acquisition_for_benchmarking.py::TestCheckpointAcquisitionWorkflow::test_acquire_from_refit_run_success PASSED
tests/evaluation/selection/integration/test_checkpoint_acquisition_for_benchmarking.py::TestCheckpointAcquisitionWorkflow::test_acquire_from_parent_hpo_run_fallback PASSED
tests/evaluation/selection/integration/test_checkpoint_acquisition_for_benchmarking.py::TestCheckpointAcquisitionWorkflow::test_no_checkpoint_found_anywhere PASSED
tests/evaluation/selection/integration/test_checkpoint_acquisition_for_benchmarking.py::TestEdgeCases::test_parent_run_id_not_available PASSED
tests/evaluation/selection/integration/test_checkpoint_acquisition_for_benchmarking.py::TestEdgeCases::test_refit_run_without_trial_key_hash PASSED
tests/evaluation/selection/integration/test_checkpoint_acquisition_for_benchmarking.py::TestEdgeCases::test_multiple_refit_runs PASSED
tests/evaluation/selection/test_champion_selection.py::TestSelectChampionPerBackbone::test_successful_selection_v2 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     evaluation.selection.trial_finder: Found 3 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 00:31:11 INFO     evaluation.selection.trial_finder: Grouped runs for distilbert: 0 v1 group(s), 1 v2 group(s)
2026-01-18 00:31:11 INFO     evaluation.selection.trial_finder: Found 1 eligible group(s) for distilbert (0 skipped due to min_trials requirement)
2026-01-18 00:31:11 INFO     evaluation.selection.artifact_unified.selectors: Found refit run refit-run-12... for trial run2... (selected latest from 1 refit run(s))
2026-01-18 00:31:11 INFO     evaluation.selection.trial_finder: Found refit run refit-run-12... for champion trial run2... (using SSOT selector: refit_preferred)
PASSED
tests/evaluation/selection/test_champion_selection.py::TestSelectChampionPerBackbone::test_successful_selection_v1 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     evaluation.selection.trial_finder: Found 3 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 00:31:11 INFO     evaluation.selection.trial_finder: Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)
2026-01-18 00:31:11 INFO     evaluation.selection.trial_finder: Found 1 eligible group(s) for distilbert (0 skipped due to min_trials requirement)
2026-01-18 00:31:11 INFO     evaluation.selection.artifact_unified.selectors: Found refit run refit-run-12... for trial run2... (selected latest from 1 refit run(s))
2026-01-18 00:31:11 INFO     evaluation.selection.trial_finder: Found refit run refit-run-12... for champion trial run2... (using SSOT selector: refit_preferred)
PASSED
tests/evaluation/selection/test_champion_selection.py::TestSelectChampionPerBackbone::test_no_runs_returns_none 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     evaluation.selection.trial_finder: No runs found with stage='hpo_trial' for distilbert, trying legacy stage='hpo'
2026-01-18 00:31:11 INFO     evaluation.selection.trial_finder: Found 0 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 00:31:11 INFO     evaluation.selection.trial_finder: Grouped runs for distilbert: 0 v1 group(s), 0 v2 group(s)
2026-01-18 00:31:11 WARNING  evaluation.selection.trial_finder: No valid groups found for distilbert. No trial runs found in HPO experiment 'test_hpo_experiment'. This may indicate:
  - HPO was not run for this backbone
  - Runs exist but don't have required tags (stage='hpo_trial' or 'hpo', backbone tag)
  - Runs exist but were filtered out (missing metrics, artifacts, or grouping tags)
Skipping champion selection for distilbert.
PASSED
tests/evaluation/selection/test_champion_selection.py::TestSelectChampionPerBackbone::test_insufficient_trials_returns_none 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     evaluation.selection.trial_finder: Found 2 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 00:31:11 INFO     evaluation.selection.trial_finder: Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)
2026-01-18 00:31:11 WARNING  evaluation.selection.trial_finder: Skipping group hash1... - only 2 valid trials (minimum: 3)
2026-01-18 00:31:11 WARNING  evaluation.selection.trial_finder: No eligible groups for distilbert. Processed 1 group(s), but 1 were skipped due to insufficient trials (minimum: 3). Remaining groups: 0
PASSED
tests/evaluation/selection/test_champion_selection.py::TestSelectChampionPerBackbone::test_missing_metrics_filtered 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     evaluation.selection.trial_finder: Found 3 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 00:31:11 INFO     evaluation.selection.trial_finder: Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)
2026-01-18 00:31:11 WARNING  evaluation.selection.trial_finder: Group hash1...: 2 valid metrics, 1 missing/invalid. Missing metric runs: ['run3']
2026-01-18 00:31:11 WARNING  evaluation.selection.trial_finder: Skipping group hash1... - only 2 valid trials (minimum: 3)
2026-01-18 00:31:11 WARNING  evaluation.selection.trial_finder: No eligible groups for distilbert. Processed 1 group(s), but 1 were skipped due to insufficient trials (minimum: 3). Remaining groups: 0
PASSED
tests/evaluation/selection/test_champion_selection.py::TestSelectChampionPerBackbone::test_nan_metrics_filtered 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     evaluation.selection.trial_finder: Found 3 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 00:31:11 INFO     evaluation.selection.trial_finder: Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)
2026-01-18 00:31:11 WARNING  evaluation.selection.trial_finder: Group hash1...: 2 valid metrics, 1 missing/invalid. Missing metric runs: []
2026-01-18 00:31:11 WARNING  evaluation.selection.trial_finder: Skipping group hash1... - only 2 valid trials (minimum: 3)
2026-01-18 00:31:11 WARNING  evaluation.selection.trial_finder: No eligible groups for distilbert. Processed 1 group(s), but 1 were skipped due to insufficient trials (minimum: 3). Remaining groups: 0
PASSED
tests/evaluation/selection/test_champion_selection.py::TestSelectChampionPerBackbone::test_artifact_availability_filter 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     evaluation.selection.trial_finder: Found 3 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 00:31:11 WARNING  evaluation.selection.trial_finder: Artifact filter: 1 run(s) have code.artifact.available='false' (explicitly marked as unavailable)
2026-01-18 00:31:11 WARNING  evaluation.selection.trial_finder: Artifact filter: 1 run(s) excluded (1 explicitly false, 0 missing/legacy allowed)
2026-01-18 00:31:11 WARNING  evaluation.selection.trial_finder: Artifact filter removed 1 runs for distilbert (2 remaining). Check that runs have 'code.artifact.available' tag set to 'true'.
2026-01-18 00:31:11 INFO     evaluation.selection.trial_finder: Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)
2026-01-18 00:31:11 WARNING  evaluation.selection.trial_finder: Skipping group hash1... - only 2 valid trials (minimum: 3)
2026-01-18 00:31:11 WARNING  evaluation.selection.trial_finder: No eligible groups for distilbert. Processed 1 group(s), but 1 were skipped due to insufficient trials (minimum: 3). Remaining groups: 0
PASSED
tests/evaluation/selection/test_champion_selection.py::TestSelectChampionPerBackbone::test_no_artifact_requirement 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     evaluation.selection.trial_finder: Found 3 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 00:31:11 INFO     evaluation.selection.trial_finder: Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)
2026-01-18 00:31:11 INFO     evaluation.selection.trial_finder: Found 1 eligible group(s) for distilbert (0 skipped due to min_trials requirement)
2026-01-18 00:31:11 INFO     evaluation.selection.artifact_unified.selectors: Found refit run refit-run-12... for trial run2... (selected latest from 1 refit run(s))
2026-01-18 00:31:11 INFO     evaluation.selection.trial_finder: Found refit run refit-run-12... for champion trial run2... (using SSOT selector: refit_preferred)
PASSED
tests/evaluation/selection/test_champion_selection.py::TestSelectChampionPerBackbone::test_never_mix_v1_v2_when_disabled 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     evaluation.selection.trial_finder: Found 5 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 00:31:11 INFO     evaluation.selection.trial_finder: Grouped runs for distilbert: 1 v1 group(s), 1 v2 group(s)
2026-01-18 00:31:11 INFO     evaluation.selection.trial_finder: Found both v1 and v2 runs for distilbert. Using 2.0 groups only (never mixing versions).
2026-01-18 00:31:11 INFO     evaluation.selection.trial_finder: Found 1 eligible group(s) for distilbert (0 skipped due to min_trials requirement)
2026-01-18 00:31:11 INFO     evaluation.selection.artifact_unified.selectors: Found refit run refit-run-12... for trial run3... (selected latest from 1 refit run(s))
2026-01-18 00:31:11 INFO     evaluation.selection.trial_finder: Found refit run refit-run-12... for champion trial run3... (using SSOT selector: refit_preferred)
PASSED
tests/evaluation/selection/test_champion_selection.py::TestSelectChampionPerBackbone::test_minimize_objective 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     evaluation.selection.trial_finder: Found 3 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 00:31:11 INFO     evaluation.selection.trial_finder: Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)
2026-01-18 00:31:11 INFO     evaluation.selection.trial_finder: Found 1 eligible group(s) for distilbert (0 skipped due to min_trials requirement)
2026-01-18 00:31:11 INFO     evaluation.selection.artifact_unified.selectors: Found refit run refit-run-12... for trial run3... (selected latest from 1 refit run(s))
2026-01-18 00:31:11 INFO     evaluation.selection.trial_finder: Found refit run refit-run-12... for champion trial run3... (using SSOT selector: refit_preferred)
PASSED
tests/evaluation/selection/test_champion_selection.py::TestSelectChampionPerBackbone::test_multiple_groups_selects_best 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     evaluation.selection.trial_finder: Found 6 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 00:31:11 INFO     evaluation.selection.trial_finder: Grouped runs for distilbert: 2 v1 group(s), 0 v2 group(s)
2026-01-18 00:31:11 INFO     evaluation.selection.trial_finder: Found 2 eligible group(s) for distilbert (0 skipped due to min_trials requirement)
2026-01-18 00:31:11 INFO     evaluation.selection.artifact_unified.selectors: Found refit run refit-run-12... for trial run6... (selected latest from 1 refit run(s))
2026-01-18 00:31:11 INFO     evaluation.selection.trial_finder: Found refit run refit-run-12... for champion trial run6... (using SSOT selector: refit_preferred)
PASSED
tests/evaluation/selection/test_champion_selection.py::TestSelectChampionPerBackbone::test_stable_score_computation 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     evaluation.selection.trial_finder: Found 5 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 00:31:11 INFO     evaluation.selection.trial_finder: Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)
2026-01-18 00:31:11 INFO     evaluation.selection.trial_finder: Found 1 eligible group(s) for distilbert (0 skipped due to min_trials requirement)
2026-01-18 00:31:11 INFO     evaluation.selection.artifact_unified.selectors: Found refit run refit-run-12... for trial run3... (selected latest from 1 refit run(s))
2026-01-18 00:31:11 INFO     evaluation.selection.trial_finder: Found refit run refit-run-12... for champion trial run3... (using SSOT selector: refit_preferred)
PASSED
tests/final_training/integration/test_final_training_component.py::test_execute_final_training_reuse_if_exists_skips_training 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     training.execution.executor: Final training config loaded from final_training.yaml
2026-01-18 00:31:11 INFO     training.execution.executor: Output directory: /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_re0/outputs/v1
2026-01-18 00:31:11 INFO     training.execution.executor: Found existing completed final training run
2026-01-18 00:31:11 INFO     training.execution.executor:   Output directory: /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_re0/outputs/v1
2026-01-18 00:31:11 INFO     training.execution.executor:   Checkpoint: /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_re0/outputs/v1/checkpoint
2026-01-18 00:31:11 INFO     training.execution.executor:   Reusing existing checkpoint (run.mode: reuse_if_exists)
PASSED
tests/final_training/integration/test_final_training_component.py::test_execute_final_training_force_new_runs_training 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     training.execution.executor: Final training config loaded from final_training.yaml
2026-01-18 00:31:11 INFO     training.execution.executor: Output directory: /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_fo0/outputs/v1
2026-01-18 00:31:11 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_fo0/config/tags.yaml, using defaults
2026-01-18 00:31:11 INFO     training.execution.mlflow_setup:  View run test_run_name at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/594159571823761936/runs/ab235bd81ea0493aa66ca7bc757a1e84
2026-01-18 00:31:11 INFO     training.execution.mlflow_setup: Created MLflow run: test_run_name (ab235bd81ea0...)
2026-01-18 00:31:11 INFO     training.execution.executor: Created MLflow run: test_run_name (ab235bd81ea0...)
2026-01-18 00:31:11 INFO     training.execution.executor: Running final training...
2026-01-18 00:31:11 INFO     infrastructure.tracking.mlflow.lifecycle: Run ab235bd81ea0... already terminated with status <Mock name='mock.get_run().info.status' id='140077261534928'> (expected FINISHED)
2026-01-18 00:31:11 INFO     training.execution.executor: Saved metadata to: None
2026-01-18 00:31:11 INFO     training.execution.executor: Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_fo0/outputs/v1/checkpoint
2026-01-18 00:31:11 INFO     training.execution.executor: MLflow run: ab235bd81ea0...
PASSED
tests/final_training/integration/test_final_training_component.py::test_execute_final_training_resume_if_incomplete_continues 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     training.execution.executor: Final training config loaded from final_training.yaml
2026-01-18 00:31:11 INFO     training.execution.executor: Output directory: /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_re1/outputs/v1
2026-01-18 00:31:11 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_re1/config/tags.yaml, using defaults
2026-01-18 00:31:11 INFO     training.execution.mlflow_setup:  View run test_run_name at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/594159571823761936/runs/f4ba5c72c8be41e385e10ee47e31935a
2026-01-18 00:31:11 INFO     training.execution.mlflow_setup: Created MLflow run: test_run_name (f4ba5c72c8be...)
2026-01-18 00:31:11 INFO     training.execution.executor: Created MLflow run: test_run_name (f4ba5c72c8be...)
2026-01-18 00:31:11 INFO     training.execution.executor: Running final training...
2026-01-18 00:31:11 INFO     infrastructure.tracking.mlflow.lifecycle: Run f4ba5c72c8be... already terminated with status <Mock name='mock.get_run().info.status' id='140077261523072'> (expected FINISHED)
2026-01-18 00:31:11 INFO     training.execution.executor: Saved metadata to: None
2026-01-18 00:31:11 INFO     training.execution.executor: Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_re1/outputs/v1/checkpoint
2026-01-18 00:31:11 INFO     training.execution.executor: MLflow run: f4ba5c72c8be...
PASSED
tests/final_training/integration/test_final_training_component.py::test_execute_final_training_missing_dataset_raises_error 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     training.execution.executor: Final training config loaded from final_training.yaml
2026-01-18 00:31:11 INFO     training.execution.executor: Output directory: /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_mi0/outputs/v1
PASSED
tests/final_training/integration/test_final_training_component.py::test_execute_final_training_local_path_override 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:11 INFO     training.execution.executor: Final training config loaded from final_training.yaml
2026-01-18 00:31:11 INFO     training.execution.executor: Output directory: /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_lo0/outputs/v1
2026-01-18 00:31:11 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_lo0/config/tags.yaml, using defaults
2026-01-18 00:31:11 INFO     training.execution.mlflow_setup:  View run test_run_name at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/594159571823761936/runs/166c04dc7e414b3fb1b1b55a94f835f8
2026-01-18 00:31:11 INFO     training.execution.mlflow_setup: Created MLflow run: test_run_name (166c04dc7e41...)
2026-01-18 00:31:11 INFO     training.execution.executor: Created MLflow run: test_run_name (166c04dc7e41...)
2026-01-18 00:31:11 INFO     training.execution.executor: Running final training...
2026-01-18 00:31:11 INFO     infrastructure.tracking.mlflow.lifecycle: Run 166c04dc7e41... already terminated with status <Mock name='mock.get_run().info.status' id='140077261531616'> (expected FINISHED)
2026-01-18 00:31:11 INFO     training.execution.executor: Saved metadata to: None
2026-01-18 00:31:12 INFO     training.execution.executor: Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_lo0/outputs/v1/checkpoint
2026-01-18 00:31:12 INFO     training.execution.executor: MLflow run: 166c04dc7e41...
PASSED
tests/final_training/integration/test_final_training_component.py::test_execute_final_training_training_failure_marks_run_failed 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:12 INFO     training.execution.executor: Final training config loaded from final_training.yaml
2026-01-18 00:31:12 INFO     training.execution.executor: Output directory: /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_tr0/outputs/v1
2026-01-18 00:31:12 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_tr0/config/tags.yaml, using defaults
2026-01-18 00:31:12 INFO     training.execution.mlflow_setup:  View run test_run_name at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/594159571823761936/runs/aa4da4c3d5de46038af882087bf60327
2026-01-18 00:31:12 INFO     training.execution.mlflow_setup: Created MLflow run: test_run_name (aa4da4c3d5de...)
2026-01-18 00:31:12 INFO     training.execution.executor: Created MLflow run: test_run_name (aa4da4c3d5de...)
2026-01-18 00:31:12 INFO     training.execution.executor: Running final training...
2026-01-18 00:31:12 INFO     infrastructure.tracking.mlflow.lifecycle: Run aa4da4c3d5de... already has status <Mock name='mock.get_run().info.status' id='140077259681136'>, skipping termination (expected RUNNING)
PASSED
tests/final_training/integration/test_final_training_component.py::test_execute_final_training_mlflow_disabled_skips_tracking 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:12 INFO     training.execution.executor: Final training config loaded from final_training.yaml
2026-01-18 00:31:12 INFO     training.execution.executor: Output directory: /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_ml0/outputs/v1
2026-01-18 00:31:12 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_ml0/config/tags.yaml, using defaults
2026-01-18 00:31:12 INFO     training.execution.mlflow_setup:  View run test_run_name at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/594159571823761936/runs/e7fb10b787d843f2b24ce00b75c12dbb
2026-01-18 00:31:12 INFO     training.execution.mlflow_setup: Created MLflow run: test_run_name (e7fb10b787d8...)
2026-01-18 00:31:12 INFO     training.execution.executor: Created MLflow run: test_run_name (e7fb10b787d8...)
2026-01-18 00:31:12 INFO     training.execution.executor: Running final training...
2026-01-18 00:31:12 INFO     infrastructure.tracking.mlflow.lifecycle: Run e7fb10b787d8... already terminated with status <Mock name='mock.get_run().info.status' id='140077259675088'> (expected FINISHED)
2026-01-18 00:31:12 INFO     training.execution.executor: Saved metadata to: None
2026-01-18 00:31:12 INFO     training.execution.executor: Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_ml0/outputs/v1/checkpoint
2026-01-18 00:31:12 INFO     training.execution.executor: MLflow run: e7fb10b787d8...
PASSED
tests/final_training/integration/test_final_training_component.py::test_execute_final_training_source_scratch_no_checkpoint 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:12 INFO     training.execution.executor: Final training config loaded from final_training.yaml
2026-01-18 00:31:12 INFO     training.execution.executor: Output directory: /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_so0/outputs/v1
2026-01-18 00:31:12 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_so0/config/tags.yaml, using defaults
2026-01-18 00:31:12 INFO     training.execution.mlflow_setup:  View run test_run_name at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/594159571823761936/runs/2584c83d1ca34001933be4479a23614d
2026-01-18 00:31:12 INFO     training.execution.mlflow_setup: Created MLflow run: test_run_name (2584c83d1ca3...)
2026-01-18 00:31:12 INFO     training.execution.executor: Created MLflow run: test_run_name (2584c83d1ca3...)
2026-01-18 00:31:12 INFO     training.execution.executor: Running final training...
2026-01-18 00:31:12 INFO     infrastructure.tracking.mlflow.lifecycle: Run 2584c83d1ca3... already terminated with status <Mock name='mock.get_run().info.status' id='140077259676144'> (expected FINISHED)
2026-01-18 00:31:12 INFO     training.execution.executor: Saved metadata to: None
2026-01-18 00:31:12 INFO     training.execution.executor: Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_so0/outputs/v1/checkpoint
2026-01-18 00:31:12 INFO     training.execution.executor: MLflow run: 2584c83d1ca3...
PASSED
tests/final_training/integration/test_final_training_component.py::test_execute_final_training_source_final_training_with_checkpoint 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:12 INFO     training.execution.executor: Final training config loaded from final_training.yaml
2026-01-18 00:31:12 INFO     training.execution.executor: Output directory: /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_so1/outputs/v1
2026-01-18 00:31:12 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_so1/config/tags.yaml, using defaults
2026-01-18 00:31:12 INFO     training.execution.mlflow_setup:  View run test_run_name at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/594159571823761936/runs/6864c44c43e74d479824b6364000933c
2026-01-18 00:31:12 INFO     training.execution.mlflow_setup: Created MLflow run: test_run_name (6864c44c43e7...)
2026-01-18 00:31:12 INFO     training.execution.executor: Created MLflow run: test_run_name (6864c44c43e7...)
2026-01-18 00:31:12 INFO     training.execution.executor: Running final training...
2026-01-18 00:31:12 INFO     infrastructure.tracking.mlflow.lifecycle: Run 6864c44c43e7... already terminated with status <Mock name='mock.get_run().info.status' id='140077259677872'> (expected FINISHED)
2026-01-18 00:31:12 INFO     training.execution.executor: Saved metadata to: None
2026-01-18 00:31:12 INFO     training.execution.executor: Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_so1/outputs/v1/checkpoint
2026-01-18 00:31:12 INFO     training.execution.executor: MLflow run: 6864c44c43e7...
PASSED
tests/final_training/integration/test_final_training_component.py::test_execute_final_training_hyperparameter_precedence 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:12 INFO     training.execution.executor: Final training config loaded from final_training.yaml
2026-01-18 00:31:12 INFO     training.execution.executor: Output directory: /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_hy0/outputs/v1
2026-01-18 00:31:12 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_hy0/config/tags.yaml, using defaults
2026-01-18 00:31:12 INFO     training.execution.mlflow_setup:  View run test_run_name at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/594159571823761936/runs/0d9ad5e92d8b49efb144d49b2d57a178
2026-01-18 00:31:12 INFO     training.execution.mlflow_setup: Created MLflow run: test_run_name (0d9ad5e92d8b...)
2026-01-18 00:31:12 INFO     training.execution.executor: Created MLflow run: test_run_name (0d9ad5e92d8b...)
2026-01-18 00:31:12 INFO     training.execution.executor: Running final training...
2026-01-18 00:31:12 INFO     infrastructure.tracking.mlflow.lifecycle: Run 0d9ad5e92d8b... already terminated with status <Mock name='mock.get_run().info.status' id='140077259675568'> (expected FINISHED)
2026-01-18 00:31:12 INFO     training.execution.executor: Saved metadata to: None
2026-01-18 00:31:12 INFO     training.execution.executor: Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_hy0/outputs/v1/checkpoint
2026-01-18 00:31:12 INFO     training.execution.executor: MLflow run: 0d9ad5e92d8b...
PASSED
tests/final_training/integration/test_final_training_component.py::test_execute_final_training_mlflow_overrides 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:12 INFO     training.execution.executor: Final training config loaded from final_training.yaml
2026-01-18 00:31:12 INFO     training.execution.executor: Output directory: /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_ml1/outputs/v1
2026-01-18 00:31:12 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_ml1/config/tags.yaml, using defaults
2026-01-18 00:31:12 INFO     training.execution.mlflow_setup:  View run default_run_name at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/966734036254045421/runs/83c02dea447e4bd7acd026eaab9b75c6
2026-01-18 00:31:12 INFO     training.execution.mlflow_setup: Created MLflow run: default_run_name (83c02dea447e...)
2026-01-18 00:31:12 INFO     training.execution.executor: Created MLflow run: default_run_name (83c02dea447e...)
2026-01-18 00:31:12 INFO     training.execution.executor: Running final training...
2026-01-18 00:31:12 INFO     infrastructure.tracking.mlflow.lifecycle: Run 83c02dea447e... already terminated with status <Mock name='mock.get_run().info.status' id='140077259676432'> (expected FINISHED)
2026-01-18 00:31:12 INFO     training.execution.executor: Saved metadata to: None
2026-01-18 00:31:12 INFO     training.execution.executor: Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_ml1/outputs/v1/checkpoint
2026-01-18 00:31:12 INFO     training.execution.executor: MLflow run: 83c02dea447e...
PASSED
tests/final_training/integration/test_final_training_logging_intervals.py::test_logging_eval_interval_loaded_from_config 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:12 INFO     training.execution.executor: Final training config loaded from final_training.yaml
2026-01-18 00:31:12 INFO     training.execution.executor: Output directory: /tmp/pytest-of-codespace/pytest-38/test_logging_eval_interval_loa0/outputs/v1
2026-01-18 00:31:12 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_logging_eval_interval_loa0/config/tags.yaml, using defaults
2026-01-18 00:31:12 INFO     training.execution.mlflow_setup:  View run test_run_name at: file:///tmp/mlflow/#/experiments/594159571823761936/runs/af3b1035cce64567a3c020fa1174452c
2026-01-18 00:31:12 INFO     training.execution.mlflow_setup: Created MLflow run: test_run_name (af3b1035cce6...)
2026-01-18 00:31:12 INFO     training.execution.executor: Created MLflow run: test_run_name (af3b1035cce6...)
2026-01-18 00:31:12 INFO     training.execution.executor: Running final training...
2026-01-18 00:31:12 INFO     infrastructure.tracking.mlflow.lifecycle: Run af3b1035cce6... already terminated with status <Mock name='mock.get_run().info.status' id='140077259673696'> (expected FINISHED)
2026-01-18 00:31:12 INFO     training.execution.executor: Saved metadata to: None
2026-01-18 00:31:12 INFO     training.execution.executor: Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-38/test_logging_eval_interval_loa0/outputs/v1/checkpoint
2026-01-18 00:31:12 INFO     training.execution.executor: MLflow run: af3b1035cce6...
PASSED
tests/final_training/integration/test_final_training_logging_intervals.py::test_logging_save_interval_loaded_from_config 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:12 INFO     training.execution.executor: Final training config loaded from final_training.yaml
2026-01-18 00:31:12 INFO     training.execution.executor: Output directory: /tmp/pytest-of-codespace/pytest-38/test_logging_save_interval_loa0/outputs/v1
2026-01-18 00:31:12 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_logging_save_interval_loa0/config/tags.yaml, using defaults
2026-01-18 00:31:12 INFO     training.execution.mlflow_setup:  View run test_run_name at: file:///tmp/mlflow/#/experiments/594159571823761936/runs/baef4609a982419d9f7f292d3376e5eb
2026-01-18 00:31:12 INFO     training.execution.mlflow_setup: Created MLflow run: test_run_name (baef4609a982...)
2026-01-18 00:31:12 INFO     training.execution.executor: Created MLflow run: test_run_name (baef4609a982...)
2026-01-18 00:31:12 INFO     training.execution.executor: Running final training...
2026-01-18 00:31:12 INFO     infrastructure.tracking.mlflow.lifecycle: Run baef4609a982... already terminated with status <Mock name='mock.get_run().info.status' id='140077259677104'> (expected FINISHED)
2026-01-18 00:31:12 INFO     training.execution.executor: Saved metadata to: None
2026-01-18 00:31:12 INFO     training.execution.executor: Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-38/test_logging_save_interval_loa0/outputs/v1/checkpoint
2026-01-18 00:31:12 INFO     training.execution.executor: MLflow run: baef4609a982...
PASSED
tests/final_training/integration/test_final_training_logging_intervals.py::test_logging_intervals_both_loaded_from_config 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:12 INFO     training.execution.executor: Final training config loaded from final_training.yaml
2026-01-18 00:31:12 INFO     training.execution.executor: Output directory: /tmp/pytest-of-codespace/pytest-38/test_logging_intervals_both_lo0/outputs/v1
2026-01-18 00:31:12 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_logging_intervals_both_lo0/config/tags.yaml, using defaults
2026-01-18 00:31:12 INFO     training.execution.mlflow_setup:  View run test_run_name at: file:///tmp/mlflow/#/experiments/594159571823761936/runs/15aefaf9490e45079b6dd8a3a15cbd13
2026-01-18 00:31:12 INFO     training.execution.mlflow_setup: Created MLflow run: test_run_name (15aefaf9490e...)
2026-01-18 00:31:12 INFO     training.execution.executor: Created MLflow run: test_run_name (15aefaf9490e...)
2026-01-18 00:31:12 INFO     training.execution.executor: Running final training...
2026-01-18 00:31:12 INFO     infrastructure.tracking.mlflow.lifecycle: Run 15aefaf9490e... already terminated with status <Mock name='mock.get_run().info.status' id='140077281274672'> (expected FINISHED)
2026-01-18 00:31:12 INFO     training.execution.executor: Saved metadata to: None
2026-01-18 00:31:12 INFO     training.execution.executor: Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-38/test_logging_intervals_both_lo0/outputs/v1/checkpoint
2026-01-18 00:31:12 INFO     training.execution.executor: MLflow run: 15aefaf9490e...
PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestSourceParentDictFormat::test_source_parent_dict_format_resolves_checkpoint PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestSourceParentDictFormat::test_source_parent_dict_format_with_validation_false PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestCheckpointSource::test_checkpoint_source_string_path PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestCheckpointSource::test_checkpoint_source_relative_path PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestCheckpointSource::test_checkpoint_source_dict_format PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestCheckpointSource::test_checkpoint_source_validation_fails PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestCheckpointSource::test_checkpoint_source_validation_false_allows_invalid PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestSeedRandomSeed::test_seed_random_seed_override PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestSeedRandomSeed::test_seed_falls_back_to_train_config PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestSeedRandomSeed::test_seed_falls_back_to_default PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestSeedRandomSeed::test_seed_precedence_final_training_over_best_config PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestVariantNumber::test_variant_number_explicit_override PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestVariantNumber::test_variant_number_ignored_when_force_new PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestVariantNumber::test_variant_number_none_auto_increments PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestEarlyStopping::test_early_stopping_enabled_override PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestEarlyStopping::test_early_stopping_patience_override PASSED
tests/hpo/integration/test_best_trial_selection_component.py::TestBestTrialSelection::test_best_trial_selected_by_optuna PASSED
tests/hpo/integration/test_best_trial_selection_component.py::TestBestTrialSelection::test_best_trial_extraction PASSED
tests/hpo/integration/test_best_trial_selection_component.py::TestBestTrialSelection::test_best_trial_with_cv_statistics PASSED
tests/hpo/integration/test_best_trial_selection_component.py::TestBestTrialSelection::test_best_trial_minimization_direction PASSED
tests/hpo/integration/test_best_trial_selection_component.py::TestBestTrialSelectionWithCriteria::test_selection_with_accuracy_threshold PASSED
tests/hpo/integration/test_best_trial_selection_component.py::TestBestTrialSelectionWithCriteria::test_selection_with_min_accuracy_gain PASSED
tests/hpo/integration/test_best_trial_selection_component.py::TestBestTrialSelectionWithCriteria::test_selection_accuracy_only_when_outside_threshold PASSED
tests/hpo/integration/test_best_trial_selection_component.py::TestBestTrialSelectionWithCriteria::test_selection_smoke_yaml_parameters PASSED
tests/hpo/integration/test_best_trial_selection_component.py::TestBestTrialIntegration::test_best_trial_used_for_refit PASSED
tests/hpo/integration/test_best_trial_selection_component.py::TestBestTrialIntegration::test_best_trial_with_no_completed_trials PASSED
tests/hpo/integration/test_best_trial_selection_component.py::TestBestTrialIntegration::test_best_trial_preserves_hyperparameters PASSED
tests/hpo/integration/test_early_termination.py::TestPrunerCreation::test_create_pruner_bandit_policy PASSED
tests/hpo/integration/test_early_termination.py::TestPrunerCreation::test_create_pruner_median_policy PASSED
tests/hpo/integration/test_early_termination.py::TestPrunerCreation::test_create_pruner_no_early_termination PASSED
tests/hpo/integration/test_early_termination.py::TestPrunerCreation::test_create_pruner_smoke_yaml_params PASSED
tests/hpo/integration/test_early_termination.py::TestPruningBehavior::test_pruner_delays_evaluation PASSED
tests/hpo/integration/test_early_termination.py::TestPruningBehavior::test_pruner_prunes_poor_trials PASSED
tests/hpo/integration/test_early_termination.py::TestPruningBehavior::test_pruner_evaluation_interval PASSED
tests/hpo/integration/test_early_termination.py::TestPruningBehavior::test_pruner_with_study_manager 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:15 INFO     training.hpo.core.study: [HPO] Starting optimization for distilbert with checkpointing...
FAILED
tests/hpo/integration/test_early_termination.py::TestPruningIntegration::test_pruning_preserves_best_trials PASSED
tests/hpo/integration/test_early_termination.py::TestPruningIntegration::test_pruning_with_checkpoint_resume 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:15 INFO     training.hpo.core.study: [HPO] Starting optimization for distilbert with checkpointing...
2026-01-18 00:31:15 INFO     training.hpo.core.study: [HPO] Starting optimization for distilbert with checkpointing...
FAILED
tests/hpo/integration/test_early_termination.py::TestPruningIntegration::test_pruning_disabled_behavior PASSED
tests/hpo/integration/test_early_termination.py::TestPruningSmokeYaml::test_pruning_smoke_yaml_config PASSED
tests/hpo/integration/test_error_handling.py::TestTrialExecutionErrors::test_training_subprocess_failure PASSED
tests/hpo/integration/test_error_handling.py::TestTrialExecutionErrors::test_training_module_not_found PASSED
tests/hpo/integration/test_error_handling.py::TestTrialExecutionErrors::test_missing_metrics_file 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:15 ERROR    training.hpo.trial.metrics: metrics.json not found at expected location: /tmp/pytest-of-codespace/pytest-38/test_missing_metrics_file0/outputs/hpo/local/distilbert/study-abc12345/trial-def67890/metrics.json. Trial output dir: /tmp/pytest-of-codespace/pytest-38/test_missing_metrics_file0/outputs/hpo/local/distilbert/study-abc12345/trial-def67890, Root dir: /tmp/pytest-of-codespace/pytest-38/test_missing_metrics_file0
PASSED
tests/hpo/integration/test_error_handling.py::TestTrialExecutionErrors::test_missing_objective_metric_in_metrics PASSED
tests/hpo/integration/test_error_handling.py::TestCVOrchestratorErrors::test_cv_trial_failure_propagates_error 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:15 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-38/test_cv_trial_failure_propagat0/config/naming.yaml, using empty policy
2026-01-18 00:31:15 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_cv_trial_failure_propagat0/config/tags.yaml, using defaults
2026-01-18 00:31:15 WARNING  infrastructure.paths.resolve: Pattern 'hpo_v2' not found in paths.yaml. Using fallback logic.
2026-01-18 00:31:15 INFO     training.hpo.execution.local.cv: [CV] Created trial folder: /workspaces/resume-ner-azureml/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-d7c6e3fb (trial 0)
PASSED
tests/hpo/integration/test_error_handling.py::TestCVOrchestratorErrors::test_cv_missing_trial_key_hash_raises_error 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:15 WARNING  infrastructure.tracking.mlflow.hash_utils: Could not get or compute study_key_hash. study_key_hash=missing, hpo_parent_run_id=provided, configs=missing
2026-01-18 00:31:15 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-38/test_cv_missing_trial_key_hash0/config/naming.yaml, using empty policy
2026-01-18 00:31:15 WARNING  training.hpo.execution.local.cv: Could not build systematic run name and tags: HPO trial run name built without study_key_hash; check study identity propagation., using fallback
2026-01-18 00:31:15 WARNING  infrastructure.tracking.mlflow.hash_utils: Could not get or compute study_key_hash. study_key_hash=missing, hpo_parent_run_id=provided, configs=missing
2026-01-18 00:31:15 ERROR    training.hpo.execution.local.cv: In v2 study folder study-abc12345 but trial_key_hash is None. study_key_hash=NO. Will attempt to compute hash.
2026-01-18 00:31:15 WARNING  training.hpo.execution.local.cv: In v2 study folder but missing hashes: study_key_hash=NO, trial_key_hash=NO
PASSED
tests/hpo/integration/test_error_handling.py::TestRefitExecutionErrors::test_refit_subprocess_failure 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:15 INFO     training.hpo.execution.local.refit: [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'batch_size': 4, 'dropout': 0.2, 'weight_decay': 0.05, 'backbone': 'distilbert'}
2026-01-18 00:31:15 INFO     training.hpo.execution.local.refit: [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 00:31:15 WARNING  infrastructure.paths.resolve: Pattern 'hpo_v2' not found in paths.yaml. Using fallback logic.
2026-01-18 00:31:15 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-38/test_refit_subprocess_failure0/config/naming.yaml, using empty policy
2026-01-18 00:31:15 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_refit_subprocess_failure0/config/tags.yaml, using defaults
2026-01-18 00:31:15 INFO     training.execution.mlflow_setup:  View run local_distilbert_hpo_refit_legacy at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/302183572435427150/runs/a9d14aa7ae8e462aa7b13f6a369b322d
2026-01-18 00:31:15 INFO     training.execution.mlflow_setup: Created MLflow run: local_distilbert_hpo_refit_legacy (a9d14aa7ae8e...)
2026-01-18 00:31:15 INFO     training.hpo.execution.local.refit: [REFIT] Attempting to link refit run a9d14aa7ae8e... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-18 00:31:15 ERROR    training.hpo.execution.local.refit: [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 548, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
2026-01-18 00:31:15 ERROR    training.hpo.execution.local.refit: Training failed with return code 1
2026-01-18 00:31:15 ERROR    training.hpo.execution.local.refit: STDOUT:

2026-01-18 00:31:15 ERROR    training.hpo.execution.local.refit: STDERR:
/opt/conda/envs/resume-ner-training/bin/python: Error while finding module specification for 'training.cli.train' (ModuleNotFoundError: No module named 'training.cli')

PASSED
tests/hpo/integration/test_error_handling.py::TestRefitExecutionErrors::test_refit_non_v2_study_folder_raises_error 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:15 INFO     training.hpo.execution.local.refit: [REFIT] Starting refit training for trial 0 with hyperparameters: {'backbone': 'distilbert'}
2026-01-18 00:31:15 INFO     training.hpo.execution.local.refit: [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 00:31:15 WARNING  training.hpo.execution.local.refit: Could not construct v2 refit folder, falling back to legacy: 'NoneType' object has no attribute 'mkdir'
PASSED
tests/hpo/integration/test_error_handling.py::TestStudyManagerErrors::test_study_creation_with_invalid_storage_uri 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:15 INFO     training.hpo.core.study: [HPO] Starting optimization for distilbert with checkpointing...
PASSED
tests/hpo/integration/test_error_handling.py::TestBestTrialSelectionErrors::test_extract_best_config_with_no_completed_trials PASSED
tests/hpo/integration/test_error_handling.py::TestBestTrialSelectionErrors::test_selection_logic_with_empty_candidates PASSED
tests/hpo/integration/test_error_handling.py::TestSearchSpaceErrors::test_invalid_search_space_type PASSED
tests/hpo/integration/test_error_handling.py::TestSearchSpaceErrors::test_invalid_float_range PASSED
tests/hpo/integration/test_error_handling.py::TestMLflowErrors::test_mlflow_run_creation_failure_handled_gracefully PASSED
tests/hpo/integration/test_error_handling.py::TestConfigurationErrors::test_missing_objective_metric_in_config PASSED
tests/hpo/integration/test_error_handling.py::TestConfigurationErrors::test_invalid_sampling_algorithm PASSED
tests/hpo/integration/test_error_handling.py::TestPathResolutionErrors::test_missing_config_dir_handled_gracefully PASSED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointCreation::test_checkpoint_storage_path_resolution PASSED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointCreation::test_checkpoint_storage_path_with_backbone_placeholder PASSED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointCreation::test_checkpoint_disabled_returns_none PASSED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointCreation::test_storage_uri_conversion PASSED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointResume::test_resume_from_existing_checkpoint 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:15 INFO     training.hpo.core.study: [HPO] Resuming optimization for distilbert from checkpoint...
2026-01-18 00:31:15 INFO     training.hpo.core.study: Loaded 0 existing trials (0 completed, 0 marked as failed)
FAILED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointResume::test_resume_preserves_trials 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:15 INFO     training.hpo.core.study: [HPO] Resuming optimization for distilbert from checkpoint...
2026-01-18 00:31:15 INFO     training.hpo.core.study: Loaded 0 existing trials (0 completed, 0 marked as failed)
FAILED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointResume::test_resume_marks_running_trials_as_failed 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:16 INFO     training.hpo.core.study: [HPO] Resuming optimization for distilbert from checkpoint...
2026-01-18 00:31:16 INFO     training.hpo.core.study: Loaded 0 existing trials (0 completed, 0 marked as failed)
FAILED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointResume::test_resume_with_auto_resume_false_raises_error 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:16 INFO     training.hpo.core.study: [HPO] auto_resume=false: Creating new study 'hpo_distilbert_no_resume_test' (ignoring existing study)
2026-01-18 00:31:16 INFO     training.hpo.core.study: [HPO] Starting optimization for distilbert with checkpointing...
FAILED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointResume::test_resume_continues_trial_numbering 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:16 INFO     training.hpo.core.study: [HPO] Resuming optimization for distilbert from checkpoint...
2026-01-18 00:31:16 INFO     training.hpo.core.study: Loaded 0 existing trials (0 completed, 0 marked as failed)
FAILED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointSmokeYaml::test_checkpoint_smoke_yaml_study_name_template PASSED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointSmokeYaml::test_checkpoint_smoke_yaml_storage_path_template PASSED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointSmokeYaml::test_checkpoint_smoke_yaml_auto_resume_true 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:16 INFO     training.hpo.core.study: [HPO] Resuming optimization for distilbert from checkpoint...
2026-01-18 00:31:16 INFO     training.hpo.core.study: Loaded 0 existing trials (0 completed, 0 marked as failed)
FAILED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointFileIO::test_checkpoint_file_exists_after_study_creation PASSED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointFileIO::test_checkpoint_file_persists_trials PASSED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointFileIO::test_checkpoint_file_can_be_moved_and_loaded PASSED
tests/hpo/integration/test_hpo_full_workflow.py::TestFullHPOWorkflow::test_full_hpo_workflow_with_cv_and_refit 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:16 INFO     training.hpo.execution.local.sweep: [EARLY HASH] Computed v2 study_key_hash=01b6aaaa11c2ca02... for folder creation
2026-01-18 00:31:16 INFO     training.hpo.core.study: [HPO] Starting optimization for distilbert with checkpointing...
2026-01-18 00:31:16 WARNING  infrastructure.naming.display_policy: [naming.yaml] Missing required 'run_names' section (/tmp/pytest-of-codespace/pytest-38/test_full_hpo_workflow_with_cv0/config/naming.yaml)
2026-01-18 00:31:16 INFO     training.hpo.execution.local.sweep: [HPO Setup] k_folds=2, fold_splits_file=None, k_fold config: {'enabled': True, 'n_splits': 2, 'random_seed': 42, 'shuffle': True, 'stratified': True}
2026-01-18 00:31:16 INFO     training.hpo.execution.local.sweep: [CV Setup]  Created 2 fold splits and saved to /tmp/pytest-of-codespace/pytest-38/test_full_hpo_workflow_with_cv0/outputs/hpo/local/distilbert/study-01b6aaaa/fold_splits.json
2026-01-18 00:31:16 WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker: Using LOCAL MLflow tracking (not Azure ML)
2026-01-18 00:31:16 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
2026-01-18 00:31:17 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=01b6aaaa11c2ca02...
2026-01-18 00:31:17 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=01b6aaaa11c2ca02..., study_family_hash=6a3fd9967110a5db...
2026-01-18 00:31:17 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [START_SWEEP_RUN] Yielding RunHandle. run_id=37cce966ef87...
2026-01-18 00:31:17 INFO     training.hpo.execution.local.sweep: [HPO] Parent run 37cce966ef87... is now visible in MLflow (status: RUNNING)
2026-01-18 00:31:17 INFO     training.hpo.execution.local.sweep: Setting Phase 2 tags on parent run 37cce966ef87... (data_config=present, train_config=present)
2026-01-18 00:31:17 WARNING  training.hpo.execution.local.sweep: Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
2026-01-18 00:31:17 WARNING  training.hpo.execution.local.sweep: Could not set data fingerprint tag: 'Missing tag key: fingerprint.data'
2026-01-18 00:31:17 WARNING  training.hpo.execution.local.sweep: Could not set eval fingerprint tag: 'Missing tag key: fingerprint.eval'
2026-01-18 00:31:17 WARNING  training.hpo.execution.local.sweep: Could not set objective direction tag: 'Missing tag key: objective.direction'
2026-01-18 00:31:17 WARNING  training.hpo.execution.local.sweep: Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
2026-01-18 00:31:17 WARNING  training.hpo.execution.local.sweep: Could not set artifact.available tag: 'Missing tag key: artifact.available'
2026-01-18 00:31:17 INFO     training.hpo.execution.local.sweep:  Set Phase 2 tags on parent run 37cce966ef87...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-18 00:31:17 INFO     training.hpo.execution.local.sweep:  Successfully completed Phase 2 tag setting for parent run 37cce966ef87...
2026-01-18 00:31:17 INFO     training.hpo.tracking.cleanup: [CLEANUP] Starting cleanup check: parent_run_id=37cce966ef87...
2026-01-18 00:31:17 INFO     training.hpo.tracking.cleanup: [CLEANUP] MLflow imported successfully. Current env: local, run_key_hash: cc90dfbe1603...
2026-01-18 00:31:17 INFO     training.hpo.tracking.cleanup: [CLEANUP] Retrieved experiment: 975860881501093750
2026-01-18 00:31:17 INFO     training.hpo.tracking.cleanup: [CLEANUP] Fetching all runs in experiment (may paginate for large experiments)...
2026-01-18 00:31:17 INFO     training.hpo.tracking.cleanup: [CLEANUP] Fetched 14 total runs from experiment
2026-01-18 00:31:17 INFO     training.hpo.tracking.cleanup: [CLEANUP] Built parentchildren map: 0 parents have children
2026-01-18 00:31:17 INFO     training.hpo.tracking.cleanup: [CLEANUP] Status breakdown: {'RUNNING': 1, 'FINISHED': 13}
2026-01-18 00:31:17 INFO     training.hpo.tracking.cleanup: [CLEANUP] Found 0 tag-based matches, 0 name-fallback matches (legacy), 0 total eligible for tagging
2026-01-18 00:31:17 INFO     training.hpo.tracking.cleanup: [CLEANUP] Found 0 orphaned child runs (RUNNING children with terminal parents)
2026-01-18 00:31:17 INFO     training.hpo.tracking.cleanup: [CLEANUP] No interrupted parent runs found to tag
2026-01-18 00:31:17 INFO     training.hpo.tracking.cleanup: [CLEANUP] No orphaned child runs found to tag
2026-01-18 00:31:17 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=37cce966ef87...
2026-01-18 00:31:17 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Found 0 completed trials out of 0 total
2026-01-18 00:31:17 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Logging metrics: n_trials=0, n_completed_trials=0
2026-01-18 00:31:17 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=<MagicMock name='mock.create_study().best_value' id='140077206668144'>
2026-01-18 00:31:17 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Logging best hyperparameters: <MagicMock name='mock.create_study().best_params' id='140077206783792'>
2026-01-18 00:31:17 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Finding and logging best trial run ID...
2026-01-18 00:31:17 WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker: Could not find MLflow run ID for best trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>. Search failed: Invalid clause(s) in filter string: 'tags.trial_number = '<MagicMock name='mock.create_study()', '.', 'best_trial.number', '' id='', '140077206848368', ''>''. This may be a timing issue - trial runs may not be created/committed yet.
2026-01-18 00:31:17 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Completed best trial ID logging
2026-01-18 00:31:17 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Checkpoint logging enabled: False (upload_checkpoint=False)
2026-01-18 00:31:17 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
2026-01-18 00:31:17 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Completed log_final_metrics successfully
2026-01-18 00:31:17 INFO     training.hpo.execution.local.sweep: [REFIT] Starting refit training for best trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>
2026-01-18 00:31:17 WARNING  training.hpo.execution.local.sweep: [REFIT] Could not determine trial_key_hash from best trial run tags. Refit run may not match trial run.
2026-01-18 00:31:17 INFO     training.hpo.execution.local.refit: [REFIT] Starting refit training for trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'> with hyperparameters: <MagicMock name='mock.create_study().best_trial.params' id='140077206865856'>
2026-01-18 00:31:17 INFO     training.hpo.execution.local.refit: [REFIT] Computed trial_id="trial_<MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>_20260118_003116", run_id='20260118_003116', trial_number=<MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>
2026-01-18 00:31:17 WARNING  infrastructure.tracking.mlflow.hash_utils: Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
2026-01-18 00:31:17 WARNING  training.hpo.execution.local.sweep: [REFIT] Refit training failed: Cannot create refit in v2 study folder study-01b6aaaa without trial_key_hash. study_key_hash=present, trial_key_hash=missing, refit_params=missing. Hash computation from trial parameters may have failed.. Continuing without refit checkpoint.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py", line 1244, in run_local_hpo_sweep
    refit_metrics, refit_checkpoint_dir, refit_run_id = run_refit_training(
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 240, in run_refit_training
    raise RuntimeError(
RuntimeError: Cannot create refit in v2 study folder study-01b6aaaa without trial_key_hash. study_key_hash=present, trial_key_hash=missing, refit_params=missing. Hash computation from trial parameters may have failed.
2026-01-18 00:31:17 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_BEST_CHECKPOINT] Falling back to standard checkpoint search
2026-01-18 00:31:17 WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker: Best trial checkpoint not found for trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>. Searched in: /tmp/pytest-of-codespace/pytest-38/test_full_hpo_workflow_with_cv0/outputs/hpo/local/distilbert. Skipping MLflow checkpoint logging.
2026-01-18 00:31:17 WARNING  training.hpo.execution.local.sweep: [REFIT] No refit_run_id available to mark as FINISHED
2026-01-18 00:31:17 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [START_SWEEP_RUN] Context manager exiting normally. run_id=37cce966ef87...
FAILED
tests/hpo/integration/test_hpo_full_workflow.py::TestFullHPOWorkflow::test_full_hpo_workflow_no_cv_no_refit 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:17 INFO     training.hpo.execution.local.sweep: [EARLY HASH] Computed v2 study_key_hash=957b9a1438354884... for folder creation
2026-01-18 00:31:18 WARNING  evaluation.selection.trial_finder: No v2 study folders found in /tmp/pytest-of-codespace/pytest-38/test_full_hpo_workflow_no_cv_n0/outputs/hpo/local/distilbert (resolved from /tmp/pytest-of-codespace/pytest-38/test_full_hpo_workflow_no_cv_n0/outputs/hpo/local/distilbert)
2026-01-18 00:31:18 WARNING  infrastructure.naming.display_policy: [naming.yaml] Missing required 'run_names' section (/tmp/pytest-of-codespace/pytest-38/test_full_hpo_workflow_no_cv_n0/config/naming.yaml)
2026-01-18 00:31:18 INFO     training.hpo.execution.local.sweep: [HPO Setup] k_folds=None, fold_splits_file=None, k_fold config: {'enabled': False}
2026-01-18 00:31:18 WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker: Using LOCAL MLflow tracking (not Azure ML)
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=957b9a1438354884...
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=957b9a1438354884..., study_family_hash=23208e5829deb236...
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [START_SWEEP_RUN] Yielding RunHandle. run_id=fbc8ed55335b...
2026-01-18 00:31:18 INFO     training.hpo.execution.local.sweep: [HPO] Parent run fbc8ed55335b... is now visible in MLflow (status: RUNNING)
2026-01-18 00:31:18 INFO     training.hpo.execution.local.sweep: Setting Phase 2 tags on parent run fbc8ed55335b... (data_config=present, train_config=present)
2026-01-18 00:31:18 WARNING  training.hpo.execution.local.sweep: Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
2026-01-18 00:31:18 WARNING  training.hpo.execution.local.sweep: Could not set data fingerprint tag: 'Missing tag key: fingerprint.data'
2026-01-18 00:31:18 WARNING  training.hpo.execution.local.sweep: Could not set eval fingerprint tag: 'Missing tag key: fingerprint.eval'
2026-01-18 00:31:18 WARNING  training.hpo.execution.local.sweep: Could not set objective direction tag: 'Missing tag key: objective.direction'
2026-01-18 00:31:18 WARNING  training.hpo.execution.local.sweep: Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
2026-01-18 00:31:18 WARNING  training.hpo.execution.local.sweep: Could not set artifact.available tag: 'Missing tag key: artifact.available'
2026-01-18 00:31:18 INFO     training.hpo.execution.local.sweep:  Set Phase 2 tags on parent run fbc8ed55335b...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-18 00:31:18 INFO     training.hpo.execution.local.sweep:  Successfully completed Phase 2 tag setting for parent run fbc8ed55335b...
2026-01-18 00:31:18 INFO     training.hpo.tracking.cleanup: [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=fbc8ed55335b...
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Found 0 completed trials out of 0 total
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Logging metrics: n_trials=0, n_completed_trials=0
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=<MagicMock name='mock.create_study().best_value' id='140077206668144'>
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Logging best hyperparameters: <MagicMock name='mock.create_study().best_params' id='140077206783792'>
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Finding and logging best trial run ID...
2026-01-18 00:31:18 WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker: Could not find MLflow run ID for best trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>. Search failed: Invalid clause(s) in filter string: 'tags.trial_number = '<MagicMock name='mock.create_study()', '.', 'best_trial.number', '' id='', '140077206848368', ''>''. This may be a timing issue - trial runs may not be created/committed yet.
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Completed best trial ID logging
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Checkpoint logging enabled: False (upload_checkpoint=False)
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Completed log_final_metrics successfully
2026-01-18 00:31:18 INFO     training.hpo.execution.local.sweep: [REFIT] Refit training is disabled in config
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [START_SWEEP_RUN] Context manager exiting normally. run_id=fbc8ed55335b...
FAILED
tests/hpo/integration/test_hpo_full_workflow.py::TestFullHPOWorkflow::test_full_hpo_workflow_creates_correct_path_structure 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:18 INFO     training.hpo.execution.local.sweep: [EARLY HASH] Computed v2 study_key_hash=dc0c421a5056294a... for folder creation
2026-01-18 00:31:18 WARNING  evaluation.selection.trial_finder: No v2 study folders found in /tmp/pytest-of-codespace/pytest-38/test_full_hpo_workflow_creates0/outputs/hpo/local/distilbert (resolved from /tmp/pytest-of-codespace/pytest-38/test_full_hpo_workflow_creates0/outputs/hpo/local/distilbert)
2026-01-18 00:31:18 WARNING  infrastructure.naming.display_policy: [naming.yaml] Missing required 'run_names' section (/tmp/pytest-of-codespace/pytest-38/test_full_hpo_workflow_creates0/config/naming.yaml)
2026-01-18 00:31:18 INFO     training.hpo.execution.local.sweep: [HPO Setup] k_folds=2, fold_splits_file=None, k_fold config: {'enabled': True, 'n_splits': 2, 'random_seed': 42, 'shuffle': True, 'stratified': True}
2026-01-18 00:31:18 INFO     training.hpo.execution.local.sweep: [CV Setup]  Created 2 fold splits and saved to /tmp/pytest-of-codespace/pytest-38/test_full_hpo_workflow_creates0/outputs/hpo/local/distilbert/study-dc0c421a/fold_splits.json
2026-01-18 00:31:18 WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker: Using LOCAL MLflow tracking (not Azure ML)
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=dc0c421a5056294a...
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=dc0c421a5056294a..., study_family_hash=d1aa8bfffdcfdb77...
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [START_SWEEP_RUN] Yielding RunHandle. run_id=bd458de544e9...
2026-01-18 00:31:18 INFO     training.hpo.execution.local.sweep: [HPO] Parent run bd458de544e9... is now visible in MLflow (status: RUNNING)
2026-01-18 00:31:18 INFO     training.hpo.execution.local.sweep: Setting Phase 2 tags on parent run bd458de544e9... (data_config=present, train_config=present)
2026-01-18 00:31:18 WARNING  training.hpo.execution.local.sweep: Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
2026-01-18 00:31:18 WARNING  training.hpo.execution.local.sweep: Could not set data fingerprint tag: 'Missing tag key: fingerprint.data'
2026-01-18 00:31:18 WARNING  training.hpo.execution.local.sweep: Could not set eval fingerprint tag: 'Missing tag key: fingerprint.eval'
2026-01-18 00:31:18 WARNING  training.hpo.execution.local.sweep: Could not set objective direction tag: 'Missing tag key: objective.direction'
2026-01-18 00:31:18 WARNING  training.hpo.execution.local.sweep: Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
2026-01-18 00:31:18 WARNING  training.hpo.execution.local.sweep: Could not set artifact.available tag: 'Missing tag key: artifact.available'
2026-01-18 00:31:18 INFO     training.hpo.execution.local.sweep:  Set Phase 2 tags on parent run bd458de544e9...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-18 00:31:18 INFO     training.hpo.execution.local.sweep:  Successfully completed Phase 2 tag setting for parent run bd458de544e9...
2026-01-18 00:31:18 INFO     training.hpo.tracking.cleanup: [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=bd458de544e9...
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Found 0 completed trials out of 0 total
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Logging metrics: n_trials=0, n_completed_trials=0
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=<MagicMock name='mock.create_study().best_value' id='140077206668144'>
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Logging best hyperparameters: <MagicMock name='mock.create_study().best_params' id='140077206783792'>
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Finding and logging best trial run ID...
2026-01-18 00:31:18 WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker: Could not find MLflow run ID for best trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>. Search failed: Invalid clause(s) in filter string: 'tags.trial_number = '<MagicMock name='mock.create_study()', '.', 'best_trial.number', '' id='', '140077206848368', ''>''. This may be a timing issue - trial runs may not be created/committed yet.
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Completed best trial ID logging
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Checkpoint logging enabled: False (upload_checkpoint=False)
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Completed log_final_metrics successfully
2026-01-18 00:31:18 INFO     training.hpo.execution.local.sweep: [REFIT] Starting refit training for best trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>
2026-01-18 00:31:18 WARNING  training.hpo.execution.local.sweep: [REFIT] Could not determine trial_key_hash from best trial run tags. Refit run may not match trial run.
2026-01-18 00:31:18 INFO     training.hpo.execution.local.refit: [REFIT] Starting refit training for trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'> with hyperparameters: <MagicMock name='mock.create_study().best_trial.params' id='140077206865856'>
2026-01-18 00:31:18 INFO     training.hpo.execution.local.refit: [REFIT] Computed trial_id="trial_<MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>_20260118_003118", run_id='20260118_003118', trial_number=<MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>
2026-01-18 00:31:18 WARNING  infrastructure.tracking.mlflow.hash_utils: Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
2026-01-18 00:31:18 WARNING  training.hpo.execution.local.sweep: [REFIT] Refit training failed: Cannot create refit in v2 study folder study-dc0c421a without trial_key_hash. study_key_hash=present, trial_key_hash=missing, refit_params=missing. Hash computation from trial parameters may have failed.. Continuing without refit checkpoint.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py", line 1244, in run_local_hpo_sweep
    refit_metrics, refit_checkpoint_dir, refit_run_id = run_refit_training(
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 240, in run_refit_training
    raise RuntimeError(
RuntimeError: Cannot create refit in v2 study folder study-dc0c421a without trial_key_hash. study_key_hash=present, trial_key_hash=missing, refit_params=missing. Hash computation from trial parameters may have failed.
2026-01-18 00:31:18 INFO     training.hpo.execution.local.sweep: [HPO] Skipping checkpoint logging (mlflow.log_best_checkpoint=false or not set)
2026-01-18 00:31:18 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_full_hpo_workflow_creates0/outputs/hpo/config/tags.yaml, using defaults
2026-01-18 00:31:18 WARNING  training.hpo.execution.local.sweep: [REFIT] No refit_run_id available to mark as FINISHED
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [START_SWEEP_RUN] Context manager exiting normally. run_id=bd458de544e9...
FAILED
tests/hpo/integration/test_hpo_resume_workflow.py::TestHPOResumeWorkflow::test_resume_workflow_preserves_trials 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:18 INFO     training.hpo.execution.local.sweep: [EARLY HASH] Computed v2 study_key_hash=957b9a1438354884... for folder creation
2026-01-18 00:31:18 INFO     training.hpo.core.study: [HPO] Starting optimization for distilbert with checkpointing...
2026-01-18 00:31:18 WARNING  infrastructure.naming.display_policy: [naming.yaml] Missing required 'run_names' section (/tmp/pytest-of-codespace/pytest-38/test_resume_workflow_preserves0/config/naming.yaml)
2026-01-18 00:31:18 INFO     training.hpo.execution.local.sweep: [HPO Setup] k_folds=None, fold_splits_file=None, k_fold config: {'enabled': False}
2026-01-18 00:31:18 WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker: Using LOCAL MLflow tracking (not Azure ML)
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=957b9a1438354884...
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=957b9a1438354884..., study_family_hash=23208e5829deb236...
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [START_SWEEP_RUN] Yielding RunHandle. run_id=67807eddde4b...
2026-01-18 00:31:18 INFO     training.hpo.execution.local.sweep: [HPO] Parent run 67807eddde4b... is now visible in MLflow (status: RUNNING)
2026-01-18 00:31:18 INFO     training.hpo.execution.local.sweep: Setting Phase 2 tags on parent run 67807eddde4b... (data_config=present, train_config=present)
2026-01-18 00:31:18 WARNING  training.hpo.execution.local.sweep: Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
2026-01-18 00:31:18 WARNING  training.hpo.execution.local.sweep: Could not set data fingerprint tag: 'Missing tag key: fingerprint.data'
2026-01-18 00:31:18 WARNING  training.hpo.execution.local.sweep: Could not set eval fingerprint tag: 'Missing tag key: fingerprint.eval'
2026-01-18 00:31:18 WARNING  training.hpo.execution.local.sweep: Could not set objective direction tag: 'Missing tag key: objective.direction'
2026-01-18 00:31:18 WARNING  training.hpo.execution.local.sweep: Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
2026-01-18 00:31:18 WARNING  training.hpo.execution.local.sweep: Could not set artifact.available tag: 'Missing tag key: artifact.available'
2026-01-18 00:31:18 INFO     training.hpo.execution.local.sweep:  Set Phase 2 tags on parent run 67807eddde4b...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-18 00:31:18 INFO     training.hpo.execution.local.sweep:  Successfully completed Phase 2 tag setting for parent run 67807eddde4b...
2026-01-18 00:31:18 INFO     training.hpo.tracking.cleanup: [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=67807eddde4b...
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Found 0 completed trials out of 0 total
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Logging metrics: n_trials=0, n_completed_trials=0
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=<MagicMock name='mock.create_study().best_value' id='140077206668144'>
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Logging best hyperparameters: <MagicMock name='mock.create_study().best_params' id='140077206783792'>
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Finding and logging best trial run ID...
2026-01-18 00:31:18 WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker: Could not find MLflow run ID for best trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>. Search failed: Invalid clause(s) in filter string: 'tags.trial_number = '<MagicMock name='mock.create_study()', '.', 'best_trial.number', '' id='', '140077206848368', ''>''. This may be a timing issue - trial runs may not be created/committed yet.
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Completed best trial ID logging
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Checkpoint logging enabled: False (upload_checkpoint=False)
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Completed log_final_metrics successfully
2026-01-18 00:31:18 INFO     training.hpo.execution.local.sweep: [REFIT] Refit training is disabled in config
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [START_SWEEP_RUN] Context manager exiting normally. run_id=67807eddde4b...
FAILED
tests/hpo/integration/test_hpo_resume_workflow.py::TestHPOResumeWorkflow::test_resume_workflow_with_different_run_id 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:18 INFO     training.hpo.execution.local.sweep: [EARLY HASH] Computed v2 study_key_hash=42ff77fc79adab6d... for folder creation
2026-01-18 00:31:18 INFO     training.hpo.core.study: [HPO] Starting optimization for distilbert with checkpointing...
2026-01-18 00:31:18 WARNING  infrastructure.naming.display_policy: [naming.yaml] Missing required 'run_names' section (/tmp/pytest-of-codespace/pytest-38/test_resume_workflow_with_diff0/config/naming.yaml)
2026-01-18 00:31:18 INFO     training.hpo.execution.local.sweep: [HPO Setup] k_folds=None, fold_splits_file=None, k_fold config: {'enabled': False}
2026-01-18 00:31:18 WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker: Using LOCAL MLflow tracking (not Azure ML)
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=42ff77fc79adab6d...
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=42ff77fc79adab6d..., study_family_hash=ce5a7f9ff304036d...
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [START_SWEEP_RUN] Yielding RunHandle. run_id=bd92d2ca20c5...
2026-01-18 00:31:18 INFO     training.hpo.execution.local.sweep: [HPO] Parent run bd92d2ca20c5... is now visible in MLflow (status: RUNNING)
2026-01-18 00:31:18 INFO     training.hpo.execution.local.sweep: Setting Phase 2 tags on parent run bd92d2ca20c5... (data_config=present, train_config=present)
2026-01-18 00:31:18 WARNING  training.hpo.execution.local.sweep: Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
2026-01-18 00:31:18 WARNING  training.hpo.execution.local.sweep: Could not set data fingerprint tag: 'Missing tag key: fingerprint.data'
2026-01-18 00:31:18 WARNING  training.hpo.execution.local.sweep: Could not set eval fingerprint tag: 'Missing tag key: fingerprint.eval'
2026-01-18 00:31:18 WARNING  training.hpo.execution.local.sweep: Could not set objective direction tag: 'Missing tag key: objective.direction'
2026-01-18 00:31:18 WARNING  training.hpo.execution.local.sweep: Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
2026-01-18 00:31:18 WARNING  training.hpo.execution.local.sweep: Could not set artifact.available tag: 'Missing tag key: artifact.available'
2026-01-18 00:31:18 INFO     training.hpo.execution.local.sweep:  Set Phase 2 tags on parent run bd92d2ca20c5...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-18 00:31:18 INFO     training.hpo.execution.local.sweep:  Successfully completed Phase 2 tag setting for parent run bd92d2ca20c5...
2026-01-18 00:31:18 INFO     training.hpo.tracking.cleanup: [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=bd92d2ca20c5...
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Found 0 completed trials out of 0 total
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Logging metrics: n_trials=0, n_completed_trials=0
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=<MagicMock name='mock.create_study().best_value' id='140077206668144'>
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Logging best hyperparameters: <MagicMock name='mock.create_study().best_params' id='140077206783792'>
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Finding and logging best trial run ID...
2026-01-18 00:31:18 WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker: Could not find MLflow run ID for best trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>. Search failed: Invalid clause(s) in filter string: 'tags.trial_number = '<MagicMock name='mock.create_study()', '.', 'best_trial.number', '' id='', '140077206848368', ''>''. This may be a timing issue - trial runs may not be created/committed yet.
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Completed best trial ID logging
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Checkpoint logging enabled: False (upload_checkpoint=False)
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Completed log_final_metrics successfully
2026-01-18 00:31:18 INFO     training.hpo.execution.local.sweep: [REFIT] Refit training is disabled in config
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [START_SWEEP_RUN] Context manager exiting normally. run_id=bd92d2ca20c5...
2026-01-18 00:31:18 INFO     training.hpo.execution.local.sweep: [EARLY HASH] Computed v2 study_key_hash=42ff77fc79adab6d... for folder creation
2026-01-18 00:31:18 INFO     training.hpo.core.study: [HPO] Starting optimization for distilbert with checkpointing...
2026-01-18 00:31:18 INFO     training.hpo.execution.local.sweep: [HPO Setup] k_folds=None, fold_splits_file=None, k_fold config: {'enabled': False}
2026-01-18 00:31:18 WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker: Using LOCAL MLflow tracking (not Azure ML)
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=42ff77fc79adab6d...
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=42ff77fc79adab6d..., study_family_hash=ce5a7f9ff304036d...
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [START_SWEEP_RUN] Yielding RunHandle. run_id=d471c7696a36...
2026-01-18 00:31:18 INFO     training.hpo.execution.local.sweep: [HPO] Parent run d471c7696a36... is now visible in MLflow (status: RUNNING)
2026-01-18 00:31:18 INFO     training.hpo.execution.local.sweep: Setting Phase 2 tags on parent run d471c7696a36... (data_config=present, train_config=present)
2026-01-18 00:31:18 WARNING  training.hpo.execution.local.sweep: Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
2026-01-18 00:31:18 WARNING  training.hpo.execution.local.sweep: Could not set data fingerprint tag: 'Missing tag key: fingerprint.data'
2026-01-18 00:31:18 WARNING  training.hpo.execution.local.sweep: Could not set eval fingerprint tag: 'Missing tag key: fingerprint.eval'
2026-01-18 00:31:18 WARNING  training.hpo.execution.local.sweep: Could not set objective direction tag: 'Missing tag key: objective.direction'
2026-01-18 00:31:18 WARNING  training.hpo.execution.local.sweep: Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
2026-01-18 00:31:18 WARNING  training.hpo.execution.local.sweep: Could not set artifact.available tag: 'Missing tag key: artifact.available'
2026-01-18 00:31:18 INFO     training.hpo.execution.local.sweep:  Set Phase 2 tags on parent run d471c7696a36...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-18 00:31:18 INFO     training.hpo.execution.local.sweep:  Successfully completed Phase 2 tag setting for parent run d471c7696a36...
2026-01-18 00:31:18 INFO     training.hpo.tracking.cleanup: [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=d471c7696a36...
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Found 0 completed trials out of 0 total
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Logging metrics: n_trials=0, n_completed_trials=0
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=<MagicMock name='mock.create_study().best_value' id='140077206668144'>
2026-01-18 00:31:18 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Logging best hyperparameters: <MagicMock name='mock.create_study().best_params' id='140077206783792'>
2026-01-18 00:31:19 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Finding and logging best trial run ID...
2026-01-18 00:31:19 WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker: Could not find MLflow run ID for best trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>. Search failed: Invalid clause(s) in filter string: 'tags.trial_number = '<MagicMock name='mock.create_study()', '.', 'best_trial.number', '' id='', '140077206848368', ''>''. This may be a timing issue - trial runs may not be created/committed yet.
2026-01-18 00:31:19 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Completed best trial ID logging
2026-01-18 00:31:19 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Checkpoint logging enabled: False (upload_checkpoint=False)
2026-01-18 00:31:19 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
2026-01-18 00:31:19 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Completed log_final_metrics successfully
2026-01-18 00:31:19 INFO     training.hpo.execution.local.sweep: [REFIT] Refit training is disabled in config
2026-01-18 00:31:19 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [START_SWEEP_RUN] Context manager exiting normally. run_id=d471c7696a36...
FAILED
tests/hpo/integration/test_hpo_resume_workflow.py::TestHPOResumeWorkflow::test_resume_workflow_with_cv 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:19 INFO     training.hpo.execution.local.sweep: [EARLY HASH] Computed v2 study_key_hash=dc0c421a5056294a... for folder creation
2026-01-18 00:31:19 INFO     training.hpo.core.study: [HPO] Starting optimization for distilbert with checkpointing...
2026-01-18 00:31:19 WARNING  infrastructure.naming.display_policy: [naming.yaml] Missing required 'run_names' section (/tmp/pytest-of-codespace/pytest-38/test_resume_workflow_with_cv0/config/naming.yaml)
2026-01-18 00:31:19 INFO     training.hpo.execution.local.sweep: [HPO Setup] k_folds=2, fold_splits_file=None, k_fold config: {'enabled': True, 'n_splits': 2, 'random_seed': 42, 'shuffle': True, 'stratified': True}
2026-01-18 00:31:19 INFO     training.hpo.execution.local.sweep: [CV Setup]  Created 2 fold splits and saved to /tmp/pytest-of-codespace/pytest-38/test_resume_workflow_with_cv0/outputs/hpo/local/distilbert/study-dc0c421a/fold_splits.json
2026-01-18 00:31:19 WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker: Using LOCAL MLflow tracking (not Azure ML)
2026-01-18 00:31:19 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
2026-01-18 00:31:19 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=dc0c421a5056294a...
2026-01-18 00:31:19 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=dc0c421a5056294a..., study_family_hash=d1aa8bfffdcfdb77...
2026-01-18 00:31:19 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [START_SWEEP_RUN] Yielding RunHandle. run_id=e9b822128b94...
2026-01-18 00:31:19 INFO     training.hpo.execution.local.sweep: [HPO] Parent run e9b822128b94... is now visible in MLflow (status: RUNNING)
2026-01-18 00:31:19 INFO     training.hpo.execution.local.sweep: Setting Phase 2 tags on parent run e9b822128b94... (data_config=present, train_config=present)
2026-01-18 00:31:19 WARNING  training.hpo.execution.local.sweep: Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
2026-01-18 00:31:19 WARNING  training.hpo.execution.local.sweep: Could not set data fingerprint tag: 'Missing tag key: fingerprint.data'
2026-01-18 00:31:19 WARNING  training.hpo.execution.local.sweep: Could not set eval fingerprint tag: 'Missing tag key: fingerprint.eval'
2026-01-18 00:31:19 WARNING  training.hpo.execution.local.sweep: Could not set objective direction tag: 'Missing tag key: objective.direction'
2026-01-18 00:31:19 WARNING  training.hpo.execution.local.sweep: Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
2026-01-18 00:31:19 WARNING  training.hpo.execution.local.sweep: Could not set artifact.available tag: 'Missing tag key: artifact.available'
2026-01-18 00:31:19 INFO     training.hpo.execution.local.sweep:  Set Phase 2 tags on parent run e9b822128b94...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-18 00:31:19 INFO     training.hpo.execution.local.sweep:  Successfully completed Phase 2 tag setting for parent run e9b822128b94...
2026-01-18 00:31:19 INFO     training.hpo.tracking.cleanup: [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.
2026-01-18 00:31:19 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=e9b822128b94...
2026-01-18 00:31:19 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Found 0 completed trials out of 0 total
2026-01-18 00:31:19 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Logging metrics: n_trials=0, n_completed_trials=0
2026-01-18 00:31:19 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=<MagicMock name='mock.create_study().best_value' id='140077206668144'>
2026-01-18 00:31:19 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Logging best hyperparameters: <MagicMock name='mock.create_study().best_params' id='140077206783792'>
2026-01-18 00:31:19 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Finding and logging best trial run ID...
2026-01-18 00:31:19 WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker: Could not find MLflow run ID for best trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>. Search failed: Invalid clause(s) in filter string: 'tags.trial_number = '<MagicMock name='mock.create_study()', '.', 'best_trial.number', '' id='', '140077206848368', ''>''. This may be a timing issue - trial runs may not be created/committed yet.
2026-01-18 00:31:19 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Completed best trial ID logging
2026-01-18 00:31:19 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Checkpoint logging enabled: False (upload_checkpoint=False)
2026-01-18 00:31:19 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
2026-01-18 00:31:19 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Completed log_final_metrics successfully
2026-01-18 00:31:19 INFO     training.hpo.execution.local.sweep: [REFIT] Refit training is disabled in config
2026-01-18 00:31:19 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [START_SWEEP_RUN] Context manager exiting normally. run_id=e9b822128b94...
2026-01-18 00:31:19 INFO     training.hpo.execution.local.sweep: [EARLY HASH] Computed v2 study_key_hash=dc0c421a5056294a... for folder creation
2026-01-18 00:31:19 INFO     training.hpo.core.study: [HPO] Starting optimization for distilbert with checkpointing...
2026-01-18 00:31:19 INFO     training.hpo.execution.local.sweep: [HPO Setup] k_folds=2, fold_splits_file=None, k_fold config: {'enabled': True, 'n_splits': 2, 'random_seed': 42, 'shuffle': True, 'stratified': True}
2026-01-18 00:31:19 INFO     training.hpo.execution.local.sweep: [CV Setup]  Created 2 fold splits and saved to /tmp/pytest-of-codespace/pytest-38/test_resume_workflow_with_cv0/outputs/hpo/local/distilbert/study-dc0c421a/fold_splits.json
2026-01-18 00:31:19 WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker: Using LOCAL MLflow tracking (not Azure ML)
2026-01-18 00:31:19 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
2026-01-18 00:31:19 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=dc0c421a5056294a...
2026-01-18 00:31:19 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=dc0c421a5056294a..., study_family_hash=d1aa8bfffdcfdb77...
2026-01-18 00:31:19 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [START_SWEEP_RUN] Yielding RunHandle. run_id=6a02791b8354...
2026-01-18 00:31:19 INFO     training.hpo.execution.local.sweep: [HPO] Parent run 6a02791b8354... is now visible in MLflow (status: RUNNING)
2026-01-18 00:31:19 INFO     training.hpo.execution.local.sweep: Setting Phase 2 tags on parent run 6a02791b8354... (data_config=present, train_config=present)
2026-01-18 00:31:19 WARNING  training.hpo.execution.local.sweep: Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
2026-01-18 00:31:19 WARNING  training.hpo.execution.local.sweep: Could not set data fingerprint tag: 'Missing tag key: fingerprint.data'
2026-01-18 00:31:19 WARNING  training.hpo.execution.local.sweep: Could not set eval fingerprint tag: 'Missing tag key: fingerprint.eval'
2026-01-18 00:31:19 WARNING  training.hpo.execution.local.sweep: Could not set objective direction tag: 'Missing tag key: objective.direction'
2026-01-18 00:31:19 WARNING  training.hpo.execution.local.sweep: Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
2026-01-18 00:31:19 WARNING  training.hpo.execution.local.sweep: Could not set artifact.available tag: 'Missing tag key: artifact.available'
2026-01-18 00:31:19 INFO     training.hpo.execution.local.sweep:  Set Phase 2 tags on parent run 6a02791b8354...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-18 00:31:19 INFO     training.hpo.execution.local.sweep:  Successfully completed Phase 2 tag setting for parent run 6a02791b8354...
2026-01-18 00:31:19 INFO     training.hpo.tracking.cleanup: [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.
2026-01-18 00:31:19 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=6a02791b8354...
2026-01-18 00:31:19 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Found 0 completed trials out of 0 total
2026-01-18 00:31:19 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Logging metrics: n_trials=0, n_completed_trials=0
2026-01-18 00:31:19 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=<MagicMock name='mock.create_study().best_value' id='140077206668144'>
2026-01-18 00:31:19 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Logging best hyperparameters: <MagicMock name='mock.create_study().best_params' id='140077206783792'>
2026-01-18 00:31:19 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Finding and logging best trial run ID...
2026-01-18 00:31:19 WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker: Could not find MLflow run ID for best trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>. Search failed: Invalid clause(s) in filter string: 'tags.trial_number = '<MagicMock name='mock.create_study()', '.', 'best_trial.number', '' id='', '140077206848368', ''>''. This may be a timing issue - trial runs may not be created/committed yet.
2026-01-18 00:31:19 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Completed best trial ID logging
2026-01-18 00:31:19 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Checkpoint logging enabled: False (upload_checkpoint=False)
2026-01-18 00:31:19 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
2026-01-18 00:31:19 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_FINAL_METRICS] Completed log_final_metrics successfully
2026-01-18 00:31:19 INFO     training.hpo.execution.local.sweep: [REFIT] Refit training is disabled in config
2026-01-18 00:31:19 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [START_SWEEP_RUN] Context manager exiting normally. run_id=6a02791b8354...
FAILED
tests/hpo/integration/test_hpo_run_mode_variants.py::TestHPORunModeForceNew::test_force_new_creates_variant_1_on_first_run PASSED
tests/hpo/integration/test_hpo_run_mode_variants.py::TestHPORunModeForceNew::test_force_new_creates_variant_2_on_second_run PASSED
tests/hpo/integration/test_hpo_run_mode_variants.py::TestHPORunModeForceNew::test_force_new_creates_variant_3_on_third_run PASSED
tests/hpo/integration/test_hpo_run_mode_variants.py::TestHPORunModeForceNew::test_force_new_with_custom_study_name PASSED
tests/hpo/integration/test_hpo_run_mode_variants.py::TestHPORunModeReuseIfExists::test_reuse_if_exists_uses_base_name PASSED
tests/hpo/integration/test_hpo_run_mode_variants.py::TestHPORunModeReuseIfExists::test_reuse_if_exists_even_with_existing_variants PASSED
tests/hpo/integration/test_hpo_run_mode_variants.py::TestStudyManagerWithRunMode::test_study_manager_extracts_run_mode PASSED
tests/hpo/integration/test_hpo_run_mode_variants.py::TestStudyManagerWithRunMode::test_study_manager_passes_run_mode_to_create_study_name 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:19 INFO     training.hpo.core.study: [HPO] Starting optimization for distilbert with checkpointing...
PASSED
tests/hpo/integration/test_hpo_run_mode_variants.py::TestSmokeYamlRunModeConfig::test_smoke_yaml_run_mode_default PASSED
tests/hpo/integration/test_hpo_run_mode_variants.py::TestSmokeYamlRunModeConfig::test_smoke_yaml_run_mode_force_new PASSED
tests/hpo/integration/test_hpo_run_mode_variants.py::TestSmokeYamlRunModeConfig::test_smoke_yaml_study_name_null_auto_generate PASSED
tests/hpo/integration/test_hpo_run_mode_variants.py::TestSmokeYamlRunModeConfig::test_smoke_yaml_variant_sequence PASSED
tests/hpo/integration/test_hpo_studies_dict_storage.py::TestHPOStudiesDictStorage::test_notebook_loop_stores_all_backbones PASSED
tests/hpo/integration/test_hpo_studies_dict_storage.py::TestHPOStudiesDictStorage::test_validate_hpo_studies_dict_helper PASSED
tests/hpo/integration/test_hpo_sweep_setup.py::TestHPOSweepSetup::test_load_configs_from_smoke_yaml PASSED
tests/hpo/integration/test_hpo_sweep_setup.py::TestHPOSweepSetup::test_setup_hpo_mlflow_run_creates_parent_run 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:19 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 00:31:19 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 00:31:19 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:b17bb0a908286809619872feca54b44e3a407d64215bc..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-18 00:31:19 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Loaded 22 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-18 00:31:19 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Found 6 allocations for counter_key (after deduplication): committed=[], reserved=[1, 2, 3, 4, 5, 6], expired=[], max_committed_version=0
2026-01-18 00:31:19 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Allocation details: [(1, 'reserved', 'pending_2026'), (2, 'reserved', 'pending_2026'), (3, 'reserved', 'pending_2026'), (4, 'reserved', 'pending_2026'), (5, 'reserved', 'pending_2026'), (6, 'reserved', 'pending_2026')]
2026-01-18 00:31:19 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Reserving next version: 7 (incremented from max_committed=0, skipped 6 reserved/expired versions)
2026-01-18 00:31:19 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version]  Successfully reserved version 7 for counter_key resume-ner:hpo:b17bb0a908286809619872feca54b44e3a4... (run_id: pending_2026...)
PASSED
tests/hpo/integration/test_hpo_sweep_setup.py::TestHPOSweepSetup::test_setup_hpo_mlflow_run_computes_study_key_hash 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:19 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 00:31:19 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 00:31:19 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:3d74e4db609045bbecfa4cc681e1ebee1a827c35aa7b8..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-18 00:31:19 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Loaded 23 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-18 00:31:19 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Found 12 allocations for counter_key (after deduplication): committed=[], reserved=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], expired=[], max_committed_version=0
2026-01-18 00:31:19 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Allocation details: [(1, 'reserved', 'pending_2026'), (2, 'reserved', 'pending_2026'), (3, 'reserved', 'pending_2026'), (4, 'reserved', 'pending_2026'), (5, 'reserved', 'pending_2026'), (6, 'reserved', 'pending_2026'), (7, 'reserved', 'pending_2026'), (8, 'reserved', 'pending_2026'), (9, 'reserved', 'pending_2026'), (10, 'reserved', 'pending_2026'), (11, 'reserved', 'pending_2026'), (12, 'reserved', 'pending_2026')]
2026-01-18 00:31:19 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Reserving next version: 13 (incremented from max_committed=0, skipped 12 reserved/expired versions)
2026-01-18 00:31:19 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version]  Successfully reserved version 13 for counter_key resume-ner:hpo:3d74e4db609045bbecfa4cc681e1ebee1a8... (run_id: pending_2026...)
PASSED
tests/hpo/integration/test_hpo_sweep_setup.py::TestHPOSweepSetup::test_create_study_name_from_checkpoint_config PASSED
tests/hpo/integration/test_hpo_sweep_setup.py::TestHPOSweepSetup::test_create_study_name_without_checkpoint PASSED
tests/hpo/integration/test_hpo_sweep_setup.py::TestHPOSweepSetup::test_setup_hpo_mlflow_run_tags 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:19 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 00:31:19 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 00:31:19 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:3d74e4db609045bbecfa4cc681e1ebee1a827c35aa7b8..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-18 00:31:19 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Loaded 24 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-18 00:31:19 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Found 13 allocations for counter_key (after deduplication): committed=[], reserved=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], expired=[], max_committed_version=0
2026-01-18 00:31:19 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Allocation details: [(1, 'reserved', 'pending_2026'), (2, 'reserved', 'pending_2026'), (3, 'reserved', 'pending_2026'), (4, 'reserved', 'pending_2026'), (5, 'reserved', 'pending_2026'), (6, 'reserved', 'pending_2026'), (7, 'reserved', 'pending_2026'), (8, 'reserved', 'pending_2026'), (9, 'reserved', 'pending_2026'), (10, 'reserved', 'pending_2026'), (11, 'reserved', 'pending_2026'), (12, 'reserved', 'pending_2026'), (13, 'reserved', 'pending_2026')]
2026-01-18 00:31:19 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Reserving next version: 14 (incremented from max_committed=0, skipped 13 reserved/expired versions)
2026-01-18 00:31:19 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version]  Successfully reserved version 14 for counter_key resume-ner:hpo:3d74e4db609045bbecfa4cc681e1ebee1a8... (run_id: pending_2026...)
PASSED
tests/hpo/integration/test_hpo_sweep_setup.py::TestHPOSweepSetup::test_checkpoint_file_created PASSED
tests/hpo/integration/test_hpo_sweep_setup.py::TestHPOSweepSetup::test_checkpoint_file_created_v2_hash_based PASSED
tests/hpo/integration/test_hpo_sweep_setup.py::TestHPOSweepSetup::test_checkpoint_path_fallback_to_legacy_when_no_hash PASSED
tests/hpo/integration/test_hpo_sweep_setup.py::TestHPOSweepSetup::test_study_key_hash_and_family_hash_computed PASSED
tests/hpo/integration/test_hpo_sweep_setup.py::TestHPOSweepSetup::test_setup_hpo_mlflow_run_trusts_provided_config_dir 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:19 WARNING  infrastructure.naming.display_policy: [naming.yaml] Missing required 'run_names' section (/tmp/pytest-of-codespace/pytest-38/test_setup_hpo_mlflow_run_trus0/project1/config/naming.yaml)
PASSED
tests/hpo/integration/test_hpo_sweep_setup.py::TestHPOSweepSetup::test_setup_hpo_mlflow_run_infers_config_dir_when_none 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:19 WARNING  infrastructure.naming.display_policy: [naming.yaml] Missing required 'run_names' section (/tmp/pytest-of-codespace/pytest-38/test_setup_hpo_mlflow_run_infe0/project/config/naming.yaml)
PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunHierarchy::test_hpo_parent_run_has_correct_tags PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunHierarchy::test_trial_run_is_child_of_hpo_parent PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunHierarchy::test_fold_run_is_child_of_trial_run PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunHierarchy::test_refit_run_is_child_of_hpo_parent PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunTags::test_hpo_parent_run_tags PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunTags::test_trial_run_tags PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunTags::test_refit_run_tags PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunTags::test_trial_run_inherits_study_key_hash_from_parent PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunMetrics::test_trial_run_logs_metrics PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunMetrics::test_trial_run_logs_hyperparameters PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunMetrics::test_cv_trial_run_logs_aggregated_metrics PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunMetrics::test_refit_run_logs_metrics PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunStructure::test_mlflow_structure_no_cv PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunStructure::test_mlflow_structure_with_cv PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunStructure::test_mlflow_runs_have_grouping_tags PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunStatus::test_trial_run_status_transitions PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunStatus::test_refit_run_status_finished_after_upload PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureV2::test_study_folder_naming PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureV2::test_trial_folder_naming PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureV2::test_refit_folder_structure PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureV2::test_cv_fold_folder_structure PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureV2::test_checkpoint_location_in_trial PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureV2::test_checkpoint_location_in_refit PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureV2::test_checkpoint_location_in_cv_fold PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureValidation::test_is_v2_path_detects_trial_folder PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureValidation::test_find_study_folder PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureValidation::test_find_trial_folder PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureWithBuildOutputPath::test_build_output_path_hpo_sweep 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:19 WARNING  infrastructure.paths.resolve: Pattern 'hpo_v2' not found in paths.yaml. Using fallback logic.
PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureWithBuildOutputPath::test_build_output_path_hpo_trial 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:19 WARNING  infrastructure.paths.resolve: Pattern 'hpo_v2' not found in paths.yaml. Using fallback logic.
PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureWithBuildOutputPath::test_build_output_path_hpo_refit 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:19 WARNING  infrastructure.paths.resolve: Pattern 'hpo_v2' not found in paths.yaml. Using fallback logic.
PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureFiles::test_study_db_location PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureFiles::test_trial_meta_json_location PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureFiles::test_fold_splits_json_location PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureFiles::test_metrics_json_location_in_trial PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureFiles::test_metrics_json_location_in_refit PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureSmokeYaml::test_path_structure_matches_smoke_yaml PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureSmokeYaml::test_path_structure_study8_trial8_format PASSED
tests/hpo/integration/test_refit_training.py::TestRefitTrainingSetup::test_refit_uses_best_trial_hyperparameters 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'batch_size': 4, 'dropout': 0.2, 'weight_decay': 0.05, 'backbone': 'distilbert'}
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 00:31:19 WARNING  infrastructure.paths.resolve: Pattern 'hpo_v2' not found in paths.yaml. Using fallback logic.
2026-01-18 00:31:19 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-38/test_refit_uses_best_trial_hyp0/config/naming.yaml, using empty policy
2026-01-18 00:31:19 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_refit_uses_best_trial_hyp0/config/tags.yaml, using defaults
2026-01-18 00:31:19 INFO     training.execution.mlflow_setup:  View run local_distilbert_hpo_refit_legacy at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/302183572435427150/runs/5d90c3981d884fd891ff636bb2765783
2026-01-18 00:31:19 INFO     training.execution.mlflow_setup: Created MLflow run: local_distilbert_hpo_refit_legacy (5d90c3981d88...)
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Attempting to link refit run 5d90c3981d88... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-18 00:31:19 ERROR    training.hpo.execution.local.refit: [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 548, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
2026-01-18 00:31:19 WARNING  training.hpo.execution.local.refit: [REFIT] Metrics file not found at /tmp/pytest-of-codespace/pytest-38/test_refit_uses_best_trial_hyp0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/metrics.json
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Attempting to link refit run 5d90c3981d88... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-18 00:31:19 ERROR    training.hpo.execution.local.refit: [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 548, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Refit training completed. Metrics: N/A, Checkpoint: /tmp/pytest-of-codespace/pytest-38/test_refit_uses_best_trial_hyp0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
PASSED
tests/hpo/integration/test_refit_training.py::TestRefitTrainingSetup::test_refit_creates_mlflow_run 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 00:31:19 WARNING  infrastructure.paths.resolve: Pattern 'hpo_v2' not found in paths.yaml. Using fallback logic.
2026-01-18 00:31:19 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-38/test_refit_creates_mlflow_run0/config/naming.yaml, using empty policy
2026-01-18 00:31:19 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_refit_creates_mlflow_run0/config/tags.yaml, using defaults
2026-01-18 00:31:19 INFO     training.execution.mlflow_setup:  View run local_distilbert_hpo_refit_legacy at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/302183572435427150/runs/9211e91bf4c64224bf0ce6f2d76f82b0
2026-01-18 00:31:19 INFO     training.execution.mlflow_setup: Created MLflow run: local_distilbert_hpo_refit_legacy (9211e91bf4c6...)
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Attempting to link refit run 9211e91bf4c6... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-18 00:31:19 ERROR    training.hpo.execution.local.refit: [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 548, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Attempting to link refit run 9211e91bf4c6... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-18 00:31:19 ERROR    training.hpo.execution.local.refit: [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 548, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Refit training completed. Metrics: 0.8, Checkpoint: /tmp/pytest-of-codespace/pytest-38/test_refit_creates_mlflow_run0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
PASSED
tests/hpo/integration/test_refit_training.py::TestRefitTrainingSetup::test_refit_creates_v2_output_directory 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 00:31:19 WARNING  infrastructure.paths.resolve: Pattern 'hpo_v2' not found in paths.yaml. Using fallback logic.
2026-01-18 00:31:19 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-38/test_refit_creates_v2_output_d0/config/naming.yaml, using empty policy
2026-01-18 00:31:19 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_refit_creates_v2_output_d0/config/tags.yaml, using defaults
2026-01-18 00:31:19 INFO     training.execution.mlflow_setup:  View run local_distilbert_hpo_refit_legacy at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/302183572435427150/runs/a633f45d5e2c4cafae8e1e6ec74bbebc
2026-01-18 00:31:19 INFO     training.execution.mlflow_setup: Created MLflow run: local_distilbert_hpo_refit_legacy (a633f45d5e2c...)
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Attempting to link refit run a633f45d5e2c... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-18 00:31:19 ERROR    training.hpo.execution.local.refit: [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 548, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
2026-01-18 00:31:19 WARNING  training.hpo.execution.local.refit: [REFIT] Metrics file not found at /tmp/pytest-of-codespace/pytest-38/test_refit_creates_v2_output_d0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/metrics.json
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Attempting to link refit run a633f45d5e2c... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-18 00:31:19 ERROR    training.hpo.execution.local.refit: [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 548, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Refit training completed. Metrics: N/A, Checkpoint: /tmp/pytest-of-codespace/pytest-38/test_refit_creates_v2_output_d0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
PASSED
tests/hpo/integration/test_refit_training.py::TestRefitTrainingExecution::test_refit_reads_metrics_from_file 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 00:31:19 WARNING  infrastructure.paths.resolve: Pattern 'hpo_v2' not found in paths.yaml. Using fallback logic.
2026-01-18 00:31:19 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-38/test_refit_reads_metrics_from_0/config/naming.yaml, using empty policy
2026-01-18 00:31:19 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_refit_reads_metrics_from_0/config/tags.yaml, using defaults
2026-01-18 00:31:19 INFO     training.execution.mlflow_setup:  View run local_distilbert_hpo_refit_legacy at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/302183572435427150/runs/b4a27ef051ae4a4f8f606c1c1be8bcd4
2026-01-18 00:31:19 INFO     training.execution.mlflow_setup: Created MLflow run: local_distilbert_hpo_refit_legacy (b4a27ef051ae...)
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Attempting to link refit run b4a27ef051ae... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-18 00:31:19 ERROR    training.hpo.execution.local.refit: [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 548, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
2026-01-18 00:31:19 WARNING  training.hpo.execution.local.refit: [REFIT] Metrics file not found at /tmp/pytest-of-codespace/pytest-38/test_refit_reads_metrics_from_0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/metrics.json
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Attempting to link refit run b4a27ef051ae... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-18 00:31:19 ERROR    training.hpo.execution.local.refit: [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 548, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Refit training completed. Metrics: N/A, Checkpoint: /tmp/pytest-of-codespace/pytest-38/test_refit_reads_metrics_from_0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
PASSED
tests/hpo/integration/test_refit_training.py::TestRefitTrainingExecution::test_refit_logs_metrics_to_mlflow 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 00:31:19 WARNING  infrastructure.paths.resolve: Pattern 'hpo_v2' not found in paths.yaml. Using fallback logic.
2026-01-18 00:31:19 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-38/test_refit_logs_metrics_to_mlf0/config/naming.yaml, using empty policy
2026-01-18 00:31:19 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_refit_logs_metrics_to_mlf0/config/tags.yaml, using defaults
2026-01-18 00:31:19 INFO     training.execution.mlflow_setup:  View run local_distilbert_hpo_refit_legacy at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/302183572435427150/runs/02b6530be01e4677bfd754e3397394a5
2026-01-18 00:31:19 INFO     training.execution.mlflow_setup: Created MLflow run: local_distilbert_hpo_refit_legacy (02b6530be01e...)
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Attempting to link refit run 02b6530be01e... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-18 00:31:19 ERROR    training.hpo.execution.local.refit: [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 548, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
2026-01-18 00:31:19 WARNING  training.hpo.execution.local.refit: [REFIT] Metrics file not found at /tmp/pytest-of-codespace/pytest-38/test_refit_logs_metrics_to_mlf0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/metrics.json
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Attempting to link refit run 02b6530be01e... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-18 00:31:19 ERROR    training.hpo.execution.local.refit: [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 548, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Refit training completed. Metrics: N/A, Checkpoint: /tmp/pytest-of-codespace/pytest-38/test_refit_logs_metrics_to_mlf0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
PASSED
tests/hpo/integration/test_refit_training.py::TestRefitTrainingExecution::test_refit_creates_checkpoint_directory 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 00:31:19 WARNING  infrastructure.paths.resolve: Pattern 'hpo_v2' not found in paths.yaml. Using fallback logic.
2026-01-18 00:31:19 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-38/test_refit_creates_checkpoint_0/config/naming.yaml, using empty policy
2026-01-18 00:31:19 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_refit_creates_checkpoint_0/config/tags.yaml, using defaults
2026-01-18 00:31:19 INFO     training.execution.mlflow_setup:  View run local_distilbert_hpo_refit_legacy at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/302183572435427150/runs/8c120a24b19d4df4b29c818ba4aa01b3
2026-01-18 00:31:19 INFO     training.execution.mlflow_setup: Created MLflow run: local_distilbert_hpo_refit_legacy (8c120a24b19d...)
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Attempting to link refit run 8c120a24b19d... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-18 00:31:19 ERROR    training.hpo.execution.local.refit: [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 548, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Attempting to link refit run 8c120a24b19d... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-18 00:31:19 ERROR    training.hpo.execution.local.refit: [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 548, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Refit training completed. Metrics: 0.8, Checkpoint: /tmp/pytest-of-codespace/pytest-38/test_refit_creates_checkpoint_0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
PASSED
tests/hpo/integration/test_refit_training.py::TestRefitTrainingSmokeYaml::test_refit_enabled_in_smoke_yaml 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 00:31:19 WARNING  infrastructure.paths.resolve: Pattern 'hpo_v2' not found in paths.yaml. Using fallback logic.
2026-01-18 00:31:19 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-38/test_refit_enabled_in_smoke_ya0/config/naming.yaml, using empty policy
2026-01-18 00:31:19 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_refit_enabled_in_smoke_ya0/config/tags.yaml, using defaults
2026-01-18 00:31:19 INFO     training.execution.mlflow_setup:  View run local_distilbert_hpo_refit_legacy at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/302183572435427150/runs/926b6bd318e246f09d290c89e81b4f51
2026-01-18 00:31:19 INFO     training.execution.mlflow_setup: Created MLflow run: local_distilbert_hpo_refit_legacy (926b6bd318e2...)
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Attempting to link refit run 926b6bd318e2... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-18 00:31:19 ERROR    training.hpo.execution.local.refit: [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 548, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
2026-01-18 00:31:19 WARNING  training.hpo.execution.local.refit: [REFIT] Metrics file not found at /workspaces/resume-ner-azureml/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/metrics.json
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Attempting to link refit run 926b6bd318e2... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-18 00:31:19 ERROR    training.hpo.execution.local.refit: [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 548, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Refit training completed. Metrics: N/A, Checkpoint: /workspaces/resume-ner-azureml/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
PASSED
tests/hpo/integration/test_refit_training.py::TestRefitTrainingSmokeYaml::test_refit_uses_full_epochs 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 00:31:19 WARNING  infrastructure.paths.resolve: Pattern 'hpo_v2' not found in paths.yaml. Using fallback logic.
2026-01-18 00:31:19 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-38/test_refit_uses_full_epochs0/config/naming.yaml, using empty policy
2026-01-18 00:31:19 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_refit_uses_full_epochs0/config/tags.yaml, using defaults
2026-01-18 00:31:19 INFO     training.execution.mlflow_setup:  View run local_distilbert_hpo_refit_legacy at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/302183572435427150/runs/6846f66874d146aa8d8e8bc71aefb2e1
2026-01-18 00:31:19 INFO     training.execution.mlflow_setup: Created MLflow run: local_distilbert_hpo_refit_legacy (6846f66874d1...)
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Attempting to link refit run 6846f66874d1... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-18 00:31:19 ERROR    training.hpo.execution.local.refit: [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 548, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Attempting to link refit run 6846f66874d1... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-18 00:31:19 ERROR    training.hpo.execution.local.refit: [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 548, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Refit training completed. Metrics: 0.8, Checkpoint: /tmp/pytest-of-codespace/pytest-38/test_refit_uses_full_epochs0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
PASSED
tests/hpo/integration/test_refit_training.py::TestRefitCheckpointDuplicationPrevention::test_refit_skips_checkpoint_folder_logging 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 00:31:19 WARNING  infrastructure.paths.resolve: Pattern 'hpo_v2' not found in paths.yaml. Using fallback logic.
2026-01-18 00:31:19 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-38/test_refit_skips_checkpoint_fo0/config/naming.yaml, using empty policy
2026-01-18 00:31:19 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_refit_skips_checkpoint_fo0/config/tags.yaml, using defaults
2026-01-18 00:31:19 INFO     training.execution.mlflow_setup:  View run local_distilbert_hpo_refit_legacy at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/302183572435427150/runs/73a6996ceb8949f78969318b32eb0f09
2026-01-18 00:31:19 INFO     training.execution.mlflow_setup: Created MLflow run: local_distilbert_hpo_refit_legacy (73a6996ceb89...)
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Attempting to link refit run 73a6996ceb89... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-18 00:31:19 ERROR    training.hpo.execution.local.refit: [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 548, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Attempting to link refit run 73a6996ceb89... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-18 00:31:19 ERROR    training.hpo.execution.local.refit: [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 548, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Refit training completed. Metrics: 0.8, Checkpoint: /tmp/pytest-of-codespace/pytest-38/test_refit_skips_checkpoint_fo0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
PASSED
tests/hpo/integration/test_refit_training.py::TestRefitCheckpointDuplicationPrevention::test_refit_prevents_duplication_only_archive_uploaded 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 00:31:19 WARNING  infrastructure.paths.resolve: Pattern 'hpo_v2' not found in paths.yaml. Using fallback logic.
2026-01-18 00:31:19 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-38/test_refit_prevents_duplicatio0/config/naming.yaml, using empty policy
2026-01-18 00:31:19 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_refit_prevents_duplicatio0/config/tags.yaml, using defaults
2026-01-18 00:31:19 INFO     training.execution.mlflow_setup:  View run local_distilbert_hpo_refit_legacy at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/302183572435427150/runs/4d5a913380c145c79779bef846238767
2026-01-18 00:31:19 INFO     training.execution.mlflow_setup: Created MLflow run: local_distilbert_hpo_refit_legacy (4d5a913380c1...)
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Attempting to link refit run 4d5a913380c1... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-18 00:31:19 ERROR    training.hpo.execution.local.refit: [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 548, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Attempting to link refit run 4d5a913380c1... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-18 00:31:19 ERROR    training.hpo.execution.local.refit: [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 548, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-18 00:31:19 INFO     training.hpo.execution.local.refit: [REFIT] Refit training completed. Metrics: 0.8, Checkpoint: /tmp/pytest-of-codespace/pytest-38/test_refit_prevents_duplicatio0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
2026-01-18 00:31:19 WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker: Could not set refit_completed tag: Run 'refit_run_123' not found
2026-01-18 00:31:19 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_BEST_CHECKPOINT] Using preferred checkpoint directory: /tmp/pytest-of-codespace/pytest-38/test_refit_prevents_duplicatio0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
2026-01-18 00:31:19 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: Uploading checkpoint archive to MLflow...
2026-01-18 00:31:19 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: Uploading checkpoint to refit run refit_run_12 (child of parent parent_123)
2026-01-18 00:31:19 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: Checkpoint upload completed successfully for trial 0
2026-01-18 00:31:19 WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker: [LOG_BEST_CHECKPOINT] Could not update artifact.available tag: 'Missing tag key: artifact.available'
2026-01-18 00:31:19 INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker: Marked study as complete with checkpoint uploaded (best trial: 0)
PASSED
tests/hpo/integration/test_smoke_yaml_options.py::TestTimeoutMinutes::test_timeout_minutes_stops_study_after_timeout PASSED
tests/hpo/integration/test_smoke_yaml_options.py::TestTimeoutMinutes::test_timeout_minutes_conversion_to_seconds PASSED
tests/hpo/integration/test_smoke_yaml_options.py::TestSaveOnlyBest::test_save_only_best_deletes_non_best_checkpoints PASSED
tests/hpo/integration/test_smoke_yaml_options.py::TestSaveOnlyBest::test_save_only_best_false_preserves_all_checkpoints PASSED
tests/hpo/integration/test_smoke_yaml_options.py::TestLogBestCheckpoint::test_log_best_checkpoint_config_enabled PASSED
tests/hpo/integration/test_smoke_yaml_options.py::TestLogBestCheckpoint::test_log_best_checkpoint_config_disabled PASSED
tests/hpo/integration/test_smoke_yaml_options.py::TestLogBestCheckpoint::test_log_best_checkpoint_conditional_call PASSED
tests/hpo/integration/test_smoke_yaml_options.py::TestDisableAutoCleanup::test_disable_auto_cleanup_false_enables_cleanup PASSED
tests/hpo/integration/test_smoke_yaml_options.py::TestDisableAutoCleanup::test_disable_auto_cleanup_true_disables_cleanup PASSED
tests/hpo/integration/test_smoke_yaml_options.py::TestDisableAutoCleanup::test_disable_auto_cleanup_default_is_disabled PASSED
tests/hpo/integration/test_smoke_yaml_options.py::TestDisableAutoOptunaMark::test_disable_auto_optuna_mark_false_enables_marking FAILED
tests/hpo/integration/test_smoke_yaml_options.py::TestDisableAutoOptunaMark::test_disable_auto_optuna_mark_true_skips_marking PASSED
tests/hpo/integration/test_trial_execution.py::TestTrialExecutionNoCV::test_trial_execution_no_cv_creates_single_run 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:20 INFO     training.hpo.execution.local.trial: [TRIAL] Training completed. Objective metric 'macro-f1': 0.75
PASSED
tests/hpo/integration/test_trial_execution.py::TestTrialExecutionNoCV::test_trial_execution_no_cv_output_path 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:20 INFO     training.hpo.execution.local.trial: [TRIAL] Training completed. Objective metric 'macro-f1': 0.8
PASSED
tests/hpo/integration/test_trial_execution.py::TestTrialExecutionNoCV::test_trial_execution_no_cv_metrics_file 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:20 INFO     training.hpo.execution.local.trial: [TRIAL] Training completed. Objective metric 'macro-f1': 0.75
PASSED
tests/hpo/integration/test_trial_execution.py::TestTrialExecutionWithCV::test_trial_execution_with_cv_creates_nested_runs 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:20 WARNING  infrastructure.tracking.mlflow.hash_utils: Could not get or compute study_key_hash. study_key_hash=missing, hpo_parent_run_id=provided, configs=missing
2026-01-18 00:31:20 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-38/test_trial_execution_with_cv_c0/config/naming.yaml, using empty policy
2026-01-18 00:31:20 WARNING  infrastructure.tracking.mlflow.hash_utils: Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=missing, hyperparameters=available
2026-01-18 00:31:20 WARNING  training.hpo.execution.local.cv: Could not compute trial_key_hash from configs
2026-01-18 00:31:20 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_trial_execution_with_cv_c0/config/tags.yaml, using defaults
2026-01-18 00:31:20 WARNING  infrastructure.tracking.mlflow.hash_utils: Could not get or compute study_key_hash. study_key_hash=missing, hpo_parent_run_id=provided, configs=missing
2026-01-18 00:31:20 ERROR    training.hpo.execution.local.cv: In v2 study folder study-abc12345 but trial_key_hash is None. study_key_hash=NO. Will attempt to compute hash.
2026-01-18 00:31:20 WARNING  training.hpo.execution.local.cv: In v2 study folder but missing hashes: study_key_hash=NO, trial_key_hash=NO
FAILED
tests/hpo/integration/test_trial_execution.py::TestTrialExecutionWithCV::test_trial_execution_with_cv_creates_fold_runs 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:20 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-38/test_trial_execution_with_cv_c1/config/naming.yaml, using empty policy
2026-01-18 00:31:20 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_trial_execution_with_cv_c1/config/tags.yaml, using defaults
2026-01-18 00:31:20 WARNING  infrastructure.paths.resolve: Pattern 'hpo_v2' not found in paths.yaml. Using fallback logic.
2026-01-18 00:31:20 INFO     training.hpo.execution.local.cv: [CV] Created trial folder: /tmp/pytest-of-codespace/pytest-38/test_trial_execution_with_cv_c1/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-aca2e73e (trial 0)
2026-01-18 00:31:20 WARNING  infrastructure.tracking.mlflow.lifecycle: Could not check run status for trial_run_12...: Run 'trial_run_123' not found
2026-01-18 00:31:20 WARNING  infrastructure.tracking.mlflow.lifecycle: Failed to terminate run trial_run_12...: Run 'trial_run_123' not found
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/lifecycle.py", line 98, in terminate_run_safe
    client.set_terminated(run_id, status=status)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 3506, in set_terminated
    self._tracking_client.set_terminated(run_id, status, end_time)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 797, in set_terminated
    self.store.update_run_info(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 676, in update_run_info
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'trial_run_123' not found
PASSED
tests/hpo/integration/test_trial_execution.py::TestTrialExecutionWithCV::test_trial_execution_with_cv_aggregates_metrics 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:20 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-38/test_trial_execution_with_cv_a0/config/naming.yaml, using empty policy
2026-01-18 00:31:20 WARNING  infrastructure.tracking.mlflow.hash_utils: Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
2026-01-18 00:31:20 WARNING  training.hpo.execution.local.cv: Could not compute trial_key_hash from configs
2026-01-18 00:31:20 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_trial_execution_with_cv_a0/config/tags.yaml, using defaults
2026-01-18 00:31:20 WARNING  infrastructure.tracking.mlflow.hash_utils: Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
2026-01-18 00:31:20 WARNING  training.hpo.execution.local.cv: Could not compute trial_key_hash
2026-01-18 00:31:20 ERROR    training.hpo.execution.local.cv: In v2 study folder study-abc12345 but trial_key_hash is None. study_key_hash=YES. Will attempt to compute hash.
2026-01-18 00:31:20 WARNING  training.hpo.execution.local.cv: In v2 study folder but missing hashes: study_key_hash=YES, trial_key_hash=NO
2026-01-18 00:31:20 WARNING  training.hpo.execution.local.cv: In v2 study folder study-abc12345 but trial_key_hash is None. Attempting to recompute from trial_params...
2026-01-18 00:31:20 WARNING  infrastructure.tracking.mlflow.hash_utils: Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
FAILED
tests/hpo/integration/test_trial_execution.py::TestTrialExecutionWithCV::test_trial_execution_with_cv_output_paths 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:20 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-38/test_trial_execution_with_cv_o0/config/naming.yaml, using empty policy
2026-01-18 00:31:20 WARNING  infrastructure.tracking.mlflow.hash_utils: Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
2026-01-18 00:31:20 WARNING  training.hpo.execution.local.cv: Could not compute trial_key_hash from configs
2026-01-18 00:31:20 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_trial_execution_with_cv_o0/config/tags.yaml, using defaults
2026-01-18 00:31:20 WARNING  infrastructure.tracking.mlflow.hash_utils: Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
2026-01-18 00:31:20 WARNING  training.hpo.execution.local.cv: Could not compute trial_key_hash
2026-01-18 00:31:20 ERROR    training.hpo.execution.local.cv: In v2 study folder study-abc12345 but trial_key_hash is None. study_key_hash=YES. Will attempt to compute hash.
2026-01-18 00:31:20 WARNING  training.hpo.execution.local.cv: In v2 study folder but missing hashes: study_key_hash=YES, trial_key_hash=NO
2026-01-18 00:31:20 WARNING  training.hpo.execution.local.cv: In v2 study folder study-abc12345 but trial_key_hash is None. Attempting to recompute from trial_params...
2026-01-18 00:31:20 WARNING  infrastructure.tracking.mlflow.hash_utils: Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
FAILED
tests/hpo/integration/test_trial_execution.py::TestTrialExecutionWithCV::test_trial_execution_with_cv_smoke_yaml_params 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:20 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-38/test_trial_execution_with_cv_s0/config/naming.yaml, using empty policy
2026-01-18 00:31:20 WARNING  infrastructure.tracking.mlflow.hash_utils: Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
2026-01-18 00:31:20 WARNING  training.hpo.execution.local.cv: Could not compute trial_key_hash from configs
2026-01-18 00:31:20 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_trial_execution_with_cv_s0/config/tags.yaml, using defaults
2026-01-18 00:31:20 WARNING  infrastructure.tracking.mlflow.hash_utils: Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
2026-01-18 00:31:20 WARNING  training.hpo.execution.local.cv: Could not compute trial_key_hash
2026-01-18 00:31:20 ERROR    training.hpo.execution.local.cv: In v2 study folder study-abc12345 but trial_key_hash is None. study_key_hash=YES. Will attempt to compute hash.
2026-01-18 00:31:20 WARNING  training.hpo.execution.local.cv: In v2 study folder but missing hashes: study_key_hash=YES, trial_key_hash=NO
2026-01-18 00:31:20 WARNING  training.hpo.execution.local.cv: In v2 study folder study-abc12345 but trial_key_hash is None. Attempting to recompute from trial_params...
2026-01-18 00:31:20 WARNING  infrastructure.tracking.mlflow.hash_utils: Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
FAILED
tests/hpo/unit/test_hpo_variant_generation.py::TestCreateStudyNameWithVariants::test_create_study_name_force_new_no_existing PASSED
tests/hpo/unit/test_hpo_variant_generation.py::TestCreateStudyNameWithVariants::test_create_study_name_force_new_with_existing PASSED
tests/hpo/unit/test_hpo_variant_generation.py::TestCreateStudyNameWithVariants::test_create_study_name_reuse_if_exists PASSED
tests/hpo/unit/test_hpo_variant_generation.py::TestCreateStudyNameWithVariants::test_create_study_name_custom_study_name_force_new PASSED
tests/hpo/unit/test_hpo_variant_generation.py::TestCreateStudyNameWithVariants::test_create_study_name_custom_study_name_reuse_if_exists PASSED
tests/hpo/unit/test_hpo_variant_generation.py::TestCreateStudyNameWithVariants::test_create_study_name_checkpoint_disabled_force_new PASSED
tests/hpo/unit/test_hpo_variant_generation.py::TestFindStudyVariants::test_find_study_variants_none PASSED
tests/hpo/unit/test_hpo_variant_generation.py::TestFindStudyVariants::test_find_study_variants_implicit_variant_1 PASSED
tests/hpo/unit/test_hpo_variant_generation.py::TestFindStudyVariants::test_find_study_variants_explicit_variants PASSED
tests/hpo/unit/test_hpo_variant_generation.py::TestFindStudyVariants::test_find_study_variants_ignores_other_folders PASSED
tests/hpo/unit/test_hpo_variant_generation.py::TestFindStudyVariants::test_find_study_variants_different_backbone PASSED
tests/hpo/unit/test_search_space.py::TestSearchSpaceTranslation::test_translate_smoke_yaml_search_space PASSED
tests/hpo/unit/test_search_space.py::TestSearchSpaceTranslation::test_backbone_choice_values PASSED
tests/hpo/unit/test_search_space.py::TestSearchSpaceTranslation::test_learning_rate_loguniform_range PASSED
tests/hpo/unit/test_search_space.py::TestSearchSpaceTranslation::test_batch_size_choice_values PASSED
tests/hpo/unit/test_search_space.py::TestSearchSpaceTranslation::test_dropout_uniform_range PASSED
tests/hpo/unit/test_search_space.py::TestSearchSpaceTranslation::test_weight_decay_loguniform_range PASSED
tests/hpo/unit/test_search_space.py::TestSearchSpaceTranslation::test_exclude_params PASSED
tests/hpo/unit/test_search_space.py::TestSearchSpaceTranslation::test_unsupported_search_space_type PASSED
tests/hpo/unit/test_search_space.py::TestSearchSpaceTranslation::test_search_space_translator_class PASSED
tests/hpo/unit/test_trial_selection.py::TestSelectionCriteria::test_selection_faster_model_within_threshold PASSED
tests/hpo/unit/test_trial_selection.py::TestSelectionCriteria::test_selection_slower_model_when_accuracy_better PASSED
tests/hpo/unit/test_trial_selection.py::TestSelectionCriteria::test_selection_min_accuracy_gain_respected PASSED
tests/hpo/unit/test_trial_selection.py::TestSelectionCriteria::test_selection_tie_breaking_deterministic PASSED
tests/hpo/unit/test_trial_selection.py::TestSelectionCriteria::test_selection_relative_threshold_calculation PASSED
tests/hpo/unit/test_trial_selection.py::TestSelectionCriteria::test_selection_smoke_yaml_parameters PASSED
tests/hpo/unit/test_trial_selection.py::TestSelectionCriteria::test_selection_no_candidates_raises_error PASSED
tests/hpo/unit/test_trial_selection.py::TestSelectionCriteria::test_selection_single_candidate PASSED
tests/hpo/unit/test_trial_selection.py::TestSelectionCriteria::test_selection_no_threshold_accuracy_only PASSED
tests/hpo/unit/test_trial_selection.py::TestSelectionCriteria::test_selection_normalize_speed_scores PASSED
tests/hpo/unit/test_trial_selection.py::TestSelectionCriteria::test_selection_apply_threshold_logic PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_force_new_with_exists_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_force_new_without_exists_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_force_new_with_exists_and_complete_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_force_new_with_exists_and_incomplete_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_reuse_if_exists_hpo_with_exists_returns_true PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_reuse_if_exists_hpo_without_exists_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_reuse_if_exists_final_training_with_exists_and_complete_returns_true PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_reuse_if_exists_final_training_with_exists_and_incomplete_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_reuse_if_exists_final_training_without_exists_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_resume_if_incomplete_with_exists_and_incomplete_returns_true PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_resume_if_incomplete_with_exists_and_complete_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_resume_if_incomplete_without_exists_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_resume_if_incomplete_hpo_treats_as_reuse_if_exists PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_default_with_exists_returns_true PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_default_without_exists_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_not_exists_always_returns_false_regardless_of_mode PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestGetLoadIfExistsFlag::test_force_new_with_checkpoint_enabled_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestGetLoadIfExistsFlag::test_force_new_with_checkpoint_disabled_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestGetLoadIfExistsFlag::test_reuse_if_exists_with_checkpoint_enabled_returns_true PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestGetLoadIfExistsFlag::test_reuse_if_exists_with_checkpoint_disabled_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestGetLoadIfExistsFlag::test_default_with_checkpoint_enabled_returns_true PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestGetLoadIfExistsFlag::test_default_with_checkpoint_disabled_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestGetLoadIfExistsFlag::test_resume_if_incomplete_with_checkpoint_enabled_returns_true PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestGetLoadIfExistsFlag::test_resume_if_incomplete_with_checkpoint_disabled_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestGetLoadIfExistsFlag::test_checkpoint_disabled_always_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestProcessTypeIntegration::test_hpo_process_type_ignores_completeness PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestProcessTypeIntegration::test_final_training_process_type_checks_completeness PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestProcessTypeIntegration::test_selection_process_type_ignores_completeness PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestProcessTypeIntegration::test_benchmarking_process_type_ignores_completeness PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetObjectiveDirection::test_direction_key_maximize PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetObjectiveDirection::test_direction_key_minimize PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetObjectiveDirection::test_default_when_missing PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetObjectiveDirection::test_default_when_objective_empty PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetChampionSelectionConfig::test_default_values PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetChampionSelectionConfig::test_custom_values PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetChampionSelectionConfig::test_top_k_clamping_when_greater_than_min_trials 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:21 WARNING  infrastructure.config.selection: top_k_for_stable_score (5) > min_trials_per_group (3). Clamping top_k to 3.
PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetChampionSelectionConfig::test_top_k_equal_to_min_trials PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetChampionSelectionConfig::test_top_k_less_than_min_trials PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetChampionSelectionConfig::test_require_artifact_available_false PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetChampionSelectionConfig::test_artifact_check_source_disk PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetChampionSelectionConfig::test_prefer_schema_version_1_0 PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetChampionSelectionConfig::test_prefer_schema_version_2_0 PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetChampionSelectionConfig::test_allow_mixed_schema_groups_true PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetChampionSelectionConfig::test_complete_config PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestComputeDataFingerprint::test_content_hash_priority PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestComputeDataFingerprint::test_manifest_hash_priority PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestComputeDataFingerprint::test_content_hash_over_manifest_hash PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestComputeDataFingerprint::test_semantic_fallback PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestComputeDataFingerprint::test_semantic_fallback_deterministic PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestComputeDataFingerprint::test_empty_config PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestComputeDataFingerprint::test_minimal_config PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestComputeEvalFingerprint::test_with_eval_config PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestComputeEvalFingerprint::test_deterministic PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestComputeEvalFingerprint::test_empty_config PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestComputeEvalFingerprint::test_with_different_configs PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestBuildHpoStudyKeyV2::test_basic_structure PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestBuildHpoStudyKeyV2::test_includes_fingerprints PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestBuildHpoStudyKeyV2::test_deterministic PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestBuildHpoStudyKeyV2::test_different_models_produce_different_keys PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestBuildHpoStudyKeyV2::test_with_benchmark_config PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestBuildHpoStudyKeyV2::test_without_benchmark_config PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestBuildHpoStudyKeyV2::test_hash_function PASSED
tests/infrastructure/naming/test_semantic_suffix.py::TestSemanticSuffixBehavior::test_auto_generated_study_name_returns_empty_semantic_suffix 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:21 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-38/test_auto_generated_study_name0/config, raw_auto_inc_config={}
2026-01-18 00:31:21 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
PASSED
tests/infrastructure/naming/test_semantic_suffix.py::TestSemanticSuffixBehavior::test_custom_study_name_includes_semantic_suffix 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:21 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-38/test_custom_study_name_include0/config, raw_auto_inc_config={}
2026-01-18 00:31:21 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
PASSED
tests/infrastructure/naming/test_semantic_suffix.py::TestSemanticSuffixBehavior::test_auto_generated_with_variant_returns_empty_semantic_suffix 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:21 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-38/test_auto_generated_with_varia0/config, raw_auto_inc_config={}
2026-01-18 00:31:21 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
PASSED
tests/infrastructure/paths/test_repo_root_detection.py::TestDetectRepoRoot::test_detects_from_config_dir PASSED
tests/infrastructure/paths/test_repo_root_detection.py::TestDetectRepoRoot::test_detects_from_output_dir PASSED
tests/infrastructure/paths/test_repo_root_detection.py::TestDetectRepoRoot::test_detects_from_start_path PASSED
tests/infrastructure/paths/test_repo_root_detection.py::TestDetectRepoRoot::test_detects_from_current_directory PASSED
tests/infrastructure/paths/test_repo_root_detection.py::TestDetectRepoRoot::test_detects_from_parent_directories PASSED
tests/infrastructure/paths/test_repo_root_detection.py::TestDetectRepoRoot::test_raises_value_error_when_not_found 
-------------------------------- live log call ---------------------------------
2026-01-18 00:31:21 WARNING  infrastructure.paths.repo: Could not find repository root. Falling back to current working directory: /tmp/pytest-of-codespace/pytest-38/test_raises_value_error_when_n0/random/structure
PASSED
tests/infrastructure/paths/test_repo_root_detection.py::TestDetectRepoRoot::test_prioritizes_config_dir_over_output_dir PASSED
tests/infrastructure/paths/test_repo_root_detection.py::TestDetectRepoRoot::test_works_with_config_file PASSED
tests/infrastructure/paths/test_repo_root_detection.py::TestValidateRepoRoot::test_validates_with_required_markers PASSED
tests/infrastructure/paths/test_repo_root_detection.py::TestValidateRepoRoot::test_rejects_without_config_dir PASSED
tests/infrastructure/paths/test_repo_root_detection.py::TestValidateRepoRoot::test_rejects_without_src_dir PASSED
tests/infrastructure/paths/test_repo_root_detection.py::TestValidateRepoRoot::test_validates_with_optional_markers PASSED
tests/infrastructure/paths/test_repo_root_detection.py::TestValidateRepoRoot::test_rejects_non_directory PASSED
tests/infrastructure/paths/test_repo_root_detection.py::TestHelperFunctions::test_infer_config_dir_uses_unified_function PASSED
tests/infrastructure/paths/test_repo_root_detection.py::TestHelperFunctions::test_resolve_project_paths_uses_unified_function PASSED
tests/infrastructure/paths/test_repo_root_detection.py::TestConfigLoading::test_loads_repository_root_config PASSED
tests/infrastructure/paths/test_repo_root_detection.py::TestConfigLoading::test_derives_markers_from_base PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestQueryRunsByTags::test_filters_finished_runs PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestQueryRunsByTags::test_filters_by_required_tags PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestQueryRunsByTags::test_handles_missing_tags PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestQueryRunsByTags::test_passes_filter_string PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestQueryRunsByTags::test_passes_max_results PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestQueryRunsByTags::test_empty_result PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestFindBestRunByMetric::test_maximize_metric PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestFindBestRunByMetric::test_minimize_metric PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestFindBestRunByMetric::test_filters_runs_without_metric PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestFindBestRunByMetric::test_returns_none_when_no_runs_have_metric PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestFindBestRunByMetric::test_empty_runs_list PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestFindBestRunByMetric::test_single_run PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestGroupRunsByVariant::test_groups_by_variant_tag PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestGroupRunsByVariant::test_default_variant_for_missing_tag PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestGroupRunsByVariant::test_custom_variant_tag PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestGroupRunsByVariant::test_empty_runs_list PASSED
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerV2HashComputation::test_v2_hash_computation_success PASSED
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerV2HashComputation::test_v2_hash_computation_missing_train_config PASSED
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerV2HashComputation::test_v2_hash_computation_empty_eval_config PASSED
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerBestTrialSearch::test_best_trial_search_by_study_key_hash_success PASSED
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerBestTrialSearch::test_best_trial_search_filters_by_parent_run_id PASSED
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerBestTrialSearch::test_best_trial_search_no_parent_study_key_hash PASSED
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerBestTrialSearch::test_best_trial_search_no_matching_runs PASSED
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerBestTrialSearch::test_best_trial_search_exception_handling PASSED
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerTrialNumberExtraction::test_extract_trial_number_from_tag PASSED
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerTrialNumberExtraction::test_extract_trial_number_from_run_name PASSED
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerTrialNumberExtraction::test_extract_trial_number_returns_none_when_not_found PASSED
tests/integration/api/test_api.py::TestHealthEndpoint::test_health_check PASSED
tests/integration/api/test_api.py::TestHealthEndpoint::test_model_info_not_loaded PASSED
tests/integration/api/test_api.py::TestHealthEndpoint::test_model_info_loaded PASSED
tests/integration/api/test_api.py::TestPredictEndpoint::test_predict_not_loaded PASSED
tests/integration/api/test_api.py::TestPredictEndpoint::test_predict_success PASSED
tests/integration/api/test_api.py::TestPredictEndpoint::test_predict_batch_success PASSED
tests/integration/api/test_api.py::TestPredictEndpoint::test_predict_batch_size_exceeded PASSED
tests/integration/api/test_api.py::TestFileEndpoints::test_predict_file_not_loaded PASSED
tests/integration/api/test_api.py::TestFileEndpoints::test_predict_file_pdf PASSED
tests/integration/api/test_api.py::TestFileEndpoints::test_predict_file_image PASSED
tests/integration/api/test_api_local_server.py::TestServerLifecycle::test_server_startup_with_valid_model SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestServerLifecycle::test_server_startup_with_invalid_model_path PASSED
tests/integration/api/test_api_local_server.py::TestServerLifecycle::test_server_graceful_shutdown SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestHealthEndpoints::test_health_check_model_loaded SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestHealthEndpoints::test_model_info_loaded SKIPPEDraining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestSingleTextPrediction::test_predict_valid_text SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestSingleTextPrediction::test_predict_empty_text SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestSingleTextPrediction::test_predict_unicode_text SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestSingleTextPrediction::test_predict_long_text SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestSingleTextPrediction::test_predict_special_characters SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestSingleTextPrediction::test_predict_whitespace_only SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestSingleTextPrediction::test_predict_non_string_value SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestBatchTextPrediction::test_predict_batch_small SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestBatchTextPrediction::test_predict_batch_medium SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestBatchTextPrediction::test_predict_batch_empty SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestBatchTextPrediction::test_predict_batch_mixed SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestBatchTextPrediction::test_predict_batch_size_exceeded SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestBatchTextPrediction::test_predict_batch_with_empty_text SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestBatchTextPrediction::test_predict_batch_missing_texts_field SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestBatchTextPrediction::test_predict_batch_non_list_value SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestFileUpload::test_predict_file_pdf SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestFileUpload::test_predict_file_png SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestFileUpload::test_predict_file_larger_pdf SKIPPEDraining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestFileUpload::test_predict_file_small_pdf SKIPPEDraining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestFileUpload::test_predict_file_missing_file SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestBatchFileUpload::test_predict_file_batch_small SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestBatchFileUpload::test_predict_file_batch_mixed_types SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestBatchFileUpload::test_predict_file_batch_medium SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestBatchFileUpload::test_predict_file_batch_empty SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestBatchFileUpload::test_predict_file_batch_size_exceeded SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestDebugEndpoint::test_predict_debug SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestErrorHandling::test_invalid_json SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestErrorHandling::test_missing_required_fields SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestErrorHandling::test_invalid_file_type SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestPerformance::test_predict_latency SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestPerformance::test_predict_batch_latency SKIPPEDraining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestStability::test_repeated_predictions_consistency SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestStability::test_repeated_file_processing SKIPPEDraining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestStability::test_comprehensive_multi_file_multi_iteration SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_inference_direct.py::test_direct_inference SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_inference_performance.py::TestInferencePerformanceIntegration::test_real_inference_performance SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_inference_performance.py::TestInferencePerformanceIntegration::test_tokenization_consistency_mock PASSED
tests/integration/api/test_onnx_inference.py::test_onnx_inference_speed SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_tokenization_speed.py::test_tokenization_speed SKIPPEDs/final_training/distilroberta/checkpoint)
tests/orchestration/jobs/hpo/local/test_backup.py::TestBackupHpoStudyToDrive::test_backup_v2_study_folder_found_locally PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestBackupHpoStudyToDrive::test_backup_skips_when_already_in_drive PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestBackupHpoStudyToDrive::test_backup_uses_v2_folder_only PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestBackupHpoStudyToDrive::test_backup_warns_when_study_folder_not_found PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestBackupHpoStudyToDrive::test_backup_checks_file_existence_not_just_path PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestBackupHpoStudyToDrive::test_backup_disabled_skips_all_operations PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestIncrementalBackupCallback::test_create_incremental_backup_callback_file PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestIncrementalBackupCallback::test_create_incremental_backup_callback_directory PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestIncrementalBackupCallback::test_incremental_backup_callback_skips_when_disabled PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestIncrementalBackupCallback::test_incremental_backup_callback_skips_drive_paths PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestIncrementalBackupCallback::test_incremental_backup_callback_skips_nonexistent_path PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestIncrementalBackupCallback::test_incremental_backup_callback_skips_non_complete_trials PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestIncrementalBackupCallback::test_incremental_backup_callback_handles_errors_gracefully PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestIncrementalBackupCallback::test_create_study_db_backup_callback PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestImmediateBackupIfNeeded::test_immediate_backup_succeeds_with_enabled_backup_local_path PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestImmediateBackupIfNeeded::test_immediate_backup_succeeds_with_directory PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestImmediateBackupIfNeeded::test_immediate_backup_skips_when_disabled PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestImmediateBackupIfNeeded::test_immediate_backup_skips_when_path_is_drive PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestImmediateBackupIfNeeded::test_immediate_backup_skips_when_path_missing PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestImmediateBackupIfNeeded::test_immediate_backup_skips_when_backup_to_drive_is_none PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestImmediateBackupIfNeeded::test_immediate_backup_handles_backup_failure_gracefully PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestImmediateBackupIfNeeded::test_immediate_backup_handles_exception_gracefully PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestShouldSkipBackup::test_should_skip_when_backup_disabled PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestShouldSkipBackup::test_should_skip_when_path_is_drive PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestShouldSkipBackup::test_should_skip_when_path_missing PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestShouldSkipBackup::test_should_not_skip_when_all_conditions_met PASSED
tests/orchestration/jobs/hpo/local/test_backup.py::TestShouldSkipBackup::test_should_skip_priority_order PASSED
tests/selection/integration/test_artifact_acquisition_edge_cases.py::TestArtifactAcquisitionEdgeCases::test_empty_priority_list PASSED
tests/selection/integration/test_artifact_acquisition_edge_cases.py::TestArtifactAcquisitionEdgeCases::test_invalid_priority_values PASSED
tests/selection/integration/test_artifact_acquisition_edge_cases.py::TestArtifactAcquisitionEdgeCases::test_missing_local_section PASSED
tests/selection/integration/test_artifact_acquisition_edge_cases.py::TestArtifactAcquisitionEdgeCases::test_missing_drive_section PASSED
tests/selection/integration/test_artifact_acquisition_edge_cases.py::TestArtifactAcquisitionEdgeCases::test_missing_mlflow_section PASSED
tests/selection/integration/test_artifact_acquisition_edge_cases.py::TestArtifactAcquisitionEdgeCases::test_validation_false_allows_invalid_checkpoints PASSED
tests/selection/integration/test_artifact_acquisition_edge_cases.py::TestArtifactAcquisitionEdgeCases::test_validation_true_rejects_invalid_checkpoints PASSED
tests/selection/integration/test_artifact_acquisition_edge_cases.py::TestArtifactAcquisitionEdgeCases::test_priority_order_affects_strategy_selection PASSED
tests/selection/integration/test_artifact_acquisition_edge_cases.py::TestArtifactAcquisitionEdgeCases::test_duplicate_priority_values PASSED
tests/selection/integration/test_artifact_acquisition_edge_cases.py::TestArtifactAcquisitionEdgeCases::test_priority_with_only_one_source PASSED
tests/selection/integration/test_artifact_acquisition_edge_cases.py::TestArtifactAcquisitionEdgeCases::test_missing_study_trial_hashes_skips_local PASSED
tests/selection/integration/test_artifact_acquisition_edge_cases.py::TestArtifactAcquisitionEdgeCases::test_config_with_all_optional_fields PASSED
tests/selection/integration/test_artifact_acquisition_logic.py::TestArtifactAcquisitionConfig::test_priority_order_local_first PASSED
tests/selection/integration/test_artifact_acquisition_logic.py::TestArtifactAcquisitionConfig::test_priority_order_mlflow_first PASSED
tests/selection/integration/test_artifact_acquisition_logic.py::TestArtifactAcquisitionConfig::test_local_validate_controls_validation PASSED
tests/selection/integration/test_artifact_acquisition_logic.py::TestArtifactAcquisitionConfig::test_drive_enabled_controls_drive_strategy 
 Backing up best model checkpoint to Google Drive...
 Successfully backed up checkpoint to Google Drive
  Drive path: <Mock name='mock.backup().dst' id='140077136865664'>
PASSED
tests/selection/integration/test_artifact_acquisition_logic.py::TestArtifactAcquisitionConfig::test_drive_validate_controls_validation 
 Backing up best model checkpoint to Google Drive...
 Successfully backed up checkpoint to Google Drive
  Drive path: <Mock name='mock.backup().dst' id='140077136755728'>

 Backing up best model checkpoint to Google Drive...
 Successfully backed up checkpoint to Google Drive
  Drive path: <Mock name='mock.backup().dst' id='140077136755728'>
PASSED
tests/selection/integration/test_artifact_acquisition_logic.py::TestArtifactAcquisitionConfig::test_mlflow_enabled_controls_mlflow_strategy PASSED
tests/selection/integration/test_artifact_acquisition_logic.py::TestArtifactAcquisitionConfig::test_mlflow_validate_controls_validation PASSED
tests/selection/integration/test_artifact_acquisition_logic.py::TestArtifactAcquisitionConfig::test_priority_order_with_some_sources_disabled PASSED
tests/selection/integration/test_artifact_acquisition_logic.py::TestArtifactAcquisitionConfig::test_all_strategies_fail_gracefully_when_disabled PASSED
tests/selection/integration/test_artifact_acquisition_unified_config.py::TestSearchRootsIntegration::test_search_roots_used_in_local_discovery PASSED
tests/selection/integration/test_artifact_acquisition_unified_config.py::TestSearchRootsIntegration::test_search_roots_default_when_missing PASSED
tests/selection/integration/test_artifact_acquisition_unified_config.py::TestArtifactKindsPriorityIntegration::test_artifact_kinds_priority_overrides_global PASSED
tests/selection/integration/test_artifact_acquisition_unified_config.py::TestArtifactKindsPriorityIntegration::test_artifact_kinds_fallback_to_global_priority PASSED
tests/selection/integration/test_artifact_acquisition_unified_config.py::TestMlflowRequireArtifactTagIntegration::test_require_artifact_tag_config_extracted PASSED
tests/selection/integration/test_artifact_acquisition_unified_config.py::TestConfigOptionCombinations::test_all_config_options_together PASSED
tests/selection/integration/test_artifact_acquisition_unified_config.py::TestConfigOptionCombinations::test_config_with_disabled_sources PASSED
tests/selection/integration/test_artifact_acquisition_workflow.py::TestArtifactAcquisitionWorkflow::test_complete_workflow_with_default_config PASSED
tests/selection/integration/test_artifact_acquisition_workflow.py::TestArtifactAcquisitionWorkflow::test_workflow_with_custom_priority_order PASSED
tests/selection/integration/test_artifact_acquisition_workflow.py::TestArtifactAcquisitionWorkflow::test_workflow_with_validation_disabled PASSED
tests/selection/integration/test_artifact_acquisition_workflow.py::TestArtifactAcquisitionWorkflow::test_workflow_with_all_sources_enabled PASSED
tests/selection/integration/test_artifact_acquisition_workflow.py::TestArtifactAcquisitionWorkflow::test_workflow_with_all_sources_disabled PASSED
tests/selection/integration/test_artifact_acquisition_workflow.py::TestArtifactAcquisitionWorkflow::test_workflow_mlflow_fallback_to_manual PASSED
tests/selection/integration/test_best_model_cache.py::test_cache_dual_file_strategy_creates_all_files PASSED
tests/selection/integration/test_best_model_cache.py::test_cache_load_valid_cache_with_mlflow_validation  Checking for cached best model selection...
  Cache file: /tmp/pytest-of-codespace/pytest-38/test_cache_load_valid_cache_wi0/outputs/cache/best_model_selection/latest_best_model_selection.json
   Cache file found
   Cache key matches
   Schema version valid
   Validating MLflow run: run-123...
   MLflow run exists and is FINISHED

 Cache validation successful - reusing cached best model selection
  Cache timestamp: None
  Run ID: run-123...
  Backbone: distilbert-base-uncased
PASSED
tests/selection/integration/test_best_model_cache.py::test_cache_load_cache_key_mismatch_returns_none  Checking for cached best model selection...
  Cache file: /tmp/pytest-of-codespace/pytest-38/test_cache_load_cache_key_mism0/outputs/cache/best_model_selection/latest_best_model_selection.json
   Cache file found
   Cache key mismatch - config changed since cache was created
    Cached key: old_cach... (from unknown)
    Current key: 54c8d4d3...
    Reason: Selection config, tags config, experiment, or benchmark experiment changed
PASSED
tests/selection/integration/test_best_model_cache.py::test_cache_partial_write_recovery  Checking for cached best model selection...
  Cache file: /tmp/pytest-of-codespace/pytest-38/test_cache_partial_write_recov0/outputs/cache/best_model_selection/latest_best_model_selection.json
   Cache file does not exist (first run or cache was cleared)
PASSED
tests/selection/integration/test_best_model_cache.py::test_cache_missing_metrics_handling  Checking for cached best model selection...
  Cache file: /tmp/pytest-of-codespace/pytest-38/test_cache_missing_metrics_han0/outputs/cache/best_model_selection/latest_best_model_selection.json
   Cache file found
   Cache key matches
   Schema version valid
   Validating MLflow run: run-123...
   MLflow run exists and is FINISHED

 Cache validation successful - reusing cached best model selection
  Cache timestamp: None
  Run ID: run-123...
  Backbone: distilbert-base-uncased
PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestLatencyAggregationIntegration::test_latency_aggregation_latest_strategy PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestLatencyAggregationIntegration::test_latency_aggregation_median_strategy PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestLatencyAggregationIntegration::test_latency_aggregation_mean_strategy PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestLatencyAggregationIntegration::test_latency_aggregation_default_when_missing PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestObjectiveDirectionMigrationIntegration::test_objective_direction_preferred_over_goal PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestObjectiveDirectionMigrationIntegration::test_objective_goal_fallback_when_direction_missing PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestObjectiveDirectionMigrationIntegration::test_objective_direction_default_when_both_missing PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestChampionSelectionConfigIntegration::test_min_trials_per_group_extraction PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestChampionSelectionConfigIntegration::test_top_k_for_stable_score_extraction PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestChampionSelectionConfigIntegration::test_require_artifact_available_extraction PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestChampionSelectionConfigIntegration::test_artifact_check_source_extraction PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestChampionSelectionConfigIntegration::test_prefer_schema_version_extraction PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestChampionSelectionConfigIntegration::test_allow_mixed_schema_groups_extraction PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestScoringConfigIntegration::test_scoring_weights_used_in_composite_score PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestScoringConfigIntegration::test_normalize_weights_controls_normalization PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestBenchmarkRequiredMetricsIntegration::test_required_metrics_filters_benchmark_runs PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestCompleteConfigWorkflow::test_all_config_options_used_together PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestCompleteConfigWorkflow::test_config_with_missing_sections_uses_defaults PASSED
tests/selection/integration/test_mlflow_selection_config.py::TestMLflowSelectionConfigUsage::test_find_best_model_uses_objective_metric PASSED
tests/selection/integration/test_mlflow_selection_config.py::TestMLflowSelectionConfigUsage::test_find_best_model_uses_scoring_weights PASSED
tests/selection/integration/test_mlflow_selection_config.py::TestMLflowSelectionConfigUsage::test_find_best_model_uses_benchmark_required_metrics PASSED
tests/selection/integration/test_mlflow_selection_config.py::TestMLflowSelectionConfigUsage::test_find_best_model_weight_normalization PASSED
tests/selection/integration/test_mlflow_selection_config.py::TestMLflowSelectionConfigUsage::test_find_best_model_composite_score_calculation 
 Best model selected:
   Run ID: refit_run_id_123
   Experiment: test_experiment-hpo-distilbert
   Backbone: distilbert
   F1 Score: 0.7500
   Latency: 5.00 ms
   Composite Score: 0.3000
PASSED
tests/selection/integration/test_mlflow_selection_config.py::TestMLflowSelectionConfigUsage::test_find_best_model_all_config_options_together PASSED
tests/selection/integration/test_mlflow_selection_config.py::TestMLflowSelectionConfigUsage::test_find_best_model_custom_config_values PASSED
tests/selection/integration/test_selection_cache_config.py::TestSelectionCacheConfig::test_compute_selection_cache_key_includes_selection_config PASSED
tests/selection/integration/test_selection_cache_config.py::TestSelectionCacheConfig::test_compute_selection_cache_key_deterministic PASSED
tests/selection/integration/test_selection_cache_config.py::TestSelectionCacheConfig::test_load_cached_best_model_validates_cache_key  Checking for cached best model selection...
  Cache file: /tmp/pytest-of-codespace/pytest-38/test_load_cached_best_model_va0/outputs/cache.json
   Cache file found
   Cache key matches
   Schema version valid
   Validating MLflow run: test_run_id_...
   MLflow run exists and is FINISHED

 Cache validation successful - reusing cached best model selection
  Cache timestamp: 2026-01-08T20:00:00Z
  Run ID: test_run_id_...
  Backbone: distilbert
PASSED
tests/selection/integration/test_selection_cache_config.py::TestSelectionCacheConfig::test_load_cached_best_model_cache_key_mismatch   Mode is 'force_new' - skipping cache
PASSED
tests/selection/integration/test_selection_cache_config.py::TestSelectionCacheConfig::test_save_best_model_cache_includes_selection_config PASSED
tests/selection/integration/test_selection_cache_config.py::TestSelectionCacheConfig::test_run_mode_reuse_if_exists_behavior  Checking for cached best model selection...
  Cache file: /tmp/pytest-of-codespace/pytest-38/test_run_mode_reuse_if_exists_0/outputs/cache.json
   Cache file found
   Cache key matches
   Schema version valid
   Validating MLflow run: test_run_id_...
   MLflow run exists and is FINISHED

 Cache validation successful - reusing cached best model selection
  Cache timestamp: 2026-01-08T20:00:00Z
  Run ID: test_run_id_...
  Backbone: distilbert
PASSED
tests/selection/integration/test_selection_cache_config.py::TestSelectionCacheConfig::test_run_mode_force_new_behavior   Mode is 'force_new' - skipping cache
PASSED
tests/selection/integration/test_selection_edge_cases.py::TestSelectionConfigEdgeCases::test_missing_run_section_uses_defaults PASSED
tests/selection/integration/test_selection_edge_cases.py::TestSelectionConfigEdgeCases::test_missing_objective_section_handled PASSED
tests/selection/integration/test_selection_edge_cases.py::TestSelectionConfigEdgeCases::test_missing_scoring_section_uses_defaults PASSED
tests/selection/integration/test_selection_edge_cases.py::TestSelectionConfigEdgeCases::test_negative_weights_handled PASSED
tests/selection/integration/test_selection_edge_cases.py::TestSelectionConfigEdgeCases::test_zero_weights_handled PASSED
tests/selection/integration/test_selection_edge_cases.py::TestSelectionConfigEdgeCases::test_weight_normalization_zero_sum PASSED
tests/selection/integration/test_selection_edge_cases.py::TestSelectionConfigEdgeCases::test_empty_required_metrics_list PASSED
tests/selection/integration/test_selection_edge_cases.py::TestSelectionConfigEdgeCases::test_missing_benchmark_section_uses_defaults PASSED
tests/selection/integration/test_selection_edge_cases.py::TestSelectionConfigEdgeCases::test_cache_key_with_missing_sections PASSED
tests/selection/integration/test_selection_edge_cases.py::TestSelectionConfigEdgeCases::test_find_best_model_missing_required_metrics PASSED
tests/selection/integration/test_selection_edge_cases.py::TestSelectionConfigEdgeCases::test_find_best_model_empty_required_metrics PASSED
tests/selection/integration/test_selection_edge_cases.py::TestSelectionConfigEdgeCases::test_invalid_run_mode_value PASSED
tests/selection/integration/test_selection_workflow.py::TestSelectionWorkflow::test_workflow_loads_config_and_uses_all_options PASSED
tests/selection/integration/test_selection_workflow.py::TestSelectionWorkflow::test_workflow_custom_config_values PASSED
tests/selection/integration/test_selection_workflow.py::TestSelectionWorkflow::test_workflow_cache_key_computation PASSED
tests/selection/integration/test_selection_workflow.py::TestSelectionWorkflow::test_workflow_cache_loading_with_config  Checking for cached best model selection...
  Cache file: /tmp/pytest-of-codespace/pytest-38/test_workflow_cache_loading_wi0/outputs/cache.json
   Cache file found
   Cache key matches
   Schema version valid
   Validating MLflow run: test_run_id_...
   MLflow run exists and is FINISHED

 Cache validation successful - reusing cached best model selection
  Cache timestamp: 2026-01-08T20:00:00Z
  Run ID: test_run_id_...
  Backbone: distilbert
PASSED
tests/selection/unit/test_artifact_acquisition_config.py::TestArtifactAcquisitionConfigLoading::test_load_yaml_loads_artifact_acquisition_config PASSED
tests/selection/unit/test_artifact_acquisition_config.py::TestArtifactAcquisitionConfigLoading::test_artifact_acquisition_config_structure PASSED
tests/selection/unit/test_artifact_acquisition_config.py::TestArtifactAcquisitionConfigLoading::test_artifact_acquisition_config_default_values PASSED
tests/selection/unit/test_artifact_acquisition_config.py::TestArtifactAcquisitionConfigLoading::test_artifact_acquisition_config_custom_values PASSED
tests/selection/unit/test_artifact_acquisition_config.py::TestArtifactAcquisitionConfigLoading::test_artifact_acquisition_config_missing_sections PASSED
tests/selection/unit/test_artifact_acquisition_config.py::TestArtifactAcquisitionConfigLoading::test_artifact_acquisition_config_types PASSED
tests/selection/unit/test_artifact_acquisition_config.py::TestArtifactAcquisitionConfigLoading::test_load_actual_artifact_acquisition_yaml PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestSearchRootsConfig::test_search_roots_extraction PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestSearchRootsConfig::test_search_roots_custom_order PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestSearchRootsConfig::test_search_roots_default PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestSearchRootsConfig::test_search_roots_empty_list PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestSearchRootsConfig::test_search_roots_passed_to_discovery PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestArtifactKindsConfig::test_artifact_kinds_extraction PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestArtifactKindsConfig::test_artifact_kinds_priority_overrides_global PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestArtifactKindsConfig::test_artifact_kinds_fallback_to_global_priority PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestArtifactKindsConfig::test_artifact_kinds_all_artifact_types PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestArtifactKindsConfig::test_artifact_kinds_missing_priority PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestMlflowRequireArtifactTag::test_require_artifact_tag_extraction PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestMlflowRequireArtifactTag::test_require_artifact_tag_true PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestMlflowRequireArtifactTag::test_require_artifact_tag_default PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestConfigIntegrationSuccessCases::test_success_with_all_config_options PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestConfigIntegrationSuccessCases::test_success_with_custom_artifact_kinds_priority PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestConfigIntegrationSuccessCases::test_success_with_custom_search_roots PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestConfigIntegrationFailureCases::test_failure_missing_search_roots_uses_defaults PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestConfigIntegrationFailureCases::test_failure_missing_artifact_kinds_uses_global_priority PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestConfigIntegrationFailureCases::test_failure_empty_priority_list PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestConfigIntegrationFailureCases::test_failure_all_sources_disabled PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestConfigIntegrationFailureCases::test_failure_invalid_priority_source PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestConfigOptionTypes::test_all_config_option_types PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestConfigDefaults::test_all_defaults_match_config_file PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestConfigCompleteCoverage::test_all_config_sections_present PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestConfigCompleteCoverage::test_all_local_options_present PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestConfigCompleteCoverage::test_all_drive_options_present PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestConfigCompleteCoverage::test_all_mlflow_options_present PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_priority_extraction PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_priority_custom_order PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_priority_default PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_local_validate_extraction PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_local_validate_false PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_local_validate_default PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_local_match_strategy_extraction PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_local_match_strategy_metadata_run_id PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_local_match_strategy_spec_fp PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_local_require_exact_match_extraction PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_local_require_exact_match_false PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_drive_enabled_extraction PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_drive_enabled_false PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_drive_enabled_default PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_drive_validate_extraction PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_drive_validate_false PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_drive_validate_default PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_drive_folder_path_extraction PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_drive_folder_path_custom PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_mlflow_enabled_extraction PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_mlflow_enabled_false PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_mlflow_enabled_default PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_mlflow_validate_extraction PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_mlflow_validate_false PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_mlflow_validate_default PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_mlflow_download_timeout_extraction PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_mlflow_download_timeout_custom PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_mlflow_download_timeout_default PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_all_options_together PASSED
tests/selection/unit/test_best_model_selection_config.py::TestBestModelSelectionConfigLoading::test_load_best_model_selection_config PASSED
tests/selection/unit/test_best_model_selection_config.py::TestBestModelSelectionConfigLoading::test_load_config_with_custom_values PASSED
tests/selection/unit/test_best_model_selection_config.py::TestBestModelSelectionConfigLoading::test_load_config_structure_validation PASSED
tests/selection/unit/test_best_model_selection_config.py::TestBestModelSelectionConfigLoading::test_load_config_matches_actual_file PASSED
tests/selection/unit/test_best_model_selection_config.py::TestBestModelSelectionConfigLoading::test_load_config_with_champion_selection_section PASSED
tests/selection/unit/test_best_model_selection_config.py::TestBestModelSelectionConfigLoading::test_load_config_champion_selection_custom_values PASSED
tests/selection/unit/test_best_model_selection_config.py::TestBestModelSelectionConfigLoading::test_load_config_champion_selection_structure_validation PASSED
tests/selection/unit/test_best_model_selection_config.py::TestBestModelSelectionConfigLoading::test_load_config_champion_selection_with_objective_direction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestRunModeConfig::test_run_mode_extraction_force_new PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestRunModeConfig::test_run_mode_extraction_reuse_if_exists PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestRunModeConfig::test_run_mode_default PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestRunModeConfig::test_run_mode_invalid_value PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestObjectiveConfig::test_objective_metric_extraction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestObjectiveConfig::test_objective_metric_default PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestObjectiveConfig::test_objective_direction_extraction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestObjectiveConfig::test_objective_direction_minimize PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestObjectiveConfig::test_objective_goal_extraction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestObjectiveConfig::test_objective_direction_and_goal_both_present PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestObjectiveConfig::test_objective_direction_preferred_over_goal PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_min_trials_per_group_extraction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_min_trials_per_group_default PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_min_trials_per_group_custom_value PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_top_k_for_stable_score_extraction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_top_k_for_stable_score_default PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_require_artifact_available_extraction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_require_artifact_available_false PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_artifact_check_source_extraction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_artifact_check_source_disk PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_prefer_schema_version_extraction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_prefer_schema_version_2_0 PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_prefer_schema_version_1_0 PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_allow_mixed_schema_groups_extraction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_allow_mixed_schema_groups_true PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_all_champion_selection_options_together PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestScoringConfig::test_f1_weight_extraction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestScoringConfig::test_f1_weight_default PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestScoringConfig::test_latency_weight_extraction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestScoringConfig::test_latency_weight_default PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestScoringConfig::test_normalize_weights_extraction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestScoringConfig::test_normalize_weights_false PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestScoringConfig::test_normalize_weights_default PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestBenchmarkConfig::test_required_metrics_extraction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestBenchmarkConfig::test_required_metrics_multiple PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestBenchmarkConfig::test_required_metrics_default PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestBenchmarkConfig::test_latency_aggregation_extraction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestBenchmarkConfig::test_latency_aggregation_median PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestBenchmarkConfig::test_latency_aggregation_mean PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestBenchmarkConfig::test_latency_aggregation_default PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestBenchmarkConfig::test_latency_aggregation_invalid_value PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigIntegrationSuccessCases::test_all_config_options_together PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigIntegrationSuccessCases::test_config_with_custom_latency_aggregation PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigIntegrationSuccessCases::test_config_with_all_champion_selection_options PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigIntegrationFailureCases::test_failure_missing_run_section PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigIntegrationFailureCases::test_failure_missing_objective_section PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigIntegrationFailureCases::test_failure_missing_champion_selection_section PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigIntegrationFailureCases::test_failure_missing_scoring_section PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigIntegrationFailureCases::test_failure_missing_benchmark_section PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigIntegrationFailureCases::test_failure_empty_required_metrics PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigIntegrationFailureCases::test_failure_zero_weights PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigTypeValidation::test_all_config_option_types PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigDefaults::test_all_defaults_match_config_file PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigCompleteCoverage::test_all_config_sections_present PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigCompleteCoverage::test_all_run_options_present PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigCompleteCoverage::test_all_objective_options_present PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigCompleteCoverage::test_all_champion_selection_options_present PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigCompleteCoverage::test_all_scoring_options_present PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigCompleteCoverage::test_all_benchmark_options_present PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_run_mode_extraction PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_run_mode_default PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_run_mode_reuse_if_exists PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_objective_metric_extraction PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_objective_metric_default PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_objective_goal_extraction PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_objective_goal_minimize PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_objective_goal_default PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_scoring_f1_weight_extraction PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_scoring_f1_weight_default PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_scoring_latency_weight_extraction PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_scoring_latency_weight_default PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_scoring_normalize_weights_extraction_true PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_scoring_normalize_weights_extraction_false PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_scoring_normalize_weights_default PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_benchmark_required_metrics_extraction PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_benchmark_required_metrics_multiple PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_benchmark_required_metrics_default PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_all_options_together PASSED
tests/selection/unit/test_study_summary.py::TestStudySummaryMultipleBackbones::test_print_study_summaries_with_multiple_backbones PASSED
tests/selection/unit/test_study_summary.py::TestStudySummaryMultipleBackbones::test_print_study_summaries_with_missing_from_dict PASSED
tests/selection/unit/test_study_summary.py::TestStudySummaryMultipleBackbones::test_print_study_summaries_skips_already_printed PASSED
tests/selection/unit/test_study_summary.py::TestStudySummaryMultipleBackbones::test_format_study_summary_line_with_cv_stats PASSED
tests/selection/unit/test_study_summary.py::TestStudySummaryMultipleBackbones::test_format_study_summary_line_without_cv_stats PASSED
tests/selection/unit/test_study_summary.py::TestStudySummaryMultipleBackbones::test_extract_cv_statistics PASSED
tests/selection/unit/test_study_summary.py::TestStudySummaryMultipleBackbones::test_extract_cv_statistics_missing PASSED
tests/shared/unit/test_drive_backup.py::TestBackupResult::test_backup_result_str_success PASSED
tests/shared/unit/test_drive_backup.py::TestBackupResult::test_backup_result_str_error PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_init_validation PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_drive_path_for_valid_path PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_drive_path_for_outside_root PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_drive_path_for_outside_outputs PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_drive_path_for_allows_outputs_when_disabled PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_backup_file PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_backup_directory PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_backup_nonexistent_file PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_backup_type_mismatch PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_backup_type_inference PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_backup_dry_run PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_restore_file PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_restore_directory PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_restore_nonexistent_backup PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_restore_overwrites_existing PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_ensure_local_exists PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_ensure_local_restores_if_missing PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_backup_exists_true PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_backup_exists_false PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_as_restore_callback PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_as_backup_callback PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_drive_path_for_rejects_drive_paths PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_backup_rejects_drive_paths PASSED
tests/shared/unit/test_drive_backup.py::TestEnsureLocalOptions::test_default_options PASSED
tests/shared/unit/test_drive_backup.py::TestColabMounting::test_mount_colab_drive_success PASSED
tests/shared/unit/test_drive_backup.py::TestColabMounting::test_mount_colab_drive_import_error PASSED
tests/shared/unit/test_drive_backup.py::TestColabMounting::test_create_colab_store_success Using configured backup location: /tmp/pytest-of-codespace/pytest-38/test_create_colab_store_succes0/drive/MyDrive/resume-ner-checkpoints
PASSED
tests/shared/unit/test_drive_backup.py::TestColabMounting::test_create_colab_store_mount_fails  Warning: Could not mount Drive: Not in Colab
PASSED
tests/shared/unit/test_drive_backup.py::TestColabMounting::test_create_colab_store_default_path Using default backup location: /tmp/pytest-of-codespace/pytest-38/test_create_colab_store_defaul0/drive/MyDrive/project
PASSED
tests/shared/unit/test_platform_detection.py::TestIsDrivePath::test_drive_path_string PASSED
tests/shared/unit/test_platform_detection.py::TestIsDrivePath::test_drive_path_path_object PASSED
tests/shared/unit/test_platform_detection.py::TestIsDrivePath::test_local_path_string PASSED
tests/shared/unit/test_platform_detection.py::TestIsDrivePath::test_local_path_path_object PASSED
tests/shared/unit/test_platform_detection.py::TestIsDrivePath::test_relative_path PASSED
tests/shared/unit/test_platform_detection.py::TestIsDrivePath::test_none_input PASSED
tests/tracking/integration/test_azureml_artifact_upload_integration.py::TestAzureMLArtifactUploadIntegration::test_artifact_upload_to_refit_run_with_monkey_patch SKIPPED
tests/tracking/integration/test_azureml_artifact_upload_integration.py::TestAzureMLArtifactUploadIntegration::test_refit_run_completion_after_upload SKIPPED
tests/tracking/integration/test_azureml_artifact_upload_integration.py::TestMonkeyPatchBehavior::test_patch_handles_tracking_uri_error PASSED
tests/tracking/integration/test_naming_integration.py::test_end_to_end_final_training PASSED
tests/tracking/integration/test_naming_integration.py::test_end_to_end_conversion PASSED
tests/tracking/integration/test_naming_integration.py::test_cross_platform_same_spec_fp PASSED
tests/tracking/integration/test_tracking_config_enabled.py::TestBenchmarkTrackingEnabled::test_benchmark_tracking_enabled_creates_run FAILED
tests/tracking/integration/test_tracking_config_enabled.py::TestBenchmarkTrackingEnabled::test_benchmark_tracking_disabled_skips_run PASSED
tests/tracking/integration/test_tracking_config_enabled.py::TestTrainingTrackingEnabled::test_training_tracking_enabled_creates_run FAILED
tests/tracking/integration/test_tracking_config_enabled.py::TestTrainingTrackingEnabled::test_training_tracking_disabled_skips_run PASSED
tests/tracking/integration/test_tracking_config_enabled.py::TestConversionTrackingEnabled::test_conversion_tracking_enabled_creates_run FAILED
tests/tracking/integration/test_tracking_config_enabled.py::TestConversionTrackingEnabled::test_conversion_tracking_disabled_skips_run PASSED
tests/tracking/integration/test_tracking_config_enabled.py::TestLogArtifactsOptions::test_benchmark_log_artifacts_disabled_skips_logging PASSED
tests/tracking/integration/test_tracking_config_enabled.py::TestLogArtifactsOptions::test_training_log_checkpoint_disabled_skips_logging PASSED
tests/tracking/integration/test_tracking_config_enabled.py::TestLogArtifactsOptions::test_training_log_metrics_json_disabled_skips_logging PASSED
tests/tracking/integration/test_tracking_config_enabled.py::TestLogArtifactsOptions::test_conversion_log_onnx_model_disabled_skips_logging PASSED
tests/tracking/integration/test_tracking_config_enabled.py::TestLogArtifactsOptions::test_conversion_log_conversion_log_disabled_skips_logging PASSED
tests/tracking/scripts/test_artifact_upload_manual.py::test_monkey_patch_registration Testing monkey-patch registration...
 Azure ML builder registered: <function azureml_artifacts_builder at 0x7f665054d360>
 Builder is patched (has __wrapped__ attribute)
PASSED
tests/tracking/scripts/test_artifact_upload_manual.py::test_artifact_upload_to_child_run 
Testing artifact upload to child run...
Tracking URI: sqlite:////workspaces/resume-ner-azureml/mlruns/mlflow.db
 No active MLflow run, skipping test
PASSED
tests/tracking/scripts/test_artifact_upload_manual.py::test_refit_run_completion 
Testing refit run completion logic...
 No active MLflow run, skipping test
PASSED
tests/tracking/unit/test_azureml_artifact_upload.py::TestAzureMLArtifactBuilderPatch::test_patch_registered_on_import PASSED
tests/tracking/unit/test_azureml_artifact_upload.py::TestAzureMLArtifactBuilderPatch::test_patch_handles_tracking_uri_parameter PASSED
tests/tracking/unit/test_azureml_artifact_upload.py::TestAzureMLArtifactBuilderPatch::test_patch_auto_applies_on_module_import PASSED
tests/tracking/unit/test_azureml_artifact_upload.py::TestArtifactUploadToChildRun::test_upload_to_refit_run_when_available PASSED
tests/tracking/unit/test_azureml_artifact_upload.py::TestArtifactUploadToChildRun::test_upload_to_parent_run_when_refit_not_available PASSED
tests/tracking/unit/test_azureml_artifact_upload.py::TestRefitRunFinishedStatus::test_refit_run_marked_finished_after_successful_upload PASSED
tests/tracking/unit/test_azureml_artifact_upload.py::TestRefitRunFinishedStatus::test_refit_run_marked_failed_after_upload_failure PASSED
tests/tracking/unit/test_azureml_artifact_upload.py::TestRefitRunFinishedStatus::test_refit_run_not_terminated_if_already_finished PASSED
tests/tracking/unit/test_azureml_artifact_upload.py::TestAzureMLCompatibility::test_azureml_mlflow_imported PASSED
tests/tracking/unit/test_azureml_artifact_upload.py::TestAzureMLCompatibility::test_artifact_repository_registry_has_azureml PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestAzureMLConfiguration::test_azure_ml_enabled PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestAzureMLConfiguration::test_azure_ml_disabled PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestExperimentNaming::test_build_mlflow_experiment_name_format PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestExperimentNaming::test_build_mlflow_experiment_name_all_stages PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestExperimentNaming::test_build_mlflow_experiment_name_special_characters PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestStageSpecificTrackingConfiguration::test_tracking_benchmark_enabled PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestStageSpecificTrackingConfiguration::test_tracking_training_config PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestStageSpecificTrackingConfiguration::test_tracking_conversion_config PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestNamingConfigurationDetails::test_naming_project_name PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestNamingConfigurationDetails::test_naming_project_name_default PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestNamingConfigurationDetails::test_naming_tags_config PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestNamingConfigurationDetails::test_naming_run_name_config PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestIndexCacheConfiguration::test_index_enabled PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestIndexCacheConfiguration::test_index_disabled PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestRunFinderConfiguration::test_run_finder_strict_mode_default_true PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestRunFinderConfiguration::test_run_finder_strict_mode_default_false PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestRunFinderConfiguration::test_run_finder_default PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestArtifactUploadUtilities::test_log_artifact_safe_with_active_run PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestArtifactUploadUtilities::test_log_artifact_safe_with_explicit_run_id PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestArtifactUploadUtilities::test_log_artifact_safe_handles_errors PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestArtifactUploadUtilities::test_log_artifacts_safe_with_directory PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestArtifactUploadUtilities::test_upload_checkpoint_archive PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestRunLifecycleUtilities::test_terminate_run_safe_with_running_run PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestRunLifecycleUtilities::test_terminate_run_safe_with_already_terminated PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestRunLifecycleUtilities::test_terminate_run_safe_with_tags PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestRunLifecycleUtilities::test_terminate_run_with_tags PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestRunLifecycleUtilities::test_ensure_run_terminated PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestRunLifecycleUtilities::test_ensure_run_terminated_already_finished PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestRunCreationUtilities::test_get_or_create_experiment_existing PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestRunCreationUtilities::test_get_or_create_experiment_new PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestRunCreationUtilities::test_resolve_experiment_id_from_parent PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestRunCreationUtilities::test_resolve_experiment_id_from_name PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestURLUtilities::test_get_mlflow_run_url_azureml PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestURLUtilities::test_get_mlflow_run_url_standard PASSED
tests/tracking/unit/test_mlflow_utils_config_inference.py::TestInferConfigDir::test_finds_config_in_parent_chain PASSED
tests/tracking/unit/test_mlflow_utils_config_inference.py::TestInferConfigDir::test_finds_config_at_root_level PASSED
tests/tracking/unit/test_mlflow_utils_config_inference.py::TestInferConfigDir::test_falls_back_to_cwd_when_not_found PASSED
tests/tracking/unit/test_mlflow_utils_config_inference.py::TestInferConfigDir::test_handles_none_path PASSED
tests/tracking/unit/test_mlflow_utils_config_inference.py::TestInferConfigDir::test_finds_first_config_in_parent_chain PASSED
tests/tracking/unit/test_mlflow_utils_config_inference.py::TestInferConfigDir::test_not_in_outputs_subdirectory PASSED
tests/tracking/unit/test_naming_centralized.py::test_naming_context_validation PASSED
tests/tracking/unit/test_naming_centralized.py::test_naming_context_final_training_requires_fingerprints PASSED
tests/tracking/unit/test_naming_centralized.py::test_naming_context_conversion_requires_parent_and_conv_fp PASSED
tests/tracking/unit/test_naming_centralized.py::test_create_naming_context_auto_detect PASSED
tests/tracking/unit/test_naming_centralized.py::test_build_output_path_hpo PASSED
tests/tracking/unit/test_naming_centralized.py::test_build_output_path_benchmarking PASSED
tests/tracking/unit/test_naming_centralized.py::test_build_output_path_final_training PASSED
tests/tracking/unit/test_naming_centralized.py::test_build_output_path_final_training_variant PASSED
tests/tracking/unit/test_naming_centralized.py::test_build_output_path_conversion PASSED
tests/tracking/unit/test_naming_centralized.py::test_build_output_path_best_configurations PASSED
tests/tracking/unit/test_naming_centralized.py::test_build_parent_training_id PASSED
tests/tracking/unit/test_naming_centralized.py::TestRunNameGeneration::test_hpo_trial_run_name PASSED
tests/tracking/unit/test_naming_centralized.py::TestRunNameGeneration::test_hpo_trial_fold_run_name PASSED
tests/tracking/unit/test_naming_centralized.py::TestRunNameGeneration::test_hpo_refit_run_name PASSED
tests/tracking/unit/test_naming_centralized.py::TestRunNameGeneration::test_hpo_sweep_run_name PASSED
tests/tracking/unit/test_naming_centralized.py::TestRunNameGeneration::test_run_name_max_length PASSED
tests/tracking/unit/test_naming_centralized.py::TestRunNameGeneration::test_run_name_forbidden_chars_removed PASSED
tests/tracking/unit/test_naming_placeholder_truncation.py::TestNamingPlaceholderTruncation::test_final_training_naming_with_unknown_placeholders PASSED
tests/tracking/unit/test_naming_placeholder_truncation.py::TestNamingPlaceholderTruncation::test_final_training_naming_pattern_with_unknown_placeholders PASSED
tests/tracking/unit/test_naming_placeholder_truncation.py::TestNamingPlaceholderTruncation::test_placeholder_truncation_to_8_chars PASSED
tests/tracking/unit/test_naming_placeholder_truncation.py::TestNamingPlaceholderTruncation::test_old_placeholder_behavior_would_truncate PASSED
tests/tracking/unit/test_naming_placeholder_truncation.py::TestNamingPlaceholderTruncation::test_naming_context_accepts_unknown_placeholders PASSED
tests/tracking/unit/test_naming_placeholder_truncation.py::TestNamingPlaceholderTruncation::test_token_values_with_unknown_placeholders PASSED
tests/tracking/unit/test_naming_policy.py::TestStudyKeyHashComputation::test_build_hpo_study_key PASSED
tests/tracking/unit/test_naming_policy.py::TestStudyKeyHashComputation::test_build_hpo_study_key_hash_length PASSED
tests/tracking/unit/test_naming_policy.py::TestStudyKeyHashComputation::test_build_hpo_study_key_hash_deterministic PASSED
tests/tracking/unit/test_naming_policy.py::TestStudyKeyHashComputation::test_build_hpo_study_key_hash_different_inputs PASSED
tests/tracking/unit/test_naming_policy.py::TestStudyKeyHashComputation::test_build_hpo_study_key_includes_all_config_components PASSED
tests/tracking/unit/test_naming_policy.py::TestStudyKeyHashComputation::test_build_hpo_study_key_without_benchmark PASSED
tests/tracking/unit/test_naming_policy.py::TestStudyKeyHashComputation::test_build_hpo_study_family_key PASSED
tests/tracking/unit/test_naming_policy.py::TestStudyKeyHashComputation::test_build_hpo_study_family_hash PASSED
tests/tracking/unit/test_naming_policy.py::TestTrialKeyHashComputation::test_build_hpo_trial_key PASSED
tests/tracking/unit/test_naming_policy.py::TestTrialKeyHashComputation::test_build_hpo_trial_key_hash_length PASSED
tests/tracking/unit/test_naming_policy.py::TestTrialKeyHashComputation::test_build_hpo_trial_key_hash_includes_study_hash PASSED
tests/tracking/unit/test_naming_policy.py::TestTrialKeyHashComputation::test_build_hpo_trial_key_same_params_same_hash PASSED
tests/tracking/unit/test_naming_policy.py::TestTrialKeyHashComputation::test_build_hpo_trial_key_different_params_different_hash PASSED
tests/tracking/unit/test_naming_policy.py::TestTrialKeyHashComputation::test_build_hpo_trial_key_normalizes_hyperparameters PASSED
tests/tracking/unit/test_naming_policy.py::TestTrialKeyHashComputation::test_build_hpo_trial_key_normalizes_strings PASSED
tests/tracking/unit/test_naming_policy.py::TestTrialKeyHashComputation::test_build_hpo_trial_key_with_smoke_yaml_params PASSED
tests/tracking/unit/test_naming_policy_details.py::TestComponentConfiguration::test_zero_pad_trial_number PASSED
tests/tracking/unit/test_naming_policy_details.py::TestComponentConfiguration::test_component_default_values PASSED
tests/tracking/unit/test_naming_policy_details.py::TestComponentConfiguration::test_component_length_truncation PASSED
tests/tracking/unit/test_naming_policy_details.py::TestSemanticSuffix::test_semantic_suffix_enabled PASSED
tests/tracking/unit/test_naming_policy_details.py::TestSemanticSuffix::test_semantic_suffix_max_length PASSED
tests/tracking/unit/test_naming_policy_details.py::TestSemanticSuffix::test_semantic_suffix_sanitization PASSED
tests/tracking/unit/test_naming_policy_details.py::TestVersionFormat::test_version_format_parsing PASSED
tests/tracking/unit/test_naming_policy_details.py::TestSeparatorPolicy::test_separator_field PASSED
tests/tracking/unit/test_naming_policy_details.py::TestNormalizationRules::test_normalization_env_replace PASSED
tests/tracking/unit/test_naming_policy_details.py::TestNormalizationRules::test_normalization_model_replace PASSED
tests/tracking/unit/test_naming_policy_details.py::TestValidationRules::test_validate_max_length PASSED
tests/tracking/unit/test_naming_policy_details.py::TestValidationRules::test_validate_forbidden_chars PASSED
tests/tracking/unit/test_naming_policy_details.py::TestValidationRules::test_validate_warn_length PASSED
tests/tracking/unit/test_sweep_tracker_config_inference.py::TestSweepTrackerConfigInference::test_log_best_trial_id_infers_config_from_output_dir PASSED
tests/tracking/unit/test_sweep_tracker_config_inference.py::TestSweepTrackerConfigInference::test_log_best_trial_id_finds_config_in_parent_chain PASSED
tests/tracking/unit/test_sweep_tracker_config_inference.py::TestSweepTrackerConfigInference::test_log_best_trial_id_falls_back_to_cwd_config_when_not_found PASSED
tests/tracking/unit/test_sweep_tracker_config_inference.py::TestSweepTrackerConfigInference::test_log_final_metrics_infers_config_correctly PASSED
tests/tracking/unit/test_sweep_tracker_config_inference.py::TestSweepTrackerConfigInference::test_log_final_metrics_uses_hpo_output_dir_when_output_dir_none PASSED
tests/tracking/unit/test_sweep_tracker_config_inference.py::TestSweepTrackerConfigInference::test_config_dir_not_in_outputs_directory PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagsRegistry::test_load_tags_registry_from_file PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagsRegistry::test_load_tags_registry_fallback_to_defaults PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagsRegistry::test_load_tags_registry_merges_with_defaults PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagsRegistry::test_load_tags_registry_module_level_caching PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagsRegistry::test_load_tags_registry_schema_version_defaults_to_0 PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagsRegistry::test_tags_registry_key_access_all_sections PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagsRegistry::test_tags_registry_raises_tagkeyerror_for_missing_keys PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagsRegistry::test_tags_registry_handles_invalid_section_types PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagsRegistry::test_tags_registry_handles_invalid_key_value_types PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagBuilding::test_build_mlflow_tags_minimal_tags PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagBuilding::test_build_mlflow_tags_hpo_process PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagBuilding::test_build_mlflow_tags_hpo_refit_process PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagBuilding::test_build_mlflow_tags_benchmarking_process PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagBuilding::test_build_mlflow_tags_final_training_process PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagBuilding::test_build_mlflow_tags_conversion_process PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagBuilding::test_build_mlflow_tags_optional_tags PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagSanitization::test_sanitize_tag_value_truncates_exceeding_max_length PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagSanitization::test_sanitize_tag_value_adds_indicator_when_truncated PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagSanitization::test_sanitize_tag_value_preserves_values_within_limit PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagSanitization::test_sanitize_tag_value_handles_empty_strings PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagSanitization::test_sanitize_tag_value_uses_max_length_from_config PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagKeyResolution::test_get_tag_key_loads_from_registry PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagKeyResolution::test_get_tag_key_falls_back_to_fallback PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagKeyResolution::test_get_tag_key_raises_when_missing_and_no_fallback PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagKeyResolution::test_get_tag_key_handles_registry_loading_failures PASSED
tests/tracking/unit/test_tags_registry.py::test_tags_registry_key_access PASSED
tests/tracking/unit/test_tags_registry.py::test_tags_registry_missing_key PASSED
tests/tracking/unit/test_tags_registry.py::test_tags_registry_validation_required_keys PASSED
tests/tracking/unit/test_tags_registry.py::test_load_tags_registry_from_file PASSED
tests/tracking/unit/test_tags_registry.py::test_load_tags_registry_missing_file PASSED
tests/tracking/unit/test_tags_registry.py::test_load_tags_registry_caching PASSED
tests/tracking/unit/test_tags_registry.py::test_load_tags_registry_merge_with_defaults PASSED
tests/tracking/unit/test_tags_registry.py::test_load_tags_registry_default_schema_version PASSED
tests/tracking/unit/test_tags_registry.py::test_tags_registry_invalid_section_type PASSED
tests/tracking/unit/test_tags_registry.py::test_tags_registry_invalid_key_type PASSED
tests/training/hpo/integration/test_hash_consistency.py::TestHashConsistency::test_v2_hash_computation_always_succeeds PASSED
tests/training/hpo/integration/test_hash_consistency.py::TestHashConsistency::test_v2_hash_with_empty_configs PASSED
tests/training/hpo/integration/test_hash_consistency.py::TestHashConsistency::test_trial_key_hash_consistency PASSED
tests/training/hpo/integration/test_hash_consistency.py::TestHashConsistency::test_hash_mismatch_detection PASSED
tests/training/hpo/integration/test_hash_consistency.py::TestRunTagConsistency::test_parent_run_has_v2_tags PASSED
tests/training/hpo/integration/test_hash_consistency.py::TestRefitLinking::test_refit_uses_parent_study_key_hash PASSED
tests/training/hpo/integration/test_hash_consistency.py::TestRefitLinking::test_refit_linking_tag_format PASSED
tests/training/hpo/integration/test_hash_consistency.py::TestParentTrialHashMismatch::test_parent_v2_trial_v1_mismatch_detection PASSED
tests/training/hpo/integration/test_hash_consistency.py::TestParentTrialHashMismatch::test_same_configs_produce_same_v2_hash PASSED
tests/training/hpo/unit/test_cv_hash_computation.py::TestCVTrialRunHashComputation::test_priority_1_use_provided_hashes PASSED
tests/training/hpo/unit/test_cv_hash_computation.py::TestCVTrialRunHashComputation::test_priority_2_get_from_parent_tags FAILED
tests/training/hpo/unit/test_cv_hash_computation.py::TestCVTrialRunHashComputation::test_priority_3_compute_v2_from_configs PASSED
tests/training/hpo/unit/test_cv_hash_computation.py::TestCVTrialRunHashComputation::test_hash_consistency_with_parent PASSED
tests/training/hpo/unit/test_cv_hash_computation.py::TestCVTrialRunHashComputation::test_missing_train_config_still_computes_hash PASSED
tests/training/hpo/unit/test_cv_hash_computation.py::TestCVTrialRunHashComputation::test_error_handling_parent_run_not_found PASSED
tests/training/hpo/unit/test_cv_hash_computation.py::TestCVTrialRunHashComputation::test_no_hpo_parent_run_id_returns_none PASSED
tests/training/hpo/unit/test_phase2_tags.py::TestSetPhase2HpoTags::test_sets_all_tags_successfully PASSED
tests/training/hpo/unit/test_phase2_tags.py::TestSetPhase2HpoTags::test_handles_missing_parent_run_id PASSED
tests/training/hpo/unit/test_phase2_tags.py::TestSetPhase2HpoTags::test_handles_none_parent_run_id PASSED
tests/training/hpo/unit/test_phase2_tags.py::TestSetPhase2HpoTags::test_handles_missing_data_config PASSED
tests/training/hpo/unit/test_phase2_tags.py::TestSetPhase2HpoTags::test_minimize_objective_direction PASSED
tests/training/hpo/unit/test_phase2_tags.py::TestSetPhase2HpoTags::test_legacy_goal_key_migration PASSED
tests/training/hpo/unit/test_phase2_tags.py::TestSetPhase2HpoTags::test_handles_exception_gracefully PASSED
tests/training/hpo/unit/test_phase2_tags.py::TestSetPhase2HpoTags::test_schema_version_1_0_fallback PASSED
tests/training/hpo/utils/test_helpers.py::TestSetupCheckpointStorage::test_uses_v2_folder_when_study_key_hash_provided PASSED
tests/training/hpo/utils/test_helpers.py::TestSetupCheckpointStorage::test_falls_back_to_resolve_storage_path_when_v2_not_found PASSED
tests/training/hpo/utils/test_helpers.py::TestSetupCheckpointStorage::test_restore_from_drive_works_with_v2_path PASSED
tests/training/hpo/utils/test_helpers.py::TestSetupCheckpointStorage::test_restore_skips_when_path_is_in_drive PASSED
tests/training/hpo/utils/test_helpers.py::TestSetupCheckpointStorage::test_should_resume_when_file_exists PASSED
tests/training/hpo/utils/test_helpers.py::TestSetupCheckpointStorage::test_should_not_resume_when_file_missing PASSED
tests/unit/api/test_extractors.py::TestFileTypeDetection::test_detect_pdf PASSED
tests/unit/api/test_extractors.py::TestFileTypeDetection::test_detect_png PASSED
tests/unit/api/test_extractors.py::TestFileTypeDetection::test_detect_jpeg PASSED
tests/unit/api/test_extractors.py::TestFileTypeDetection::test_invalid_file_type PASSED
tests/unit/api/test_extractors.py::TestPDFExtraction::test_extract_pdf_pymupdf PASSED
tests/unit/api/test_extractors.py::TestPDFExtraction::test_extract_pdf_pdfplumber PASSED
tests/unit/api/test_extractors.py::TestPDFExtraction::test_extract_pdf_invalid_extractor PASSED
tests/unit/api/test_extractors.py::TestImageExtraction::test_extract_image_easyocr PASSED
tests/unit/api/test_extractors.py::TestImageExtraction::test_extract_image_pytesseract PASSED
tests/unit/api/test_extractors.py::TestImageExtraction::test_extract_image_invalid_extractor PASSED
tests/unit/api/test_extractors.py::TestFileValidation::test_validate_file_success PASSED
tests/unit/api/test_extractors.py::TestFileValidation::test_validate_file_size_exceeded PASSED
tests/unit/api/test_inference.py::TestONNXInferenceEngine::test_load_model_success PASSED
tests/unit/api/test_inference.py::TestONNXInferenceEngine::test_load_model_file_not_found PASSED
tests/unit/api/test_inference.py::TestONNXInferenceEngine::test_predict_tokens PASSED
tests/unit/api/test_inference.py::TestONNXInferenceEngine::test_model_not_loaded_error PASSED
tests/unit/api/test_inference_fixes.py::TestInferenceFixes::test_tokenization_returns_numpy_arrays PASSED
tests/unit/api/test_inference_fixes.py::TestInferenceFixes::test_offset_mapping_extraction PASSED
tests/unit/api/test_inference_fixes.py::TestInferenceFixes::test_entity_extraction_with_offsets PASSED
tests/unit/api/test_inference_fixes.py::TestInferenceFixes::test_no_hanging_on_special_tokens PASSED
tests/unit/api/test_inference_performance.py::TestInferencePerformance::test_tokenization_does_not_hang FAILED
tests/unit/api/test_inference_performance.py::TestInferencePerformance::test_entity_extraction_with_offset_mapping PASSED
tests/unit/api/test_inference_performance.py::TestInferencePerformance::test_empty_text_handling PASSED
tests/unit/api/test_inference_performance.py::TestInferencePerformance::test_special_characters_handling PASSED
tests/unit/api/test_inference_performance.py::TestInferencePerformance::test_tokenization_consistency PASSED
tests/unit/shared/test_mlflow_setup.py::TestGetLocalTrackingUri::test_local_platform PASSED
tests/unit/shared/test_mlflow_setup.py::TestGetLocalTrackingUri::test_colab_with_drive PASSED
tests/unit/shared/test_mlflow_setup.py::TestGetLocalTrackingUri::test_colab_without_drive PASSED
tests/unit/shared/test_mlflow_setup.py::TestGetLocalTrackingUri::test_kaggle_platform PASSED
tests/unit/shared/test_mlflow_setup.py::TestGetAzureMlTrackingUri::test_success PASSED
tests/unit/shared/test_mlflow_setup.py::TestGetAzureMlTrackingUri::test_import_error PASSED
tests/unit/shared/test_mlflow_setup.py::TestGetAzureMlTrackingUri::test_workspace_access_error PASSED
tests/unit/shared/test_mlflow_setup.py::TestSetupMlflowCrossPlatform::test_local_fallback_no_ml_client PASSED
tests/unit/shared/test_mlflow_setup.py::TestSetupMlflowCrossPlatform::test_azure_ml_success PASSED
tests/unit/shared/test_mlflow_setup.py::TestSetupMlflowCrossPlatform::test_azure_ml_failure_with_fallback PASSED
tests/unit/shared/test_mlflow_setup.py::TestSetupMlflowCrossPlatform::test_azure_ml_failure_no_fallback PASSED
tests/unit/shared/test_mlflow_setup.py::TestSetupMlflowCrossPlatform::test_mlflow_not_installed PASSED
tests/unit/shared/test_mlflow_setup.py::TestPlatformSpecificBehavior::test_colab_drive_mounted PASSED
tests/unit/shared/test_mlflow_setup.py::TestPlatformSpecificBehavior::test_kaggle_platform PASSED
tests/unit/shared/test_mlflow_setup.py::TestPlatformSpecificBehavior::test_local_platform PASSED
tests/unit/shared/test_mlflow_setup.py::TestCreateMlClientFromConfig::test_success_with_env_vars PASSED
tests/unit/shared/test_mlflow_setup.py::TestCreateMlClientFromConfig::test_azure_ml_disabled PASSED
tests/unit/shared/test_mlflow_setup.py::TestCreateMlClientFromConfig::test_config_missing_azure_ml_section PASSED
tests/unit/shared/test_mlflow_setup.py::TestCreateMlClientFromConfig::test_missing_credentials PASSED
tests/unit/shared/test_mlflow_setup.py::TestCreateMlClientFromConfig::test_import_error PASSED
tests/unit/shared/test_mlflow_setup.py::TestSetupMlflowFromConfig::test_config_file_exists_azure_enabled PASSED
tests/unit/shared/test_mlflow_setup.py::TestSetupMlflowFromConfig::test_config_file_exists_azure_disabled PASSED
tests/unit/shared/test_mlflow_setup.py::TestSetupMlflowFromConfig::test_config_file_missing PASSED
tests/unit/shared/test_mlflow_setup.py::TestAzureMlNamespaceCollision::test_import_with_local_azureml_shadowing SKIPPEDe
'ensure_data_asset_uploaded' from 'azureml' (/opt/conda/envs/resume-ner-
training/lib/python3.10/site-packages/azureml/__init__.py))
tests/unit/shared/test_mlflow_setup.py::TestAzureMlNamespaceCollision::test_check_azureml_mlflow_available_with_shadowing SKIPPEDe
'ensure_data_asset_uploaded' from 'azureml' (/opt/conda/envs/resume-ner-
training/lib/python3.10/site-packages/azureml/__init__.py))
tests/unit/shared/test_mlflow_setup.py::TestAzureMlNamespaceCollision::test_local_azureml_functions_still_work_after_import SKIPPEDe
'ensure_data_asset_uploaded' from 'azureml' (/opt/conda/envs/resume-ner-
training/lib/python3.10/site-packages/azureml/__init__.py))
tests/unit/training/test_checkpoint_loader.py::TestValidateCheckpoint::test_validate_checkpoint_valid PASSED
tests/unit/training/test_checkpoint_loader.py::TestValidateCheckpoint::test_validate_checkpoint_with_safetensors PASSED
tests/unit/training/test_checkpoint_loader.py::TestValidateCheckpoint::test_validate_checkpoint_missing_config PASSED
tests/unit/training/test_checkpoint_loader.py::TestValidateCheckpoint::test_validate_checkpoint_missing_model PASSED
tests/unit/training/test_checkpoint_loader.py::TestValidateCheckpoint::test_validate_checkpoint_nonexistent PASSED
tests/unit/training/test_checkpoint_loader.py::TestResolveCheckpointPath::test_resolve_from_env_var PASSED
tests/unit/training/test_checkpoint_loader.py::TestResolveCheckpointPath::test_resolve_from_config PASSED
tests/unit/training/test_checkpoint_loader.py::TestResolveCheckpointPath::test_resolve_from_cache PASSED
tests/unit/training/test_checkpoint_loader.py::TestResolveCheckpointPath::test_resolve_with_pattern PASSED
tests/unit/training/test_checkpoint_loader.py::TestResolveCheckpointPath::test_resolve_none_when_invalid PASSED
tests/unit/training/test_checkpoint_loader.py::TestResolveCheckpointPath::test_resolve_none_when_not_configured PASSED
tests/unit/training/test_checkpoint_loader.py::TestResolveCheckpointPath::test_resolve_priority_env_over_config PASSED
tests/unit/training/test_cv_utils.py::TestKfoldSplitCreation::test_create_kfold_splits_returns_k_folds PASSED
tests/unit/training/test_cv_utils.py::TestKfoldSplitCreation::test_create_kfold_splits_all_samples_in_one_fold PASSED
tests/unit/training/test_cv_utils.py::TestKfoldSplitCreation::test_create_kfold_splits_stratified PASSED
tests/unit/training/test_cv_utils.py::TestKfoldSplitCreation::test_create_kfold_splits_same_seed_same_splits PASSED
tests/unit/training/test_cv_utils.py::TestKfoldSplitCreation::test_create_kfold_splits_different_seed_different_splits PASSED
tests/unit/training/test_cv_utils.py::TestKfoldSplitCreation::test_create_kfold_splits_smoke_yaml_params PASSED
tests/unit/training/test_cv_utils.py::TestKfoldSplitCreation::test_create_kfold_splits_no_shuffle PASSED
tests/unit/training/test_cv_utils.py::TestKfoldSplitCreation::test_create_kfold_splits_insufficient_samples PASSED
tests/unit/training/test_cv_utils.py::TestSaveLoadFoldSplits::test_save_fold_splits PASSED
tests/unit/training/test_cv_utils.py::TestSaveLoadFoldSplits::test_load_fold_splits PASSED
tests/unit/training/test_cv_utils.py::TestSaveLoadFoldSplits::test_load_fold_splits_file_not_found PASSED
tests/unit/training/test_cv_utils.py::TestSaveLoadFoldSplits::test_save_and_load_roundtrip PASSED
tests/unit/training/test_cv_utils.py::TestGetFoldData::test_get_fold_data PASSED
tests/unit/training/test_cv_utils.py::TestValidateSplits::test_validate_splits [CV] Fold 0: {'PERSON': 1, 'ORG': 1} | Missing: []
[CV] Fold 1: {'PERSON': 1, 'ORG': 1} | Missing: []
PASSED
tests/unit/training/test_data_combiner.py::TestCombineDatasets::test_new_only_strategy PASSED
tests/unit/training/test_data_combiner.py::TestCombineDatasets::test_combined_strategy PASSED
tests/unit/training/test_data_combiner.py::TestCombineDatasets::test_append_strategy PASSED
tests/unit/training/test_data_combiner.py::TestCombineDatasets::test_invalid_strategy PASSED
tests/unit/training/test_data_combiner.py::TestCombineDatasets::test_combined_requires_old_dataset PASSED
tests/unit/training/test_data_combiner.py::TestCombineDatasets::test_append_requires_old_dataset PASSED
tests/unit/training/test_data_combiner.py::TestCombineDatasets::test_new_dataset_not_found PASSED
tests/unit/training/test_data_combiner.py::TestCombineDatasets::test_old_dataset_not_found PASSED
tests/unit/training/test_data_combiner.py::TestCombineDatasets::test_no_validation_in_new_dataset PASSED
tests/unit/training/test_data_combiner.py::TestCombineDatasets::test_create_validation_split PASSED
tests/unit/training/test_train_config_defaults.py::test_core_training_defaults PASSED
tests/unit/training/test_train_config_defaults.py::test_metric_defaults PASSED
tests/unit/training/test_train_config_defaults.py::test_early_stopping_defaults PASSED
tests/unit/training/test_trainer.py::TestPrepareDataLoaders::test_prepare_data_loaders_basic PASSED
tests/unit/training/test_trainer.py::TestPrepareDataLoaders::test_prepare_data_loaders_with_indices PASSED
tests/unit/training/test_trainer.py::TestPrepareDataLoaders::test_prepare_data_loaders_use_all_data PASSED
tests/unit/training/test_trainer.py::TestPrepareDataLoaders::test_prepare_data_loaders_deberta_batch_size_cap PASSED
tests/unit/training/test_trainer.py::TestPrepareDataLoaders::test_prepare_data_loaders_val_split_fallback PASSED
tests/unit/training/test_trainer.py::TestPrepareDataLoaders::test_prepare_data_loaders_kfold_cv_val_from_train_data PASSED
tests/unit/training/test_trainer.py::TestCreateOptimizerAndScheduler::test_create_optimizer_and_scheduler_defaults PASSED
tests/unit/training/test_trainer.py::TestCreateOptimizerAndScheduler::test_create_optimizer_and_scheduler_custom PASSED
tests/unit/training/test_trainer.py::TestCreateOptimizerAndScheduler::test_create_optimizer_and_scheduler_warmup_capped PASSED
tests/unit/training/test_trainer.py::TestRunTrainingLoop::test_run_training_loop_basic PASSED
tests/unit/training/test_trainer.py::TestRunTrainingLoop::test_run_training_loop_multiple_epochs PASSED
tests/unit/training/test_trainer.py::TestSaveCheckpoint::test_save_checkpoint_success PASSED
tests/unit/training/test_trainer.py::TestSaveCheckpoint::test_save_checkpoint_creates_directory PASSED
tests/unit/training/test_trainer.py::TestTrainModel::test_train_model_basic PASSED
tests/unit/training/test_trainer.py::TestTrainModel::test_train_model_with_fold_splits PASSED
tests/unit/training/test_trainer.py::TestTrainModel::test_train_model_use_all_data PASSED
tests/unit/training/test_trainer.py::TestTrainModel::test_train_model_invalid_fold_idx PASSED
tests/workflows/test_full_workflow_e2e.py::test_full_workflow_e2e FAILED
tests/workflows/test_notebook_01_e2e.py::TestNotebookE2E_Core::test_environment_detection PASSED
tests/workflows/test_notebook_01_e2e.py::TestNotebookE2E_Core::test_path_setup PASSED
tests/workflows/test_notebook_01_e2e.py::TestNotebookE2E_Core::test_config_loading PASSED
tests/workflows/test_notebook_01_e2e.py::TestNotebookE2E_Core::test_dataset_verification PASSED
tests/workflows/test_notebook_01_e2e.py::TestNotebookE2E_Core::test_mlflow_setup PASSED
tests/workflows/test_notebook_01_e2e.py::TestNotebookE2E_Core::test_hpo_sweep_execution_mocked FAILED
tests/workflows/test_notebook_01_e2e.py::TestNotebookE2E_Core::test_benchmarking_execution_mocked PASSED
tests/workflows/test_notebook_01_e2e.py::TestNotebookE2E_Core::test_output_validation PASSED
tests/workflows/test_notebook_01_e2e.py::TestNotebookE2E_Full::test_repository_setup SKIPPED
tests/workflows/test_notebook_01_e2e.py::TestNotebookE2E_Full::test_dependency_check SKIPPED
tests/workflows/test_notebook_02_e2e.py::test_best_config_selection_e2e PASSED

=================================== FAILURES ===================================
______________ TestPruningBehavior.test_pruner_with_study_manager ______________
tests/hpo/integration/test_early_termination.py:281: in test_pruner_with_study_manager
    assert isinstance(study.pruner, MedianPruner)
E   AssertionError: assert False
E    +  where False = isinstance(<MagicMock name='mock.create_study().pruner' id='140077259720208'>, MedianPruner)
E    +    where <MagicMock name='mock.create_study().pruner' id='140077259720208'> = <MagicMock name='mock.create_study()' id='140077279450096'>.pruner
------------------------------ Captured log call -------------------------------
INFO     training.hpo.core.study:study.py:368 [HPO] Starting optimization for distilbert with checkpointing...
__________ TestPruningIntegration.test_pruning_with_checkpoint_resume __________
tests/hpo/integration/test_early_termination.py:394: in test_pruning_with_checkpoint_resume
    assert should_resume is True
E   assert False is True
------------------------------ Captured log call -------------------------------
INFO     training.hpo.core.study:study.py:368 [HPO] Starting optimization for distilbert with checkpointing...
INFO     training.hpo.core.study:study.py:368 [HPO] Starting optimization for distilbert with checkpointing...
__________ TestCheckpointResume.test_resume_from_existing_checkpoint ___________
tests/hpo/integration/test_hpo_checkpoint_resume.py:149: in test_resume_from_existing_checkpoint
    assert len(study2.trials) == 1
E   AssertionError: assert 0 == 1
E    +  where 0 = len(<MagicMock name='mock.create_study().trials' id='140077280918304'>)
E    +    where <MagicMock name='mock.create_study().trials' id='140077280918304'> = <MagicMock name='mock.create_study()' id='140077279450096'>.trials
------------------------------ Captured log call -------------------------------
INFO     training.hpo.core.study:study.py:288 [HPO] Resuming optimization for distilbert from checkpoint...
INFO     training.hpo.core.study:study.py:341 Loaded 0 existing trials (0 completed, 0 marked as failed)
______________ TestCheckpointResume.test_resume_preserves_trials _______________
tests/hpo/integration/test_hpo_checkpoint_resume.py:204: in test_resume_preserves_trials
    assert len(study2.trials) == 3
E   AssertionError: assert 0 == 3
E    +  where 0 = len(<MagicMock name='mock.create_study().trials' id='140077280918304'>)
E    +    where <MagicMock name='mock.create_study().trials' id='140077280918304'> = <MagicMock name='mock.create_study()' id='140077279450096'>.trials
------------------------------ Captured log call -------------------------------
INFO     training.hpo.core.study:study.py:288 [HPO] Resuming optimization for distilbert from checkpoint...
INFO     training.hpo.core.study:study.py:341 Loaded 0 existing trials (0 completed, 0 marked as failed)
_______ TestCheckpointResume.test_resume_marks_running_trials_as_failed ________
tests/hpo/integration/test_hpo_checkpoint_resume.py:264: in test_resume_marks_running_trials_as_failed
    assert len(study2.trials) == 2
E   AssertionError: assert 0 == 2
E    +  where 0 = len(<MagicMock name='mock.create_study().trials' id='140077280918304'>)
E    +    where <MagicMock name='mock.create_study().trials' id='140077280918304'> = <MagicMock name='mock.create_study()' id='140077279450096'>.trials
------------------------------ Captured log call -------------------------------
INFO     training.hpo.core.study:study.py:288 [HPO] Resuming optimization for distilbert from checkpoint...
INFO     training.hpo.core.study:study.py:341 Loaded 0 existing trials (0 completed, 0 marked as failed)
_____ TestCheckpointResume.test_resume_with_auto_resume_false_raises_error _____
tests/hpo/integration/test_hpo_checkpoint_resume.py:315: in test_resume_with_auto_resume_false_raises_error
    with pytest.raises(ValueError, match="auto_resume=false"):
E   Failed: DID NOT RAISE <class 'ValueError'>
------------------------------ Captured log call -------------------------------
INFO     training.hpo.core.study:study.py:274 [HPO] auto_resume=false: Creating new study 'hpo_distilbert_no_resume_test' (ignoring existing study)
INFO     training.hpo.core.study:study.py:368 [HPO] Starting optimization for distilbert with checkpointing...
__________ TestCheckpointResume.test_resume_continues_trial_numbering __________
tests/hpo/integration/test_hpo_checkpoint_resume.py:372: in test_resume_continues_trial_numbering
    assert len(study2.trials) == 2
E   AssertionError: assert 0 == 2
E    +  where 0 = len(<MagicMock name='mock.create_study().trials' id='140077280918304'>)
E    +    where <MagicMock name='mock.create_study().trials' id='140077280918304'> = <MagicMock name='mock.create_study()' id='140077279450096'>.trials
------------------------------ Captured log call -------------------------------
INFO     training.hpo.core.study:study.py:288 [HPO] Resuming optimization for distilbert from checkpoint...
INFO     training.hpo.core.study:study.py:341 Loaded 0 existing trials (0 completed, 0 marked as failed)
_____ TestCheckpointSmokeYaml.test_checkpoint_smoke_yaml_auto_resume_true ______
tests/hpo/integration/test_hpo_checkpoint_resume.py:481: in test_checkpoint_smoke_yaml_auto_resume_true
    assert len(study2.trials) == 1
E   AssertionError: assert 0 == 1
E    +  where 0 = len(<MagicMock name='mock.create_study().trials' id='140077280918304'>)
E    +    where <MagicMock name='mock.create_study().trials' id='140077280918304'> = <MagicMock name='mock.create_study()' id='140077279450096'>.trials
------------------------------ Captured log call -------------------------------
INFO     training.hpo.core.study:study.py:288 [HPO] Resuming optimization for distilbert from checkpoint...
INFO     training.hpo.core.study:study.py:341 Loaded 0 existing trials (0 completed, 0 marked as failed)
_________ TestFullHPOWorkflow.test_full_hpo_workflow_with_cv_and_refit _________
tests/hpo/integration/test_hpo_full_workflow.py:315: in test_full_hpo_workflow_with_cv_and_refit
    assert len(trial_folders) > 0
E   assert 0 > 0
E    +  where 0 = len([])
------------------------------ Captured log call -------------------------------
INFO     training.hpo.execution.local.sweep:sweep.py:658 [EARLY HASH] Computed v2 study_key_hash=01b6aaaa11c2ca02... for folder creation
INFO     training.hpo.core.study:study.py:368 [HPO] Starting optimization for distilbert with checkpointing...
WARNING  infrastructure.naming.display_policy:display_policy.py:140 [naming.yaml] Missing required 'run_names' section (/tmp/pytest-of-codespace/pytest-38/test_full_hpo_workflow_with_cv0/config/naming.yaml)
INFO     training.hpo.execution.local.sweep:sweep.py:843 [HPO Setup] k_folds=2, fold_splits_file=None, k_fold config: {'enabled': True, 'n_splits': 2, 'random_seed': 42, 'shuffle': True, 'stratified': True}
INFO     training.hpo.execution.local.sweep:sweep.py:173 [CV Setup]  Created 2 fold splits and saved to /tmp/pytest-of-codespace/pytest-38/test_full_hpo_workflow_with_cv0/outputs/hpo/local/distilbert/study-01b6aaaa/fold_splits.json
WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:1057 Using LOCAL MLflow tracking (not Azure ML)
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:1059 To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
DEBUG    git.cmd:cmd.py:1270 Popen(['git', 'version'], cwd=/workspaces/resume-ner-azureml, stdin=None, shell=False, universal_newlines=False)
DEBUG    git.cmd:cmd.py:1270 Popen(['git', 'version'], cwd=/workspaces/resume-ner-azureml, stdin=None, shell=False, universal_newlines=False)
DEBUG    git.util:util.py:494 sys.platform='linux', git_executable='git'
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:143 [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=01b6aaaa11c2ca02...
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:161 [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=01b6aaaa11c2ca02..., study_family_hash=6a3fd9967110a5db...
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:221 [START_SWEEP_RUN] Yielding RunHandle. run_id=37cce966ef87...
INFO     training.hpo.execution.local.sweep:sweep.py:953 [HPO] Parent run 37cce966ef87... is now visible in MLflow (status: RUNNING)
INFO     training.hpo.execution.local.sweep:sweep.py:995 Setting Phase 2 tags on parent run 37cce966ef87... (data_config=present, train_config=present)
WARNING  training.hpo.execution.local.sweep:sweep.py:530 Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
WARNING  training.hpo.execution.local.sweep:sweep.py:538 Could not set data fingerprint tag: 'Missing tag key: fingerprint.data'
WARNING  training.hpo.execution.local.sweep:sweep.py:545 Could not set eval fingerprint tag: 'Missing tag key: fingerprint.eval'
WARNING  training.hpo.execution.local.sweep:sweep.py:556 Could not set objective direction tag: 'Missing tag key: objective.direction'
WARNING  training.hpo.execution.local.sweep:sweep.py:563 Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
WARNING  training.hpo.execution.local.sweep:sweep.py:570 Could not set artifact.available tag: 'Missing tag key: artifact.available'
INFO     training.hpo.execution.local.sweep:sweep.py:573  Set Phase 2 tags on parent run 37cce966ef87...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
INFO     training.hpo.execution.local.sweep:sweep.py:1008  Successfully completed Phase 2 tag setting for parent run 37cce966ef87...
INFO     training.hpo.tracking.cleanup:cleanup.py:132 [CLEANUP] Starting cleanup check: parent_run_id=37cce966ef87...
INFO     training.hpo.tracking.cleanup:cleanup.py:178 [CLEANUP] MLflow imported successfully. Current env: local, run_key_hash: cc90dfbe1603...
INFO     training.hpo.tracking.cleanup:cleanup.py:190 [CLEANUP] Retrieved experiment: 975860881501093750
INFO     training.hpo.tracking.cleanup:cleanup.py:196 [CLEANUP] Fetching all runs in experiment (may paginate for large experiments)...
INFO     training.hpo.tracking.cleanup:cleanup.py:219 [CLEANUP] Fetched 14 total runs from experiment
INFO     training.hpo.tracking.cleanup:cleanup.py:231 [CLEANUP] Built parentchildren map: 0 parents have children
INFO     training.hpo.tracking.cleanup:cleanup.py:352 [CLEANUP] Status breakdown: {'RUNNING': 1, 'FINISHED': 13}
INFO     training.hpo.tracking.cleanup:cleanup.py:353 [CLEANUP] Found 0 tag-based matches, 0 name-fallback matches (legacy), 0 total eligible for tagging
INFO     training.hpo.tracking.cleanup:cleanup.py:391 [CLEANUP] Found 0 orphaned child runs (RUNNING children with terminal parents)
INFO     training.hpo.tracking.cleanup:cleanup.py:499 [CLEANUP] No interrupted parent runs found to tag
INFO     training.hpo.tracking.cleanup:cleanup.py:539 [CLEANUP] No orphaned child runs found to tag
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:318 [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=37cce966ef87...
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:342 [LOG_FINAL_METRICS] Found 0 completed trials out of 0 total
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:353 [LOG_FINAL_METRICS] Logging metrics: n_trials=0, n_completed_trials=0
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:362 [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=<MagicMock name='mock.create_study().best_value' id='140077206668144'>
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:371 [LOG_FINAL_METRICS] Logging best hyperparameters: <MagicMock name='mock.create_study().best_params' id='140077206783792'>
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:378 [LOG_FINAL_METRICS] Finding and logging best trial run ID...
WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:610 Could not find MLflow run ID for best trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>. Search failed: Invalid clause(s) in filter string: 'tags.trial_number = '<MagicMock name='mock.create_study()', '.', 'best_trial.number', '' id='', '140077206848368', ''>''. This may be a timing issue - trial runs may not be created/committed yet.
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:389 [LOG_FINAL_METRICS] Completed best trial ID logging
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:399 [LOG_FINAL_METRICS] Checkpoint logging enabled: False (upload_checkpoint=False)
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:432 [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:449 [LOG_FINAL_METRICS] Completed log_final_metrics successfully
INFO     training.hpo.execution.local.sweep:sweep.py:1115 [REFIT] Starting refit training for best trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>
WARNING  training.hpo.execution.local.sweep:sweep.py:1205 [REFIT] Could not determine trial_key_hash from best trial run tags. Refit run may not match trial run.
INFO     training.hpo.execution.local.refit:refit.py:107 [REFIT] Starting refit training for trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'> with hyperparameters: <MagicMock name='mock.create_study().best_trial.params' id='140077206865856'>
INFO     training.hpo.execution.local.refit:refit.py:130 [REFIT] Computed trial_id="trial_<MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>_20260118_003116", run_id='20260118_003116', trial_number=<MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>
WARNING  infrastructure.tracking.mlflow.hash_utils:hash_utils.py:515 Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
WARNING  training.hpo.execution.local.sweep:sweep.py:1304 [REFIT] Refit training failed: Cannot create refit in v2 study folder study-01b6aaaa without trial_key_hash. study_key_hash=present, trial_key_hash=missing, refit_params=missing. Hash computation from trial parameters may have failed.. Continuing without refit checkpoint.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py", line 1244, in run_local_hpo_sweep
    refit_metrics, refit_checkpoint_dir, refit_run_id = run_refit_training(
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 240, in run_refit_training
    raise RuntimeError(
RuntimeError: Cannot create refit in v2 study folder study-01b6aaaa without trial_key_hash. study_key_hash=present, trial_key_hash=missing, refit_params=missing. Hash computation from trial parameters may have failed.
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:948 [LOG_BEST_CHECKPOINT] Falling back to standard checkpoint search
WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:757 Best trial checkpoint not found for trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>. Searched in: /tmp/pytest-of-codespace/pytest-38/test_full_hpo_workflow_with_cv0/outputs/hpo/local/distilbert. Skipping MLflow checkpoint logging.
WARNING  training.hpo.execution.local.sweep:sweep.py:1428 [REFIT] No refit_run_id available to mark as FINISHED
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:224 [START_SWEEP_RUN] Context manager exiting normally. run_id=37cce966ef87...
__________ TestFullHPOWorkflow.test_full_hpo_workflow_no_cv_no_refit ___________
tests/hpo/integration/test_hpo_full_workflow.py:467: in test_full_hpo_workflow_no_cv_no_refit
    assert len(study.trials) >= 1
E   AssertionError: assert 0 >= 1
E    +  where 0 = len(<MagicMock name='mock.create_study().trials' id='140077280918304'>)
E    +    where <MagicMock name='mock.create_study().trials' id='140077280918304'> = <MagicMock name='mock.create_study()' id='140077279450096'>.trials
------------------------------ Captured log call -------------------------------
INFO     training.hpo.execution.local.sweep:sweep.py:658 [EARLY HASH] Computed v2 study_key_hash=957b9a1438354884... for folder creation
WARNING  evaluation.selection.trial_finder:trial_finder.py:506 No v2 study folders found in /tmp/pytest-of-codespace/pytest-38/test_full_hpo_workflow_no_cv_n0/outputs/hpo/local/distilbert (resolved from /tmp/pytest-of-codespace/pytest-38/test_full_hpo_workflow_no_cv_n0/outputs/hpo/local/distilbert)
WARNING  infrastructure.naming.display_policy:display_policy.py:140 [naming.yaml] Missing required 'run_names' section (/tmp/pytest-of-codespace/pytest-38/test_full_hpo_workflow_no_cv_n0/config/naming.yaml)
INFO     training.hpo.execution.local.sweep:sweep.py:843 [HPO Setup] k_folds=None, fold_splits_file=None, k_fold config: {'enabled': False}
WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:1057 Using LOCAL MLflow tracking (not Azure ML)
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:1059 To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:143 [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=957b9a1438354884...
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:161 [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=957b9a1438354884..., study_family_hash=23208e5829deb236...
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:221 [START_SWEEP_RUN] Yielding RunHandle. run_id=fbc8ed55335b...
INFO     training.hpo.execution.local.sweep:sweep.py:953 [HPO] Parent run fbc8ed55335b... is now visible in MLflow (status: RUNNING)
INFO     training.hpo.execution.local.sweep:sweep.py:995 Setting Phase 2 tags on parent run fbc8ed55335b... (data_config=present, train_config=present)
WARNING  training.hpo.execution.local.sweep:sweep.py:530 Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
WARNING  training.hpo.execution.local.sweep:sweep.py:538 Could not set data fingerprint tag: 'Missing tag key: fingerprint.data'
WARNING  training.hpo.execution.local.sweep:sweep.py:545 Could not set eval fingerprint tag: 'Missing tag key: fingerprint.eval'
WARNING  training.hpo.execution.local.sweep:sweep.py:556 Could not set objective direction tag: 'Missing tag key: objective.direction'
WARNING  training.hpo.execution.local.sweep:sweep.py:563 Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
WARNING  training.hpo.execution.local.sweep:sweep.py:570 Could not set artifact.available tag: 'Missing tag key: artifact.available'
INFO     training.hpo.execution.local.sweep:sweep.py:573  Set Phase 2 tags on parent run fbc8ed55335b...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
INFO     training.hpo.execution.local.sweep:sweep.py:1008  Successfully completed Phase 2 tag setting for parent run fbc8ed55335b...
INFO     training.hpo.tracking.cleanup:cleanup.py:121 [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:318 [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=fbc8ed55335b...
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:342 [LOG_FINAL_METRICS] Found 0 completed trials out of 0 total
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:353 [LOG_FINAL_METRICS] Logging metrics: n_trials=0, n_completed_trials=0
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:362 [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=<MagicMock name='mock.create_study().best_value' id='140077206668144'>
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:371 [LOG_FINAL_METRICS] Logging best hyperparameters: <MagicMock name='mock.create_study().best_params' id='140077206783792'>
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:378 [LOG_FINAL_METRICS] Finding and logging best trial run ID...
WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:610 Could not find MLflow run ID for best trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>. Search failed: Invalid clause(s) in filter string: 'tags.trial_number = '<MagicMock name='mock.create_study()', '.', 'best_trial.number', '' id='', '140077206848368', ''>''. This may be a timing issue - trial runs may not be created/committed yet.
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:389 [LOG_FINAL_METRICS] Completed best trial ID logging
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:399 [LOG_FINAL_METRICS] Checkpoint logging enabled: False (upload_checkpoint=False)
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:432 [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:449 [LOG_FINAL_METRICS] Completed log_final_metrics successfully
INFO     training.hpo.execution.local.sweep:sweep.py:1440 [REFIT] Refit training is disabled in config
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:224 [START_SWEEP_RUN] Context manager exiting normally. run_id=fbc8ed55335b...
__ TestFullHPOWorkflow.test_full_hpo_workflow_creates_correct_path_structure ___
tests/hpo/integration/test_hpo_full_workflow.py:641: in test_full_hpo_workflow_creates_correct_path_structure
    assert len(trial_folders) > 0
E   assert 0 > 0
E    +  where 0 = len([])
------------------------------ Captured log call -------------------------------
INFO     training.hpo.execution.local.sweep:sweep.py:658 [EARLY HASH] Computed v2 study_key_hash=dc0c421a5056294a... for folder creation
WARNING  evaluation.selection.trial_finder:trial_finder.py:506 No v2 study folders found in /tmp/pytest-of-codespace/pytest-38/test_full_hpo_workflow_creates0/outputs/hpo/local/distilbert (resolved from /tmp/pytest-of-codespace/pytest-38/test_full_hpo_workflow_creates0/outputs/hpo/local/distilbert)
WARNING  infrastructure.naming.display_policy:display_policy.py:140 [naming.yaml] Missing required 'run_names' section (/tmp/pytest-of-codespace/pytest-38/test_full_hpo_workflow_creates0/config/naming.yaml)
INFO     training.hpo.execution.local.sweep:sweep.py:843 [HPO Setup] k_folds=2, fold_splits_file=None, k_fold config: {'enabled': True, 'n_splits': 2, 'random_seed': 42, 'shuffle': True, 'stratified': True}
INFO     training.hpo.execution.local.sweep:sweep.py:173 [CV Setup]  Created 2 fold splits and saved to /tmp/pytest-of-codespace/pytest-38/test_full_hpo_workflow_creates0/outputs/hpo/local/distilbert/study-dc0c421a/fold_splits.json
WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:1057 Using LOCAL MLflow tracking (not Azure ML)
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:1059 To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:143 [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=dc0c421a5056294a...
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:161 [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=dc0c421a5056294a..., study_family_hash=d1aa8bfffdcfdb77...
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:221 [START_SWEEP_RUN] Yielding RunHandle. run_id=bd458de544e9...
INFO     training.hpo.execution.local.sweep:sweep.py:953 [HPO] Parent run bd458de544e9... is now visible in MLflow (status: RUNNING)
INFO     training.hpo.execution.local.sweep:sweep.py:995 Setting Phase 2 tags on parent run bd458de544e9... (data_config=present, train_config=present)
WARNING  training.hpo.execution.local.sweep:sweep.py:530 Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
WARNING  training.hpo.execution.local.sweep:sweep.py:538 Could not set data fingerprint tag: 'Missing tag key: fingerprint.data'
WARNING  training.hpo.execution.local.sweep:sweep.py:545 Could not set eval fingerprint tag: 'Missing tag key: fingerprint.eval'
WARNING  training.hpo.execution.local.sweep:sweep.py:556 Could not set objective direction tag: 'Missing tag key: objective.direction'
WARNING  training.hpo.execution.local.sweep:sweep.py:563 Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
WARNING  training.hpo.execution.local.sweep:sweep.py:570 Could not set artifact.available tag: 'Missing tag key: artifact.available'
INFO     training.hpo.execution.local.sweep:sweep.py:573  Set Phase 2 tags on parent run bd458de544e9...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
INFO     training.hpo.execution.local.sweep:sweep.py:1008  Successfully completed Phase 2 tag setting for parent run bd458de544e9...
INFO     training.hpo.tracking.cleanup:cleanup.py:121 [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:318 [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=bd458de544e9...
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:342 [LOG_FINAL_METRICS] Found 0 completed trials out of 0 total
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:353 [LOG_FINAL_METRICS] Logging metrics: n_trials=0, n_completed_trials=0
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:362 [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=<MagicMock name='mock.create_study().best_value' id='140077206668144'>
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:371 [LOG_FINAL_METRICS] Logging best hyperparameters: <MagicMock name='mock.create_study().best_params' id='140077206783792'>
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:378 [LOG_FINAL_METRICS] Finding and logging best trial run ID...
WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:610 Could not find MLflow run ID for best trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>. Search failed: Invalid clause(s) in filter string: 'tags.trial_number = '<MagicMock name='mock.create_study()', '.', 'best_trial.number', '' id='', '140077206848368', ''>''. This may be a timing issue - trial runs may not be created/committed yet.
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:389 [LOG_FINAL_METRICS] Completed best trial ID logging
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:399 [LOG_FINAL_METRICS] Checkpoint logging enabled: False (upload_checkpoint=False)
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:432 [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:449 [LOG_FINAL_METRICS] Completed log_final_metrics successfully
INFO     training.hpo.execution.local.sweep:sweep.py:1115 [REFIT] Starting refit training for best trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>
WARNING  training.hpo.execution.local.sweep:sweep.py:1205 [REFIT] Could not determine trial_key_hash from best trial run tags. Refit run may not match trial run.
INFO     training.hpo.execution.local.refit:refit.py:107 [REFIT] Starting refit training for trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'> with hyperparameters: <MagicMock name='mock.create_study().best_trial.params' id='140077206865856'>
INFO     training.hpo.execution.local.refit:refit.py:130 [REFIT] Computed trial_id="trial_<MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>_20260118_003118", run_id='20260118_003118', trial_number=<MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>
WARNING  infrastructure.tracking.mlflow.hash_utils:hash_utils.py:515 Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
WARNING  training.hpo.execution.local.sweep:sweep.py:1304 [REFIT] Refit training failed: Cannot create refit in v2 study folder study-dc0c421a without trial_key_hash. study_key_hash=present, trial_key_hash=missing, refit_params=missing. Hash computation from trial parameters may have failed.. Continuing without refit checkpoint.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py", line 1244, in run_local_hpo_sweep
    refit_metrics, refit_checkpoint_dir, refit_run_id = run_refit_training(
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 240, in run_refit_training
    raise RuntimeError(
RuntimeError: Cannot create refit in v2 study folder study-dc0c421a without trial_key_hash. study_key_hash=present, trial_key_hash=missing, refit_params=missing. Hash computation from trial parameters may have failed.
INFO     training.hpo.execution.local.sweep:sweep.py:1340 [HPO] Skipping checkpoint logging (mlflow.log_best_checkpoint=false or not set)
INFO     infrastructure.naming.mlflow.tags_registry:tags_registry.py:224 [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_full_hpo_workflow_creates0/outputs/hpo/config/tags.yaml, using defaults
WARNING  training.hpo.execution.local.sweep:sweep.py:1428 [REFIT] No refit_run_id available to mark as FINISHED
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:224 [START_SWEEP_RUN] Context manager exiting normally. run_id=bd458de544e9...
_________ TestHPOResumeWorkflow.test_resume_workflow_preserves_trials __________
tests/hpo/integration/test_hpo_resume_workflow.py:147: in test_resume_workflow_preserves_trials
    assert len(study1.trials) == 1
E   AssertionError: assert 0 == 1
E    +  where 0 = len(<MagicMock name='mock.create_study().trials' id='140077280918304'>)
E    +    where <MagicMock name='mock.create_study().trials' id='140077280918304'> = <MagicMock name='mock.create_study()' id='140077279450096'>.trials
------------------------------ Captured log call -------------------------------
INFO     training.hpo.execution.local.sweep:sweep.py:658 [EARLY HASH] Computed v2 study_key_hash=957b9a1438354884... for folder creation
INFO     training.hpo.core.study:study.py:368 [HPO] Starting optimization for distilbert with checkpointing...
WARNING  infrastructure.naming.display_policy:display_policy.py:140 [naming.yaml] Missing required 'run_names' section (/tmp/pytest-of-codespace/pytest-38/test_resume_workflow_preserves0/config/naming.yaml)
INFO     training.hpo.execution.local.sweep:sweep.py:843 [HPO Setup] k_folds=None, fold_splits_file=None, k_fold config: {'enabled': False}
WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:1057 Using LOCAL MLflow tracking (not Azure ML)
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:1059 To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:143 [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=957b9a1438354884...
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:161 [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=957b9a1438354884..., study_family_hash=23208e5829deb236...
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:221 [START_SWEEP_RUN] Yielding RunHandle. run_id=67807eddde4b...
INFO     training.hpo.execution.local.sweep:sweep.py:953 [HPO] Parent run 67807eddde4b... is now visible in MLflow (status: RUNNING)
INFO     training.hpo.execution.local.sweep:sweep.py:995 Setting Phase 2 tags on parent run 67807eddde4b... (data_config=present, train_config=present)
WARNING  training.hpo.execution.local.sweep:sweep.py:530 Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
WARNING  training.hpo.execution.local.sweep:sweep.py:538 Could not set data fingerprint tag: 'Missing tag key: fingerprint.data'
WARNING  training.hpo.execution.local.sweep:sweep.py:545 Could not set eval fingerprint tag: 'Missing tag key: fingerprint.eval'
WARNING  training.hpo.execution.local.sweep:sweep.py:556 Could not set objective direction tag: 'Missing tag key: objective.direction'
WARNING  training.hpo.execution.local.sweep:sweep.py:563 Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
WARNING  training.hpo.execution.local.sweep:sweep.py:570 Could not set artifact.available tag: 'Missing tag key: artifact.available'
INFO     training.hpo.execution.local.sweep:sweep.py:573  Set Phase 2 tags on parent run 67807eddde4b...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
INFO     training.hpo.execution.local.sweep:sweep.py:1008  Successfully completed Phase 2 tag setting for parent run 67807eddde4b...
INFO     training.hpo.tracking.cleanup:cleanup.py:121 [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:318 [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=67807eddde4b...
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:342 [LOG_FINAL_METRICS] Found 0 completed trials out of 0 total
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:353 [LOG_FINAL_METRICS] Logging metrics: n_trials=0, n_completed_trials=0
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:362 [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=<MagicMock name='mock.create_study().best_value' id='140077206668144'>
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:371 [LOG_FINAL_METRICS] Logging best hyperparameters: <MagicMock name='mock.create_study().best_params' id='140077206783792'>
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:378 [LOG_FINAL_METRICS] Finding and logging best trial run ID...
WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:610 Could not find MLflow run ID for best trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>. Search failed: Invalid clause(s) in filter string: 'tags.trial_number = '<MagicMock name='mock.create_study()', '.', 'best_trial.number', '' id='', '140077206848368', ''>''. This may be a timing issue - trial runs may not be created/committed yet.
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:389 [LOG_FINAL_METRICS] Completed best trial ID logging
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:399 [LOG_FINAL_METRICS] Checkpoint logging enabled: False (upload_checkpoint=False)
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:432 [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:449 [LOG_FINAL_METRICS] Completed log_final_metrics successfully
INFO     training.hpo.execution.local.sweep:sweep.py:1440 [REFIT] Refit training is disabled in config
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:224 [START_SWEEP_RUN] Context manager exiting normally. run_id=67807eddde4b...
_______ TestHPOResumeWorkflow.test_resume_workflow_with_different_run_id _______
tests/hpo/integration/test_hpo_resume_workflow.py:339: in test_resume_workflow_with_different_run_id
    assert len(study2.trials) > first_trial_count  # New trial added
E   AssertionError: assert 0 > 0
E    +  where 0 = len(<MagicMock name='mock.create_study().trials' id='140077280918304'>)
E    +    where <MagicMock name='mock.create_study().trials' id='140077280918304'> = <MagicMock name='mock.create_study()' id='140077279450096'>.trials
------------------------------ Captured log call -------------------------------
INFO     training.hpo.execution.local.sweep:sweep.py:658 [EARLY HASH] Computed v2 study_key_hash=42ff77fc79adab6d... for folder creation
INFO     training.hpo.core.study:study.py:368 [HPO] Starting optimization for distilbert with checkpointing...
WARNING  infrastructure.naming.display_policy:display_policy.py:140 [naming.yaml] Missing required 'run_names' section (/tmp/pytest-of-codespace/pytest-38/test_resume_workflow_with_diff0/config/naming.yaml)
INFO     training.hpo.execution.local.sweep:sweep.py:843 [HPO Setup] k_folds=None, fold_splits_file=None, k_fold config: {'enabled': False}
WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:1057 Using LOCAL MLflow tracking (not Azure ML)
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:1059 To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:143 [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=42ff77fc79adab6d...
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:161 [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=42ff77fc79adab6d..., study_family_hash=ce5a7f9ff304036d...
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:221 [START_SWEEP_RUN] Yielding RunHandle. run_id=bd92d2ca20c5...
INFO     training.hpo.execution.local.sweep:sweep.py:953 [HPO] Parent run bd92d2ca20c5... is now visible in MLflow (status: RUNNING)
INFO     training.hpo.execution.local.sweep:sweep.py:995 Setting Phase 2 tags on parent run bd92d2ca20c5... (data_config=present, train_config=present)
WARNING  training.hpo.execution.local.sweep:sweep.py:530 Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
WARNING  training.hpo.execution.local.sweep:sweep.py:538 Could not set data fingerprint tag: 'Missing tag key: fingerprint.data'
WARNING  training.hpo.execution.local.sweep:sweep.py:545 Could not set eval fingerprint tag: 'Missing tag key: fingerprint.eval'
WARNING  training.hpo.execution.local.sweep:sweep.py:556 Could not set objective direction tag: 'Missing tag key: objective.direction'
WARNING  training.hpo.execution.local.sweep:sweep.py:563 Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
WARNING  training.hpo.execution.local.sweep:sweep.py:570 Could not set artifact.available tag: 'Missing tag key: artifact.available'
INFO     training.hpo.execution.local.sweep:sweep.py:573  Set Phase 2 tags on parent run bd92d2ca20c5...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
INFO     training.hpo.execution.local.sweep:sweep.py:1008  Successfully completed Phase 2 tag setting for parent run bd92d2ca20c5...
INFO     training.hpo.tracking.cleanup:cleanup.py:121 [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:318 [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=bd92d2ca20c5...
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:342 [LOG_FINAL_METRICS] Found 0 completed trials out of 0 total
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:353 [LOG_FINAL_METRICS] Logging metrics: n_trials=0, n_completed_trials=0
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:362 [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=<MagicMock name='mock.create_study().best_value' id='140077206668144'>
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:371 [LOG_FINAL_METRICS] Logging best hyperparameters: <MagicMock name='mock.create_study().best_params' id='140077206783792'>
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:378 [LOG_FINAL_METRICS] Finding and logging best trial run ID...
WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:610 Could not find MLflow run ID for best trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>. Search failed: Invalid clause(s) in filter string: 'tags.trial_number = '<MagicMock name='mock.create_study()', '.', 'best_trial.number', '' id='', '140077206848368', ''>''. This may be a timing issue - trial runs may not be created/committed yet.
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:389 [LOG_FINAL_METRICS] Completed best trial ID logging
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:399 [LOG_FINAL_METRICS] Checkpoint logging enabled: False (upload_checkpoint=False)
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:432 [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:449 [LOG_FINAL_METRICS] Completed log_final_metrics successfully
INFO     training.hpo.execution.local.sweep:sweep.py:1440 [REFIT] Refit training is disabled in config
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:224 [START_SWEEP_RUN] Context manager exiting normally. run_id=bd92d2ca20c5...
INFO     training.hpo.execution.local.sweep:sweep.py:658 [EARLY HASH] Computed v2 study_key_hash=42ff77fc79adab6d... for folder creation
INFO     training.hpo.core.study:study.py:368 [HPO] Starting optimization for distilbert with checkpointing...
INFO     training.hpo.execution.local.sweep:sweep.py:843 [HPO Setup] k_folds=None, fold_splits_file=None, k_fold config: {'enabled': False}
WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:1057 Using LOCAL MLflow tracking (not Azure ML)
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:1059 To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:143 [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=42ff77fc79adab6d...
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:161 [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=42ff77fc79adab6d..., study_family_hash=ce5a7f9ff304036d...
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:221 [START_SWEEP_RUN] Yielding RunHandle. run_id=d471c7696a36...
INFO     training.hpo.execution.local.sweep:sweep.py:953 [HPO] Parent run d471c7696a36... is now visible in MLflow (status: RUNNING)
INFO     training.hpo.execution.local.sweep:sweep.py:995 Setting Phase 2 tags on parent run d471c7696a36... (data_config=present, train_config=present)
WARNING  training.hpo.execution.local.sweep:sweep.py:530 Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
WARNING  training.hpo.execution.local.sweep:sweep.py:538 Could not set data fingerprint tag: 'Missing tag key: fingerprint.data'
WARNING  training.hpo.execution.local.sweep:sweep.py:545 Could not set eval fingerprint tag: 'Missing tag key: fingerprint.eval'
WARNING  training.hpo.execution.local.sweep:sweep.py:556 Could not set objective direction tag: 'Missing tag key: objective.direction'
WARNING  training.hpo.execution.local.sweep:sweep.py:563 Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
WARNING  training.hpo.execution.local.sweep:sweep.py:570 Could not set artifact.available tag: 'Missing tag key: artifact.available'
INFO     training.hpo.execution.local.sweep:sweep.py:573  Set Phase 2 tags on parent run d471c7696a36...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
INFO     training.hpo.execution.local.sweep:sweep.py:1008  Successfully completed Phase 2 tag setting for parent run d471c7696a36...
INFO     training.hpo.tracking.cleanup:cleanup.py:121 [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:318 [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=d471c7696a36...
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:342 [LOG_FINAL_METRICS] Found 0 completed trials out of 0 total
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:353 [LOG_FINAL_METRICS] Logging metrics: n_trials=0, n_completed_trials=0
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:362 [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=<MagicMock name='mock.create_study().best_value' id='140077206668144'>
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:371 [LOG_FINAL_METRICS] Logging best hyperparameters: <MagicMock name='mock.create_study().best_params' id='140077206783792'>
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:378 [LOG_FINAL_METRICS] Finding and logging best trial run ID...
WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:610 Could not find MLflow run ID for best trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>. Search failed: Invalid clause(s) in filter string: 'tags.trial_number = '<MagicMock name='mock.create_study()', '.', 'best_trial.number', '' id='', '140077206848368', ''>''. This may be a timing issue - trial runs may not be created/committed yet.
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:389 [LOG_FINAL_METRICS] Completed best trial ID logging
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:399 [LOG_FINAL_METRICS] Checkpoint logging enabled: False (upload_checkpoint=False)
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:432 [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:449 [LOG_FINAL_METRICS] Completed log_final_metrics successfully
INFO     training.hpo.execution.local.sweep:sweep.py:1440 [REFIT] Refit training is disabled in config
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:224 [START_SWEEP_RUN] Context manager exiting normally. run_id=d471c7696a36...
______________ TestHPOResumeWorkflow.test_resume_workflow_with_cv ______________
tests/hpo/integration/test_hpo_resume_workflow.py:509: in test_resume_workflow_with_cv
    assert len(study2.trials) > first_trial_count
E   AssertionError: assert 0 > 0
E    +  where 0 = len(<MagicMock name='mock.create_study().trials' id='140077280918304'>)
E    +    where <MagicMock name='mock.create_study().trials' id='140077280918304'> = <MagicMock name='mock.create_study()' id='140077279450096'>.trials
------------------------------ Captured log call -------------------------------
INFO     training.hpo.execution.local.sweep:sweep.py:658 [EARLY HASH] Computed v2 study_key_hash=dc0c421a5056294a... for folder creation
INFO     training.hpo.core.study:study.py:368 [HPO] Starting optimization for distilbert with checkpointing...
WARNING  infrastructure.naming.display_policy:display_policy.py:140 [naming.yaml] Missing required 'run_names' section (/tmp/pytest-of-codespace/pytest-38/test_resume_workflow_with_cv0/config/naming.yaml)
INFO     training.hpo.execution.local.sweep:sweep.py:843 [HPO Setup] k_folds=2, fold_splits_file=None, k_fold config: {'enabled': True, 'n_splits': 2, 'random_seed': 42, 'shuffle': True, 'stratified': True}
INFO     training.hpo.execution.local.sweep:sweep.py:173 [CV Setup]  Created 2 fold splits and saved to /tmp/pytest-of-codespace/pytest-38/test_resume_workflow_with_cv0/outputs/hpo/local/distilbert/study-dc0c421a/fold_splits.json
WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:1057 Using LOCAL MLflow tracking (not Azure ML)
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:1059 To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:143 [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=dc0c421a5056294a...
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:161 [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=dc0c421a5056294a..., study_family_hash=d1aa8bfffdcfdb77...
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:221 [START_SWEEP_RUN] Yielding RunHandle. run_id=e9b822128b94...
INFO     training.hpo.execution.local.sweep:sweep.py:953 [HPO] Parent run e9b822128b94... is now visible in MLflow (status: RUNNING)
INFO     training.hpo.execution.local.sweep:sweep.py:995 Setting Phase 2 tags on parent run e9b822128b94... (data_config=present, train_config=present)
WARNING  training.hpo.execution.local.sweep:sweep.py:530 Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
WARNING  training.hpo.execution.local.sweep:sweep.py:538 Could not set data fingerprint tag: 'Missing tag key: fingerprint.data'
WARNING  training.hpo.execution.local.sweep:sweep.py:545 Could not set eval fingerprint tag: 'Missing tag key: fingerprint.eval'
WARNING  training.hpo.execution.local.sweep:sweep.py:556 Could not set objective direction tag: 'Missing tag key: objective.direction'
WARNING  training.hpo.execution.local.sweep:sweep.py:563 Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
WARNING  training.hpo.execution.local.sweep:sweep.py:570 Could not set artifact.available tag: 'Missing tag key: artifact.available'
INFO     training.hpo.execution.local.sweep:sweep.py:573  Set Phase 2 tags on parent run e9b822128b94...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
INFO     training.hpo.execution.local.sweep:sweep.py:1008  Successfully completed Phase 2 tag setting for parent run e9b822128b94...
INFO     training.hpo.tracking.cleanup:cleanup.py:121 [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:318 [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=e9b822128b94...
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:342 [LOG_FINAL_METRICS] Found 0 completed trials out of 0 total
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:353 [LOG_FINAL_METRICS] Logging metrics: n_trials=0, n_completed_trials=0
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:362 [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=<MagicMock name='mock.create_study().best_value' id='140077206668144'>
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:371 [LOG_FINAL_METRICS] Logging best hyperparameters: <MagicMock name='mock.create_study().best_params' id='140077206783792'>
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:378 [LOG_FINAL_METRICS] Finding and logging best trial run ID...
WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:610 Could not find MLflow run ID for best trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>. Search failed: Invalid clause(s) in filter string: 'tags.trial_number = '<MagicMock name='mock.create_study()', '.', 'best_trial.number', '' id='', '140077206848368', ''>''. This may be a timing issue - trial runs may not be created/committed yet.
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:389 [LOG_FINAL_METRICS] Completed best trial ID logging
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:399 [LOG_FINAL_METRICS] Checkpoint logging enabled: False (upload_checkpoint=False)
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:432 [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:449 [LOG_FINAL_METRICS] Completed log_final_metrics successfully
INFO     training.hpo.execution.local.sweep:sweep.py:1440 [REFIT] Refit training is disabled in config
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:224 [START_SWEEP_RUN] Context manager exiting normally. run_id=e9b822128b94...
INFO     training.hpo.execution.local.sweep:sweep.py:658 [EARLY HASH] Computed v2 study_key_hash=dc0c421a5056294a... for folder creation
INFO     training.hpo.core.study:study.py:368 [HPO] Starting optimization for distilbert with checkpointing...
INFO     training.hpo.execution.local.sweep:sweep.py:843 [HPO Setup] k_folds=2, fold_splits_file=None, k_fold config: {'enabled': True, 'n_splits': 2, 'random_seed': 42, 'shuffle': True, 'stratified': True}
INFO     training.hpo.execution.local.sweep:sweep.py:173 [CV Setup]  Created 2 fold splits and saved to /tmp/pytest-of-codespace/pytest-38/test_resume_workflow_with_cv0/outputs/hpo/local/distilbert/study-dc0c421a/fold_splits.json
WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:1057 Using LOCAL MLflow tracking (not Azure ML)
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:1059 To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:143 [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=dc0c421a5056294a...
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:161 [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=dc0c421a5056294a..., study_family_hash=d1aa8bfffdcfdb77...
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:221 [START_SWEEP_RUN] Yielding RunHandle. run_id=6a02791b8354...
INFO     training.hpo.execution.local.sweep:sweep.py:953 [HPO] Parent run 6a02791b8354... is now visible in MLflow (status: RUNNING)
INFO     training.hpo.execution.local.sweep:sweep.py:995 Setting Phase 2 tags on parent run 6a02791b8354... (data_config=present, train_config=present)
WARNING  training.hpo.execution.local.sweep:sweep.py:530 Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
WARNING  training.hpo.execution.local.sweep:sweep.py:538 Could not set data fingerprint tag: 'Missing tag key: fingerprint.data'
WARNING  training.hpo.execution.local.sweep:sweep.py:545 Could not set eval fingerprint tag: 'Missing tag key: fingerprint.eval'
WARNING  training.hpo.execution.local.sweep:sweep.py:556 Could not set objective direction tag: 'Missing tag key: objective.direction'
WARNING  training.hpo.execution.local.sweep:sweep.py:563 Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
WARNING  training.hpo.execution.local.sweep:sweep.py:570 Could not set artifact.available tag: 'Missing tag key: artifact.available'
INFO     training.hpo.execution.local.sweep:sweep.py:573  Set Phase 2 tags on parent run 6a02791b8354...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
INFO     training.hpo.execution.local.sweep:sweep.py:1008  Successfully completed Phase 2 tag setting for parent run 6a02791b8354...
INFO     training.hpo.tracking.cleanup:cleanup.py:121 [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:318 [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=6a02791b8354...
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:342 [LOG_FINAL_METRICS] Found 0 completed trials out of 0 total
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:353 [LOG_FINAL_METRICS] Logging metrics: n_trials=0, n_completed_trials=0
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:362 [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=<MagicMock name='mock.create_study().best_value' id='140077206668144'>
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:371 [LOG_FINAL_METRICS] Logging best hyperparameters: <MagicMock name='mock.create_study().best_params' id='140077206783792'>
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:378 [LOG_FINAL_METRICS] Finding and logging best trial run ID...
WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:610 Could not find MLflow run ID for best trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>. Search failed: Invalid clause(s) in filter string: 'tags.trial_number = '<MagicMock name='mock.create_study()', '.', 'best_trial.number', '' id='', '140077206848368', ''>''. This may be a timing issue - trial runs may not be created/committed yet.
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:389 [LOG_FINAL_METRICS] Completed best trial ID logging
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:399 [LOG_FINAL_METRICS] Checkpoint logging enabled: False (upload_checkpoint=False)
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:432 [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:449 [LOG_FINAL_METRICS] Completed log_final_metrics successfully
INFO     training.hpo.execution.local.sweep:sweep.py:1440 [REFIT] Refit training is disabled in config
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:224 [START_SWEEP_RUN] Context manager exiting normally. run_id=6a02791b8354...
_ TestDisableAutoOptunaMark.test_disable_auto_optuna_mark_false_enables_marking _
tests/hpo/integration/test_smoke_yaml_options.py:298: in test_disable_auto_optuna_mark_false_enables_marking
    assert len(running_trials) == 0, "RUNNING trials should be marked FAILED when disable_auto_optuna_mark=false"
E   AssertionError: RUNNING trials should be marked FAILED when disable_auto_optuna_mark=false
E   assert 1 == 0
E    +  where 1 = len([FrozenTrial(number=0, state=<TrialState.RUNNING: 0>, values=None, datetime_start=datetime.datetime(2026, 1, 18, 0, 31, 20, 651883), datetime_complete=None, params={}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={}, trial_id=1, value=None)])
__ TestTrialExecutionWithCV.test_trial_execution_with_cv_creates_nested_runs ___
tests/hpo/integration/test_trial_execution.py:203: in test_trial_execution_with_cv_creates_nested_runs
    avg_metric, fold_metrics = run_training_trial_with_cv(
src/training/hpo/execution/local/cv.py:271: in run_training_trial_with_cv
    raise RuntimeError(
E   RuntimeError: Cannot create trial in v2 study folder study-abc12345 without trial_key_hash. study_key_hash=NO, trial_key_hash=NO. This is a critical error that must be fixed.
------------------------------ Captured log call -------------------------------
WARNING  infrastructure.tracking.mlflow.hash_utils:hash_utils.py:445 Could not get or compute study_key_hash. study_key_hash=missing, hpo_parent_run_id=provided, configs=missing
WARNING  infrastructure.naming.display_policy:display_policy.py:89 [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-38/test_trial_execution_with_cv_c0/config/naming.yaml, using empty policy
WARNING  infrastructure.tracking.mlflow.hash_utils:hash_utils.py:515 Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=missing, hyperparameters=available
WARNING  training.hpo.execution.local.cv:cv.py:511 Could not compute trial_key_hash from configs
INFO     infrastructure.naming.mlflow.tags_registry:tags_registry.py:224 [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_trial_execution_with_cv_c0/config/tags.yaml, using defaults
WARNING  infrastructure.tracking.mlflow.hash_utils:hash_utils.py:445 Could not get or compute study_key_hash. study_key_hash=missing, hpo_parent_run_id=provided, configs=missing
ERROR    training.hpo.execution.local.cv:cv.py:155 In v2 study folder study-abc12345 but trial_key_hash is None. study_key_hash=NO. Will attempt to compute hash.
WARNING  training.hpo.execution.local.cv:cv.py:198 In v2 study folder but missing hashes: study_key_hash=NO, trial_key_hash=NO
___ TestTrialExecutionWithCV.test_trial_execution_with_cv_aggregates_metrics ___
tests/hpo/integration/test_trial_execution.py:341: in test_trial_execution_with_cv_aggregates_metrics
    avg_metric, fold_metrics = run_training_trial_with_cv(
src/training/hpo/execution/local/cv.py:271: in run_training_trial_with_cv
    raise RuntimeError(
E   RuntimeError: Cannot create trial in v2 study folder study-abc12345 without trial_key_hash. study_key_hash=YES, trial_key_hash=NO. This is a critical error that must be fixed.
------------------------------ Captured log call -------------------------------
WARNING  infrastructure.naming.display_policy:display_policy.py:89 [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-38/test_trial_execution_with_cv_a0/config/naming.yaml, using empty policy
WARNING  infrastructure.tracking.mlflow.hash_utils:hash_utils.py:515 Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
WARNING  training.hpo.execution.local.cv:cv.py:511 Could not compute trial_key_hash from configs
INFO     infrastructure.naming.mlflow.tags_registry:tags_registry.py:224 [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_trial_execution_with_cv_a0/config/tags.yaml, using defaults
WARNING  infrastructure.tracking.mlflow.hash_utils:hash_utils.py:515 Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
WARNING  training.hpo.execution.local.cv:cv.py:145 Could not compute trial_key_hash
ERROR    training.hpo.execution.local.cv:cv.py:155 In v2 study folder study-abc12345 but trial_key_hash is None. study_key_hash=YES. Will attempt to compute hash.
WARNING  training.hpo.execution.local.cv:cv.py:198 In v2 study folder but missing hashes: study_key_hash=YES, trial_key_hash=NO
WARNING  training.hpo.execution.local.cv:cv.py:213 In v2 study folder study-abc12345 but trial_key_hash is None. Attempting to recompute from trial_params...
WARNING  infrastructure.tracking.mlflow.hash_utils:hash_utils.py:515 Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
______ TestTrialExecutionWithCV.test_trial_execution_with_cv_output_paths ______
tests/hpo/integration/test_trial_execution.py:420: in test_trial_execution_with_cv_output_paths
    avg_metric, fold_metrics = run_training_trial_with_cv(
src/training/hpo/execution/local/cv.py:271: in run_training_trial_with_cv
    raise RuntimeError(
E   RuntimeError: Cannot create trial in v2 study folder study-abc12345 without trial_key_hash. study_key_hash=YES, trial_key_hash=NO. This is a critical error that must be fixed.
------------------------------ Captured log call -------------------------------
WARNING  infrastructure.naming.display_policy:display_policy.py:89 [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-38/test_trial_execution_with_cv_o0/config/naming.yaml, using empty policy
WARNING  infrastructure.tracking.mlflow.hash_utils:hash_utils.py:515 Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
WARNING  training.hpo.execution.local.cv:cv.py:511 Could not compute trial_key_hash from configs
INFO     infrastructure.naming.mlflow.tags_registry:tags_registry.py:224 [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_trial_execution_with_cv_o0/config/tags.yaml, using defaults
WARNING  infrastructure.tracking.mlflow.hash_utils:hash_utils.py:515 Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
WARNING  training.hpo.execution.local.cv:cv.py:145 Could not compute trial_key_hash
ERROR    training.hpo.execution.local.cv:cv.py:155 In v2 study folder study-abc12345 but trial_key_hash is None. study_key_hash=YES. Will attempt to compute hash.
WARNING  training.hpo.execution.local.cv:cv.py:198 In v2 study folder but missing hashes: study_key_hash=YES, trial_key_hash=NO
WARNING  training.hpo.execution.local.cv:cv.py:213 In v2 study folder study-abc12345 but trial_key_hash is None. Attempting to recompute from trial_params...
WARNING  infrastructure.tracking.mlflow.hash_utils:hash_utils.py:515 Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
___ TestTrialExecutionWithCV.test_trial_execution_with_cv_smoke_yaml_params ____
tests/hpo/integration/test_trial_execution.py:490: in test_trial_execution_with_cv_smoke_yaml_params
    avg_metric, fold_metrics = run_training_trial_with_cv(
src/training/hpo/execution/local/cv.py:271: in run_training_trial_with_cv
    raise RuntimeError(
E   RuntimeError: Cannot create trial in v2 study folder study-abc12345 without trial_key_hash. study_key_hash=YES, trial_key_hash=NO. This is a critical error that must be fixed.
------------------------------ Captured log call -------------------------------
WARNING  infrastructure.naming.display_policy:display_policy.py:89 [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-38/test_trial_execution_with_cv_s0/config/naming.yaml, using empty policy
WARNING  infrastructure.tracking.mlflow.hash_utils:hash_utils.py:515 Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
WARNING  training.hpo.execution.local.cv:cv.py:511 Could not compute trial_key_hash from configs
INFO     infrastructure.naming.mlflow.tags_registry:tags_registry.py:224 [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_trial_execution_with_cv_s0/config/tags.yaml, using defaults
WARNING  infrastructure.tracking.mlflow.hash_utils:hash_utils.py:515 Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
WARNING  training.hpo.execution.local.cv:cv.py:145 Could not compute trial_key_hash
ERROR    training.hpo.execution.local.cv:cv.py:155 In v2 study folder study-abc12345 but trial_key_hash is None. study_key_hash=YES. Will attempt to compute hash.
WARNING  training.hpo.execution.local.cv:cv.py:198 In v2 study folder but missing hashes: study_key_hash=YES, trial_key_hash=NO
WARNING  training.hpo.execution.local.cv:cv.py:213 In v2 study folder study-abc12345 but trial_key_hash is None. Attempting to recompute from trial_params...
WARNING  infrastructure.tracking.mlflow.hash_utils:hash_utils.py:515 Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
___ TestBenchmarkTrackingEnabled.test_benchmark_tracking_enabled_creates_run ___
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1967: in _exec_single_context
    self.dialect.do_execute(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py:952: in do_execute
    cursor.execute(statement, parameters)
E   sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:160: in make_managed_session
    yield session
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:586: in create_run
    inputs_list = self._get_run_inputs(session, [run_id])
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:626: in _get_run_inputs
    ).all()
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py:2704: in all
    return self._iter().all()  # type: ignore
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py:2857: in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py:2351: in execute
    return self._execute_internal(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py:2228: in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py:577: in orm_pre_session_exec
    session._autoflush()
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py:3051: in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py:3040: in _autoflush
    self.flush()
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py:4331: in flush
    self._flush(objects)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py:4466: in _flush
    with util.safe_reraise():
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py:224: in __exit__
    raise exc_value.with_traceback(exc_tb)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py:4427: in _flush
    flush_context.execute()
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py:466: in execute
    rec.execute(self)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py:642: in execute
    util.preloaded.orm_persistence.save_obj(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py:93: in save_obj
    _emit_insert_statements(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py:1048: in _emit_insert_statements
    result = connection.execute(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1419: in execute
    return meth(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py:527: in _execute_on_connection
    return connection._execute_clauseelement(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1641: in _execute_clauseelement
    ret = self._execute_context(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1846: in _execute_context
    return self._exec_single_context(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1986: in _exec_single_context
    self._handle_dbapi_exception(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py:2363: in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1967: in _exec_single_context
    self.dialect.do_execute(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py:952: in do_execute
    cursor.execute(statement, parameters)
E   sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
E   (sqlite3.OperationalError) attempt to write a readonly database
E   [SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
E   [parameters: ('1f6ad882e9674bcdaf62bd4d3d3a3e5c', 'nebulous-lark-516', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696283447, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/1f6ad882e9674bcdaf62bd4d3d3a3e5c/artifacts', '1')]
E   (Background on this error at: https://sqlalche.me/e/20/e3q8)

The above exception was the direct cause of the following exception:
/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/trackers/benchmark_tracker.py:246: in start_benchmark_run
    mlflow.set_tags(tags)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/fluent.py:1404: in set_tags
    run_id = _get_or_start_run().info.run_id
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/fluent.py:3024: in _get_or_start_run
    return start_run()
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/fluent.py:475: in start_run
    active_run_obj = client.create_run(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py:479: in create_run
    return self._tracking_client.create_run(experiment_id, start_time, tags, run_name)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/telemetry/track.py:30: in wrapper
    result = func(*args, **kwargs)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py:183: in create_run
    return self.store.create_run(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:540: in create_run
    with self.ManagedSessionMaker() as session:
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:153: in __exit__
    self.gen.throw(typ, value, traceback)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:171: in make_managed_session
    raise MlflowException(message=e, error_code=TEMPORARILY_UNAVAILABLE) from e
E   mlflow.exceptions.MlflowException: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
E   (sqlite3.OperationalError) attempt to write a readonly database
E   [SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
E   [parameters: ('1f6ad882e9674bcdaf62bd4d3d3a3e5c', 'nebulous-lark-516', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696283447, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/1f6ad882e9674bcdaf62bd4d3d3a3e5c/artifacts', '1')]
E   (Background on this error at: https://sqlalche.me/e/20/e3q8)

During handling of the above exception, another exception occurred:
/workspaces/resume-ner-azureml/tests/tracking/integration/test_tracking_config_enabled.py:76: in test_benchmark_tracking_enabled_creates_run
    assert handle is not None
E   assert None is not None
------------------------------ Captured log call -------------------------------
WARNING  infrastructure.tracking.mlflow.trackers.base_tracker:base_tracker.py:74 Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696283410, 1768696283410)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
WARNING  infrastructure.tracking.mlflow.trackers.base_tracker:base_tracker.py:75 Continuing without MLflow tracking...
INFO     infrastructure.tracking.mlflow.trackers.benchmark_tracker:benchmark_tracker.py:158 [START_BENCHMARK_RUN] Building tags: context=None, study_key_hash=None..., trial_key_hash=None..., context.model=None, context.process_type=None
INFO     infrastructure.naming.mlflow.tags_registry:tags_registry.py:224 [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_benchmark_tracking_enable0/config/tags.yaml, using defaults
INFO     infrastructure.tracking.mlflow.trackers.benchmark_tracker:benchmark_tracker.py:188 [START_BENCHMARK_RUN] Built tags, grouping tags present: study_key_hash=False, trial_key_hash=False, code.model=unknown, code.stage=unknown
WARNING  infrastructure.tracking.mlflow.trackers.benchmark_tracker:benchmark_tracker.py:229 [START_BENCHMARK_RUN] No valid parent run IDs found. Received: trial=None, refit=None, sweep=None. These may be timestamps or None - will query MLflow if trial_key_hash available.
WARNING  infrastructure.tracking.mlflow.trackers.benchmark_tracker:benchmark_tracker.py:373 MLflow tracking failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('1f6ad882e9674bcdaf62bd4d3d3a3e5c', 'nebulous-lark-516', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696283447, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/1f6ad882e9674bcdaf62bd4d3d3a3e5c/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
WARNING  infrastructure.tracking.mlflow.trackers.benchmark_tracker:benchmark_tracker.py:374 Continuing benchmarking without MLflow tracking...
____ TestTrainingTrackingEnabled.test_training_tracking_enabled_creates_run ____
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1967: in _exec_single_context
    self.dialect.do_execute(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py:952: in do_execute
    cursor.execute(statement, parameters)
E   sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:160: in make_managed_session
    yield session
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:586: in create_run
    inputs_list = self._get_run_inputs(session, [run_id])
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:626: in _get_run_inputs
    ).all()
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py:2704: in all
    return self._iter().all()  # type: ignore
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py:2857: in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py:2351: in execute
    return self._execute_internal(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py:2228: in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py:577: in orm_pre_session_exec
    session._autoflush()
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py:3051: in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py:3040: in _autoflush
    self.flush()
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py:4331: in flush
    self._flush(objects)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py:4466: in _flush
    with util.safe_reraise():
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py:224: in __exit__
    raise exc_value.with_traceback(exc_tb)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py:4427: in _flush
    flush_context.execute()
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py:466: in execute
    rec.execute(self)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py:642: in execute
    util.preloaded.orm_persistence.save_obj(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py:93: in save_obj
    _emit_insert_statements(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py:1048: in _emit_insert_statements
    result = connection.execute(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1419: in execute
    return meth(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py:527: in _execute_on_connection
    return connection._execute_clauseelement(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1641: in _execute_clauseelement
    ret = self._execute_context(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1846: in _execute_context
    return self._exec_single_context(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1986: in _exec_single_context
    self._handle_dbapi_exception(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py:2363: in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1967: in _exec_single_context
    self.dialect.do_execute(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py:952: in do_execute
    cursor.execute(statement, parameters)
E   sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
E   (sqlite3.OperationalError) attempt to write a readonly database
E   [SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
E   [parameters: ('a394c0b04a5a4d78be1230c0b7d4ef3d', 'upbeat-kit-767', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696284671, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/a394c0b04a5a4d78be1230c0b7d4ef3d/artifacts', '1')]
E   (Background on this error at: https://sqlalche.me/e/20/e3q8)

The above exception was the direct cause of the following exception:
src/infrastructure/tracking/mlflow/trackers/training_tracker.py:133: in start_training_run
    mlflow.set_tags(tags)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/fluent.py:1404: in set_tags
    run_id = _get_or_start_run().info.run_id
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/fluent.py:3024: in _get_or_start_run
    return start_run()
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/fluent.py:475: in start_run
    active_run_obj = client.create_run(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py:479: in create_run
    return self._tracking_client.create_run(experiment_id, start_time, tags, run_name)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/telemetry/track.py:30: in wrapper
    result = func(*args, **kwargs)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py:183: in create_run
    return self.store.create_run(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:540: in create_run
    with self.ManagedSessionMaker() as session:
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:153: in __exit__
    self.gen.throw(typ, value, traceback)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:171: in make_managed_session
    raise MlflowException(message=e, error_code=TEMPORARILY_UNAVAILABLE) from e
E   mlflow.exceptions.MlflowException: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
E   (sqlite3.OperationalError) attempt to write a readonly database
E   [SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
E   [parameters: ('a394c0b04a5a4d78be1230c0b7d4ef3d', 'upbeat-kit-767', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696284671, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/a394c0b04a5a4d78be1230c0b7d4ef3d/artifacts', '1')]
E   (Background on this error at: https://sqlalche.me/e/20/e3q8)

During handling of the above exception, another exception occurred:
tests/tracking/integration/test_tracking_config_enabled.py:148: in test_training_tracking_enabled_creates_run
    assert handle is not None
E   assert None is not None
------------------------------ Captured log call -------------------------------
WARNING  infrastructure.tracking.mlflow.trackers.base_tracker:base_tracker.py:74 Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696284663, 1768696284663)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
WARNING  infrastructure.tracking.mlflow.trackers.base_tracker:base_tracker.py:75 Continuing without MLflow tracking...
INFO     infrastructure.naming.mlflow.tags_registry:tags_registry.py:224 [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_training_tracking_enabled0/config/tags.yaml, using defaults
WARNING  infrastructure.tracking.mlflow.trackers.training_tracker:training_tracker.py:197 MLflow tracking failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('a394c0b04a5a4d78be1230c0b7d4ef3d', 'upbeat-kit-767', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696284671, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/a394c0b04a5a4d78be1230c0b7d4ef3d/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
WARNING  infrastructure.tracking.mlflow.trackers.training_tracker:training_tracker.py:198 Continuing training without MLflow tracking...
__ TestConversionTrackingEnabled.test_conversion_tracking_enabled_creates_run __
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1967: in _exec_single_context
    self.dialect.do_execute(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py:952: in do_execute
    cursor.execute(statement, parameters)
E   sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:160: in make_managed_session
    yield session
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:586: in create_run
    inputs_list = self._get_run_inputs(session, [run_id])
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:626: in _get_run_inputs
    ).all()
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py:2704: in all
    return self._iter().all()  # type: ignore
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py:2857: in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py:2351: in execute
    return self._execute_internal(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py:2228: in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py:577: in orm_pre_session_exec
    session._autoflush()
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py:3051: in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py:3040: in _autoflush
    self.flush()
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py:4331: in flush
    self._flush(objects)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py:4466: in _flush
    with util.safe_reraise():
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py:224: in __exit__
    raise exc_value.with_traceback(exc_tb)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py:4427: in _flush
    flush_context.execute()
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py:466: in execute
    rec.execute(self)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py:642: in execute
    util.preloaded.orm_persistence.save_obj(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py:93: in save_obj
    _emit_insert_statements(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py:1048: in _emit_insert_statements
    result = connection.execute(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1419: in execute
    return meth(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py:527: in _execute_on_connection
    return connection._execute_clauseelement(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1641: in _execute_clauseelement
    ret = self._execute_context(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1846: in _execute_context
    return self._exec_single_context(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1986: in _exec_single_context
    self._handle_dbapi_exception(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py:2363: in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py:1967: in _exec_single_context
    self.dialect.do_execute(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py:952: in do_execute
    cursor.execute(statement, parameters)
E   sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
E   (sqlite3.OperationalError) attempt to write a readonly database
E   [SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
E   [parameters: ('62e396c0710a44b3bcaba0cc27138242', 'big-kit-958', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696286045, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/62e396c0710a44b3bcaba0cc27138242/artifacts', '1')]
E   (Background on this error at: https://sqlalche.me/e/20/e3q8)

The above exception was the direct cause of the following exception:
src/infrastructure/tracking/mlflow/trackers/conversion_tracker.py:116: in start_conversion_run
    mlflow.set_tags(tags)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/fluent.py:1404: in set_tags
    run_id = _get_or_start_run().info.run_id
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/fluent.py:3024: in _get_or_start_run
    return start_run()
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/fluent.py:475: in start_run
    active_run_obj = client.create_run(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py:479: in create_run
    return self._tracking_client.create_run(experiment_id, start_time, tags, run_name)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/telemetry/track.py:30: in wrapper
    result = func(*args, **kwargs)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py:183: in create_run
    return self.store.create_run(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py:540: in create_run
    with self.ManagedSessionMaker() as session:
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:153: in __exit__
    self.gen.throw(typ, value, traceback)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py:171: in make_managed_session
    raise MlflowException(message=e, error_code=TEMPORARILY_UNAVAILABLE) from e
E   mlflow.exceptions.MlflowException: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
E   (sqlite3.OperationalError) attempt to write a readonly database
E   [SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
E   [parameters: ('62e396c0710a44b3bcaba0cc27138242', 'big-kit-958', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696286045, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/62e396c0710a44b3bcaba0cc27138242/artifacts', '1')]
E   (Background on this error at: https://sqlalche.me/e/20/e3q8)

During handling of the above exception, another exception occurred:
tests/tracking/integration/test_tracking_config_enabled.py:218: in test_conversion_tracking_enabled_creates_run
    assert handle is not None
E   assert None is not None
------------------------------ Captured log call -------------------------------
WARNING  infrastructure.tracking.mlflow.trackers.base_tracker:base_tracker.py:74 Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696286038, 1768696286038)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
WARNING  infrastructure.tracking.mlflow.trackers.base_tracker:base_tracker.py:75 Continuing without MLflow tracking...
INFO     infrastructure.naming.mlflow.tags_registry:tags_registry.py:224 [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_conversion_tracking_enabl0/config/tags.yaml, using defaults
WARNING  infrastructure.tracking.mlflow.trackers.conversion_tracker:conversion_tracker.py:151 MLflow tracking failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('62e396c0710a44b3bcaba0cc27138242', 'big-kit-958', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696286045, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/62e396c0710a44b3bcaba0cc27138242/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
WARNING  infrastructure.tracking.mlflow.trackers.conversion_tracker:conversion_tracker.py:152 Continuing conversion without MLflow tracking...
______ TestCVTrialRunHashComputation.test_priority_2_get_from_parent_tags ______
tests/training/hpo/unit/test_cv_hash_computation.py:168: in test_priority_2_get_from_parent_tags
    assert tags.get("code.study_key_hash") == parent_study_key_hash
E   AssertionError: assert '8c4c43347d560c8ac472b5a715bac7d333bdfabbcad47278bf4f468d1a3ae15d' == 'parent_study_hash_aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa'
E     
E     - parent_study_hash_aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa
E     + 8c4c43347d560c8ac472b5a715bac7d333bdfabbcad47278bf4f468d1a3ae15d
------------------------------ Captured log call -------------------------------
WARNING  infrastructure.naming.display_policy:display_policy.py:89 [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-38/test_priority_2_get_from_paren0/workspace/config/naming.yaml, using empty policy
___________ TestInferencePerformance.test_tokenization_does_not_hang ___________
tests/unit/api/test_inference_performance.py:205: in test_tokenization_does_not_hang
    assert elapsed < 1.0, f"Tokenization took {elapsed:.2f} seconds, expected < 1.0"
E   AssertionError: Tokenization took 1.22 seconds, expected < 1.0
E   assert 1.2226388454437256 < 1.0
____________________________ test_full_workflow_e2e ____________________________
tests/workflows/test_full_workflow_e2e.py:246: in test_full_workflow_e2e
    assert len(study.trials) > 0
E   AssertionError: assert 0 > 0
E    +  where 0 = len(<MagicMock name='mock.create_study().trials' id='140077280918304'>)
E    +    where <MagicMock name='mock.create_study().trials' id='140077280918304'> = <MagicMock name='mock.create_study()' id='140077279450096'>.trials
------------------------------ Captured log call -------------------------------
INFO     training.hpo.execution.local.sweep:sweep.py:658 [EARLY HASH] Computed v2 study_key_hash=79a646c77bb15106... for folder creation
INFO     training.hpo.core.study:study.py:368 [HPO] Starting optimization for distilroberta with checkpointing...
INFO     infrastructure.naming.mlflow.config:config.py:365 [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
INFO     infrastructure.naming.mlflow.config:config.py:372 [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
INFO     orchestration.jobs.tracking.index.version_counter:version_counter.py:92 [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:7202413b67e3126708d0544fb08f79f5b50640effd450..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
INFO     orchestration.jobs.tracking.index.version_counter:version_counter.py:109 [Reserve Version] Loaded 25 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
INFO     orchestration.jobs.tracking.index.version_counter:version_counter.py:157 [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0
INFO     orchestration.jobs.tracking.index.version_counter:version_counter.py:201 [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)
INFO     orchestration.jobs.tracking.index.version_counter:version_counter.py:229 [Reserve Version]  Successfully reserved version 1 for counter_key resume-ner:hpo:7202413b67e3126708d0544fb08f79f5b50... (run_id: pending_2026...)
INFO     training.hpo.execution.local.sweep:sweep.py:843 [HPO Setup] k_folds=5, fold_splits_file=None, k_fold config: {'enabled': True, 'n_splits': 5, 'random_seed': 42, 'shuffle': True, 'stratified': True}
INFO     training.hpo.execution.local.sweep:sweep.py:173 [CV Setup]  Created 5 fold splits and saved to /workspaces/resume-ner-azureml/outputs/hpo/local/distilroberta/study-79a646c7/fold_splits.json
WARNING  infrastructure.tracking.mlflow.trackers.base_tracker:base_tracker.py:74 Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('resume_ner_baseline-hpo-distilroberta', None, 'active', 1768696299439, 1768696299439)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
WARNING  infrastructure.tracking.mlflow.trackers.base_tracker:base_tracker.py:75 Continuing without MLflow tracking...
WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:1057 Using LOCAL MLflow tracking (not Azure ML)
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:1059 To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
ERROR    infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:228 [START_SWEEP_RUN] MLflow tracking failed: No Experiment with id=546145851228734692 exists
ERROR    infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:229 [START_SWEEP_RUN] Traceback: Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/trackers/sweep_tracker.py", line 120, in start_sweep_run
    with mlflow.start_run(run_name=run_name) as parent_run:
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/fluent.py", line 475, in start_run
    active_run_obj = client.create_run(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 479, in create_run
    return self._tracking_client.create_run(experiment_id, start_time, tags, run_name)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/telemetry/track.py", line 30, in wrapper
    result = func(*args, **kwargs)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 183, in create_run
    return self.store.create_run(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 541, in create_run
    experiment = self.get_experiment(experiment_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 462, in get_experiment
    return self._get_experiment(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 440, in _get_experiment
    raise MlflowException(
mlflow.exceptions.MlflowException: No Experiment with id=546145851228734692 exists

WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:231 Continuing HPO without MLflow tracking...
INFO     training.hpo.execution.local.sweep:sweep.py:995 Setting Phase 2 tags on parent run None... (data_config=present, train_config=present)
WARNING  training.hpo.execution.local.sweep:sweep.py:467 _set_phase2_hpo_tags: parent_run_id is empty, skipping
INFO     training.hpo.execution.local.sweep:sweep.py:1008  Successfully completed Phase 2 tag setting for parent run None...
INFO     training.hpo.tracking.cleanup:cleanup.py:121 [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.
INFO     training.hpo.execution.local.sweep:sweep.py:1115 [REFIT] Starting refit training for best trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>
WARNING  training.hpo.execution.local.sweep:sweep.py:1205 [REFIT] Could not determine trial_key_hash from best trial run tags. Refit run may not match trial run.
INFO     training.hpo.execution.local.refit:refit.py:107 [REFIT] Starting refit training for trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'> with hyperparameters: <MagicMock name='mock.create_study().best_trial.params' id='140077206865856'>
INFO     training.hpo.execution.local.refit:refit.py:130 [REFIT] Computed trial_id="trial_<MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>_20260118_003139", run_id='20260118_003139', trial_number=<MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>
WARNING  infrastructure.tracking.mlflow.hash_utils:hash_utils.py:515 Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
WARNING  training.hpo.execution.local.sweep:sweep.py:1304 [REFIT] Refit training failed: Cannot create refit in v2 study folder study-79a646c7 without trial_key_hash. study_key_hash=present, trial_key_hash=missing, refit_params=missing. Hash computation from trial parameters may have failed.. Continuing without refit checkpoint.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py", line 1244, in run_local_hpo_sweep
    refit_metrics, refit_checkpoint_dir, refit_run_id = run_refit_training(
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 240, in run_refit_training
    raise RuntimeError(
RuntimeError: Cannot create refit in v2 study folder study-79a646c7 without trial_key_hash. study_key_hash=present, trial_key_hash=missing, refit_params=missing. Hash computation from trial parameters may have failed.
INFO     infrastructure.naming.mlflow.tags_registry:tags_registry.py:224 [Tags Registry] Config file not found at /workspaces/resume-ner-azureml/outputs/hpo/config/tags.yaml, using defaults
WARNING  training.hpo.execution.local.sweep:sweep.py:1428 [REFIT] No refit_run_id available to mark as FINISHED
_____________ TestNotebookE2E_Core.test_hpo_sweep_execution_mocked _____________
tests/workflows/test_notebook_01_e2e.py:519: in test_hpo_sweep_execution_mocked
    assert len(study.trials) > 0
E   AssertionError: assert 0 > 0
E    +  where 0 = len(<MagicMock name='mock.create_study().trials' id='140077280918304'>)
E    +    where <MagicMock name='mock.create_study().trials' id='140077280918304'> = <MagicMock name='mock.create_study()' id='140077279450096'>.trials
------------------------------ Captured log call -------------------------------
INFO     training.hpo.execution.local.sweep:sweep.py:658 [EARLY HASH] Computed v2 study_key_hash=c3659feaead8e1ec... for folder creation
INFO     training.hpo.core.study:study.py:368 [HPO] Starting optimization for distilbert with checkpointing...
INFO     infrastructure.naming.mlflow.config:config.py:365 [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
INFO     infrastructure.naming.mlflow.config:config.py:372 [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
INFO     orchestration.jobs.tracking.index.version_counter:version_counter.py:92 [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:3639b270abbc20a6f2b828357499e2f4d7eaa262dcff6..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
INFO     orchestration.jobs.tracking.index.version_counter:version_counter.py:109 [Reserve Version] Loaded 26 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
INFO     orchestration.jobs.tracking.index.version_counter:version_counter.py:157 [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0
INFO     orchestration.jobs.tracking.index.version_counter:version_counter.py:201 [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)
INFO     orchestration.jobs.tracking.index.version_counter:version_counter.py:229 [Reserve Version]  Successfully reserved version 1 for counter_key resume-ner:hpo:3639b270abbc20a6f2b828357499e2f4d7e... (run_id: pending_2026...)
INFO     training.hpo.execution.local.sweep:sweep.py:843 [HPO Setup] k_folds=2, fold_splits_file=None, k_fold config: {'enabled': True, 'n_splits': 2, 'random_seed': 42, 'shuffle': True, 'stratified': True}
INFO     training.hpo.execution.local.sweep:sweep.py:173 [CV Setup]  Created 2 fold splits and saved to /workspaces/resume-ner-azureml/outputs/hpo/local/distilbert/study-c3659fea/fold_splits.json
WARNING  infrastructure.tracking.mlflow.trackers.base_tracker:base_tracker.py:74 Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('resume_ner_baseline-hpo-distilbert', None, 'active', 1768696299539, 1768696299539)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
WARNING  infrastructure.tracking.mlflow.trackers.base_tracker:base_tracker.py:75 Continuing without MLflow tracking...
WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:1057 Using LOCAL MLflow tracking (not Azure ML)
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:1059 To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
ERROR    infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:228 [START_SWEEP_RUN] MLflow tracking failed: No Experiment with id=679667700195200030 exists
ERROR    infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:229 [START_SWEEP_RUN] Traceback: Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/trackers/sweep_tracker.py", line 120, in start_sweep_run
    with mlflow.start_run(run_name=run_name) as parent_run:
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/fluent.py", line 475, in start_run
    active_run_obj = client.create_run(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 479, in create_run
    return self._tracking_client.create_run(experiment_id, start_time, tags, run_name)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/telemetry/track.py", line 30, in wrapper
    result = func(*args, **kwargs)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 183, in create_run
    return self.store.create_run(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 541, in create_run
    experiment = self.get_experiment(experiment_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 462, in get_experiment
    return self._get_experiment(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 440, in _get_experiment
    raise MlflowException(
mlflow.exceptions.MlflowException: No Experiment with id=679667700195200030 exists

WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:231 Continuing HPO without MLflow tracking...
INFO     training.hpo.execution.local.sweep:sweep.py:995 Setting Phase 2 tags on parent run None... (data_config=present, train_config=present)
WARNING  training.hpo.execution.local.sweep:sweep.py:467 _set_phase2_hpo_tags: parent_run_id is empty, skipping
INFO     training.hpo.execution.local.sweep:sweep.py:1008  Successfully completed Phase 2 tag setting for parent run None...
INFO     training.hpo.execution.local.sweep:sweep.py:1115 [REFIT] Starting refit training for best trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>
WARNING  training.hpo.execution.local.sweep:sweep.py:1205 [REFIT] Could not determine trial_key_hash from best trial run tags. Refit run may not match trial run.
INFO     training.hpo.execution.local.refit:refit.py:107 [REFIT] Starting refit training for trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'> with hyperparameters: <MagicMock name='mock.create_study().best_trial.params' id='140077206865856'>
INFO     training.hpo.execution.local.refit:refit.py:130 [REFIT] Computed trial_id="trial_<MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>_20260118_003139", run_id='20260118_003139', trial_number=<MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>
WARNING  infrastructure.tracking.mlflow.hash_utils:hash_utils.py:515 Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
WARNING  training.hpo.execution.local.sweep:sweep.py:1304 [REFIT] Refit training failed: Cannot create refit in v2 study folder study-c3659fea without trial_key_hash. study_key_hash=present, trial_key_hash=missing, refit_params=missing. Hash computation from trial parameters may have failed.. Continuing without refit checkpoint.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py", line 1244, in run_local_hpo_sweep
    refit_metrics, refit_checkpoint_dir, refit_run_id = run_refit_training(
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 240, in run_refit_training
    raise RuntimeError(
RuntimeError: Cannot create refit in v2 study folder study-c3659fea without trial_key_hash. study_key_hash=present, trial_key_hash=missing, refit_params=missing. Hash computation from trial parameters may have failed.
WARNING  training.hpo.execution.local.sweep:sweep.py:1428 [REFIT] No refit_run_id available to mark as FINISHED
=============================== warnings summary ===============================
tests/config/unit/test_fingerprint_placeholder_fallback.py::TestFingerprintPlaceholderFallback::test_placeholder_values_are_short_enough_for_naming
  /workspaces/resume-ner-azureml/tests/config/unit/test_fingerprint_placeholder_fallback.py:128: RuntimeWarning: Fingerprint computation functions not available. Using placeholder fingerprints.
    spec_fp, exec_fp = _compute_fingerprints(

tests/tracking/scripts/test_artifact_upload_manual.py::test_monkey_patch_registration
  /opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/_pytest/python.py:170: PytestReturnNotNoneWarning: Test functions should return None, but tests/tracking/scripts/test_artifact_upload_manual.py::test_monkey_patch_registration returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/tracking/scripts/test_artifact_upload_manual.py::test_artifact_upload_to_child_run
  /opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/_pytest/python.py:170: PytestReturnNotNoneWarning: Test functions should return None, but tests/tracking/scripts/test_artifact_upload_manual.py::test_artifact_upload_to_child_run returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/tracking/scripts/test_artifact_upload_manual.py::test_refit_run_completion
  /opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/_pytest/python.py:170: PytestReturnNotNoneWarning: Test functions should return None, but tests/tracking/scripts/test_artifact_upload_manual.py::test_refit_run_completion returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED tests/hpo/integration/test_early_termination.py::TestPruningBehavior::test_pruner_with_study_manager - AssertionError: assert False
 +  where False = isinstance(<MagicMock name='mock.create_study().pruner' id='140077259720208'>, MedianPruner)
 +    where <MagicMock name='mock.create_study().pruner' id='140077259720208'> = <MagicMock name='mock.create_study()' id='140077279450096'>.pruner
FAILED tests/hpo/integration/test_early_termination.py::TestPruningIntegration::test_pruning_with_checkpoint_resume - assert False is True
FAILED tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointResume::test_resume_from_existing_checkpoint - AssertionError: assert 0 == 1
 +  where 0 = len(<MagicMock name='mock.create_study().trials' id='140077280918304'>)
 +    where <MagicMock name='mock.create_study().trials' id='140077280918304'> = <MagicMock name='mock.create_study()' id='140077279450096'>.trials
FAILED tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointResume::test_resume_preserves_trials - AssertionError: assert 0 == 3
 +  where 0 = len(<MagicMock name='mock.create_study().trials' id='140077280918304'>)
 +    where <MagicMock name='mock.create_study().trials' id='140077280918304'> = <MagicMock name='mock.create_study()' id='140077279450096'>.trials
FAILED tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointResume::test_resume_marks_running_trials_as_failed - AssertionError: assert 0 == 2
 +  where 0 = len(<MagicMock name='mock.create_study().trials' id='140077280918304'>)
 +    where <MagicMock name='mock.create_study().trials' id='140077280918304'> = <MagicMock name='mock.create_study()' id='140077279450096'>.trials
FAILED tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointResume::test_resume_with_auto_resume_false_raises_error - Failed: DID NOT RAISE <class 'ValueError'>
FAILED tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointResume::test_resume_continues_trial_numbering - AssertionError: assert 0 == 2
 +  where 0 = len(<MagicMock name='mock.create_study().trials' id='140077280918304'>)
 +    where <MagicMock name='mock.create_study().trials' id='140077280918304'> = <MagicMock name='mock.create_study()' id='140077279450096'>.trials
FAILED tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointSmokeYaml::test_checkpoint_smoke_yaml_auto_resume_true - AssertionError: assert 0 == 1
 +  where 0 = len(<MagicMock name='mock.create_study().trials' id='140077280918304'>)
 +    where <MagicMock name='mock.create_study().trials' id='140077280918304'> = <MagicMock name='mock.create_study()' id='140077279450096'>.trials
FAILED tests/hpo/integration/test_hpo_full_workflow.py::TestFullHPOWorkflow::test_full_hpo_workflow_with_cv_and_refit - assert 0 > 0
 +  where 0 = len([])
FAILED tests/hpo/integration/test_hpo_full_workflow.py::TestFullHPOWorkflow::test_full_hpo_workflow_no_cv_no_refit - AssertionError: assert 0 >= 1
 +  where 0 = len(<MagicMock name='mock.create_study().trials' id='140077280918304'>)
 +    where <MagicMock name='mock.create_study().trials' id='140077280918304'> = <MagicMock name='mock.create_study()' id='140077279450096'>.trials
FAILED tests/hpo/integration/test_hpo_full_workflow.py::TestFullHPOWorkflow::test_full_hpo_workflow_creates_correct_path_structure - assert 0 > 0
 +  where 0 = len([])
FAILED tests/hpo/integration/test_hpo_resume_workflow.py::TestHPOResumeWorkflow::test_resume_workflow_preserves_trials - AssertionError: assert 0 == 1
 +  where 0 = len(<MagicMock name='mock.create_study().trials' id='140077280918304'>)
 +    where <MagicMock name='mock.create_study().trials' id='140077280918304'> = <MagicMock name='mock.create_study()' id='140077279450096'>.trials
FAILED tests/hpo/integration/test_hpo_resume_workflow.py::TestHPOResumeWorkflow::test_resume_workflow_with_different_run_id - AssertionError: assert 0 > 0
 +  where 0 = len(<MagicMock name='mock.create_study().trials' id='140077280918304'>)
 +    where <MagicMock name='mock.create_study().trials' id='140077280918304'> = <MagicMock name='mock.create_study()' id='140077279450096'>.trials
FAILED tests/hpo/integration/test_hpo_resume_workflow.py::TestHPOResumeWorkflow::test_resume_workflow_with_cv - AssertionError: assert 0 > 0
 +  where 0 = len(<MagicMock name='mock.create_study().trials' id='140077280918304'>)
 +    where <MagicMock name='mock.create_study().trials' id='140077280918304'> = <MagicMock name='mock.create_study()' id='140077279450096'>.trials
FAILED tests/hpo/integration/test_smoke_yaml_options.py::TestDisableAutoOptunaMark::test_disable_auto_optuna_mark_false_enables_marking - AssertionError: RUNNING trials should be marked FAILED when disable_auto_optuna_mark=false
assert 1 == 0
 +  where 1 = len([FrozenTrial(number=0, state=<TrialState.RUNNING: 0>, values=None, datetime_start=datetime.datetime(2026, 1, 18, 0, 31, 20, 651883), datetime_complete=None, params={}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={}, trial_id=1, value=None)])
FAILED tests/hpo/integration/test_trial_execution.py::TestTrialExecutionWithCV::test_trial_execution_with_cv_creates_nested_runs - RuntimeError: Cannot create trial in v2 study folder study-abc12345 without trial_key_hash. study_key_hash=NO, trial_key_hash=NO. This is a critical error that must be fixed.
FAILED tests/hpo/integration/test_trial_execution.py::TestTrialExecutionWithCV::test_trial_execution_with_cv_aggregates_metrics - RuntimeError: Cannot create trial in v2 study folder study-abc12345 without trial_key_hash. study_key_hash=YES, trial_key_hash=NO. This is a critical error that must be fixed.
FAILED tests/hpo/integration/test_trial_execution.py::TestTrialExecutionWithCV::test_trial_execution_with_cv_output_paths - RuntimeError: Cannot create trial in v2 study folder study-abc12345 without trial_key_hash. study_key_hash=YES, trial_key_hash=NO. This is a critical error that must be fixed.
FAILED tests/hpo/integration/test_trial_execution.py::TestTrialExecutionWithCV::test_trial_execution_with_cv_smoke_yaml_params - RuntimeError: Cannot create trial in v2 study folder study-abc12345 without trial_key_hash. study_key_hash=YES, trial_key_hash=NO. This is a critical error that must be fixed.
FAILED tests/tracking/integration/test_tracking_config_enabled.py::TestBenchmarkTrackingEnabled::test_benchmark_tracking_enabled_creates_run - assert None is not None
FAILED tests/tracking/integration/test_tracking_config_enabled.py::TestTrainingTrackingEnabled::test_training_tracking_enabled_creates_run - assert None is not None
FAILED tests/tracking/integration/test_tracking_config_enabled.py::TestConversionTrackingEnabled::test_conversion_tracking_enabled_creates_run - assert None is not None
FAILED tests/training/hpo/unit/test_cv_hash_computation.py::TestCVTrialRunHashComputation::test_priority_2_get_from_parent_tags - AssertionError: assert '8c4c43347d560c8ac472b5a715bac7d333bdfabbcad47278bf4f468d1a3ae15d' == 'parent_study_hash_aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa'
  
  - parent_study_hash_aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa
  + 8c4c43347d560c8ac472b5a715bac7d333bdfabbcad47278bf4f468d1a3ae15d
FAILED tests/unit/api/test_inference_performance.py::TestInferencePerformance::test_tokenization_does_not_hang - AssertionError: Tokenization took 1.22 seconds, expected < 1.0
assert 1.2226388454437256 < 1.0
FAILED tests/workflows/test_full_workflow_e2e.py::test_full_workflow_e2e - AssertionError: assert 0 > 0
 +  where 0 = len(<MagicMock name='mock.create_study().trials' id='140077280918304'>)
 +    where <MagicMock name='mock.create_study().trials' id='140077280918304'> = <MagicMock name='mock.create_study()' id='140077279450096'>.trials
FAILED tests/workflows/test_notebook_01_e2e.py::TestNotebookE2E_Core::test_hpo_sweep_execution_mocked - AssertionError: assert 0 > 0
 +  where 0 = len(<MagicMock name='mock.create_study().trials' id='140077280918304'>)
 +    where <MagicMock name='mock.create_study().trials' id='140077280918304'> = <MagicMock name='mock.create_study()' id='140077279450096'>.trials
=========== 26 failed, 1370 passed, 53 skipped, 4 warnings in 38.80s ===========

============================================================
[INFO] Pytest log file: /workspaces/resume-ner-azureml/outputs/pytest_logs/pytest_20260118_003101.log
============================================================

2026-01-18 00:31:05,772 - infrastructure.tracking.mlflow.compatibility - INFO - Applied Azure ML artifact compatibility patch
2026-01-18 00:31:10,821 - evaluation.benchmarking.orchestrator - INFO - Skipping benchmarking (test data not available)
2026-01-18 00:31:10,825 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_handles_1/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_handles_1/checkpoint --test-data /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_handles_1/test.json --batch-sizes 1 8 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_handles_1/benchmark.json
2026-01-18 00:31:10,826 - evaluation.benchmarking.utils - ERROR - Benchmarking failed with return code 1
2026-01-18 00:31:10,829 - evaluation.benchmarking.orchestrator - INFO - Benchmarking distilbert (trial-25d03eeb)...
2026-01-18 00:31:10,842 - evaluation.benchmarking.orchestrator - WARNING - [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=xyz789uvw012...
2026-01-18 00:31:10,843 - evaluation.benchmarking.orchestrator - INFO - [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 00:31:10,844 - evaluation.benchmarking.orchestrator - INFO - Benchmark completed: /tmp/pytest-of-codespace/pytest-38/test_benchmark_passes_trial_id0/outputs/benchmarking/test/benchmark.json
2026-01-18 00:31:10,844 - evaluation.benchmarking.orchestrator - INFO - Benchmarking complete. 1/1 trials benchmarked.
2026-01-18 00:31:10,847 - evaluation.benchmarking.orchestrator - INFO - Benchmarking distilbert (trial_1_20251231_161745)...
2026-01-18 00:31:10,849 - evaluation.benchmarking.orchestrator - WARNING - [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=xyz789uvw012...
2026-01-18 00:31:10,850 - evaluation.benchmarking.orchestrator - INFO - [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 00:31:10,850 - evaluation.benchmarking.orchestrator - INFO - Benchmark completed: /tmp/pytest-of-codespace/pytest-38/test_benchmark_passes_trial_id1/outputs/benchmarking/test/benchmark.json
2026-01-18 00:31:10,850 - evaluation.benchmarking.orchestrator - INFO - Benchmarking complete. 1/1 trials benchmarked.
2026-01-18 00:31:10,854 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_mlflow_t0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_mlflow_t0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_mlflow_t0/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_mlflow_t0/benchmark.json
2026-01-18 00:31:10,854 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Building run name: trial_id=trial-25d03eeb, root_dir=/tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_mlflow_t0, config_dir=/tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_mlflow_t0/config
2026-01-18 00:31:10,855 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Generated run name: <Mock name='create_naming_context().storage_env' id='140077280139472'>_<Mock name='create_naming_context().model' id='140077280138224'>_<Mock name='create_naming_context().process_type' id='140077280137360'>_legacy
2026-01-18 00:31:10,858 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_mlflow_t1/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_mlflow_t1/checkpoint --test-data /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_mlflow_t1/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_mlflow_t1/benchmark.json
2026-01-18 00:31:10,859 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Constructed trial_id from trial_key_hash: trial-25d03eeb
2026-01-18 00:31:10,859 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Building run name: trial_id=trial-25d03eeb, root_dir=/tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_mlflow_t1, config_dir=/tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_mlflow_t1/config
2026-01-18 00:31:10,859 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Generated run name: <Mock name='create_naming_context().storage_env' id='140077279578144'>_<Mock name='create_naming_context().model' id='140077279576608'>_<Mock name='create_naming_context().process_type' id='140077279576752'>_legacy
2026-01-18 00:31:10,864 - evaluation.benchmarking.orchestrator - INFO - Benchmarking distilbert (trial_0)...
2026-01-18 00:31:10,866 - evaluation.benchmarking.orchestrator - WARNING - [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 00:31:10,867 - evaluation.benchmarking.orchestrator - INFO - [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 00:31:10,867 - evaluation.benchmarking.orchestrator - INFO - Benchmark completed: /tmp/pytest-of-codespace/pytest-38/test_benchmark_best_trials_use0/outputs/benchmarking/test/benchmark.json
2026-01-18 00:31:10,867 - evaluation.benchmarking.orchestrator - INFO - Benchmarking complete. 1/1 trials benchmarked.
2026-01-18 00:31:10,871 - evaluation.benchmarking.orchestrator - INFO - Benchmarking distilbert (trial_0)...
2026-01-18 00:31:10,872 - evaluation.benchmarking.orchestrator - WARNING - [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 00:31:10,874 - evaluation.benchmarking.orchestrator - INFO - [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 00:31:10,874 - evaluation.benchmarking.orchestrator - INFO - Benchmark completed: /tmp/pytest-of-codespace/pytest-38/test_benchmark_best_trials_use1/outputs/benchmarking/test/benchmark.json
2026-01-18 00:31:10,874 - evaluation.benchmarking.orchestrator - INFO - Benchmarking complete. 1/1 trials benchmarked.
2026-01-18 00:31:10,878 - evaluation.benchmarking.orchestrator - INFO - Benchmarking distilbert (trial_0)...
2026-01-18 00:31:10,879 - evaluation.benchmarking.orchestrator - WARNING - [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 00:31:10,881 - evaluation.benchmarking.orchestrator - INFO - [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 00:31:10,881 - evaluation.benchmarking.orchestrator - INFO - Benchmark completed: /tmp/pytest-of-codespace/pytest-38/test_benchmark_best_trials_use2/outputs/benchmarking/test/benchmark.json
2026-01-18 00:31:10,881 - evaluation.benchmarking.orchestrator - INFO - Benchmarking complete. 1/1 trials benchmarked.
2026-01-18 00:31:10,885 - evaluation.benchmarking.orchestrator - INFO - Benchmarking distilbert (trial_0)...
2026-01-18 00:31:10,887 - evaluation.benchmarking.orchestrator - WARNING - [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 00:31:10,888 - evaluation.benchmarking.orchestrator - INFO - [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 00:31:10,888 - evaluation.benchmarking.orchestrator - INFO - Benchmark completed: /tmp/pytest-of-codespace/pytest-38/test_benchmark_best_trials_use3/outputs/benchmarking/test/benchmark.json
2026-01-18 00:31:10,888 - evaluation.benchmarking.orchestrator - INFO - Benchmarking complete. 1/1 trials benchmarked.
2026-01-18 00:31:10,892 - evaluation.benchmarking.orchestrator - INFO - Benchmarking distilbert (trial_0)...
2026-01-18 00:31:10,893 - evaluation.benchmarking.orchestrator - WARNING - [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 00:31:10,895 - evaluation.benchmarking.orchestrator - INFO - [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 00:31:10,895 - evaluation.benchmarking.orchestrator - INFO - Benchmark completed: /tmp/pytest-of-codespace/pytest-38/test_benchmark_best_trials_use4/outputs/benchmarking/test/custom_benchmark.json
2026-01-18 00:31:10,895 - evaluation.benchmarking.orchestrator - INFO - Benchmarking complete. 1/1 trials benchmarked.
2026-01-18 00:31:10,900 - evaluation.benchmarking.orchestrator - INFO - Benchmarking distilbert (trial_0)...
2026-01-18 00:31:10,901 - evaluation.benchmarking.orchestrator - WARNING - [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 00:31:10,902 - evaluation.benchmarking.orchestrator - INFO - [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 00:31:10,902 - evaluation.benchmarking.orchestrator - INFO - Benchmark completed: /tmp/pytest-of-codespace/pytest-38/test_benchmark_best_trials_use5/outputs/benchmarking/test/benchmark.json
2026-01-18 00:31:10,903 - evaluation.benchmarking.orchestrator - INFO - Benchmarking complete. 1/1 trials benchmarked.
2026-01-18 00:31:10,906 - evaluation.benchmarking.orchestrator - INFO - Benchmarking distilbert (trial_0)...
2026-01-18 00:31:10,908 - evaluation.benchmarking.orchestrator - WARNING - [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 00:31:10,909 - evaluation.benchmarking.orchestrator - INFO - [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 00:31:10,909 - evaluation.benchmarking.orchestrator - INFO - Benchmark completed: /tmp/pytest-of-codespace/pytest-38/test_benchmark_best_trials_use6/outputs/benchmarking/test/custom_benchmark.json
2026-01-18 00:31:10,909 - evaluation.benchmarking.orchestrator - INFO - Benchmarking complete. 1/1 trials benchmarked.
2026-01-18 00:31:10,913 - evaluation.benchmarking.orchestrator - INFO - Benchmarking distilbert (trial_0)...
2026-01-18 00:31:10,915 - evaluation.benchmarking.orchestrator - WARNING - [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 00:31:10,916 - evaluation.benchmarking.orchestrator - INFO - [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 00:31:10,916 - evaluation.benchmarking.orchestrator - INFO - Benchmark completed: /tmp/pytest-of-codespace/pytest-38/test_benchmark_best_trials_all0/outputs/benchmarking/test/custom_benchmark.json
2026-01-18 00:31:10,916 - evaluation.benchmarking.orchestrator - INFO - Benchmarking complete. 1/1 trials benchmarked.
2026-01-18 00:31:10,919 - evaluation.benchmarking.orchestrator - INFO - Benchmarking distilbert (trial_0)...
2026-01-18 00:31:10,921 - evaluation.benchmarking.orchestrator - WARNING - [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 00:31:10,922 - evaluation.benchmarking.orchestrator - INFO - [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 00:31:10,922 - evaluation.benchmarking.orchestrator - ERROR - Benchmark failed for distilbert
2026-01-18 00:31:10,922 - evaluation.benchmarking.orchestrator - INFO - Benchmarking complete. 0/1 trials benchmarked.
2026-01-18 00:31:10,924 - evaluation.benchmarking.orchestrator - INFO - Run mode is 'force_new' - skipping idempotency check, will benchmark all champions
2026-01-18 00:31:10,926 - evaluation.benchmarking.orchestrator - INFO - Skipping distilbert - benchmark already exists (trial_key_hash=trial_hash_456...)
2026-01-18 00:31:10,937 - evaluation.benchmarking.orchestrator - WARNING - Found 1 finished benchmark run(s) by hash (fallback). Consider setting benchmark_key tag for more reliable idempotency. trial_key_hash=trial_hash_456...
2026-01-18 00:31:10,941 - evaluation.benchmarking.orchestrator - WARNING - Found 1 finished benchmark run(s) by hash (fallback). Consider setting benchmark_key tag for more reliable idempotency. trial_key_hash=trial_hash_456...
2026-01-18 00:31:10,944 - evaluation.benchmarking.orchestrator - INFO - Skipping distilbert - benchmark already exists (trial_key_hash=trial_hash_456...)
2026-01-18 00:31:10,948 - evaluation.benchmarking.orchestrator - INFO - Run mode is 'force_new' - skipping idempotency check, will benchmark all champions
2026-01-18 00:31:10,954 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_bat0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_bat0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_bat0/test.json --batch-sizes 1 8 16 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_bat0/benchmark.json
2026-01-18 00:31:10,957 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_ite0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_ite0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_ite0/test.json --batch-sizes 1 8 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_ite0/benchmark.json
2026-01-18 00:31:10,960 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_war0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_war0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_war0/test.json --batch-sizes 1 8 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_war0/benchmark.json
2026-01-18 00:31:10,962 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_max0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_max0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_max0/test.json --batch-sizes 1 8 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_max0/benchmark.json
2026-01-18 00:31:10,964 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_dev0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_dev0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_dev0/test.json --batch-sizes 1 8 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_dev0/benchmark.json --device cuda
2026-01-18 00:31:10,967 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_skips_de0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_skips_de0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_skips_de0/test.json --batch-sizes 1 8 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_skips_de0/benchmark.json
2026-01-18 00:31:10,969 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_out0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_out0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_out0/test.json --batch-sizes 1 8 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_uses_out0/benchmark.json
2026-01-18 00:31:10,972 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_all_conf0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_all_conf0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_all_conf0/test.json --batch-sizes 2 4 32 --iterations 200 --warmup 20 --max-length 256 --output /tmp/pytest-of-codespace/pytest-38/test_run_benchmarking_all_conf0/custom_benchmark.json --device cuda
2026-01-18 00:31:10,978 - evaluation.benchmarking.orchestrator - INFO - Benchmarking distilbert (trial_0)...
2026-01-18 00:31:10,980 - evaluation.benchmarking.orchestrator - WARNING - [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 00:31:10,981 - evaluation.benchmarking.orchestrator - INFO - [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 00:31:10,981 - evaluation.benchmarking.orchestrator - INFO - Benchmark completed: /tmp/pytest-of-codespace/pytest-38/test_workflow_loads_config_and0/outputs/benchmarking/test/benchmark.json
2026-01-18 00:31:10,981 - evaluation.benchmarking.orchestrator - INFO - Benchmarking complete. 1/1 trials benchmarked.
2026-01-18 00:31:10,985 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-38/test_workflow_custom_config_va0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-38/test_workflow_custom_config_va0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-38/test_workflow_custom_config_va0/test.json --batch-sizes 2 4 32 --iterations 200 --warmup 20 --max-length 256 --output /tmp/pytest-of-codespace/pytest-38/test_workflow_custom_config_va0/custom_benchmark.json --device cuda
2026-01-18 00:31:10,990 - evaluation.benchmarking.orchestrator - INFO - Benchmarking distilbert (trial_0)...
2026-01-18 00:31:10,991 - evaluation.benchmarking.orchestrator - WARNING - [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=bbbbbbbbbbbbbbbb...
2026-01-18 00:31:10,993 - evaluation.benchmarking.orchestrator - INFO - [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-18 00:31:10,993 - evaluation.benchmarking.orchestrator - INFO - Benchmark completed: /tmp/pytest-of-codespace/pytest-38/test_workflow_defaults_when_co0/outputs/benchmarking/test/benchmark.json
2026-01-18 00:31:10,993 - evaluation.benchmarking.orchestrator - INFO - Benchmarking complete. 1/1 trials benchmarked.
2026-01-18 00:31:11,035 - evaluation.benchmarking.orchestrator - INFO - Run mode is 'force_new' - skipping idempotency check, will benchmark all champions
2026-01-18 00:31:11,037 - evaluation.benchmarking.orchestrator - INFO - Skipping distilbert - benchmark already exists (trial_key_hash=trial_hash_456...)
2026-01-18 00:31:11,037 - evaluation.benchmarking.orchestrator - INFO - Skipping deberta - benchmark already exists (trial_key_hash=trial_hash_789...)
2026-01-18 00:31:11,046 - evaluation.benchmarking.orchestrator - INFO - Skipping distilbert - benchmark already exists (trial_key_hash=trial_hash_456...)
2026-01-18 00:31:11,048 - evaluation.benchmarking.orchestrator - INFO - Skipping distilbert - benchmark already exists (trial_key_hash=trial_hash_456...)
2026-01-18 00:31:11,053 - evaluation.benchmarking.orchestrator - WARNING - No run_id found for champion distilbert, skipping idempotency check
2026-01-18 00:31:11,056 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_failure_mlflow_client_una0/config/tags.yaml, using defaults
2026-01-18 00:31:11,064 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-38/test_trial_id_from_parameter_o0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-38/test_trial_id_from_parameter_o0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-38/test_trial_id_from_parameter_o0/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-38/test_trial_id_from_parameter_o0/benchmark.json
2026-01-18 00:31:11,065 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Building run name: trial_id=1_20251231_161745, root_dir=/tmp/pytest-of-codespace/pytest-38/test_trial_id_from_parameter_o0, config_dir=/tmp/pytest-of-codespace/pytest-38/test_trial_id_from_parameter_o0/config
2026-01-18 00:31:11,065 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Generated run name: <Mock name='create_naming_context().storage_env' id='140077279584144'>_<Mock name='create_naming_context().model' id='140077279582080'>_<Mock name='create_naming_context().process_type' id='140077279577808'>_legacy
2026-01-18 00:31:11,069 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-38/test_trial_id_from_parameter_n0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-38/test_trial_id_from_parameter_n0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-38/test_trial_id_from_parameter_n0/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-38/test_trial_id_from_parameter_n0/benchmark.json
2026-01-18 00:31:11,069 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Building run name: trial_id=trial-25d03eeb, root_dir=/tmp/pytest-of-codespace/pytest-38/test_trial_id_from_parameter_n0, config_dir=/tmp/pytest-of-codespace/pytest-38/test_trial_id_from_parameter_n0/config
2026-01-18 00:31:11,069 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Generated run name: <Mock name='create_naming_context().storage_env' id='140077280119488'>_<Mock name='create_naming_context().model' id='140077280121120'>_<Mock name='create_naming_context().process_type' id='140077280120208'>_legacy
2026-01-18 00:31:11,073 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-38/test_trial_id_extraction_from_0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-38/test_trial_id_extraction_from_0/hpo/local/distilbert/study-abc123/trial_1_20251231_161745/checkpoint --test-data /tmp/pytest-of-codespace/pytest-38/test_trial_id_extraction_from_0/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-38/test_trial_id_extraction_from_0/benchmark.json
2026-01-18 00:31:11,073 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Extracted trial_id from checkpoint path: trial_1_20251231_161745 (at level 1)
2026-01-18 00:31:11,073 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Building run name: trial_id=trial_1_20251231_161745, root_dir=/tmp/pytest-of-codespace/pytest-38/test_trial_id_extraction_from_0, config_dir=/tmp/pytest-of-codespace/pytest-38/test_trial_id_extraction_from_0/config
2026-01-18 00:31:11,074 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Generated run name: <Mock name='create_naming_context().storage_env' id='140077280128128'>_<Mock name='create_naming_context().model' id='140077280133024'>_<Mock name='create_naming_context().process_type' id='140077280132928'>_legacy
2026-01-18 00:31:11,077 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-38/test_trial_id_extraction_from_1/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-38/test_trial_id_extraction_from_1/hpo/local/distilbert/study-abc123/trial-25d03eeb/checkpoint --test-data /tmp/pytest-of-codespace/pytest-38/test_trial_id_extraction_from_1/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-38/test_trial_id_extraction_from_1/benchmark.json
2026-01-18 00:31:11,078 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Extracted trial_id from checkpoint path: trial-25d03eeb (at level 1)
2026-01-18 00:31:11,078 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Building run name: trial_id=trial-25d03eeb, root_dir=/tmp/pytest-of-codespace/pytest-38/test_trial_id_extraction_from_1, config_dir=/tmp/pytest-of-codespace/pytest-38/test_trial_id_extraction_from_1/config
2026-01-18 00:31:11,078 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Generated run name: <Mock name='create_naming_context().storage_env' id='140077279557712'>_<Mock name='create_naming_context().model' id='140077279558432'>_<Mock name='create_naming_context().process_type' id='140077279557520'>_legacy
2026-01-18 00:31:11,082 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-38/test_trial_id_extraction_from_2/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-38/test_trial_id_extraction_from_2/hpo/local/distilbert/study-abc123/trial-25d03eeb/refit/checkpoint --test-data /tmp/pytest-of-codespace/pytest-38/test_trial_id_extraction_from_2/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-38/test_trial_id_extraction_from_2/benchmark.json
2026-01-18 00:31:11,082 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Extracted trial_id from checkpoint path: trial-25d03eeb (at level 2)
2026-01-18 00:31:11,082 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Building run name: trial_id=trial-25d03eeb, root_dir=/tmp/pytest-of-codespace/pytest-38/test_trial_id_extraction_from_2, config_dir=/tmp/pytest-of-codespace/pytest-38/test_trial_id_extraction_from_2/config
2026-01-18 00:31:11,083 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Generated run name: <Mock name='create_naming_context().storage_env' id='140077280339200'>_<Mock name='create_naming_context().model' id='140077280339248'>_<Mock name='create_naming_context().process_type' id='140077280339344'>_legacy
2026-01-18 00:31:11,086 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-38/test_trial_id_fallback_to_tria0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-38/test_trial_id_fallback_to_tria0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-38/test_trial_id_fallback_to_tria0/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-38/test_trial_id_fallback_to_tria0/benchmark.json
2026-01-18 00:31:11,087 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Constructed trial_id from trial_key_hash: trial-25d03eeb
2026-01-18 00:31:11,087 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Building run name: trial_id=trial-25d03eeb, root_dir=/tmp/pytest-of-codespace/pytest-38/test_trial_id_fallback_to_tria0, config_dir=/tmp/pytest-of-codespace/pytest-38/test_trial_id_fallback_to_tria0/config
2026-01-18 00:31:11,087 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Generated run name: <Mock name='create_naming_context().storage_env' id='140077280106368'>_<Mock name='create_naming_context().model' id='140077280106512'>_<Mock name='create_naming_context().process_type' id='140077280105840'>_legacy
2026-01-18 00:31:11,091 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-38/test_trial_id_parameter_overri0/src/evaluation/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-38/test_trial_id_parameter_overri0/hpo/local/distilbert/study-abc123/trial-25d03eeb/checkpoint --test-data /tmp/pytest-of-codespace/pytest-38/test_trial_id_parameter_overri0/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-38/test_trial_id_parameter_overri0/benchmark.json
2026-01-18 00:31:11,091 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Building run name: trial_id=trial-custom123, root_dir=/tmp/pytest-of-codespace/pytest-38/test_trial_id_parameter_overri0, config_dir=/tmp/pytest-of-codespace/pytest-38/test_trial_id_parameter_overri0/config
2026-01-18 00:31:11,092 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Generated run name: <Mock name='create_naming_context().storage_env' id='140077280296816'>_<Mock name='create_naming_context().model' id='140077280293168'>_<Mock name='create_naming_context().process_type' id='140077280285008'>_legacy
2026-01-18 00:31:11,099 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-38/test_hpo_trial_workflow0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 00:31:11,099 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 00:31:11,099 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:e0fbae020bc656b7e6425344765f3df149e5f6831d2b7..., root_dir=/tmp/pytest-of-codespace/pytest-38/test_hpo_trial_workflow0, config_dir=/tmp/pytest-of-codespace/pytest-38/test_hpo_trial_workflow0/config, counter_path=/tmp/pytest-of-codespace/pytest-38/test_hpo_trial_workflow0/outputs/cache/run_name_counter.json
2026-01-18 00:31:11,099 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Loaded 0 existing allocations from /tmp/pytest-of-codespace/pytest-38/test_hpo_trial_workflow0/outputs/cache/run_name_counter.json
2026-01-18 00:31:11,100 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0
2026-01-18 00:31:11,100 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)
2026-01-18 00:31:11,100 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version]  Successfully reserved version 1 for counter_key resume-ner:hpo:e0fbae020bc656b7e6425344765f3df149e... (run_id: pending_2026...)
2026-01-18 00:31:11,107 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-38/test_final_training_workflow0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 00:31:11,107 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=final_training
2026-01-18 00:31:11,114 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-38/test_benchmarking_workflow0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 00:31:11,115 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=benchmarking
2026-01-18 00:31:11,122 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-38/test_paths_match_naming_patter0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 00:31:11,122 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 00:31:11,122 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:fa924e9d2ac5675b719f2281bdcff80badd52b78fb7ef..., root_dir=/tmp/pytest-of-codespace/pytest-38/test_paths_match_naming_patter0, config_dir=/tmp/pytest-of-codespace/pytest-38/test_paths_match_naming_patter0/config, counter_path=/tmp/pytest-of-codespace/pytest-38/test_paths_match_naming_patter0/outputs/cache/run_name_counter.json
2026-01-18 00:31:11,122 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Loaded 0 existing allocations from /tmp/pytest-of-codespace/pytest-38/test_paths_match_naming_patter0/outputs/cache/run_name_counter.json
2026-01-18 00:31:11,122 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0
2026-01-18 00:31:11,123 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)
2026-01-18 00:31:11,123 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version]  Successfully reserved version 1 for counter_key resume-ner:hpo:fa924e9d2ac5675b719f2281bdcff80badd... (run_id: pending_2026...)
2026-01-18 00:31:11,137 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-38/test_naming_patterns_from_nami0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 00:31:11,137 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 00:31:11,137 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:e0fbae020bc656b7e6425344765f3df149e5f6831d2b7..., root_dir=/tmp/pytest-of-codespace/pytest-38/test_naming_patterns_from_nami0, config_dir=/tmp/pytest-of-codespace/pytest-38/test_naming_patterns_from_nami0/config, counter_path=/tmp/pytest-of-codespace/pytest-38/test_naming_patterns_from_nami0/outputs/cache/run_name_counter.json
2026-01-18 00:31:11,138 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Loaded 0 existing allocations from /tmp/pytest-of-codespace/pytest-38/test_naming_patterns_from_nami0/outputs/cache/run_name_counter.json
2026-01-18 00:31:11,138 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0
2026-01-18 00:31:11,138 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)
2026-01-18 00:31:11,138 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version]  Successfully reserved version 1 for counter_key resume-ner:hpo:e0fbae020bc656b7e6425344765f3df149e... (run_id: pending_2026...)
2026-01-18 00:31:11,153 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-38/test_naming_conventions_consis0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 00:31:11,154 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 00:31:11,154 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:e0fbae020bc656b7e6425344765f3df149e5f6831d2b7..., root_dir=/tmp/pytest-of-codespace/pytest-38/test_naming_conventions_consis0, config_dir=/tmp/pytest-of-codespace/pytest-38/test_naming_conventions_consis0/config, counter_path=/tmp/pytest-of-codespace/pytest-38/test_naming_conventions_consis0/outputs/cache/run_name_counter.json
2026-01-18 00:31:11,154 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Loaded 0 existing allocations from /tmp/pytest-of-codespace/pytest-38/test_naming_conventions_consis0/outputs/cache/run_name_counter.json
2026-01-18 00:31:11,154 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0
2026-01-18 00:31:11,154 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)
2026-01-18 00:31:11,154 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version]  Successfully reserved version 1 for counter_key resume-ner:hpo:e0fbae020bc656b7e6425344765f3df149e... (run_id: pending_2026...)
2026-01-18 00:31:11,379 - orchestration.jobs.tracking.config.loader - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-38/test_naming_run_name_auto_incr0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 00:31:11,380 - orchestration.jobs.tracking.config.loader - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 00:31:11,382 - orchestration.jobs.tracking.config.loader - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-38/test_naming_run_name_auto_incr1/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 00:31:11,382 - orchestration.jobs.tracking.config.loader - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 00:31:11,385 - orchestration.jobs.tracking.config.loader - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-38/test_naming_run_name_auto_incr2/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 00:31:11,385 - orchestration.jobs.tracking.config.loader - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=benchmarking
2026-01-18 00:31:11,387 - orchestration.jobs.tracking.config.loader - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-38/test_naming_run_name_auto_incr3/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 00:31:11,387 - orchestration.jobs.tracking.config.loader - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 00:31:11,707 - deployment.conversion.orchestration - INFO - Output directory: /tmp/pytest-of-codespace/pytest-38/test_opset_version_passed_to_s0/outputs/conversion/test
2026-01-18 00:31:11,728 - deployment.conversion.orchestration - INFO - Created MLflow run: test_run_name (test_run_id...)
2026-01-18 00:31:11,729 - deployment.conversion.orchestration - INFO - Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /path/to/checkpoint --config-dir /tmp/pytest-of-codespace/pytest-38/test_opset_version_passed_to_s0/config --backbone distilbert-base-uncased --output-dir /tmp/pytest-of-codespace/pytest-38/test_opset_version_passed_to_s0/outputs/conversion/test --opset-version 19 --run-smoke-test
2026-01-18 00:31:11,730 - infrastructure.tracking.mlflow.lifecycle - WARNING - Could not ensure run test_run_id... is terminated: Run 'test_run_id' not found
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/lifecycle.py", line 130, in ensure_run_terminated
    run = client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'test_run_id' not found
2026-01-18 00:31:11,741 - deployment.conversion.orchestration - INFO - Conversion completed. ONNX model: /tmp/pytest-of-codespace/pytest-38/test_opset_version_passed_to_s0/outputs/conversion/test/model.onnx
2026-01-18 00:31:11,747 - deployment.conversion.orchestration - INFO - Output directory: /tmp/pytest-of-codespace/pytest-38/test_quantization_int8_adds_fl0/outputs/conversion/test
2026-01-18 00:31:11,750 - deployment.conversion.orchestration - INFO - Created MLflow run: test_run_name (test_run_id...)
2026-01-18 00:31:11,750 - deployment.conversion.orchestration - INFO - Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /path/to/checkpoint --config-dir /tmp/pytest-of-codespace/pytest-38/test_quantization_int8_adds_fl0/config --backbone distilbert-base-uncased --output-dir /tmp/pytest-of-codespace/pytest-38/test_quantization_int8_adds_fl0/outputs/conversion/test --opset-version 18 --quantize-int8 --run-smoke-test
2026-01-18 00:31:11,753 - infrastructure.tracking.mlflow.lifecycle - WARNING - Could not ensure run test_run_id... is terminated: Run 'test_run_id' not found
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/lifecycle.py", line 130, in ensure_run_terminated
    run = client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'test_run_id' not found
2026-01-18 00:31:11,753 - deployment.conversion.orchestration - INFO - Conversion completed. ONNX model: /tmp/pytest-of-codespace/pytest-38/test_quantization_int8_adds_fl0/outputs/conversion/test/model_int8.onnx
2026-01-18 00:31:11,758 - deployment.conversion.orchestration - INFO - Output directory: /tmp/pytest-of-codespace/pytest-38/test_quantization_none_no_flag0/outputs/conversion/test
2026-01-18 00:31:11,761 - deployment.conversion.orchestration - INFO - Created MLflow run: test_run_name (test_run_id...)
2026-01-18 00:31:11,762 - deployment.conversion.orchestration - INFO - Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /path/to/checkpoint --config-dir /tmp/pytest-of-codespace/pytest-38/test_quantization_none_no_flag0/config --backbone distilbert-base-uncased --output-dir /tmp/pytest-of-codespace/pytest-38/test_quantization_none_no_flag0/outputs/conversion/test --opset-version 18 --run-smoke-test
2026-01-18 00:31:11,763 - infrastructure.tracking.mlflow.lifecycle - WARNING - Could not ensure run test_run_id... is terminated: Run 'test_run_id' not found
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/lifecycle.py", line 130, in ensure_run_terminated
    run = client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'test_run_id' not found
2026-01-18 00:31:11,763 - deployment.conversion.orchestration - INFO - Conversion completed. ONNX model: /tmp/pytest-of-codespace/pytest-38/test_quantization_none_no_flag0/outputs/conversion/test/model.onnx
2026-01-18 00:31:11,768 - deployment.conversion.orchestration - INFO - Output directory: /tmp/pytest-of-codespace/pytest-38/test_run_smoke_test_true_adds_0/outputs/conversion/test
2026-01-18 00:31:11,772 - deployment.conversion.orchestration - INFO - Created MLflow run: test_run_name (test_run_id...)
2026-01-18 00:31:11,772 - deployment.conversion.orchestration - INFO - Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /path/to/checkpoint --config-dir /tmp/pytest-of-codespace/pytest-38/test_run_smoke_test_true_adds_0/config --backbone distilbert-base-uncased --output-dir /tmp/pytest-of-codespace/pytest-38/test_run_smoke_test_true_adds_0/outputs/conversion/test --opset-version 18 --run-smoke-test
2026-01-18 00:31:11,773 - infrastructure.tracking.mlflow.lifecycle - WARNING - Could not ensure run test_run_id... is terminated: Run 'test_run_id' not found
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/lifecycle.py", line 130, in ensure_run_terminated
    run = client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'test_run_id' not found
2026-01-18 00:31:11,773 - deployment.conversion.orchestration - INFO - Conversion completed. ONNX model: /tmp/pytest-of-codespace/pytest-38/test_run_smoke_test_true_adds_0/outputs/conversion/test/model.onnx
2026-01-18 00:31:11,779 - deployment.conversion.orchestration - INFO - Output directory: /tmp/pytest-of-codespace/pytest-38/test_run_smoke_test_false_no_f0/outputs/conversion/test
2026-01-18 00:31:11,782 - deployment.conversion.orchestration - INFO - Created MLflow run: test_run_name (test_run_id...)
2026-01-18 00:31:11,782 - deployment.conversion.orchestration - INFO - Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /path/to/checkpoint --config-dir /tmp/pytest-of-codespace/pytest-38/test_run_smoke_test_false_no_f0/config --backbone distilbert-base-uncased --output-dir /tmp/pytest-of-codespace/pytest-38/test_run_smoke_test_false_no_f0/outputs/conversion/test --opset-version 18
2026-01-18 00:31:11,783 - infrastructure.tracking.mlflow.lifecycle - WARNING - Could not ensure run test_run_id... is terminated: Run 'test_run_id' not found
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/lifecycle.py", line 130, in ensure_run_terminated
    run = client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'test_run_id' not found
2026-01-18 00:31:11,784 - deployment.conversion.orchestration - INFO - Conversion completed. ONNX model: /tmp/pytest-of-codespace/pytest-38/test_run_smoke_test_false_no_f0/outputs/conversion/test/model.onnx
2026-01-18 00:31:11,789 - deployment.conversion.orchestration - INFO - Output directory: /tmp/pytest-of-codespace/pytest-38/test_filename_pattern_used_in_0/outputs/conversion/test
2026-01-18 00:31:11,792 - deployment.conversion.orchestration - INFO - Created MLflow run: test_run_name (test_run_id...)
2026-01-18 00:31:11,792 - deployment.conversion.orchestration - INFO - Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /path/to/checkpoint --config-dir /tmp/pytest-of-codespace/pytest-38/test_filename_pattern_used_in_0/config --backbone distilbert-base-uncased --output-dir /tmp/pytest-of-codespace/pytest-38/test_filename_pattern_used_in_0/outputs/conversion/test --opset-version 18 --run-smoke-test
2026-01-18 00:31:11,793 - infrastructure.tracking.mlflow.lifecycle - WARNING - Could not ensure run test_run_id... is terminated: Run 'test_run_id' not found
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/lifecycle.py", line 130, in ensure_run_terminated
    run = client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'test_run_id' not found
2026-01-18 00:31:11,794 - deployment.conversion.orchestration - INFO - Conversion completed. ONNX model: /tmp/pytest-of-codespace/pytest-38/test_filename_pattern_used_in_0/outputs/conversion/test/custom_fp32_model.onnx
2026-01-18 00:31:11,877 - evaluation.selection.trial_finder - INFO - Found 3 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 00:31:11,877 - evaluation.selection.trial_finder - INFO - Grouped runs for distilbert: 0 v1 group(s), 1 v2 group(s)
2026-01-18 00:31:11,885 - evaluation.selection.trial_finder - INFO - Found 1 eligible group(s) for distilbert (0 skipped due to min_trials requirement)
2026-01-18 00:31:11,886 - evaluation.selection.artifact_unified.selectors - INFO - Found refit run refit-run-12... for trial run2... (selected latest from 1 refit run(s))
2026-01-18 00:31:11,886 - evaluation.selection.trial_finder - INFO - Found refit run refit-run-12... for champion trial run2... (using SSOT selector: refit_preferred)
2026-01-18 00:31:11,888 - evaluation.selection.trial_finder - INFO - Found 3 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 00:31:11,889 - evaluation.selection.trial_finder - INFO - Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)
2026-01-18 00:31:11,889 - evaluation.selection.trial_finder - INFO - Found 1 eligible group(s) for distilbert (0 skipped due to min_trials requirement)
2026-01-18 00:31:11,889 - evaluation.selection.artifact_unified.selectors - INFO - Found refit run refit-run-12... for trial run2... (selected latest from 1 refit run(s))
2026-01-18 00:31:11,889 - evaluation.selection.trial_finder - INFO - Found refit run refit-run-12... for champion trial run2... (using SSOT selector: refit_preferred)
2026-01-18 00:31:11,891 - evaluation.selection.trial_finder - INFO - No runs found with stage='hpo_trial' for distilbert, trying legacy stage='hpo'
2026-01-18 00:31:11,892 - evaluation.selection.trial_finder - INFO - Found 0 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 00:31:11,892 - evaluation.selection.trial_finder - INFO - Grouped runs for distilbert: 0 v1 group(s), 0 v2 group(s)
2026-01-18 00:31:11,892 - evaluation.selection.trial_finder - WARNING - No valid groups found for distilbert. No trial runs found in HPO experiment 'test_hpo_experiment'. This may indicate:
  - HPO was not run for this backbone
  - Runs exist but don't have required tags (stage='hpo_trial' or 'hpo', backbone tag)
  - Runs exist but were filtered out (missing metrics, artifacts, or grouping tags)
Skipping champion selection for distilbert.
2026-01-18 00:31:11,894 - evaluation.selection.trial_finder - INFO - Found 2 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 00:31:11,895 - evaluation.selection.trial_finder - INFO - Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)
2026-01-18 00:31:11,895 - evaluation.selection.trial_finder - WARNING - Skipping group hash1... - only 2 valid trials (minimum: 3)
2026-01-18 00:31:11,895 - evaluation.selection.trial_finder - WARNING - No eligible groups for distilbert. Processed 1 group(s), but 1 were skipped due to insufficient trials (minimum: 3). Remaining groups: 0
2026-01-18 00:31:11,898 - evaluation.selection.trial_finder - INFO - Found 3 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 00:31:11,898 - evaluation.selection.trial_finder - INFO - Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)
2026-01-18 00:31:11,899 - evaluation.selection.trial_finder - WARNING - Group hash1...: 2 valid metrics, 1 missing/invalid. Missing metric runs: ['run3']
2026-01-18 00:31:11,899 - evaluation.selection.trial_finder - WARNING - Skipping group hash1... - only 2 valid trials (minimum: 3)
2026-01-18 00:31:11,899 - evaluation.selection.trial_finder - WARNING - No eligible groups for distilbert. Processed 1 group(s), but 1 were skipped due to insufficient trials (minimum: 3). Remaining groups: 0
2026-01-18 00:31:11,901 - evaluation.selection.trial_finder - INFO - Found 3 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 00:31:11,902 - evaluation.selection.trial_finder - INFO - Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)
2026-01-18 00:31:11,902 - evaluation.selection.trial_finder - WARNING - Group hash1...: 2 valid metrics, 1 missing/invalid. Missing metric runs: []
2026-01-18 00:31:11,902 - evaluation.selection.trial_finder - WARNING - Skipping group hash1... - only 2 valid trials (minimum: 3)
2026-01-18 00:31:11,902 - evaluation.selection.trial_finder - WARNING - No eligible groups for distilbert. Processed 1 group(s), but 1 were skipped due to insufficient trials (minimum: 3). Remaining groups: 0
2026-01-18 00:31:11,905 - evaluation.selection.trial_finder - INFO - Found 3 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 00:31:11,906 - evaluation.selection.trial_finder - WARNING - Artifact filter: 1 run(s) have code.artifact.available='false' (explicitly marked as unavailable)
2026-01-18 00:31:11,906 - evaluation.selection.trial_finder - WARNING - Artifact filter: 1 run(s) excluded (1 explicitly false, 0 missing/legacy allowed)
2026-01-18 00:31:11,906 - evaluation.selection.trial_finder - WARNING - Artifact filter removed 1 runs for distilbert (2 remaining). Check that runs have 'code.artifact.available' tag set to 'true'.
2026-01-18 00:31:11,906 - evaluation.selection.trial_finder - INFO - Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)
2026-01-18 00:31:11,906 - evaluation.selection.trial_finder - WARNING - Skipping group hash1... - only 2 valid trials (minimum: 3)
2026-01-18 00:31:11,906 - evaluation.selection.trial_finder - WARNING - No eligible groups for distilbert. Processed 1 group(s), but 1 were skipped due to insufficient trials (minimum: 3). Remaining groups: 0
2026-01-18 00:31:11,909 - evaluation.selection.trial_finder - INFO - Found 3 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 00:31:11,909 - evaluation.selection.trial_finder - INFO - Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)
2026-01-18 00:31:11,909 - evaluation.selection.trial_finder - INFO - Found 1 eligible group(s) for distilbert (0 skipped due to min_trials requirement)
2026-01-18 00:31:11,909 - evaluation.selection.artifact_unified.selectors - INFO - Found refit run refit-run-12... for trial run2... (selected latest from 1 refit run(s))
2026-01-18 00:31:11,909 - evaluation.selection.trial_finder - INFO - Found refit run refit-run-12... for champion trial run2... (using SSOT selector: refit_preferred)
2026-01-18 00:31:11,912 - evaluation.selection.trial_finder - INFO - Found 5 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 00:31:11,913 - evaluation.selection.trial_finder - INFO - Grouped runs for distilbert: 1 v1 group(s), 1 v2 group(s)
2026-01-18 00:31:11,913 - evaluation.selection.trial_finder - INFO - Found both v1 and v2 runs for distilbert. Using 2.0 groups only (never mixing versions).
2026-01-18 00:31:11,913 - evaluation.selection.trial_finder - INFO - Found 1 eligible group(s) for distilbert (0 skipped due to min_trials requirement)
2026-01-18 00:31:11,913 - evaluation.selection.artifact_unified.selectors - INFO - Found refit run refit-run-12... for trial run3... (selected latest from 1 refit run(s))
2026-01-18 00:31:11,913 - evaluation.selection.trial_finder - INFO - Found refit run refit-run-12... for champion trial run3... (using SSOT selector: refit_preferred)
2026-01-18 00:31:11,916 - evaluation.selection.trial_finder - INFO - Found 3 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 00:31:11,916 - evaluation.selection.trial_finder - INFO - Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)
2026-01-18 00:31:11,916 - evaluation.selection.trial_finder - INFO - Found 1 eligible group(s) for distilbert (0 skipped due to min_trials requirement)
2026-01-18 00:31:11,917 - evaluation.selection.artifact_unified.selectors - INFO - Found refit run refit-run-12... for trial run3... (selected latest from 1 refit run(s))
2026-01-18 00:31:11,917 - evaluation.selection.trial_finder - INFO - Found refit run refit-run-12... for champion trial run3... (using SSOT selector: refit_preferred)
2026-01-18 00:31:11,920 - evaluation.selection.trial_finder - INFO - Found 6 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 00:31:11,921 - evaluation.selection.trial_finder - INFO - Grouped runs for distilbert: 2 v1 group(s), 0 v2 group(s)
2026-01-18 00:31:11,921 - evaluation.selection.trial_finder - INFO - Found 2 eligible group(s) for distilbert (0 skipped due to min_trials requirement)
2026-01-18 00:31:11,922 - evaluation.selection.artifact_unified.selectors - INFO - Found refit run refit-run-12... for trial run6... (selected latest from 1 refit run(s))
2026-01-18 00:31:11,922 - evaluation.selection.trial_finder - INFO - Found refit run refit-run-12... for champion trial run6... (using SSOT selector: refit_preferred)
2026-01-18 00:31:11,925 - evaluation.selection.trial_finder - INFO - Found 5 runs with stage tag for distilbert (backbone=distilbert)
2026-01-18 00:31:11,925 - evaluation.selection.trial_finder - INFO - Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)
2026-01-18 00:31:11,925 - evaluation.selection.trial_finder - INFO - Found 1 eligible group(s) for distilbert (0 skipped due to min_trials requirement)
2026-01-18 00:31:11,926 - evaluation.selection.artifact_unified.selectors - INFO - Found refit run refit-run-12... for trial run3... (selected latest from 1 refit run(s))
2026-01-18 00:31:11,926 - evaluation.selection.trial_finder - INFO - Found refit run refit-run-12... for champion trial run3... (using SSOT selector: refit_preferred)
2026-01-18 00:31:11,928 - training.execution.executor - INFO - Final training config loaded from final_training.yaml
2026-01-18 00:31:11,929 - training.execution.executor - INFO - Output directory: /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_re0/outputs/v1
2026-01-18 00:31:11,929 - training.execution.executor - INFO - Found existing completed final training run
2026-01-18 00:31:11,929 - training.execution.executor - INFO -   Output directory: /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_re0/outputs/v1
2026-01-18 00:31:11,929 - training.execution.executor - INFO -   Checkpoint: /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_re0/outputs/v1/checkpoint
2026-01-18 00:31:11,929 - training.execution.executor - INFO -   Reusing existing checkpoint (run.mode: reuse_if_exists)
2026-01-18 00:31:11,932 - training.execution.executor - INFO - Final training config loaded from final_training.yaml
2026-01-18 00:31:11,932 - training.execution.executor - INFO - Output directory: /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_fo0/outputs/v1
2026-01-18 00:31:11,938 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_fo0/config/tags.yaml, using defaults
2026-01-18 00:31:11,949 - training.execution.mlflow_setup - INFO -  View run test_run_name at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/594159571823761936/runs/ab235bd81ea0493aa66ca7bc757a1e84
2026-01-18 00:31:11,949 - training.execution.mlflow_setup - INFO - Created MLflow run: test_run_name (ab235bd81ea0...)
2026-01-18 00:31:11,950 - training.execution.executor - INFO - Created MLflow run: test_run_name (ab235bd81ea0...)
2026-01-18 00:31:11,950 - training.execution.executor - INFO - Running final training...
2026-01-18 00:31:11,951 - infrastructure.tracking.mlflow.lifecycle - INFO - Run ab235bd81ea0... already terminated with status <Mock name='mock.get_run().info.status' id='140077261534928'> (expected FINISHED)
2026-01-18 00:31:11,951 - training.execution.executor - INFO - Saved metadata to: None
2026-01-18 00:31:11,951 - training.execution.executor - INFO - Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_fo0/outputs/v1/checkpoint
2026-01-18 00:31:11,951 - training.execution.executor - INFO - MLflow run: ab235bd81ea0...
2026-01-18 00:31:11,956 - training.execution.executor - INFO - Final training config loaded from final_training.yaml
2026-01-18 00:31:11,956 - training.execution.executor - INFO - Output directory: /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_re1/outputs/v1
2026-01-18 00:31:11,961 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_re1/config/tags.yaml, using defaults
2026-01-18 00:31:11,970 - training.execution.mlflow_setup - INFO -  View run test_run_name at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/594159571823761936/runs/f4ba5c72c8be41e385e10ee47e31935a
2026-01-18 00:31:11,970 - training.execution.mlflow_setup - INFO - Created MLflow run: test_run_name (f4ba5c72c8be...)
2026-01-18 00:31:11,970 - training.execution.executor - INFO - Created MLflow run: test_run_name (f4ba5c72c8be...)
2026-01-18 00:31:11,970 - training.execution.executor - INFO - Running final training...
2026-01-18 00:31:11,971 - infrastructure.tracking.mlflow.lifecycle - INFO - Run f4ba5c72c8be... already terminated with status <Mock name='mock.get_run().info.status' id='140077261523072'> (expected FINISHED)
2026-01-18 00:31:11,971 - training.execution.executor - INFO - Saved metadata to: None
2026-01-18 00:31:11,971 - training.execution.executor - INFO - Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_re1/outputs/v1/checkpoint
2026-01-18 00:31:11,971 - training.execution.executor - INFO - MLflow run: f4ba5c72c8be...
2026-01-18 00:31:11,974 - training.execution.executor - INFO - Final training config loaded from final_training.yaml
2026-01-18 00:31:11,974 - training.execution.executor - INFO - Output directory: /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_mi0/outputs/v1
2026-01-18 00:31:11,977 - training.execution.executor - INFO - Final training config loaded from final_training.yaml
2026-01-18 00:31:11,977 - training.execution.executor - INFO - Output directory: /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_lo0/outputs/v1
2026-01-18 00:31:11,987 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_lo0/config/tags.yaml, using defaults
2026-01-18 00:31:11,998 - training.execution.mlflow_setup - INFO -  View run test_run_name at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/594159571823761936/runs/166c04dc7e414b3fb1b1b55a94f835f8
2026-01-18 00:31:11,999 - training.execution.mlflow_setup - INFO - Created MLflow run: test_run_name (166c04dc7e41...)
2026-01-18 00:31:11,999 - training.execution.executor - INFO - Created MLflow run: test_run_name (166c04dc7e41...)
2026-01-18 00:31:11,999 - training.execution.executor - INFO - Running final training...
2026-01-18 00:31:11,999 - infrastructure.tracking.mlflow.lifecycle - INFO - Run 166c04dc7e41... already terminated with status <Mock name='mock.get_run().info.status' id='140077261531616'> (expected FINISHED)
2026-01-18 00:31:11,999 - training.execution.executor - INFO - Saved metadata to: None
2026-01-18 00:31:12,000 - training.execution.executor - INFO - Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_lo0/outputs/v1/checkpoint
2026-01-18 00:31:12,000 - training.execution.executor - INFO - MLflow run: 166c04dc7e41...
2026-01-18 00:31:12,005 - training.execution.executor - INFO - Final training config loaded from final_training.yaml
2026-01-18 00:31:12,005 - training.execution.executor - INFO - Output directory: /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_tr0/outputs/v1
2026-01-18 00:31:12,010 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_tr0/config/tags.yaml, using defaults
2026-01-18 00:31:12,019 - training.execution.mlflow_setup - INFO -  View run test_run_name at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/594159571823761936/runs/aa4da4c3d5de46038af882087bf60327
2026-01-18 00:31:12,019 - training.execution.mlflow_setup - INFO - Created MLflow run: test_run_name (aa4da4c3d5de...)
2026-01-18 00:31:12,019 - training.execution.executor - INFO - Created MLflow run: test_run_name (aa4da4c3d5de...)
2026-01-18 00:31:12,019 - training.execution.executor - INFO - Running final training...
2026-01-18 00:31:12,019 - infrastructure.tracking.mlflow.lifecycle - INFO - Run aa4da4c3d5de... already has status <Mock name='mock.get_run().info.status' id='140077259681136'>, skipping termination (expected RUNNING)
2026-01-18 00:31:12,023 - training.execution.executor - INFO - Final training config loaded from final_training.yaml
2026-01-18 00:31:12,023 - training.execution.executor - INFO - Output directory: /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_ml0/outputs/v1
2026-01-18 00:31:12,028 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_ml0/config/tags.yaml, using defaults
2026-01-18 00:31:12,036 - training.execution.mlflow_setup - INFO -  View run test_run_name at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/594159571823761936/runs/e7fb10b787d843f2b24ce00b75c12dbb
2026-01-18 00:31:12,036 - training.execution.mlflow_setup - INFO - Created MLflow run: test_run_name (e7fb10b787d8...)
2026-01-18 00:31:12,037 - training.execution.executor - INFO - Created MLflow run: test_run_name (e7fb10b787d8...)
2026-01-18 00:31:12,037 - training.execution.executor - INFO - Running final training...
2026-01-18 00:31:12,037 - infrastructure.tracking.mlflow.lifecycle - INFO - Run e7fb10b787d8... already terminated with status <Mock name='mock.get_run().info.status' id='140077259675088'> (expected FINISHED)
2026-01-18 00:31:12,037 - training.execution.executor - INFO - Saved metadata to: None
2026-01-18 00:31:12,037 - training.execution.executor - INFO - Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_ml0/outputs/v1/checkpoint
2026-01-18 00:31:12,037 - training.execution.executor - INFO - MLflow run: e7fb10b787d8...
2026-01-18 00:31:12,040 - training.execution.executor - INFO - Final training config loaded from final_training.yaml
2026-01-18 00:31:12,040 - training.execution.executor - INFO - Output directory: /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_so0/outputs/v1
2026-01-18 00:31:12,045 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_so0/config/tags.yaml, using defaults
2026-01-18 00:31:12,053 - training.execution.mlflow_setup - INFO -  View run test_run_name at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/594159571823761936/runs/2584c83d1ca34001933be4479a23614d
2026-01-18 00:31:12,054 - training.execution.mlflow_setup - INFO - Created MLflow run: test_run_name (2584c83d1ca3...)
2026-01-18 00:31:12,054 - training.execution.executor - INFO - Created MLflow run: test_run_name (2584c83d1ca3...)
2026-01-18 00:31:12,054 - training.execution.executor - INFO - Running final training...
2026-01-18 00:31:12,054 - infrastructure.tracking.mlflow.lifecycle - INFO - Run 2584c83d1ca3... already terminated with status <Mock name='mock.get_run().info.status' id='140077259676144'> (expected FINISHED)
2026-01-18 00:31:12,054 - training.execution.executor - INFO - Saved metadata to: None
2026-01-18 00:31:12,055 - training.execution.executor - INFO - Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_so0/outputs/v1/checkpoint
2026-01-18 00:31:12,055 - training.execution.executor - INFO - MLflow run: 2584c83d1ca3...
2026-01-18 00:31:12,057 - training.execution.executor - INFO - Final training config loaded from final_training.yaml
2026-01-18 00:31:12,057 - training.execution.executor - INFO - Output directory: /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_so1/outputs/v1
2026-01-18 00:31:12,063 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_so1/config/tags.yaml, using defaults
2026-01-18 00:31:12,073 - training.execution.mlflow_setup - INFO -  View run test_run_name at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/594159571823761936/runs/6864c44c43e74d479824b6364000933c
2026-01-18 00:31:12,073 - training.execution.mlflow_setup - INFO - Created MLflow run: test_run_name (6864c44c43e7...)
2026-01-18 00:31:12,073 - training.execution.executor - INFO - Created MLflow run: test_run_name (6864c44c43e7...)
2026-01-18 00:31:12,073 - training.execution.executor - INFO - Running final training...
2026-01-18 00:31:12,074 - infrastructure.tracking.mlflow.lifecycle - INFO - Run 6864c44c43e7... already terminated with status <Mock name='mock.get_run().info.status' id='140077259677872'> (expected FINISHED)
2026-01-18 00:31:12,074 - training.execution.executor - INFO - Saved metadata to: None
2026-01-18 00:31:12,074 - training.execution.executor - INFO - Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_so1/outputs/v1/checkpoint
2026-01-18 00:31:12,074 - training.execution.executor - INFO - MLflow run: 6864c44c43e7...
2026-01-18 00:31:12,077 - training.execution.executor - INFO - Final training config loaded from final_training.yaml
2026-01-18 00:31:12,077 - training.execution.executor - INFO - Output directory: /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_hy0/outputs/v1
2026-01-18 00:31:12,081 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_hy0/config/tags.yaml, using defaults
2026-01-18 00:31:12,090 - training.execution.mlflow_setup - INFO -  View run test_run_name at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/594159571823761936/runs/0d9ad5e92d8b49efb144d49b2d57a178
2026-01-18 00:31:12,090 - training.execution.mlflow_setup - INFO - Created MLflow run: test_run_name (0d9ad5e92d8b...)
2026-01-18 00:31:12,090 - training.execution.executor - INFO - Created MLflow run: test_run_name (0d9ad5e92d8b...)
2026-01-18 00:31:12,090 - training.execution.executor - INFO - Running final training...
2026-01-18 00:31:12,090 - infrastructure.tracking.mlflow.lifecycle - INFO - Run 0d9ad5e92d8b... already terminated with status <Mock name='mock.get_run().info.status' id='140077259675568'> (expected FINISHED)
2026-01-18 00:31:12,091 - training.execution.executor - INFO - Saved metadata to: None
2026-01-18 00:31:12,091 - training.execution.executor - INFO - Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_hy0/outputs/v1/checkpoint
2026-01-18 00:31:12,091 - training.execution.executor - INFO - MLflow run: 0d9ad5e92d8b...
2026-01-18 00:31:12,093 - training.execution.executor - INFO - Final training config loaded from final_training.yaml
2026-01-18 00:31:12,093 - training.execution.executor - INFO - Output directory: /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_ml1/outputs/v1
2026-01-18 00:31:12,098 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_ml1/config/tags.yaml, using defaults
2026-01-18 00:31:12,109 - training.execution.mlflow_setup - INFO -  View run default_run_name at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/966734036254045421/runs/83c02dea447e4bd7acd026eaab9b75c6
2026-01-18 00:31:12,110 - training.execution.mlflow_setup - INFO - Created MLflow run: default_run_name (83c02dea447e...)
2026-01-18 00:31:12,110 - training.execution.executor - INFO - Created MLflow run: default_run_name (83c02dea447e...)
2026-01-18 00:31:12,110 - training.execution.executor - INFO - Running final training...
2026-01-18 00:31:12,110 - infrastructure.tracking.mlflow.lifecycle - INFO - Run 83c02dea447e... already terminated with status <Mock name='mock.get_run().info.status' id='140077259676432'> (expected FINISHED)
2026-01-18 00:31:12,110 - training.execution.executor - INFO - Saved metadata to: None
2026-01-18 00:31:12,111 - training.execution.executor - INFO - Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-38/test_execute_final_training_ml1/outputs/v1/checkpoint
2026-01-18 00:31:12,111 - training.execution.executor - INFO - MLflow run: 83c02dea447e...
2026-01-18 00:31:12,113 - training.execution.executor - INFO - Final training config loaded from final_training.yaml
2026-01-18 00:31:12,113 - training.execution.executor - INFO - Output directory: /tmp/pytest-of-codespace/pytest-38/test_logging_eval_interval_loa0/outputs/v1
2026-01-18 00:31:12,118 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_logging_eval_interval_loa0/config/tags.yaml, using defaults
2026-01-18 00:31:12,126 - training.execution.mlflow_setup - INFO -  View run test_run_name at: file:///tmp/mlflow/#/experiments/594159571823761936/runs/af3b1035cce64567a3c020fa1174452c
2026-01-18 00:31:12,126 - training.execution.mlflow_setup - INFO - Created MLflow run: test_run_name (af3b1035cce6...)
2026-01-18 00:31:12,126 - training.execution.executor - INFO - Created MLflow run: test_run_name (af3b1035cce6...)
2026-01-18 00:31:12,126 - training.execution.executor - INFO - Running final training...
2026-01-18 00:31:12,127 - infrastructure.tracking.mlflow.lifecycle - INFO - Run af3b1035cce6... already terminated with status <Mock name='mock.get_run().info.status' id='140077259673696'> (expected FINISHED)
2026-01-18 00:31:12,127 - training.execution.executor - INFO - Saved metadata to: None
2026-01-18 00:31:12,127 - training.execution.executor - INFO - Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-38/test_logging_eval_interval_loa0/outputs/v1/checkpoint
2026-01-18 00:31:12,127 - training.execution.executor - INFO - MLflow run: af3b1035cce6...
2026-01-18 00:31:12,130 - training.execution.executor - INFO - Final training config loaded from final_training.yaml
2026-01-18 00:31:12,130 - training.execution.executor - INFO - Output directory: /tmp/pytest-of-codespace/pytest-38/test_logging_save_interval_loa0/outputs/v1
2026-01-18 00:31:12,135 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_logging_save_interval_loa0/config/tags.yaml, using defaults
2026-01-18 00:31:12,143 - training.execution.mlflow_setup - INFO -  View run test_run_name at: file:///tmp/mlflow/#/experiments/594159571823761936/runs/baef4609a982419d9f7f292d3376e5eb
2026-01-18 00:31:12,143 - training.execution.mlflow_setup - INFO - Created MLflow run: test_run_name (baef4609a982...)
2026-01-18 00:31:12,143 - training.execution.executor - INFO - Created MLflow run: test_run_name (baef4609a982...)
2026-01-18 00:31:12,143 - training.execution.executor - INFO - Running final training...
2026-01-18 00:31:12,144 - infrastructure.tracking.mlflow.lifecycle - INFO - Run baef4609a982... already terminated with status <Mock name='mock.get_run().info.status' id='140077259677104'> (expected FINISHED)
2026-01-18 00:31:12,144 - training.execution.executor - INFO - Saved metadata to: None
2026-01-18 00:31:12,144 - training.execution.executor - INFO - Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-38/test_logging_save_interval_loa0/outputs/v1/checkpoint
2026-01-18 00:31:12,144 - training.execution.executor - INFO - MLflow run: baef4609a982...
2026-01-18 00:31:12,146 - training.execution.executor - INFO - Final training config loaded from final_training.yaml
2026-01-18 00:31:12,147 - training.execution.executor - INFO - Output directory: /tmp/pytest-of-codespace/pytest-38/test_logging_intervals_both_lo0/outputs/v1
2026-01-18 00:31:12,154 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_logging_intervals_both_lo0/config/tags.yaml, using defaults
2026-01-18 00:31:12,164 - training.execution.mlflow_setup - INFO -  View run test_run_name at: file:///tmp/mlflow/#/experiments/594159571823761936/runs/15aefaf9490e45079b6dd8a3a15cbd13
2026-01-18 00:31:12,164 - training.execution.mlflow_setup - INFO - Created MLflow run: test_run_name (15aefaf9490e...)
2026-01-18 00:31:12,164 - training.execution.executor - INFO - Created MLflow run: test_run_name (15aefaf9490e...)
2026-01-18 00:31:12,164 - training.execution.executor - INFO - Running final training...
2026-01-18 00:31:12,164 - infrastructure.tracking.mlflow.lifecycle - INFO - Run 15aefaf9490e... already terminated with status <Mock name='mock.get_run().info.status' id='140077281274672'> (expected FINISHED)
2026-01-18 00:31:12,165 - training.execution.executor - INFO - Saved metadata to: None
2026-01-18 00:31:12,165 - training.execution.executor - INFO - Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-38/test_logging_intervals_both_lo0/outputs/v1/checkpoint
2026-01-18 00:31:12,165 - training.execution.executor - INFO - MLflow run: 15aefaf9490e...
[I 2026-01-18 00:31:13,107] A new study created in RDB with name: test_best
[I 2026-01-18 00:31:13,137] Trial 0 finished with value: 0.7 and parameters: {}. Best is trial 0 with value: 0.7.
[I 2026-01-18 00:31:13,154] Trial 1 finished with value: 0.85 and parameters: {}. Best is trial 1 with value: 0.85.
[I 2026-01-18 00:31:13,172] Trial 2 finished with value: 0.75 and parameters: {}. Best is trial 1 with value: 0.85.
[I 2026-01-18 00:31:13,273] A new study created in RDB with name: test_extract
[I 2026-01-18 00:31:13,312] Trial 0 finished with value: 0.8 and parameters: {'learning_rate': 1.2737149778041263e-05, 'batch_size': 6}. Best is trial 0 with value: 0.8.
[I 2026-01-18 00:31:13,411] A new study created in RDB with name: test_cv
[I 2026-01-18 00:31:13,446] Trial 0 finished with value: 0.8 and parameters: {}. Best is trial 0 with value: 0.8.
[I 2026-01-18 00:31:13,546] A new study created in RDB with name: test_minimize
[I 2026-01-18 00:31:13,571] Trial 0 finished with value: 0.1 and parameters: {}. Best is trial 0 with value: 0.1.
[I 2026-01-18 00:31:13,590] Trial 1 finished with value: 0.2 and parameters: {}. Best is trial 0 with value: 0.1.
[I 2026-01-18 00:31:13,607] Trial 2 finished with value: 0.15 and parameters: {}. Best is trial 0 with value: 0.1.
[I 2026-01-18 00:31:13,807] A new study created in RDB with name: test_refit
[I 2026-01-18 00:31:13,875] Trial 0 finished with value: 0.7 and parameters: {'learning_rate': 4.678318382911412e-05}. Best is trial 0 with value: 0.7.
[I 2026-01-18 00:31:13,902] Trial 1 finished with value: 0.85 and parameters: {'learning_rate': 2.126968246381897e-05}. Best is trial 1 with value: 0.85.
[I 2026-01-18 00:31:14,439] A new study created in RDB with name: test_empty
[I 2026-01-18 00:31:14,540] A new study created in RDB with name: test_params
[I 2026-01-18 00:31:14,608] Trial 0 finished with value: 0.8 and parameters: {'learning_rate': 3.598573020095394e-05, 'batch_size': 4, 'dropout': 0.2116008898176649, 'weight_decay': 0.00554133656481045}. Best is trial 0 with value: 0.8.
[I 2026-01-18 00:31:14,717] A new study created in RDB with name: test_delay
[I 2026-01-18 00:31:14,754] Trial 0 finished with value: 0.5 and parameters: {}. Best is trial 0 with value: 0.5.
[I 2026-01-18 00:31:14,779] Trial 1 finished with value: 0.3 and parameters: {}. Best is trial 0 with value: 0.5.
[I 2026-01-18 00:31:14,882] A new study created in RDB with name: test_prune
[I 2026-01-18 00:31:14,918] Trial 0 finished with value: 0.8 and parameters: {}. Best is trial 0 with value: 0.8.
[I 2026-01-18 00:31:14,939] Trial 1 finished with value: 0.75 and parameters: {}. Best is trial 0 with value: 0.8.
[I 2026-01-18 00:31:14,958] Trial 2 pruned. 
[I 2026-01-18 00:31:15,052] A new study created in RDB with name: test_interval
[I 2026-01-18 00:31:15,092] Trial 0 finished with value: 0.3 and parameters: {}. Best is trial 0 with value: 0.3.
2026-01-18 00:31:15,099 - training.hpo.core.study - INFO - [HPO] Starting optimization for distilbert with checkpointing...
[I 2026-01-18 00:31:15,194] A new study created in RDB with name: test_best
[I 2026-01-18 00:31:15,228] Trial 0 finished with value: 0.9 and parameters: {}. Best is trial 0 with value: 0.9.
[I 2026-01-18 00:31:15,250] Trial 1 finished with value: 0.7 and parameters: {}. Best is trial 0 with value: 0.9.
[I 2026-01-18 00:31:15,271] Trial 2 finished with value: 0.8 and parameters: {}. Best is trial 0 with value: 0.9.
2026-01-18 00:31:15,278 - training.hpo.core.study - INFO - [HPO] Starting optimization for distilbert with checkpointing...
2026-01-18 00:31:15,279 - training.hpo.core.study - INFO - [HPO] Starting optimization for distilbert with checkpointing...
[I 2026-01-18 00:31:15,376] A new study created in RDB with name: test_no_prune
[I 2026-01-18 00:31:15,403] Trial 0 finished with value: 0.5 and parameters: {}. Best is trial 0 with value: 0.5.
[I 2026-01-18 00:31:15,425] Trial 1 finished with value: 0.5 and parameters: {}. Best is trial 0 with value: 0.5.
[I 2026-01-18 00:31:15,442] Trial 2 finished with value: 0.5 and parameters: {}. Best is trial 0 with value: 0.5.
2026-01-18 00:31:15,463 - training.hpo.trial.metrics - ERROR - metrics.json not found at expected location: /tmp/pytest-of-codespace/pytest-38/test_missing_metrics_file0/outputs/hpo/local/distilbert/study-abc12345/trial-def67890/metrics.json. Trial output dir: /tmp/pytest-of-codespace/pytest-38/test_missing_metrics_file0/outputs/hpo/local/distilbert/study-abc12345/trial-def67890, Root dir: /tmp/pytest-of-codespace/pytest-38/test_missing_metrics_file0
2026-01-18 00:31:15,474 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_cv_trial_failure_propagat0/config/tags.yaml, using defaults
2026-01-18 00:31:15,477 - training.hpo.execution.local.cv - INFO - [CV] Created trial folder: /workspaces/resume-ner-azureml/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-d7c6e3fb (trial 0)
2026-01-18 00:31:15,482 - infrastructure.tracking.mlflow.hash_utils - WARNING - Could not get or compute study_key_hash. study_key_hash=missing, hpo_parent_run_id=provided, configs=missing
2026-01-18 00:31:15,483 - training.hpo.execution.local.cv - WARNING - Could not build systematic run name and tags: HPO trial run name built without study_key_hash; check study identity propagation., using fallback
2026-01-18 00:31:15,484 - infrastructure.tracking.mlflow.hash_utils - WARNING - Could not get or compute study_key_hash. study_key_hash=missing, hpo_parent_run_id=provided, configs=missing
2026-01-18 00:31:15,484 - training.hpo.execution.local.cv - ERROR - In v2 study folder study-abc12345 but trial_key_hash is None. study_key_hash=NO. Will attempt to compute hash.
2026-01-18 00:31:15,484 - training.hpo.execution.local.cv - WARNING - In v2 study folder but missing hashes: study_key_hash=NO, trial_key_hash=NO
2026-01-18 00:31:15,489 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'batch_size': 4, 'dropout': 0.2, 'weight_decay': 0.05, 'backbone': 'distilbert'}
2026-01-18 00:31:15,489 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 00:31:15,493 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_refit_subprocess_failure0/config/tags.yaml, using defaults
2026-01-18 00:31:15,510 - training.execution.mlflow_setup - INFO -  View run local_distilbert_hpo_refit_legacy at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/302183572435427150/runs/a9d14aa7ae8e462aa7b13f6a369b322d
2026-01-18 00:31:15,511 - training.execution.mlflow_setup - INFO - Created MLflow run: local_distilbert_hpo_refit_legacy (a9d14aa7ae8e...)
2026-01-18 00:31:15,511 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run a9d14aa7ae8e... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-18 00:31:15,512 - training.hpo.execution.local.refit - ERROR - [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 548, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
2026-01-18 00:31:15,559 - training.hpo.execution.local.refit - ERROR - Training failed with return code 1
2026-01-18 00:31:15,559 - training.hpo.execution.local.refit - ERROR - STDOUT:

2026-01-18 00:31:15,559 - training.hpo.execution.local.refit - ERROR - STDERR:
/opt/conda/envs/resume-ner-training/bin/python: Error while finding module specification for 'training.cli.train' (ModuleNotFoundError: No module named 'training.cli')

2026-01-18 00:31:15,564 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial 0 with hyperparameters: {'backbone': 'distilbert'}
2026-01-18 00:31:15,564 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 00:31:15,564 - training.hpo.execution.local.refit - WARNING - Could not construct v2 refit folder, falling back to legacy: 'NoneType' object has no attribute 'mkdir'
2026-01-18 00:31:15,567 - training.hpo.core.study - INFO - [HPO] Starting optimization for distilbert with checkpointing...
[I 2026-01-18 00:31:15,665] A new study created in RDB with name: empty_study
[I 2026-01-18 00:31:15,778] A new study created in RDB with name: hpo_distilbert_test
2026-01-18 00:31:15,801 - training.hpo.core.study - INFO - [HPO] Resuming optimization for distilbert from checkpoint...
2026-01-18 00:31:15,803 - training.hpo.core.study - INFO - Loaded 0 existing trials (0 completed, 0 marked as failed)
[I 2026-01-18 00:31:15,903] A new study created in RDB with name: hpo_distilbert_resume_test
2026-01-18 00:31:15,956 - training.hpo.core.study - INFO - [HPO] Resuming optimization for distilbert from checkpoint...
2026-01-18 00:31:15,957 - training.hpo.core.study - INFO - Loaded 0 existing trials (0 completed, 0 marked as failed)
[I 2026-01-18 00:31:16,050] A new study created in RDB with name: hpo_distilbert_interrupted_test
2026-01-18 00:31:16,084 - training.hpo.core.study - INFO - [HPO] Resuming optimization for distilbert from checkpoint...
2026-01-18 00:31:16,084 - training.hpo.core.study - INFO - Loaded 0 existing trials (0 completed, 0 marked as failed)
[I 2026-01-18 00:31:16,181] A new study created in RDB with name: hpo_distilbert_no_resume_test
2026-01-18 00:31:16,207 - training.hpo.core.study - INFO - [HPO] auto_resume=false: Creating new study 'hpo_distilbert_no_resume_test' (ignoring existing study)
2026-01-18 00:31:16,207 - training.hpo.core.study - INFO - [HPO] Starting optimization for distilbert with checkpointing...
[I 2026-01-18 00:31:16,301] A new study created in RDB with name: hpo_distilbert_numbering_test
2026-01-18 00:31:16,340 - training.hpo.core.study - INFO - [HPO] Resuming optimization for distilbert from checkpoint...
2026-01-18 00:31:16,341 - training.hpo.core.study - INFO - Loaded 0 existing trials (0 completed, 0 marked as failed)
[I 2026-01-18 00:31:16,439] A new study created in RDB with name: hpo_distilbert_smoke_resume
2026-01-18 00:31:16,461 - training.hpo.core.study - INFO - [HPO] Resuming optimization for distilbert from checkpoint...
2026-01-18 00:31:16,461 - training.hpo.core.study - INFO - Loaded 0 existing trials (0 completed, 0 marked as failed)
[I 2026-01-18 00:31:16,557] A new study created in RDB with name: hpo_distilbert_file_test
[I 2026-01-18 00:31:16,666] A new study created in RDB with name: hpo_distilbert_persist_test
[I 2026-01-18 00:31:16,839] A new study created in RDB with name: hpo_distilbert_move_test
2026-01-18 00:31:16,904 - training.hpo.execution.local.sweep - INFO - [EARLY HASH] Computed v2 study_key_hash=01b6aaaa11c2ca02... for folder creation
2026-01-18 00:31:16,905 - training.hpo.core.study - INFO - [HPO] Starting optimization for distilbert with checkpointing...
2026-01-18 00:31:16,909 - training.hpo.execution.local.sweep - INFO - [HPO Setup] k_folds=2, fold_splits_file=None, k_fold config: {'enabled': True, 'n_splits': 2, 'random_seed': 42, 'shuffle': True, 'stratified': True}
2026-01-18 00:31:16,910 - training.hpo.execution.local.sweep - INFO - [CV Setup]  Created 2 fold splits and saved to /tmp/pytest-of-codespace/pytest-38/test_full_hpo_workflow_with_cv0/outputs/hpo/local/distilbert/study-01b6aaaa/fold_splits.json
2026-01-18 00:31:16,913 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Using LOCAL MLflow tracking (not Azure ML)
2026-01-18 00:31:16,913 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
2026-01-18 00:31:17,490 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=01b6aaaa11c2ca02...
2026-01-18 00:31:17,490 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=01b6aaaa11c2ca02..., study_family_hash=6a3fd9967110a5db...
2026-01-18 00:31:17,525 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Yielding RunHandle. run_id=37cce966ef87...
2026-01-18 00:31:17,527 - training.hpo.execution.local.sweep - INFO - [HPO] Parent run 37cce966ef87... is now visible in MLflow (status: RUNNING)
2026-01-18 00:31:17,530 - training.hpo.execution.local.sweep - INFO - Setting Phase 2 tags on parent run 37cce966ef87... (data_config=present, train_config=present)
2026-01-18 00:31:17,531 - training.hpo.execution.local.sweep - WARNING - Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
2026-01-18 00:31:17,531 - training.hpo.execution.local.sweep - WARNING - Could not set data fingerprint tag: 'Missing tag key: fingerprint.data'
2026-01-18 00:31:17,531 - training.hpo.execution.local.sweep - WARNING - Could not set eval fingerprint tag: 'Missing tag key: fingerprint.eval'
2026-01-18 00:31:17,532 - training.hpo.execution.local.sweep - WARNING - Could not set objective direction tag: 'Missing tag key: objective.direction'
2026-01-18 00:31:17,532 - training.hpo.execution.local.sweep - WARNING - Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
2026-01-18 00:31:17,532 - training.hpo.execution.local.sweep - WARNING - Could not set artifact.available tag: 'Missing tag key: artifact.available'
2026-01-18 00:31:17,532 - training.hpo.execution.local.sweep - INFO -  Set Phase 2 tags on parent run 37cce966ef87...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-18 00:31:17,532 - training.hpo.execution.local.sweep - INFO -  Successfully completed Phase 2 tag setting for parent run 37cce966ef87...
2026-01-18 00:31:17,532 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Starting cleanup check: parent_run_id=37cce966ef87...
2026-01-18 00:31:17,534 - training.hpo.tracking.cleanup - INFO - [CLEANUP] MLflow imported successfully. Current env: local, run_key_hash: cc90dfbe1603...
2026-01-18 00:31:17,537 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Retrieved experiment: 975860881501093750
2026-01-18 00:31:17,537 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Fetching all runs in experiment (may paginate for large experiments)...
2026-01-18 00:31:17,829 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Fetched 14 total runs from experiment
2026-01-18 00:31:17,830 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Built parentchildren map: 0 parents have children
2026-01-18 00:31:17,830 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Status breakdown: {'RUNNING': 1, 'FINISHED': 13}
2026-01-18 00:31:17,832 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Found 0 tag-based matches, 0 name-fallback matches (legacy), 0 total eligible for tagging
2026-01-18 00:31:17,832 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Found 0 orphaned child runs (RUNNING children with terminal parents)
2026-01-18 00:31:17,832 - training.hpo.tracking.cleanup - INFO - [CLEANUP] No interrupted parent runs found to tag
2026-01-18 00:31:17,832 - training.hpo.tracking.cleanup - INFO - [CLEANUP] No orphaned child runs found to tag
2026-01-18 00:31:17,833 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=37cce966ef87...
2026-01-18 00:31:17,833 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Found 0 completed trials out of 0 total
2026-01-18 00:31:17,833 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging metrics: n_trials=0, n_completed_trials=0
2026-01-18 00:31:17,847 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=<MagicMock name='mock.create_study().best_value' id='140077206668144'>
2026-01-18 00:31:17,854 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging best hyperparameters: <MagicMock name='mock.create_study().best_params' id='140077206783792'>
2026-01-18 00:31:17,855 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Finding and logging best trial run ID...
2026-01-18 00:31:17,952 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Could not find MLflow run ID for best trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>. Search failed: Invalid clause(s) in filter string: 'tags.trial_number = '<MagicMock name='mock.create_study()', '.', 'best_trial.number', '' id='', '140077206848368', ''>''. This may be a timing issue - trial runs may not be created/committed yet.
2026-01-18 00:31:17,952 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Completed best trial ID logging
2026-01-18 00:31:17,952 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Checkpoint logging enabled: False (upload_checkpoint=False)
2026-01-18 00:31:17,952 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
2026-01-18 00:31:17,963 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Completed log_final_metrics successfully
2026-01-18 00:31:17,963 - training.hpo.execution.local.sweep - INFO - [REFIT] Starting refit training for best trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>
2026-01-18 00:31:17,975 - training.hpo.execution.local.sweep - WARNING - [REFIT] Could not determine trial_key_hash from best trial run tags. Refit run may not match trial run.
2026-01-18 00:31:17,976 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'> with hyperparameters: <MagicMock name='mock.create_study().best_trial.params' id='140077206865856'>
2026-01-18 00:31:17,977 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id="trial_<MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>_20260118_003116", run_id='20260118_003116', trial_number=<MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>
2026-01-18 00:31:17,977 - infrastructure.tracking.mlflow.hash_utils - WARNING - Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
2026-01-18 00:31:17,977 - training.hpo.execution.local.sweep - WARNING - [REFIT] Refit training failed: Cannot create refit in v2 study folder study-01b6aaaa without trial_key_hash. study_key_hash=present, trial_key_hash=missing, refit_params=missing. Hash computation from trial parameters may have failed.. Continuing without refit checkpoint.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py", line 1244, in run_local_hpo_sweep
    refit_metrics, refit_checkpoint_dir, refit_run_id = run_refit_training(
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 240, in run_refit_training
    raise RuntimeError(
RuntimeError: Cannot create refit in v2 study folder study-01b6aaaa without trial_key_hash. study_key_hash=present, trial_key_hash=missing, refit_params=missing. Hash computation from trial parameters may have failed.
2026-01-18 00:31:17,979 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_BEST_CHECKPOINT] Falling back to standard checkpoint search
2026-01-18 00:31:17,980 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Best trial checkpoint not found for trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>. Searched in: /tmp/pytest-of-codespace/pytest-38/test_full_hpo_workflow_with_cv0/outputs/hpo/local/distilbert. Skipping MLflow checkpoint logging.
2026-01-18 00:31:17,980 - training.hpo.execution.local.sweep - WARNING - [REFIT] No refit_run_id available to mark as FINISHED
2026-01-18 00:31:17,980 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Context manager exiting normally. run_id=37cce966ef87...
2026-01-18 00:31:17,998 - training.hpo.execution.local.sweep - INFO - [EARLY HASH] Computed v2 study_key_hash=957b9a1438354884... for folder creation
2026-01-18 00:31:18,001 - evaluation.selection.trial_finder - WARNING - No v2 study folders found in /tmp/pytest-of-codespace/pytest-38/test_full_hpo_workflow_no_cv_n0/outputs/hpo/local/distilbert (resolved from /tmp/pytest-of-codespace/pytest-38/test_full_hpo_workflow_no_cv_n0/outputs/hpo/local/distilbert)
2026-01-18 00:31:18,003 - training.hpo.execution.local.sweep - INFO - [HPO Setup] k_folds=None, fold_splits_file=None, k_fold config: {'enabled': False}
2026-01-18 00:31:18,006 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Using LOCAL MLflow tracking (not Azure ML)
2026-01-18 00:31:18,006 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
2026-01-18 00:31:18,017 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=957b9a1438354884...
2026-01-18 00:31:18,019 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=957b9a1438354884..., study_family_hash=23208e5829deb236...
2026-01-18 00:31:18,051 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Yielding RunHandle. run_id=fbc8ed55335b...
2026-01-18 00:31:18,055 - training.hpo.execution.local.sweep - INFO - [HPO] Parent run fbc8ed55335b... is now visible in MLflow (status: RUNNING)
2026-01-18 00:31:18,059 - training.hpo.execution.local.sweep - INFO - Setting Phase 2 tags on parent run fbc8ed55335b... (data_config=present, train_config=present)
2026-01-18 00:31:18,061 - training.hpo.execution.local.sweep - WARNING - Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
2026-01-18 00:31:18,062 - training.hpo.execution.local.sweep - WARNING - Could not set data fingerprint tag: 'Missing tag key: fingerprint.data'
2026-01-18 00:31:18,062 - training.hpo.execution.local.sweep - WARNING - Could not set eval fingerprint tag: 'Missing tag key: fingerprint.eval'
2026-01-18 00:31:18,062 - training.hpo.execution.local.sweep - WARNING - Could not set objective direction tag: 'Missing tag key: objective.direction'
2026-01-18 00:31:18,062 - training.hpo.execution.local.sweep - WARNING - Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
2026-01-18 00:31:18,062 - training.hpo.execution.local.sweep - WARNING - Could not set artifact.available tag: 'Missing tag key: artifact.available'
2026-01-18 00:31:18,062 - training.hpo.execution.local.sweep - INFO -  Set Phase 2 tags on parent run fbc8ed55335b...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-18 00:31:18,062 - training.hpo.execution.local.sweep - INFO -  Successfully completed Phase 2 tag setting for parent run fbc8ed55335b...
2026-01-18 00:31:18,063 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.
2026-01-18 00:31:18,063 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=fbc8ed55335b...
2026-01-18 00:31:18,063 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Found 0 completed trials out of 0 total
2026-01-18 00:31:18,064 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging metrics: n_trials=0, n_completed_trials=0
2026-01-18 00:31:18,069 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=<MagicMock name='mock.create_study().best_value' id='140077206668144'>
2026-01-18 00:31:18,072 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging best hyperparameters: <MagicMock name='mock.create_study().best_params' id='140077206783792'>
2026-01-18 00:31:18,073 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Finding and logging best trial run ID...
2026-01-18 00:31:18,102 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Could not find MLflow run ID for best trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>. Search failed: Invalid clause(s) in filter string: 'tags.trial_number = '<MagicMock name='mock.create_study()', '.', 'best_trial.number', '' id='', '140077206848368', ''>''. This may be a timing issue - trial runs may not be created/committed yet.
2026-01-18 00:31:18,102 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Completed best trial ID logging
2026-01-18 00:31:18,102 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Checkpoint logging enabled: False (upload_checkpoint=False)
2026-01-18 00:31:18,102 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
2026-01-18 00:31:18,102 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Completed log_final_metrics successfully
2026-01-18 00:31:18,102 - training.hpo.execution.local.sweep - INFO - [REFIT] Refit training is disabled in config
2026-01-18 00:31:18,102 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Context manager exiting normally. run_id=fbc8ed55335b...
2026-01-18 00:31:18,118 - training.hpo.execution.local.sweep - INFO - [EARLY HASH] Computed v2 study_key_hash=dc0c421a5056294a... for folder creation
2026-01-18 00:31:18,120 - evaluation.selection.trial_finder - WARNING - No v2 study folders found in /tmp/pytest-of-codespace/pytest-38/test_full_hpo_workflow_creates0/outputs/hpo/local/distilbert (resolved from /tmp/pytest-of-codespace/pytest-38/test_full_hpo_workflow_creates0/outputs/hpo/local/distilbert)
2026-01-18 00:31:18,121 - training.hpo.execution.local.sweep - INFO - [HPO Setup] k_folds=2, fold_splits_file=None, k_fold config: {'enabled': True, 'n_splits': 2, 'random_seed': 42, 'shuffle': True, 'stratified': True}
2026-01-18 00:31:18,121 - training.hpo.execution.local.sweep - INFO - [CV Setup]  Created 2 fold splits and saved to /tmp/pytest-of-codespace/pytest-38/test_full_hpo_workflow_creates0/outputs/hpo/local/distilbert/study-dc0c421a/fold_splits.json
2026-01-18 00:31:18,124 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Using LOCAL MLflow tracking (not Azure ML)
2026-01-18 00:31:18,124 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
2026-01-18 00:31:18,134 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=dc0c421a5056294a...
2026-01-18 00:31:18,134 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=dc0c421a5056294a..., study_family_hash=d1aa8bfffdcfdb77...
2026-01-18 00:31:18,152 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Yielding RunHandle. run_id=bd458de544e9...
2026-01-18 00:31:18,154 - training.hpo.execution.local.sweep - INFO - [HPO] Parent run bd458de544e9... is now visible in MLflow (status: RUNNING)
2026-01-18 00:31:18,157 - training.hpo.execution.local.sweep - INFO - Setting Phase 2 tags on parent run bd458de544e9... (data_config=present, train_config=present)
2026-01-18 00:31:18,158 - training.hpo.execution.local.sweep - WARNING - Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
2026-01-18 00:31:18,158 - training.hpo.execution.local.sweep - WARNING - Could not set data fingerprint tag: 'Missing tag key: fingerprint.data'
2026-01-18 00:31:18,158 - training.hpo.execution.local.sweep - WARNING - Could not set eval fingerprint tag: 'Missing tag key: fingerprint.eval'
2026-01-18 00:31:18,158 - training.hpo.execution.local.sweep - WARNING - Could not set objective direction tag: 'Missing tag key: objective.direction'
2026-01-18 00:31:18,158 - training.hpo.execution.local.sweep - WARNING - Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
2026-01-18 00:31:18,158 - training.hpo.execution.local.sweep - WARNING - Could not set artifact.available tag: 'Missing tag key: artifact.available'
2026-01-18 00:31:18,158 - training.hpo.execution.local.sweep - INFO -  Set Phase 2 tags on parent run bd458de544e9...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-18 00:31:18,159 - training.hpo.execution.local.sweep - INFO -  Successfully completed Phase 2 tag setting for parent run bd458de544e9...
2026-01-18 00:31:18,159 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.
2026-01-18 00:31:18,159 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=bd458de544e9...
2026-01-18 00:31:18,159 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Found 0 completed trials out of 0 total
2026-01-18 00:31:18,159 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging metrics: n_trials=0, n_completed_trials=0
2026-01-18 00:31:18,163 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=<MagicMock name='mock.create_study().best_value' id='140077206668144'>
2026-01-18 00:31:18,164 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging best hyperparameters: <MagicMock name='mock.create_study().best_params' id='140077206783792'>
2026-01-18 00:31:18,165 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Finding and logging best trial run ID...
2026-01-18 00:31:18,191 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Could not find MLflow run ID for best trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>. Search failed: Invalid clause(s) in filter string: 'tags.trial_number = '<MagicMock name='mock.create_study()', '.', 'best_trial.number', '' id='', '140077206848368', ''>''. This may be a timing issue - trial runs may not be created/committed yet.
2026-01-18 00:31:18,191 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Completed best trial ID logging
2026-01-18 00:31:18,191 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Checkpoint logging enabled: False (upload_checkpoint=False)
2026-01-18 00:31:18,191 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
2026-01-18 00:31:18,192 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Completed log_final_metrics successfully
2026-01-18 00:31:18,192 - training.hpo.execution.local.sweep - INFO - [REFIT] Starting refit training for best trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>
2026-01-18 00:31:18,198 - training.hpo.execution.local.sweep - WARNING - [REFIT] Could not determine trial_key_hash from best trial run tags. Refit run may not match trial run.
2026-01-18 00:31:18,198 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'> with hyperparameters: <MagicMock name='mock.create_study().best_trial.params' id='140077206865856'>
2026-01-18 00:31:18,199 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id="trial_<MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>_20260118_003118", run_id='20260118_003118', trial_number=<MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>
2026-01-18 00:31:18,199 - infrastructure.tracking.mlflow.hash_utils - WARNING - Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
2026-01-18 00:31:18,199 - training.hpo.execution.local.sweep - WARNING - [REFIT] Refit training failed: Cannot create refit in v2 study folder study-dc0c421a without trial_key_hash. study_key_hash=present, trial_key_hash=missing, refit_params=missing. Hash computation from trial parameters may have failed.. Continuing without refit checkpoint.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py", line 1244, in run_local_hpo_sweep
    refit_metrics, refit_checkpoint_dir, refit_run_id = run_refit_training(
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 240, in run_refit_training
    raise RuntimeError(
RuntimeError: Cannot create refit in v2 study folder study-dc0c421a without trial_key_hash. study_key_hash=present, trial_key_hash=missing, refit_params=missing. Hash computation from trial parameters may have failed.
2026-01-18 00:31:18,199 - training.hpo.execution.local.sweep - INFO - [HPO] Skipping checkpoint logging (mlflow.log_best_checkpoint=false or not set)
2026-01-18 00:31:18,199 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_full_hpo_workflow_creates0/outputs/hpo/config/tags.yaml, using defaults
2026-01-18 00:31:18,199 - training.hpo.execution.local.sweep - WARNING - [REFIT] No refit_run_id available to mark as FINISHED
2026-01-18 00:31:18,199 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Context manager exiting normally. run_id=bd458de544e9...
2026-01-18 00:31:18,788 - training.hpo.execution.local.sweep - INFO - [EARLY HASH] Computed v2 study_key_hash=957b9a1438354884... for folder creation
2026-01-18 00:31:18,790 - training.hpo.core.study - INFO - [HPO] Starting optimization for distilbert with checkpointing...
2026-01-18 00:31:18,791 - training.hpo.execution.local.sweep - INFO - [HPO Setup] k_folds=None, fold_splits_file=None, k_fold config: {'enabled': False}
2026-01-18 00:31:18,794 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Using LOCAL MLflow tracking (not Azure ML)
2026-01-18 00:31:18,794 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
2026-01-18 00:31:18,805 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=957b9a1438354884...
2026-01-18 00:31:18,805 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=957b9a1438354884..., study_family_hash=23208e5829deb236...
2026-01-18 00:31:18,823 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Yielding RunHandle. run_id=67807eddde4b...
2026-01-18 00:31:18,826 - training.hpo.execution.local.sweep - INFO - [HPO] Parent run 67807eddde4b... is now visible in MLflow (status: RUNNING)
2026-01-18 00:31:18,828 - training.hpo.execution.local.sweep - INFO - Setting Phase 2 tags on parent run 67807eddde4b... (data_config=present, train_config=present)
2026-01-18 00:31:18,829 - training.hpo.execution.local.sweep - WARNING - Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
2026-01-18 00:31:18,830 - training.hpo.execution.local.sweep - WARNING - Could not set data fingerprint tag: 'Missing tag key: fingerprint.data'
2026-01-18 00:31:18,830 - training.hpo.execution.local.sweep - WARNING - Could not set eval fingerprint tag: 'Missing tag key: fingerprint.eval'
2026-01-18 00:31:18,830 - training.hpo.execution.local.sweep - WARNING - Could not set objective direction tag: 'Missing tag key: objective.direction'
2026-01-18 00:31:18,830 - training.hpo.execution.local.sweep - WARNING - Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
2026-01-18 00:31:18,830 - training.hpo.execution.local.sweep - WARNING - Could not set artifact.available tag: 'Missing tag key: artifact.available'
2026-01-18 00:31:18,830 - training.hpo.execution.local.sweep - INFO -  Set Phase 2 tags on parent run 67807eddde4b...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-18 00:31:18,830 - training.hpo.execution.local.sweep - INFO -  Successfully completed Phase 2 tag setting for parent run 67807eddde4b...
2026-01-18 00:31:18,830 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.
2026-01-18 00:31:18,830 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=67807eddde4b...
2026-01-18 00:31:18,831 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Found 0 completed trials out of 0 total
2026-01-18 00:31:18,831 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging metrics: n_trials=0, n_completed_trials=0
2026-01-18 00:31:18,834 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=<MagicMock name='mock.create_study().best_value' id='140077206668144'>
2026-01-18 00:31:18,836 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging best hyperparameters: <MagicMock name='mock.create_study().best_params' id='140077206783792'>
2026-01-18 00:31:18,836 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Finding and logging best trial run ID...
2026-01-18 00:31:18,862 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Could not find MLflow run ID for best trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>. Search failed: Invalid clause(s) in filter string: 'tags.trial_number = '<MagicMock name='mock.create_study()', '.', 'best_trial.number', '' id='', '140077206848368', ''>''. This may be a timing issue - trial runs may not be created/committed yet.
2026-01-18 00:31:18,862 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Completed best trial ID logging
2026-01-18 00:31:18,862 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Checkpoint logging enabled: False (upload_checkpoint=False)
2026-01-18 00:31:18,862 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
2026-01-18 00:31:18,862 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Completed log_final_metrics successfully
2026-01-18 00:31:18,862 - training.hpo.execution.local.sweep - INFO - [REFIT] Refit training is disabled in config
2026-01-18 00:31:18,862 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Context manager exiting normally. run_id=67807eddde4b...
2026-01-18 00:31:18,877 - training.hpo.execution.local.sweep - INFO - [EARLY HASH] Computed v2 study_key_hash=42ff77fc79adab6d... for folder creation
2026-01-18 00:31:18,879 - training.hpo.core.study - INFO - [HPO] Starting optimization for distilbert with checkpointing...
2026-01-18 00:31:18,880 - training.hpo.execution.local.sweep - INFO - [HPO Setup] k_folds=None, fold_splits_file=None, k_fold config: {'enabled': False}
2026-01-18 00:31:18,883 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Using LOCAL MLflow tracking (not Azure ML)
2026-01-18 00:31:18,883 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
2026-01-18 00:31:18,893 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=42ff77fc79adab6d...
2026-01-18 00:31:18,893 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=42ff77fc79adab6d..., study_family_hash=ce5a7f9ff304036d...
2026-01-18 00:31:18,911 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Yielding RunHandle. run_id=bd92d2ca20c5...
2026-01-18 00:31:18,915 - training.hpo.execution.local.sweep - INFO - [HPO] Parent run bd92d2ca20c5... is now visible in MLflow (status: RUNNING)
2026-01-18 00:31:18,917 - training.hpo.execution.local.sweep - INFO - Setting Phase 2 tags on parent run bd92d2ca20c5... (data_config=present, train_config=present)
2026-01-18 00:31:18,920 - training.hpo.execution.local.sweep - WARNING - Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
2026-01-18 00:31:18,920 - training.hpo.execution.local.sweep - WARNING - Could not set data fingerprint tag: 'Missing tag key: fingerprint.data'
2026-01-18 00:31:18,920 - training.hpo.execution.local.sweep - WARNING - Could not set eval fingerprint tag: 'Missing tag key: fingerprint.eval'
2026-01-18 00:31:18,920 - training.hpo.execution.local.sweep - WARNING - Could not set objective direction tag: 'Missing tag key: objective.direction'
2026-01-18 00:31:18,920 - training.hpo.execution.local.sweep - WARNING - Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
2026-01-18 00:31:18,920 - training.hpo.execution.local.sweep - WARNING - Could not set artifact.available tag: 'Missing tag key: artifact.available'
2026-01-18 00:31:18,920 - training.hpo.execution.local.sweep - INFO -  Set Phase 2 tags on parent run bd92d2ca20c5...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-18 00:31:18,920 - training.hpo.execution.local.sweep - INFO -  Successfully completed Phase 2 tag setting for parent run bd92d2ca20c5...
2026-01-18 00:31:18,920 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.
2026-01-18 00:31:18,921 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=bd92d2ca20c5...
2026-01-18 00:31:18,921 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Found 0 completed trials out of 0 total
2026-01-18 00:31:18,921 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging metrics: n_trials=0, n_completed_trials=0
2026-01-18 00:31:18,924 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=<MagicMock name='mock.create_study().best_value' id='140077206668144'>
2026-01-18 00:31:18,926 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging best hyperparameters: <MagicMock name='mock.create_study().best_params' id='140077206783792'>
2026-01-18 00:31:18,926 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Finding and logging best trial run ID...
2026-01-18 00:31:18,953 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Could not find MLflow run ID for best trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>. Search failed: Invalid clause(s) in filter string: 'tags.trial_number = '<MagicMock name='mock.create_study()', '.', 'best_trial.number', '' id='', '140077206848368', ''>''. This may be a timing issue - trial runs may not be created/committed yet.
2026-01-18 00:31:18,954 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Completed best trial ID logging
2026-01-18 00:31:18,954 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Checkpoint logging enabled: False (upload_checkpoint=False)
2026-01-18 00:31:18,954 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
2026-01-18 00:31:18,954 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Completed log_final_metrics successfully
2026-01-18 00:31:18,954 - training.hpo.execution.local.sweep - INFO - [REFIT] Refit training is disabled in config
2026-01-18 00:31:18,954 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Context manager exiting normally. run_id=bd92d2ca20c5...
2026-01-18 00:31:18,956 - training.hpo.execution.local.sweep - INFO - [EARLY HASH] Computed v2 study_key_hash=42ff77fc79adab6d... for folder creation
2026-01-18 00:31:18,956 - training.hpo.core.study - INFO - [HPO] Starting optimization for distilbert with checkpointing...
2026-01-18 00:31:18,957 - training.hpo.execution.local.sweep - INFO - [HPO Setup] k_folds=None, fold_splits_file=None, k_fold config: {'enabled': False}
2026-01-18 00:31:18,959 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Using LOCAL MLflow tracking (not Azure ML)
2026-01-18 00:31:18,959 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
2026-01-18 00:31:18,969 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=42ff77fc79adab6d...
2026-01-18 00:31:18,969 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=42ff77fc79adab6d..., study_family_hash=ce5a7f9ff304036d...
2026-01-18 00:31:18,987 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Yielding RunHandle. run_id=d471c7696a36...
2026-01-18 00:31:18,989 - training.hpo.execution.local.sweep - INFO - [HPO] Parent run d471c7696a36... is now visible in MLflow (status: RUNNING)
2026-01-18 00:31:18,991 - training.hpo.execution.local.sweep - INFO - Setting Phase 2 tags on parent run d471c7696a36... (data_config=present, train_config=present)
2026-01-18 00:31:18,993 - training.hpo.execution.local.sweep - WARNING - Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
2026-01-18 00:31:18,993 - training.hpo.execution.local.sweep - WARNING - Could not set data fingerprint tag: 'Missing tag key: fingerprint.data'
2026-01-18 00:31:18,993 - training.hpo.execution.local.sweep - WARNING - Could not set eval fingerprint tag: 'Missing tag key: fingerprint.eval'
2026-01-18 00:31:18,993 - training.hpo.execution.local.sweep - WARNING - Could not set objective direction tag: 'Missing tag key: objective.direction'
2026-01-18 00:31:18,993 - training.hpo.execution.local.sweep - WARNING - Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
2026-01-18 00:31:18,993 - training.hpo.execution.local.sweep - WARNING - Could not set artifact.available tag: 'Missing tag key: artifact.available'
2026-01-18 00:31:18,993 - training.hpo.execution.local.sweep - INFO -  Set Phase 2 tags on parent run d471c7696a36...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-18 00:31:18,994 - training.hpo.execution.local.sweep - INFO -  Successfully completed Phase 2 tag setting for parent run d471c7696a36...
2026-01-18 00:31:18,994 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.
2026-01-18 00:31:18,994 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=d471c7696a36...
2026-01-18 00:31:18,994 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Found 0 completed trials out of 0 total
2026-01-18 00:31:18,994 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging metrics: n_trials=0, n_completed_trials=0
2026-01-18 00:31:18,998 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=<MagicMock name='mock.create_study().best_value' id='140077206668144'>
2026-01-18 00:31:18,999 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging best hyperparameters: <MagicMock name='mock.create_study().best_params' id='140077206783792'>
2026-01-18 00:31:19,000 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Finding and logging best trial run ID...
2026-01-18 00:31:19,028 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Could not find MLflow run ID for best trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>. Search failed: Invalid clause(s) in filter string: 'tags.trial_number = '<MagicMock name='mock.create_study()', '.', 'best_trial.number', '' id='', '140077206848368', ''>''. This may be a timing issue - trial runs may not be created/committed yet.
2026-01-18 00:31:19,028 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Completed best trial ID logging
2026-01-18 00:31:19,028 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Checkpoint logging enabled: False (upload_checkpoint=False)
2026-01-18 00:31:19,028 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
2026-01-18 00:31:19,028 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Completed log_final_metrics successfully
2026-01-18 00:31:19,028 - training.hpo.execution.local.sweep - INFO - [REFIT] Refit training is disabled in config
2026-01-18 00:31:19,028 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Context manager exiting normally. run_id=d471c7696a36...
2026-01-18 00:31:19,043 - training.hpo.execution.local.sweep - INFO - [EARLY HASH] Computed v2 study_key_hash=dc0c421a5056294a... for folder creation
2026-01-18 00:31:19,044 - training.hpo.core.study - INFO - [HPO] Starting optimization for distilbert with checkpointing...
2026-01-18 00:31:19,045 - training.hpo.execution.local.sweep - INFO - [HPO Setup] k_folds=2, fold_splits_file=None, k_fold config: {'enabled': True, 'n_splits': 2, 'random_seed': 42, 'shuffle': True, 'stratified': True}
2026-01-18 00:31:19,046 - training.hpo.execution.local.sweep - INFO - [CV Setup]  Created 2 fold splits and saved to /tmp/pytest-of-codespace/pytest-38/test_resume_workflow_with_cv0/outputs/hpo/local/distilbert/study-dc0c421a/fold_splits.json
2026-01-18 00:31:19,048 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Using LOCAL MLflow tracking (not Azure ML)
2026-01-18 00:31:19,049 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
2026-01-18 00:31:19,059 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=dc0c421a5056294a...
2026-01-18 00:31:19,059 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=dc0c421a5056294a..., study_family_hash=d1aa8bfffdcfdb77...
2026-01-18 00:31:19,078 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Yielding RunHandle. run_id=e9b822128b94...
2026-01-18 00:31:19,080 - training.hpo.execution.local.sweep - INFO - [HPO] Parent run e9b822128b94... is now visible in MLflow (status: RUNNING)
2026-01-18 00:31:19,082 - training.hpo.execution.local.sweep - INFO - Setting Phase 2 tags on parent run e9b822128b94... (data_config=present, train_config=present)
2026-01-18 00:31:19,084 - training.hpo.execution.local.sweep - WARNING - Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
2026-01-18 00:31:19,084 - training.hpo.execution.local.sweep - WARNING - Could not set data fingerprint tag: 'Missing tag key: fingerprint.data'
2026-01-18 00:31:19,084 - training.hpo.execution.local.sweep - WARNING - Could not set eval fingerprint tag: 'Missing tag key: fingerprint.eval'
2026-01-18 00:31:19,084 - training.hpo.execution.local.sweep - WARNING - Could not set objective direction tag: 'Missing tag key: objective.direction'
2026-01-18 00:31:19,084 - training.hpo.execution.local.sweep - WARNING - Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
2026-01-18 00:31:19,084 - training.hpo.execution.local.sweep - WARNING - Could not set artifact.available tag: 'Missing tag key: artifact.available'
2026-01-18 00:31:19,084 - training.hpo.execution.local.sweep - INFO -  Set Phase 2 tags on parent run e9b822128b94...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-18 00:31:19,084 - training.hpo.execution.local.sweep - INFO -  Successfully completed Phase 2 tag setting for parent run e9b822128b94...
2026-01-18 00:31:19,085 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.
2026-01-18 00:31:19,085 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=e9b822128b94...
2026-01-18 00:31:19,085 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Found 0 completed trials out of 0 total
2026-01-18 00:31:19,085 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging metrics: n_trials=0, n_completed_trials=0
2026-01-18 00:31:19,089 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=<MagicMock name='mock.create_study().best_value' id='140077206668144'>
2026-01-18 00:31:19,090 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging best hyperparameters: <MagicMock name='mock.create_study().best_params' id='140077206783792'>
2026-01-18 00:31:19,091 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Finding and logging best trial run ID...
2026-01-18 00:31:19,120 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Could not find MLflow run ID for best trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>. Search failed: Invalid clause(s) in filter string: 'tags.trial_number = '<MagicMock name='mock.create_study()', '.', 'best_trial.number', '' id='', '140077206848368', ''>''. This may be a timing issue - trial runs may not be created/committed yet.
2026-01-18 00:31:19,120 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Completed best trial ID logging
2026-01-18 00:31:19,120 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Checkpoint logging enabled: False (upload_checkpoint=False)
2026-01-18 00:31:19,120 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
2026-01-18 00:31:19,120 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Completed log_final_metrics successfully
2026-01-18 00:31:19,121 - training.hpo.execution.local.sweep - INFO - [REFIT] Refit training is disabled in config
2026-01-18 00:31:19,121 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Context manager exiting normally. run_id=e9b822128b94...
2026-01-18 00:31:19,123 - training.hpo.execution.local.sweep - INFO - [EARLY HASH] Computed v2 study_key_hash=dc0c421a5056294a... for folder creation
2026-01-18 00:31:19,123 - training.hpo.core.study - INFO - [HPO] Starting optimization for distilbert with checkpointing...
2026-01-18 00:31:19,124 - training.hpo.execution.local.sweep - INFO - [HPO Setup] k_folds=2, fold_splits_file=None, k_fold config: {'enabled': True, 'n_splits': 2, 'random_seed': 42, 'shuffle': True, 'stratified': True}
2026-01-18 00:31:19,125 - training.hpo.execution.local.sweep - INFO - [CV Setup]  Created 2 fold splits and saved to /tmp/pytest-of-codespace/pytest-38/test_resume_workflow_with_cv0/outputs/hpo/local/distilbert/study-dc0c421a/fold_splits.json
2026-01-18 00:31:19,127 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Using LOCAL MLflow tracking (not Azure ML)
2026-01-18 00:31:19,127 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
2026-01-18 00:31:19,137 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=dc0c421a5056294a...
2026-01-18 00:31:19,137 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=dc0c421a5056294a..., study_family_hash=d1aa8bfffdcfdb77...
2026-01-18 00:31:19,155 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Yielding RunHandle. run_id=6a02791b8354...
2026-01-18 00:31:19,158 - training.hpo.execution.local.sweep - INFO - [HPO] Parent run 6a02791b8354... is now visible in MLflow (status: RUNNING)
2026-01-18 00:31:19,160 - training.hpo.execution.local.sweep - INFO - Setting Phase 2 tags on parent run 6a02791b8354... (data_config=present, train_config=present)
2026-01-18 00:31:19,162 - training.hpo.execution.local.sweep - WARNING - Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
2026-01-18 00:31:19,162 - training.hpo.execution.local.sweep - WARNING - Could not set data fingerprint tag: 'Missing tag key: fingerprint.data'
2026-01-18 00:31:19,162 - training.hpo.execution.local.sweep - WARNING - Could not set eval fingerprint tag: 'Missing tag key: fingerprint.eval'
2026-01-18 00:31:19,162 - training.hpo.execution.local.sweep - WARNING - Could not set objective direction tag: 'Missing tag key: objective.direction'
2026-01-18 00:31:19,162 - training.hpo.execution.local.sweep - WARNING - Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
2026-01-18 00:31:19,162 - training.hpo.execution.local.sweep - WARNING - Could not set artifact.available tag: 'Missing tag key: artifact.available'
2026-01-18 00:31:19,162 - training.hpo.execution.local.sweep - INFO -  Set Phase 2 tags on parent run 6a02791b8354...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-18 00:31:19,162 - training.hpo.execution.local.sweep - INFO -  Successfully completed Phase 2 tag setting for parent run 6a02791b8354...
2026-01-18 00:31:19,162 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.
2026-01-18 00:31:19,163 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=6a02791b8354...
2026-01-18 00:31:19,163 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Found 0 completed trials out of 0 total
2026-01-18 00:31:19,163 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging metrics: n_trials=0, n_completed_trials=0
2026-01-18 00:31:19,167 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=<MagicMock name='mock.create_study().best_value' id='140077206668144'>
2026-01-18 00:31:19,169 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging best hyperparameters: <MagicMock name='mock.create_study().best_params' id='140077206783792'>
2026-01-18 00:31:19,169 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Finding and logging best trial run ID...
2026-01-18 00:31:19,200 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Could not find MLflow run ID for best trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>. Search failed: Invalid clause(s) in filter string: 'tags.trial_number = '<MagicMock name='mock.create_study()', '.', 'best_trial.number', '' id='', '140077206848368', ''>''. This may be a timing issue - trial runs may not be created/committed yet.
2026-01-18 00:31:19,200 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Completed best trial ID logging
2026-01-18 00:31:19,200 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Checkpoint logging enabled: False (upload_checkpoint=False)
2026-01-18 00:31:19,201 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
2026-01-18 00:31:19,201 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Completed log_final_metrics successfully
2026-01-18 00:31:19,201 - training.hpo.execution.local.sweep - INFO - [REFIT] Refit training is disabled in config
2026-01-18 00:31:19,201 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Context manager exiting normally. run_id=6a02791b8354...
2026-01-18 00:31:19,230 - training.hpo.core.study - INFO - [HPO] Starting optimization for distilbert with checkpointing...
2026-01-18 00:31:19,274 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 00:31:19,274 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 00:31:19,274 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:b17bb0a908286809619872feca54b44e3a407d64215bc..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-18 00:31:19,277 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Loaded 22 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-18 00:31:19,277 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Found 6 allocations for counter_key (after deduplication): committed=[], reserved=[1, 2, 3, 4, 5, 6], expired=[], max_committed_version=0
2026-01-18 00:31:19,277 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Allocation details: [(1, 'reserved', 'pending_2026'), (2, 'reserved', 'pending_2026'), (3, 'reserved', 'pending_2026'), (4, 'reserved', 'pending_2026'), (5, 'reserved', 'pending_2026'), (6, 'reserved', 'pending_2026')]
2026-01-18 00:31:19,278 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Reserving next version: 7 (incremented from max_committed=0, skipped 6 reserved/expired versions)
2026-01-18 00:31:19,278 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version]  Successfully reserved version 7 for counter_key resume-ner:hpo:b17bb0a908286809619872feca54b44e3a4... (run_id: pending_2026...)
2026-01-18 00:31:19,281 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 00:31:19,282 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 00:31:19,282 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:3d74e4db609045bbecfa4cc681e1ebee1a827c35aa7b8..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-18 00:31:19,282 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Loaded 23 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-18 00:31:19,282 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Found 12 allocations for counter_key (after deduplication): committed=[], reserved=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], expired=[], max_committed_version=0
2026-01-18 00:31:19,282 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Allocation details: [(1, 'reserved', 'pending_2026'), (2, 'reserved', 'pending_2026'), (3, 'reserved', 'pending_2026'), (4, 'reserved', 'pending_2026'), (5, 'reserved', 'pending_2026'), (6, 'reserved', 'pending_2026'), (7, 'reserved', 'pending_2026'), (8, 'reserved', 'pending_2026'), (9, 'reserved', 'pending_2026'), (10, 'reserved', 'pending_2026'), (11, 'reserved', 'pending_2026'), (12, 'reserved', 'pending_2026')]
2026-01-18 00:31:19,282 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Reserving next version: 13 (incremented from max_committed=0, skipped 12 reserved/expired versions)
2026-01-18 00:31:19,283 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version]  Successfully reserved version 13 for counter_key resume-ner:hpo:3d74e4db609045bbecfa4cc681e1ebee1a8... (run_id: pending_2026...)
2026-01-18 00:31:19,288 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 00:31:19,288 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 00:31:19,288 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:3d74e4db609045bbecfa4cc681e1ebee1a827c35aa7b8..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-18 00:31:19,288 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Loaded 24 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-18 00:31:19,288 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Found 13 allocations for counter_key (after deduplication): committed=[], reserved=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], expired=[], max_committed_version=0
2026-01-18 00:31:19,289 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Allocation details: [(1, 'reserved', 'pending_2026'), (2, 'reserved', 'pending_2026'), (3, 'reserved', 'pending_2026'), (4, 'reserved', 'pending_2026'), (5, 'reserved', 'pending_2026'), (6, 'reserved', 'pending_2026'), (7, 'reserved', 'pending_2026'), (8, 'reserved', 'pending_2026'), (9, 'reserved', 'pending_2026'), (10, 'reserved', 'pending_2026'), (11, 'reserved', 'pending_2026'), (12, 'reserved', 'pending_2026'), (13, 'reserved', 'pending_2026')]
2026-01-18 00:31:19,289 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Reserving next version: 14 (incremented from max_committed=0, skipped 13 reserved/expired versions)
2026-01-18 00:31:19,289 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version]  Successfully reserved version 14 for counter_key resume-ner:hpo:3d74e4db609045bbecfa4cc681e1ebee1a8... (run_id: pending_2026...)
2026-01-18 00:31:19,454 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'batch_size': 4, 'dropout': 0.2, 'weight_decay': 0.05, 'backbone': 'distilbert'}
2026-01-18 00:31:19,454 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 00:31:19,461 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_refit_uses_best_trial_hyp0/config/tags.yaml, using defaults
2026-01-18 00:31:19,491 - training.execution.mlflow_setup - INFO -  View run local_distilbert_hpo_refit_legacy at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/302183572435427150/runs/5d90c3981d884fd891ff636bb2765783
2026-01-18 00:31:19,491 - training.execution.mlflow_setup - INFO - Created MLflow run: local_distilbert_hpo_refit_legacy (5d90c3981d88...)
2026-01-18 00:31:19,491 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run 5d90c3981d88... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-18 00:31:19,492 - training.hpo.execution.local.refit - ERROR - [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 548, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
2026-01-18 00:31:19,493 - training.hpo.execution.local.refit - WARNING - [REFIT] Metrics file not found at /tmp/pytest-of-codespace/pytest-38/test_refit_uses_best_trial_hyp0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/metrics.json
2026-01-18 00:31:19,494 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run 5d90c3981d88... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-18 00:31:19,495 - training.hpo.execution.local.refit - ERROR - [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 548, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
2026-01-18 00:31:19,496 - training.hpo.execution.local.refit - INFO - [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-18 00:31:19,496 - training.hpo.execution.local.refit - INFO - [REFIT] Refit training completed. Metrics: N/A, Checkpoint: /tmp/pytest-of-codespace/pytest-38/test_refit_uses_best_trial_hyp0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
2026-01-18 00:31:19,503 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-18 00:31:19,503 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 00:31:19,511 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_refit_creates_mlflow_run0/config/tags.yaml, using defaults
2026-01-18 00:31:19,543 - training.execution.mlflow_setup - INFO -  View run local_distilbert_hpo_refit_legacy at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/302183572435427150/runs/9211e91bf4c64224bf0ce6f2d76f82b0
2026-01-18 00:31:19,543 - training.execution.mlflow_setup - INFO - Created MLflow run: local_distilbert_hpo_refit_legacy (9211e91bf4c6...)
2026-01-18 00:31:19,543 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run 9211e91bf4c6... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-18 00:31:19,545 - training.hpo.execution.local.refit - ERROR - [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 548, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
2026-01-18 00:31:19,547 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run 9211e91bf4c6... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-18 00:31:19,548 - training.hpo.execution.local.refit - ERROR - [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 548, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
2026-01-18 00:31:19,549 - training.hpo.execution.local.refit - INFO - [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-18 00:31:19,549 - training.hpo.execution.local.refit - INFO - [REFIT] Refit training completed. Metrics: 0.8, Checkpoint: /tmp/pytest-of-codespace/pytest-38/test_refit_creates_mlflow_run0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
2026-01-18 00:31:19,557 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-18 00:31:19,557 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 00:31:19,563 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_refit_creates_v2_output_d0/config/tags.yaml, using defaults
2026-01-18 00:31:19,596 - training.execution.mlflow_setup - INFO -  View run local_distilbert_hpo_refit_legacy at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/302183572435427150/runs/a633f45d5e2c4cafae8e1e6ec74bbebc
2026-01-18 00:31:19,596 - training.execution.mlflow_setup - INFO - Created MLflow run: local_distilbert_hpo_refit_legacy (a633f45d5e2c...)
2026-01-18 00:31:19,596 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run a633f45d5e2c... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-18 00:31:19,597 - training.hpo.execution.local.refit - ERROR - [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 548, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
2026-01-18 00:31:19,598 - training.hpo.execution.local.refit - WARNING - [REFIT] Metrics file not found at /tmp/pytest-of-codespace/pytest-38/test_refit_creates_v2_output_d0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/metrics.json
2026-01-18 00:31:19,598 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run a633f45d5e2c... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-18 00:31:19,599 - training.hpo.execution.local.refit - ERROR - [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 548, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
2026-01-18 00:31:19,599 - training.hpo.execution.local.refit - INFO - [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-18 00:31:19,599 - training.hpo.execution.local.refit - INFO - [REFIT] Refit training completed. Metrics: N/A, Checkpoint: /tmp/pytest-of-codespace/pytest-38/test_refit_creates_v2_output_d0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
2026-01-18 00:31:19,604 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-18 00:31:19,604 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 00:31:19,608 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_refit_reads_metrics_from_0/config/tags.yaml, using defaults
2026-01-18 00:31:19,625 - training.execution.mlflow_setup - INFO -  View run local_distilbert_hpo_refit_legacy at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/302183572435427150/runs/b4a27ef051ae4a4f8f606c1c1be8bcd4
2026-01-18 00:31:19,625 - training.execution.mlflow_setup - INFO - Created MLflow run: local_distilbert_hpo_refit_legacy (b4a27ef051ae...)
2026-01-18 00:31:19,626 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run b4a27ef051ae... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-18 00:31:19,626 - training.hpo.execution.local.refit - ERROR - [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 548, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
2026-01-18 00:31:19,627 - training.hpo.execution.local.refit - WARNING - [REFIT] Metrics file not found at /tmp/pytest-of-codespace/pytest-38/test_refit_reads_metrics_from_0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/metrics.json
2026-01-18 00:31:19,627 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run b4a27ef051ae... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-18 00:31:19,628 - training.hpo.execution.local.refit - ERROR - [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 548, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
2026-01-18 00:31:19,628 - training.hpo.execution.local.refit - INFO - [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-18 00:31:19,628 - training.hpo.execution.local.refit - INFO - [REFIT] Refit training completed. Metrics: N/A, Checkpoint: /tmp/pytest-of-codespace/pytest-38/test_refit_reads_metrics_from_0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
2026-01-18 00:31:19,633 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-18 00:31:19,633 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 00:31:19,636 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_refit_logs_metrics_to_mlf0/config/tags.yaml, using defaults
2026-01-18 00:31:19,654 - training.execution.mlflow_setup - INFO -  View run local_distilbert_hpo_refit_legacy at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/302183572435427150/runs/02b6530be01e4677bfd754e3397394a5
2026-01-18 00:31:19,654 - training.execution.mlflow_setup - INFO - Created MLflow run: local_distilbert_hpo_refit_legacy (02b6530be01e...)
2026-01-18 00:31:19,654 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run 02b6530be01e... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-18 00:31:19,655 - training.hpo.execution.local.refit - ERROR - [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 548, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
2026-01-18 00:31:19,656 - training.hpo.execution.local.refit - WARNING - [REFIT] Metrics file not found at /tmp/pytest-of-codespace/pytest-38/test_refit_logs_metrics_to_mlf0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/metrics.json
2026-01-18 00:31:19,656 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run 02b6530be01e... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-18 00:31:19,657 - training.hpo.execution.local.refit - ERROR - [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 548, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
2026-01-18 00:31:19,657 - training.hpo.execution.local.refit - INFO - [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-18 00:31:19,657 - training.hpo.execution.local.refit - INFO - [REFIT] Refit training completed. Metrics: N/A, Checkpoint: /tmp/pytest-of-codespace/pytest-38/test_refit_logs_metrics_to_mlf0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
2026-01-18 00:31:19,663 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-18 00:31:19,663 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 00:31:19,666 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_refit_creates_checkpoint_0/config/tags.yaml, using defaults
2026-01-18 00:31:19,684 - training.execution.mlflow_setup - INFO -  View run local_distilbert_hpo_refit_legacy at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/302183572435427150/runs/8c120a24b19d4df4b29c818ba4aa01b3
2026-01-18 00:31:19,684 - training.execution.mlflow_setup - INFO - Created MLflow run: local_distilbert_hpo_refit_legacy (8c120a24b19d...)
2026-01-18 00:31:19,684 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run 8c120a24b19d... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-18 00:31:19,685 - training.hpo.execution.local.refit - ERROR - [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 548, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
2026-01-18 00:31:19,686 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run 8c120a24b19d... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-18 00:31:19,687 - training.hpo.execution.local.refit - ERROR - [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 548, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
2026-01-18 00:31:19,687 - training.hpo.execution.local.refit - INFO - [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-18 00:31:19,687 - training.hpo.execution.local.refit - INFO - [REFIT] Refit training completed. Metrics: 0.8, Checkpoint: /tmp/pytest-of-codespace/pytest-38/test_refit_creates_checkpoint_0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
2026-01-18 00:31:19,691 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-18 00:31:19,692 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 00:31:19,695 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_refit_enabled_in_smoke_ya0/config/tags.yaml, using defaults
2026-01-18 00:31:19,713 - training.execution.mlflow_setup - INFO -  View run local_distilbert_hpo_refit_legacy at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/302183572435427150/runs/926b6bd318e246f09d290c89e81b4f51
2026-01-18 00:31:19,713 - training.execution.mlflow_setup - INFO - Created MLflow run: local_distilbert_hpo_refit_legacy (926b6bd318e2...)
2026-01-18 00:31:19,713 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run 926b6bd318e2... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-18 00:31:19,714 - training.hpo.execution.local.refit - ERROR - [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 548, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
2026-01-18 00:31:19,714 - training.hpo.execution.local.refit - WARNING - [REFIT] Metrics file not found at /workspaces/resume-ner-azureml/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/metrics.json
2026-01-18 00:31:19,715 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run 926b6bd318e2... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-18 00:31:19,716 - training.hpo.execution.local.refit - ERROR - [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 548, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
2026-01-18 00:31:19,716 - training.hpo.execution.local.refit - INFO - [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-18 00:31:19,716 - training.hpo.execution.local.refit - INFO - [REFIT] Refit training completed. Metrics: N/A, Checkpoint: /workspaces/resume-ner-azureml/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
2026-01-18 00:31:19,721 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-18 00:31:19,721 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 00:31:19,724 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_refit_uses_full_epochs0/config/tags.yaml, using defaults
2026-01-18 00:31:19,749 - training.execution.mlflow_setup - INFO -  View run local_distilbert_hpo_refit_legacy at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/302183572435427150/runs/6846f66874d146aa8d8e8bc71aefb2e1
2026-01-18 00:31:19,749 - training.execution.mlflow_setup - INFO - Created MLflow run: local_distilbert_hpo_refit_legacy (6846f66874d1...)
2026-01-18 00:31:19,749 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run 6846f66874d1... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-18 00:31:19,751 - training.hpo.execution.local.refit - ERROR - [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 548, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
2026-01-18 00:31:19,752 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run 6846f66874d1... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-18 00:31:19,753 - training.hpo.execution.local.refit - ERROR - [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 548, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
2026-01-18 00:31:19,754 - training.hpo.execution.local.refit - INFO - [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-18 00:31:19,754 - training.hpo.execution.local.refit - INFO - [REFIT] Refit training completed. Metrics: 0.8, Checkpoint: /tmp/pytest-of-codespace/pytest-38/test_refit_uses_full_epochs0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
2026-01-18 00:31:19,764 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-18 00:31:19,765 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 00:31:19,771 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_refit_skips_checkpoint_fo0/config/tags.yaml, using defaults
2026-01-18 00:31:19,792 - training.execution.mlflow_setup - INFO -  View run local_distilbert_hpo_refit_legacy at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/302183572435427150/runs/73a6996ceb8949f78969318b32eb0f09
2026-01-18 00:31:19,792 - training.execution.mlflow_setup - INFO - Created MLflow run: local_distilbert_hpo_refit_legacy (73a6996ceb89...)
2026-01-18 00:31:19,793 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run 73a6996ceb89... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-18 00:31:19,794 - training.hpo.execution.local.refit - ERROR - [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 548, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
2026-01-18 00:31:19,795 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run 73a6996ceb89... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-18 00:31:19,797 - training.hpo.execution.local.refit - ERROR - [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 548, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
2026-01-18 00:31:19,797 - training.hpo.execution.local.refit - INFO - [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-18 00:31:19,797 - training.hpo.execution.local.refit - INFO - [REFIT] Refit training completed. Metrics: 0.8, Checkpoint: /tmp/pytest-of-codespace/pytest-38/test_refit_skips_checkpoint_fo0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
2026-01-18 00:31:19,802 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-18 00:31:19,803 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-18 00:31:19,806 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_refit_prevents_duplicatio0/config/tags.yaml, using defaults
2026-01-18 00:31:19,824 - training.execution.mlflow_setup - INFO -  View run local_distilbert_hpo_refit_legacy at: file:///workspaces/resume-ner-azureml/mlruns/#/experiments/302183572435427150/runs/4d5a913380c145c79779bef846238767
2026-01-18 00:31:19,824 - training.execution.mlflow_setup - INFO - Created MLflow run: local_distilbert_hpo_refit_legacy (4d5a913380c1...)
2026-01-18 00:31:19,824 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run 4d5a913380c1... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-18 00:31:19,825 - training.hpo.execution.local.refit - ERROR - [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 548, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
2026-01-18 00:31:19,826 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run 4d5a913380c1... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-18 00:31:19,827 - training.hpo.execution.local.refit - ERROR - [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 548, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
2026-01-18 00:31:19,827 - training.hpo.execution.local.refit - INFO - [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-18 00:31:19,827 - training.hpo.execution.local.refit - INFO - [REFIT] Refit training completed. Metrics: 0.8, Checkpoint: /tmp/pytest-of-codespace/pytest-38/test_refit_prevents_duplicatio0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
2026-01-18 00:31:19,830 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Could not set refit_completed tag: Run 'refit_run_123' not found
2026-01-18 00:31:19,830 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_BEST_CHECKPOINT] Using preferred checkpoint directory: /tmp/pytest-of-codespace/pytest-38/test_refit_prevents_duplicatio0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
2026-01-18 00:31:19,831 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - Uploading checkpoint archive to MLflow...
2026-01-18 00:31:19,831 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - Uploading checkpoint to refit run refit_run_12 (child of parent parent_123)
2026-01-18 00:31:19,831 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - Checkpoint upload completed successfully for trial 0
2026-01-18 00:31:19,831 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - [LOG_BEST_CHECKPOINT] Could not update artifact.available tag: 'Missing tag key: artifact.available'
2026-01-18 00:31:19,831 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - Marked study as complete with checkpoint uploaded (best trial: 0)
[I 2026-01-18 00:31:19,925] A new study created in RDB with name: test_timeout
[I 2026-01-18 00:31:20,053] Trial 0 finished with value: 0.75 and parameters: {}. Best is trial 0 with value: 0.75.
[I 2026-01-18 00:31:20,175] Trial 1 finished with value: 0.75 and parameters: {}. Best is trial 0 with value: 0.75.
[I 2026-01-18 00:31:20,299] Trial 2 finished with value: 0.75 and parameters: {}. Best is trial 0 with value: 0.75.
[I 2026-01-18 00:31:20,417] Trial 3 finished with value: 0.75 and parameters: {}. Best is trial 0 with value: 0.75.
[I 2026-01-18 00:31:20,534] Trial 4 finished with value: 0.75 and parameters: {}. Best is trial 0 with value: 0.75.
[I 2026-01-18 00:31:20,646] A new study created in RDB with name: test_mark
[I 2026-01-18 00:31:20,794] A new study created in RDB with name: test_skip_mark
2026-01-18 00:31:20,873 - training.hpo.execution.local.trial - INFO - [TRIAL] Training completed. Objective metric 'macro-f1': 0.75
2026-01-18 00:31:20,879 - training.hpo.execution.local.trial - INFO - [TRIAL] Training completed. Objective metric 'macro-f1': 0.8
2026-01-18 00:31:20,885 - training.hpo.execution.local.trial - INFO - [TRIAL] Training completed. Objective metric 'macro-f1': 0.75
2026-01-18 00:31:20,891 - infrastructure.tracking.mlflow.hash_utils - WARNING - Could not get or compute study_key_hash. study_key_hash=missing, hpo_parent_run_id=provided, configs=missing
2026-01-18 00:31:20,892 - infrastructure.tracking.mlflow.hash_utils - WARNING - Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=missing, hyperparameters=available
2026-01-18 00:31:20,892 - training.hpo.execution.local.cv - WARNING - Could not compute trial_key_hash from configs
2026-01-18 00:31:20,892 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_trial_execution_with_cv_c0/config/tags.yaml, using defaults
2026-01-18 00:31:20,893 - infrastructure.tracking.mlflow.hash_utils - WARNING - Could not get or compute study_key_hash. study_key_hash=missing, hpo_parent_run_id=provided, configs=missing
2026-01-18 00:31:20,893 - training.hpo.execution.local.cv - ERROR - In v2 study folder study-abc12345 but trial_key_hash is None. study_key_hash=NO. Will attempt to compute hash.
2026-01-18 00:31:20,893 - training.hpo.execution.local.cv - WARNING - In v2 study folder but missing hashes: study_key_hash=NO, trial_key_hash=NO
2026-01-18 00:31:20,912 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_trial_execution_with_cv_c1/config/tags.yaml, using defaults
2026-01-18 00:31:20,913 - training.hpo.execution.local.cv - INFO - [CV] Created trial folder: /tmp/pytest-of-codespace/pytest-38/test_trial_execution_with_cv_c1/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-aca2e73e (trial 0)
2026-01-18 00:31:20,914 - infrastructure.tracking.mlflow.lifecycle - WARNING - Could not check run status for trial_run_12...: Run 'trial_run_123' not found
2026-01-18 00:31:20,915 - infrastructure.tracking.mlflow.lifecycle - WARNING - Failed to terminate run trial_run_12...: Run 'trial_run_123' not found
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/lifecycle.py", line 98, in terminate_run_safe
    client.set_terminated(run_id, status=status)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 3506, in set_terminated
    self._tracking_client.set_terminated(run_id, status, end_time)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 797, in set_terminated
    self.store.update_run_info(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 676, in update_run_info
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'trial_run_123' not found
2026-01-18 00:31:20,920 - infrastructure.tracking.mlflow.hash_utils - WARNING - Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
2026-01-18 00:31:20,920 - training.hpo.execution.local.cv - WARNING - Could not compute trial_key_hash from configs
2026-01-18 00:31:20,920 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_trial_execution_with_cv_a0/config/tags.yaml, using defaults
2026-01-18 00:31:20,920 - infrastructure.tracking.mlflow.hash_utils - WARNING - Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
2026-01-18 00:31:20,921 - training.hpo.execution.local.cv - WARNING - Could not compute trial_key_hash
2026-01-18 00:31:20,921 - training.hpo.execution.local.cv - ERROR - In v2 study folder study-abc12345 but trial_key_hash is None. study_key_hash=YES. Will attempt to compute hash.
2026-01-18 00:31:20,921 - training.hpo.execution.local.cv - WARNING - In v2 study folder but missing hashes: study_key_hash=YES, trial_key_hash=NO
2026-01-18 00:31:20,921 - training.hpo.execution.local.cv - WARNING - In v2 study folder study-abc12345 but trial_key_hash is None. Attempting to recompute from trial_params...
2026-01-18 00:31:20,921 - infrastructure.tracking.mlflow.hash_utils - WARNING - Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
2026-01-18 00:31:20,937 - infrastructure.tracking.mlflow.hash_utils - WARNING - Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
2026-01-18 00:31:20,937 - training.hpo.execution.local.cv - WARNING - Could not compute trial_key_hash from configs
2026-01-18 00:31:20,937 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_trial_execution_with_cv_o0/config/tags.yaml, using defaults
2026-01-18 00:31:20,938 - infrastructure.tracking.mlflow.hash_utils - WARNING - Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
2026-01-18 00:31:20,938 - training.hpo.execution.local.cv - WARNING - Could not compute trial_key_hash
2026-01-18 00:31:20,938 - training.hpo.execution.local.cv - ERROR - In v2 study folder study-abc12345 but trial_key_hash is None. study_key_hash=YES. Will attempt to compute hash.
2026-01-18 00:31:20,938 - training.hpo.execution.local.cv - WARNING - In v2 study folder but missing hashes: study_key_hash=YES, trial_key_hash=NO
2026-01-18 00:31:20,938 - training.hpo.execution.local.cv - WARNING - In v2 study folder study-abc12345 but trial_key_hash is None. Attempting to recompute from trial_params...
2026-01-18 00:31:20,938 - infrastructure.tracking.mlflow.hash_utils - WARNING - Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
2026-01-18 00:31:20,954 - infrastructure.tracking.mlflow.hash_utils - WARNING - Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
2026-01-18 00:31:20,954 - training.hpo.execution.local.cv - WARNING - Could not compute trial_key_hash from configs
2026-01-18 00:31:20,954 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_trial_execution_with_cv_s0/config/tags.yaml, using defaults
2026-01-18 00:31:20,954 - infrastructure.tracking.mlflow.hash_utils - WARNING - Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
2026-01-18 00:31:20,955 - training.hpo.execution.local.cv - WARNING - Could not compute trial_key_hash
2026-01-18 00:31:20,955 - training.hpo.execution.local.cv - ERROR - In v2 study folder study-abc12345 but trial_key_hash is None. study_key_hash=YES. Will attempt to compute hash.
2026-01-18 00:31:20,955 - training.hpo.execution.local.cv - WARNING - In v2 study folder but missing hashes: study_key_hash=YES, trial_key_hash=NO
2026-01-18 00:31:20,955 - training.hpo.execution.local.cv - WARNING - In v2 study folder study-abc12345 but trial_key_hash is None. Attempting to recompute from trial_params...
2026-01-18 00:31:20,955 - infrastructure.tracking.mlflow.hash_utils - WARNING - Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
2026-01-18 00:31:21,032 - infrastructure.config.selection - WARNING - top_k_for_stable_score (5) > min_trials_per_group (3). Clamping top_k to 3.
2026-01-18 00:31:21,058 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-38/test_auto_generated_study_name0/config, raw_auto_inc_config={}
2026-01-18 00:31:21,058 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 00:31:21,061 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-38/test_custom_study_name_include0/config, raw_auto_inc_config={}
2026-01-18 00:31:21,061 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 00:31:21,065 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-38/test_auto_generated_with_varia0/config, raw_auto_inc_config={}
2026-01-18 00:31:21,065 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 00:31:21,078 - infrastructure.paths.repo - WARNING - Could not find repository root. Falling back to current working directory: /tmp/pytest-of-codespace/pytest-38/test_raises_value_error_when_n0/random/structure
2026/01/18 00:31:21 INFO mlflow.store.db.utils: Creating initial MLflow database tables...
2026/01/18 00:31:21 INFO mlflow.store.db.utils: Updating database tables
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Running upgrade  -> 451aebb31d03, add metric step
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Running upgrade 451aebb31d03 -> 90e64c465722, migrate user column to tags
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Running upgrade 90e64c465722 -> 181f10493468, allow nulls for metric values
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Running upgrade 181f10493468 -> df50e92ffc5e, Add Experiment Tags Table
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Running upgrade df50e92ffc5e -> 7ac759974ad8, Update run tags with larger limit
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Running upgrade 7ac759974ad8 -> 89d4b8295536, create latest metrics table
2026-01-18 00:31:21 INFO  [89d4b8295536_create_latest_metrics_table_py] Migration complete!
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Running upgrade 89d4b8295536 -> 2b4d017a5e9b, add model registry tables to db
2026-01-18 00:31:21 INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Adding registered_models and model_versions tables to database.
2026-01-18 00:31:21 INFO  [2b4d017a5e9b_add_model_registry_tables_to_db_py] Migration complete!
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Running upgrade 2b4d017a5e9b -> cfd24bdc0731, Update run status constraint with killed
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Running upgrade cfd24bdc0731 -> 0a8213491aaa, drop_duplicate_killed_constraint
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Running upgrade 0a8213491aaa -> 728d730b5ebd, add registered model tags table
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Running upgrade 728d730b5ebd -> 27a6a02d2cf1, add model version tags table
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Running upgrade 27a6a02d2cf1 -> 84291f40a231, add run_link to model_version
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Running upgrade 84291f40a231 -> a8c4a736bde6, allow nulls for run_id
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Running upgrade a8c4a736bde6 -> 39d1c3be5f05, add_is_nan_constraint_for_metrics_tables_if_necessary
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Running upgrade 39d1c3be5f05 -> c48cb773bb87, reset_default_value_for_is_nan_in_metrics_table_for_mysql
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Running upgrade c48cb773bb87 -> bd07f7e963c5, create index on run_uuid
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Running upgrade bd07f7e963c5 -> 0c779009ac13, add deleted_time field to runs table
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Running upgrade 0c779009ac13 -> cc1f77228345, change param value length to 500
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Running upgrade cc1f77228345 -> 97727af70f4d, Add creation_time and last_update_time to experiments table
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Running upgrade 97727af70f4d -> 3500859a5d39, Add Model Aliases table
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Running upgrade 3500859a5d39 -> 7f2a7d5fae7d, add datasets inputs input_tags tables
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Running upgrade 7f2a7d5fae7d -> 2d6e25af4d3e, increase max param val length from 500 to 8000
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Running upgrade 2d6e25af4d3e -> acf3f17fdcc7, add storage location field to model versions
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Running upgrade acf3f17fdcc7 -> 867495a8f9d4, add trace tables
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Running upgrade 867495a8f9d4 -> 5b0e9adcef9c, add cascade deletion to trace tables foreign keys
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Running upgrade 5b0e9adcef9c -> 4465047574b1, increase max dataset schema size
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Running upgrade 4465047574b1 -> f5a4f2784254, increase run tag value limit to 8000
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Running upgrade f5a4f2784254 -> 0584bdc529eb, add cascading deletion to datasets from experiments
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Running upgrade 0584bdc529eb -> 400f98739977, add logged model tables
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Running upgrade 400f98739977 -> 6953534de441, add step to inputs table
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Running upgrade 6953534de441 -> bda7b8c39065, increase_model_version_tag_value_limit
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Running upgrade bda7b8c39065 -> cbc13b556ace, add V3 trace schema columns
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Running upgrade cbc13b556ace -> 770bee3ae1dd, add assessments table
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Running upgrade 770bee3ae1dd -> a1b2c3d4e5f6, add spans table
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Running upgrade a1b2c3d4e5f6 -> de4033877273, create entity_associations table
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Running upgrade de4033877273 -> 1a0cddfcaa16, Add webhooks and webhook_events tables
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Running upgrade 1a0cddfcaa16 -> 534353b11cbc, add scorer tables
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Running upgrade 534353b11cbc -> 71994744cf8e, add evaluation datasets
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Running upgrade 71994744cf8e -> 3da73c924c2f, add outputs to dataset record
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Running upgrade 3da73c924c2f -> bf29a5ff90ea, add jobs table
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2026-01-18 00:31:21 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
2026/01/18 00:31:21 INFO mlflow.tracking.fluent: Experiment with name 'test' does not exist. Creating a new experiment.
2026-01-18 00:31:21,923 - common.shared.mlflow_setup - INFO - Using local tracking: sqlite:////workspaces/resume-ner-azureml/mlruns/mlflow.db
2026-01-18 00:31:21 INFO  [common.shared.mlflow_setup] Using local tracking: sqlite:////workspaces/resume-ner-azureml/mlruns/mlflow.db
2026-01-18 00:31:21,926 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=407d63530d6a4828...
2026-01-18 00:31:21 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=407d63530d6a4828...
2026-01-18 00:31:21,926 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=407d63530d6a4828..., study_family_hash=c70336361bc2d8f5...
2026-01-18 00:31:21 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=407d63530d6a4828..., study_family_hash=c70336361bc2d8f5...
2026-01-18 00:31:21,927 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Yielding RunHandle. run_id=test-run-123...
2026-01-18 00:31:21 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Yielding RunHandle. run_id=test-run-123...
2026-01-18 00:31:21,928 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Context manager exiting normally. run_id=test-run-123...
2026-01-18 00:31:21 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Context manager exiting normally. run_id=test-run-123...
2026-01-18 00:31:21,937 - common.shared.mlflow_setup - INFO - Using local tracking: sqlite:////workspaces/resume-ner-azureml/mlruns/mlflow.db
2026-01-18 00:31:21 INFO  [common.shared.mlflow_setup] Using local tracking: sqlite:////workspaces/resume-ner-azureml/mlruns/mlflow.db
2026-01-18 00:31:21,940 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Yielding RunHandle. run_id=test-run-123...
2026-01-18 00:31:21 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Yielding RunHandle. run_id=test-run-123...
2026-01-18 00:31:21,941 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Context manager exiting normally. run_id=test-run-123...
2026-01-18 00:31:21 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Context manager exiting normally. run_id=test-run-123...
2026-01-18 00:31:21,948 - common.shared.mlflow_setup - INFO - Using local tracking: sqlite:////workspaces/resume-ner-azureml/mlruns/mlflow.db
2026-01-18 00:31:21 INFO  [common.shared.mlflow_setup] Using local tracking: sqlite:////workspaces/resume-ner-azureml/mlruns/mlflow.db
2026-01-18 00:31:21,950 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=ebdca5d9f9571af3...
2026-01-18 00:31:21 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=ebdca5d9f9571af3...
2026-01-18 00:31:21,950 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=ebdca5d9f9571af3..., study_family_hash=c70336361bc2d8f5...
2026-01-18 00:31:21 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=ebdca5d9f9571af3..., study_family_hash=c70336361bc2d8f5...
2026-01-18 00:31:21,951 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Yielding RunHandle. run_id=test-run-123...
2026-01-18 00:31:21 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Yielding RunHandle. run_id=test-run-123...
2026-01-18 00:31:21,951 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Context manager exiting normally. run_id=test-run-123...
2026-01-18 00:31:21 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Context manager exiting normally. run_id=test-run-123...
2026-01-18 00:31:21,962 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - Found best trial 0 (study_key_hash + trial_number + parentRunId): trial-run-co...
2026-01-18 00:31:21 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] Found best trial 0 (study_key_hash + trial_number + parentRunId): trial-run-co...
2026-01-18 00:31:21,978 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - Found best trial 0 (study_key_hash + trial_number + parentRunId): trial-run-co...
2026-01-18 00:31:21 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] Found best trial 0 (study_key_hash + trial_number + parentRunId): trial-run-co...
2026-01-18 00:31:21,985 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Could not find MLflow run ID for best trial 0. Parent run parent-run-1... missing study_key_hash tag. This may be a timing issue - parent run tags may not be set yet.
2026-01-18 00:31:21 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker] Could not find MLflow run ID for best trial 0. Parent run parent-run-1... missing study_key_hash tag. This may be a timing issue - parent run tags may not be set yet.
2026-01-18 00:31:21,994 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Could not find MLflow run ID for best trial 0. Found 0 runs with matching study_key_hash and trial_number, but none belong to parent parent-run-1... This may be a timing issue - trial runs may not be created/committed yet.
2026-01-18 00:31:21 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker] Could not find MLflow run ID for best trial 0. Found 0 runs with matching study_key_hash and trial_number, but none belong to parent parent-run-1... This may be a timing issue - trial runs may not be created/committed yet.
2026-01-18 00:31:22,000 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Could not find MLflow run ID for best trial 0. Search failed: MLflow error. This may be a timing issue - trial runs may not be created/committed yet.
2026-01-18 00:31:22 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker] Could not find MLflow run ID for best trial 0. Search failed: MLflow error. This may be a timing issue - trial runs may not be created/committed yet.
2026-01-18 00:31:22,720 - orchestration.jobs.hpo.local.backup - INFO - Backed up HPO checkpoint database to Drive: study.db
2026-01-18 00:31:22 INFO  [orchestration.jobs.hpo.local.backup] Backed up HPO checkpoint database to Drive: study.db
2026-01-18 00:31:22,720 - orchestration.jobs.hpo.local.backup - INFO - Backed up entire study folder to Drive: study-c3659fea
2026-01-18 00:31:22 INFO  [orchestration.jobs.hpo.local.backup] Backed up entire study folder to Drive: study-c3659fea
2026-01-18 00:31:22,724 - orchestration.jobs.hpo.local.backup - INFO - Backed up HPO checkpoint database to Drive: study.db
2026-01-18 00:31:22 INFO  [orchestration.jobs.hpo.local.backup] Backed up HPO checkpoint database to Drive: study.db
2026-01-18 00:31:22,724 - orchestration.jobs.hpo.local.backup - INFO - Backed up entire study folder to Drive: study-c3659fea
2026-01-18 00:31:22 INFO  [orchestration.jobs.hpo.local.backup] Backed up entire study folder to Drive: study-c3659fea
2026-01-18 00:31:22,727 - orchestration.jobs.hpo.local.backup - INFO - Backed up HPO checkpoint database to Drive: study.db
2026-01-18 00:31:22 INFO  [orchestration.jobs.hpo.local.backup] Backed up HPO checkpoint database to Drive: study.db
2026-01-18 00:31:22,727 - orchestration.jobs.hpo.local.backup - INFO - Backed up entire study folder to Drive: study-c3659fea
2026-01-18 00:31:22 INFO  [orchestration.jobs.hpo.local.backup] Backed up entire study folder to Drive: study-c3659fea
2026-01-18 00:31:22,730 - evaluation.selection.trial_finder - WARNING - No v2 study folders found in /tmp/pytest-of-codespace/pytest-38/test_backup_warns_when_study_f0/outputs/hpo/colab/distilbert (resolved from /tmp/pytest-of-codespace/pytest-38/test_backup_warns_when_study_f0/outputs/hpo/colab/distilbert)
2026-01-18 00:31:22 WARNI [evaluation.selection.trial_finder] No v2 study folders found in /tmp/pytest-of-codespace/pytest-38/test_backup_warns_when_study_f0/outputs/hpo/colab/distilbert (resolved from /tmp/pytest-of-codespace/pytest-38/test_backup_warns_when_study_f0/outputs/hpo/colab/distilbert)
2026-01-18 00:31:22,730 - orchestration.jobs.hpo.local.backup - WARNING - Study folder not found in /tmp/pytest-of-codespace/pytest-38/test_backup_warns_when_study_f0/outputs/hpo/colab/distilbert (resolved from /tmp/pytest-of-codespace/pytest-38/test_backup_warns_when_study_f0/outputs/hpo/colab/distilbert)
2026-01-18 00:31:22 WARNI [orchestration.jobs.hpo.local.backup] Study folder not found in /tmp/pytest-of-codespace/pytest-38/test_backup_warns_when_study_f0/outputs/hpo/colab/distilbert (resolved from /tmp/pytest-of-codespace/pytest-38/test_backup_warns_when_study_f0/outputs/hpo/colab/distilbert)
2026-01-18 00:31:22,732 - orchestration.jobs.hpo.local.backup - WARNING - study.db not found: /tmp/pytest-of-codespace/pytest-38/test_backup_checks_file_existe0/outputs/hpo/colab/distilbert/study-c3659fea/study.db
2026-01-18 00:31:22 WARNI [orchestration.jobs.hpo.local.backup] study.db not found: /tmp/pytest-of-codespace/pytest-38/test_backup_checks_file_existe0/outputs/hpo/colab/distilbert/study-c3659fea/study.db
2026-01-18 00:31:22,733 - orchestration.jobs.hpo.local.backup - INFO - Backed up entire study folder to Drive: study-c3659fea
2026-01-18 00:31:22 INFO  [orchestration.jobs.hpo.local.backup] Backed up entire study folder to Drive: study-c3659fea
2026-01-18 00:31:22,760 - orchestration.jobs.hpo.local.backup - WARNING - Incremental backup error for study.db (trial 0): Backup failed
2026-01-18 00:31:22 WARNI [orchestration.jobs.hpo.local.backup] Incremental backup error for study.db (trial 0): Backup failed
2026-01-18 00:31:22,766 - orchestration.jobs.hpo.local.backup - INFO - Immediate backup successful: checkpoint.tar.gz
2026-01-18 00:31:22 INFO  [orchestration.jobs.hpo.local.backup] Immediate backup successful: checkpoint.tar.gz
2026-01-18 00:31:22,768 - orchestration.jobs.hpo.local.backup - INFO - Immediate backup successful: checkpoint_dir
2026-01-18 00:31:22 INFO  [orchestration.jobs.hpo.local.backup] Immediate backup successful: checkpoint_dir
2026-01-18 00:31:22,781 - orchestration.jobs.hpo.local.backup - WARNING - Immediate backup failed: checkpoint.tar.gz
2026-01-18 00:31:22 WARNI [orchestration.jobs.hpo.local.backup] Immediate backup failed: checkpoint.tar.gz
2026-01-18 00:31:22,784 - orchestration.jobs.hpo.local.backup - WARNING - Immediate backup error for checkpoint.tar.gz: Backup error
2026-01-18 00:31:22 WARNI [orchestration.jobs.hpo.local.backup] Immediate backup error for checkpoint.tar.gz: Backup error
2026-01-18 00:31:22,805 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:22,815 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:22,824 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:22,827 - evaluation.selection.artifact_unified.acquisition - INFO - Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_id_..., study_key_hash=N/A..., trial_key_hash=N/A...
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.acquisition] Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_id_..., study_key_hash=N/A..., trial_key_hash=N/A...
2026-01-18 00:31:22,834 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:22,841 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:22,847 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:22,849 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:22,855 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:22,860 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:22,868 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:22,872 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:22,881 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:22,885 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:22,895 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:22,904 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:22,913 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:22,922 - evaluation.selection.artifact_unified.acquisition - INFO - Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_123..., study_key_hash=study123..., trial_key_hash=trial876...
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.acquisition] Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_123..., study_key_hash=study123..., trial_key_hash=trial876...
2026-01-18 00:31:22,923 - evaluation.selection.artifact_unified.acquisition - INFO - Selected artifact run: artifact_run_id=refit_run_12..., trial_run_id=test_run_123...
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.acquisition] Selected artifact run: artifact_run_id=refit_run_12..., trial_run_id=test_run_123...
2026-01-18 00:31:22,923 - evaluation.selection.artifact_unified.acquisition - INFO - Artifact acquisition config for checkpoint: priority=['local'], local.enabled=True, local.validate=True, drive.enabled=True, mlflow.enabled=True
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.acquisition] Artifact acquisition config for checkpoint: priority=['local'], local.enabled=True, local.validate=True, drive.enabled=True, mlflow.enabled=True
2026-01-18 00:31:22,923 - evaluation.selection.artifact_unified.acquisition - INFO - Acquiring artifact from local: run_id=refit_run_12..., artifact_kind=checkpoint, backbone=distilbert
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.acquisition] Acquiring artifact from local: run_id=refit_run_12..., artifact_kind=checkpoint, backbone=distilbert
2026-01-18 00:31:22,924 - evaluation.selection.artifact_unified.acquisition - INFO - Copied artifact from local: /tmp/pytest-of-codespace/pytest-38/test_search_roots_used_in_loca0/checkpoint -> /tmp/pytest-of-codespace/pytest-38/test_search_roots_used_in_loca0/outputs/outputs/best_model_selection/local/distilbert/checkpoint_study123_trial876
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.acquisition] Copied artifact from local: /tmp/pytest-of-codespace/pytest-38/test_search_roots_used_in_loca0/checkpoint -> /tmp/pytest-of-codespace/pytest-38/test_search_roots_used_in_loca0/outputs/outputs/best_model_selection/local/distilbert/checkpoint_study123_trial876
2026-01-18 00:31:22,924 - evaluation.selection.artifact_unified.acquisition - INFO - Artifact validation passed: /tmp/pytest-of-codespace/pytest-38/test_search_roots_used_in_loca0/outputs/outputs/best_model_selection/local/distilbert/checkpoint_study123_trial876
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.acquisition] Artifact validation passed: /tmp/pytest-of-codespace/pytest-38/test_search_roots_used_in_loca0/outputs/outputs/best_model_selection/local/distilbert/checkpoint_study123_trial876
2026-01-18 00:31:22,931 - evaluation.selection.artifact_unified.acquisition - INFO - Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_123..., study_key_hash=study123..., trial_key_hash=trial876...
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.acquisition] Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_123..., study_key_hash=study123..., trial_key_hash=trial876...
2026-01-18 00:31:22,932 - evaluation.selection.artifact_unified.acquisition - INFO - Selected artifact run: artifact_run_id=refit_run_12..., trial_run_id=test_run_123...
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.acquisition] Selected artifact run: artifact_run_id=refit_run_12..., trial_run_id=test_run_123...
2026-01-18 00:31:22,932 - evaluation.selection.artifact_unified.acquisition - INFO - Artifact acquisition config for checkpoint: priority=['mlflow', 'local'], local.enabled=True, local.validate=True, drive.enabled=True, mlflow.enabled=True
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.acquisition] Artifact acquisition config for checkpoint: priority=['mlflow', 'local'], local.enabled=True, local.validate=True, drive.enabled=True, mlflow.enabled=True
2026-01-18 00:31:22,932 - evaluation.selection.artifact_unified.acquisition - INFO - Acquiring artifact from mlflow: run_id=refit_run_12..., artifact_kind=checkpoint, backbone=distilbert
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.acquisition] Acquiring artifact from mlflow: run_id=refit_run_12..., artifact_kind=checkpoint, backbone=distilbert
2026-01-18 00:31:22,933 - evaluation.selection.artifact_unified.acquisition - INFO - Downloading artifact from MLflow: run_id=refit_run_12..., destination=/tmp/pytest-of-codespace/pytest-38/test_artifact_kinds_priority_o0/outputs/outputs/best_model_selection/local/distilbert/checkpoint_study123_trial876
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.acquisition] Downloading artifact from MLflow: run_id=refit_run_12..., destination=/tmp/pytest-of-codespace/pytest-38/test_artifact_kinds_priority_o0/outputs/outputs/best_model_selection/local/distilbert/checkpoint_study123_trial876
2026-01-18 00:31:22,933 - evaluation.selection.artifact_unified.acquisition - ERROR - MLflow download failed: run_id=refit_run_12..., artifact_kind=checkpoint, error='Mock' object is not iterable
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/acquisition.py", line 588, in _download_from_mlflow
    artifact_paths = [a.path for a in artifacts]
TypeError: 'Mock' object is not iterable
2026-01-18 00:31:22 ERROR [evaluation.selection.artifact_unified.acquisition] MLflow download failed: run_id=refit_run_12..., artifact_kind=checkpoint, error='Mock' object is not iterable
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/acquisition.py", line 588, in _download_from_mlflow
    artifact_paths = [a.path for a in artifacts]
TypeError: 'Mock' object is not iterable
2026-01-18 00:31:22,933 - evaluation.selection.artifact_unified.acquisition - ERROR - Failed to acquire artifact from mlflow
2026-01-18 00:31:22 ERROR [evaluation.selection.artifact_unified.acquisition] Failed to acquire artifact from mlflow
2026-01-18 00:31:22,939 - evaluation.selection.artifact_unified.acquisition - INFO - Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_123..., study_key_hash=study123..., trial_key_hash=trial876...
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.acquisition] Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_123..., study_key_hash=study123..., trial_key_hash=trial876...
2026-01-18 00:31:22,940 - evaluation.selection.artifact_unified.acquisition - INFO - Selected artifact run: artifact_run_id=refit_run_12..., trial_run_id=test_run_123...
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.acquisition] Selected artifact run: artifact_run_id=refit_run_12..., trial_run_id=test_run_123...
2026-01-18 00:31:22,940 - evaluation.selection.artifact_unified.acquisition - INFO - Artifact acquisition config for checkpoint: priority=['local', 'mlflow'], local.enabled=True, local.validate=True, drive.enabled=True, mlflow.enabled=True
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.acquisition] Artifact acquisition config for checkpoint: priority=['local', 'mlflow'], local.enabled=True, local.validate=True, drive.enabled=True, mlflow.enabled=True
2026-01-18 00:31:22,940 - evaluation.selection.artifact_unified.acquisition - INFO - Acquiring artifact from local: run_id=refit_run_12..., artifact_kind=checkpoint, backbone=distilbert
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.acquisition] Acquiring artifact from local: run_id=refit_run_12..., artifact_kind=checkpoint, backbone=distilbert
2026-01-18 00:31:22,941 - evaluation.selection.artifact_unified.acquisition - INFO - Copied artifact from local: /tmp/pytest-of-codespace/pytest-38/test_artifact_kinds_fallback_t0/checkpoint -> /tmp/pytest-of-codespace/pytest-38/test_artifact_kinds_fallback_t0/outputs/outputs/best_model_selection/local/distilbert/checkpoint_study123_trial876
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.acquisition] Copied artifact from local: /tmp/pytest-of-codespace/pytest-38/test_artifact_kinds_fallback_t0/checkpoint -> /tmp/pytest-of-codespace/pytest-38/test_artifact_kinds_fallback_t0/outputs/outputs/best_model_selection/local/distilbert/checkpoint_study123_trial876
2026-01-18 00:31:22,941 - evaluation.selection.artifact_unified.acquisition - INFO - Artifact validation passed: /tmp/pytest-of-codespace/pytest-38/test_artifact_kinds_fallback_t0/outputs/outputs/best_model_selection/local/distilbert/checkpoint_study123_trial876
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.acquisition] Artifact validation passed: /tmp/pytest-of-codespace/pytest-38/test_artifact_kinds_fallback_t0/outputs/outputs/best_model_selection/local/distilbert/checkpoint_study123_trial876
2026-01-18 00:31:22,949 - evaluation.selection.artifact_unified.acquisition - INFO - Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_123..., study_key_hash=study123..., trial_key_hash=trial876...
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.acquisition] Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_123..., study_key_hash=study123..., trial_key_hash=trial876...
2026-01-18 00:31:22,949 - evaluation.selection.artifact_unified.acquisition - INFO - Selected artifact run: artifact_run_id=refit_run_12..., trial_run_id=test_run_123...
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.acquisition] Selected artifact run: artifact_run_id=refit_run_12..., trial_run_id=test_run_123...
2026-01-18 00:31:22,949 - evaluation.selection.artifact_unified.acquisition - INFO - Artifact acquisition config for checkpoint: priority=['local', 'drive', 'mlflow'], local.enabled=True, local.validate=True, drive.enabled=False, mlflow.enabled=False
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.acquisition] Artifact acquisition config for checkpoint: priority=['local', 'drive', 'mlflow'], local.enabled=True, local.validate=True, drive.enabled=False, mlflow.enabled=False
2026-01-18 00:31:22,950 - evaluation.selection.artifact_unified.acquisition - INFO - Acquiring artifact from local: run_id=refit_run_12..., artifact_kind=checkpoint, backbone=distilbert
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.acquisition] Acquiring artifact from local: run_id=refit_run_12..., artifact_kind=checkpoint, backbone=distilbert
2026-01-18 00:31:22,950 - evaluation.selection.artifact_unified.acquisition - INFO - Copied artifact from local: /tmp/pytest-of-codespace/pytest-38/test_config_with_disabled_sour0/checkpoint -> /tmp/pytest-of-codespace/pytest-38/test_config_with_disabled_sour0/outputs/outputs/best_model_selection/local/distilbert/checkpoint_study123_trial876
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.acquisition] Copied artifact from local: /tmp/pytest-of-codespace/pytest-38/test_config_with_disabled_sour0/checkpoint -> /tmp/pytest-of-codespace/pytest-38/test_config_with_disabled_sour0/outputs/outputs/best_model_selection/local/distilbert/checkpoint_study123_trial876
2026-01-18 00:31:22,951 - evaluation.selection.artifact_unified.acquisition - INFO - Artifact validation passed: /tmp/pytest-of-codespace/pytest-38/test_config_with_disabled_sour0/outputs/outputs/best_model_selection/local/distilbert/checkpoint_study123_trial876
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.acquisition] Artifact validation passed: /tmp/pytest-of-codespace/pytest-38/test_config_with_disabled_sour0/outputs/outputs/best_model_selection/local/distilbert/checkpoint_study123_trial876
2026-01-18 00:31:22,955 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:22,963 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:22,976 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:22,984 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:22,990 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:22,996 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:22 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-18 00:31:23,002 - evaluation.selection.cache - INFO - Saved best model selection cache: /tmp/pytest-of-codespace/pytest-38/test_cache_dual_file_strategy_0/outputs/cache/best_model_selection/latest_best_model_selection.json
2026-01-18 00:31:23 INFO  [evaluation.selection.cache] Saved best model selection cache: /tmp/pytest-of-codespace/pytest-38/test_cache_dual_file_strategy_0/outputs/cache/best_model_selection/latest_best_model_selection.json
2026-01-18 00:31:23,006 - evaluation.selection.cache - INFO - Using cached best model selection: run_id=run-123...
2026-01-18 00:31:23 INFO  [evaluation.selection.cache] Using cached best model selection: run_id=run-123...
2026-01-18 00:31:23,016 - evaluation.selection.cache - INFO - Using cached best model selection: run_id=run-123...
2026-01-18 00:31:23 INFO  [evaluation.selection.cache] Using cached best model selection: run_id=run-123...
2026-01-18 00:31:23,018 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-18 00:31:23,018 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-18 00:31:23,018 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 1
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 1
2026-01-18 00:31:23,018 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-18 00:31:23,019 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.70, Latency=0.30
2026-01-18 00:31:23,019 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from config file, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from config file, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 00:31:23,019 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-18 00:31:23,019 - evaluation.selection.mlflow_selection - INFO - Found 0 finished benchmark runs
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Found 0 finished benchmark runs
2026-01-18 00:31:23,019 - evaluation.selection.mlflow_selection - INFO - Found 0 benchmark runs with required metrics and grouping tags
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Found 0 benchmark runs with required metrics and grouping tags
2026-01-18 00:31:23,019 - evaluation.selection.mlflow_selection - WARNING - No valid benchmark runs found
2026-01-18 00:31:23 WARNI [evaluation.selection.mlflow_selection] No valid benchmark runs found
2026-01-18 00:31:23,021 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-18 00:31:23,021 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-18 00:31:23,021 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 1
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 1
2026-01-18 00:31:23,021 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-18 00:31:23,021 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.70, Latency=0.30
2026-01-18 00:31:23,021 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: median (from config file, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: median (from config file, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 00:31:23,021 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-18 00:31:23,021 - evaluation.selection.mlflow_selection - INFO - Found 0 finished benchmark runs
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Found 0 finished benchmark runs
2026-01-18 00:31:23,021 - evaluation.selection.mlflow_selection - INFO - Found 0 benchmark runs with required metrics and grouping tags
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Found 0 benchmark runs with required metrics and grouping tags
2026-01-18 00:31:23,021 - evaluation.selection.mlflow_selection - WARNING - No valid benchmark runs found
2026-01-18 00:31:23 WARNI [evaluation.selection.mlflow_selection] No valid benchmark runs found
2026-01-18 00:31:23,023 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-18 00:31:23,023 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-18 00:31:23,023 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 1
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 1
2026-01-18 00:31:23,023 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-18 00:31:23,023 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.70, Latency=0.30
2026-01-18 00:31:23,023 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: mean (from config file, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: mean (from config file, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 00:31:23,023 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-18 00:31:23,023 - evaluation.selection.mlflow_selection - INFO - Found 0 finished benchmark runs
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Found 0 finished benchmark runs
2026-01-18 00:31:23,023 - evaluation.selection.mlflow_selection - INFO - Found 0 benchmark runs with required metrics and grouping tags
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Found 0 benchmark runs with required metrics and grouping tags
2026-01-18 00:31:23,023 - evaluation.selection.mlflow_selection - WARNING - No valid benchmark runs found
2026-01-18 00:31:23 WARNI [evaluation.selection.mlflow_selection] No valid benchmark runs found
2026-01-18 00:31:23,036 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-18 00:31:23,036 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-18 00:31:23,036 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 1
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 1
2026-01-18 00:31:23,036 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-18 00:31:23,036 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.80, Latency=0.20
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.80, Latency=0.20
2026-01-18 00:31:23,036 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 00:31:23,036 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-18 00:31:23,037 - evaluation.selection.mlflow_selection - INFO - Found 0 finished benchmark runs
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Found 0 finished benchmark runs
2026-01-18 00:31:23,037 - evaluation.selection.mlflow_selection - INFO - Found 0 benchmark runs with required metrics and grouping tags
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Found 0 benchmark runs with required metrics and grouping tags
2026-01-18 00:31:23,037 - evaluation.selection.mlflow_selection - WARNING - No valid benchmark runs found
2026-01-18 00:31:23 WARNI [evaluation.selection.mlflow_selection] No valid benchmark runs found
2026-01-18 00:31:23,039 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-18 00:31:23,039 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-18 00:31:23,039 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 1
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 1
2026-01-18 00:31:23,039 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-18 00:31:23,039 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.70, Latency=0.30
2026-01-18 00:31:23,039 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 00:31:23,039 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-18 00:31:23,039 - evaluation.selection.mlflow_selection - INFO - Found 0 finished benchmark runs
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Found 0 finished benchmark runs
2026-01-18 00:31:23,039 - evaluation.selection.mlflow_selection - INFO - Found 0 benchmark runs with required metrics and grouping tags
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Found 0 benchmark runs with required metrics and grouping tags
2026-01-18 00:31:23,040 - evaluation.selection.mlflow_selection - WARNING - No valid benchmark runs found
2026-01-18 00:31:23 WARNI [evaluation.selection.mlflow_selection] No valid benchmark runs found
2026-01-18 00:31:23,041 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-18 00:31:23,042 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-18 00:31:23,042 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 1
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 1
2026-01-18 00:31:23,042 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-18 00:31:23,042 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.70, Latency=0.30
2026-01-18 00:31:23,042 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 00:31:23,042 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-18 00:31:23,042 - evaluation.selection.mlflow_selection - INFO - Found 0 finished benchmark runs
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Found 0 finished benchmark runs
2026-01-18 00:31:23,042 - evaluation.selection.mlflow_selection - INFO - Found 0 benchmark runs with required metrics and grouping tags
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Found 0 benchmark runs with required metrics and grouping tags
2026-01-18 00:31:23,042 - evaluation.selection.mlflow_selection - WARNING - No valid benchmark runs found
2026-01-18 00:31:23 WARNI [evaluation.selection.mlflow_selection] No valid benchmark runs found
2026-01-18 00:31:23,044 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-18 00:31:23,044 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-18 00:31:23,044 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 1
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 1
2026-01-18 00:31:23,044 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-18 00:31:23,044 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.70, Latency=0.30
2026-01-18 00:31:23,044 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: median (from config file, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: median (from config file, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 00:31:23,044 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-18 00:31:23,044 - evaluation.selection.mlflow_selection - INFO - Found 0 finished benchmark runs
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Found 0 finished benchmark runs
2026-01-18 00:31:23,044 - evaluation.selection.mlflow_selection - INFO - Found 0 benchmark runs with required metrics and grouping tags
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Found 0 benchmark runs with required metrics and grouping tags
2026-01-18 00:31:23,044 - evaluation.selection.mlflow_selection - WARNING - No valid benchmark runs found
2026-01-18 00:31:23 WARNI [evaluation.selection.mlflow_selection] No valid benchmark runs found
2026-01-18 00:31:23,047 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-18 00:31:23,047 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-18 00:31:23,047 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 2
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 2
2026-01-18 00:31:23,048 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-18 00:31:23,048 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.70, Latency=0.30
2026-01-18 00:31:23,048 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 00:31:23,048 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-18 00:31:23,048 - evaluation.selection.mlflow_selection - INFO - Found 0 finished benchmark runs
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Found 0 finished benchmark runs
2026-01-18 00:31:23,048 - evaluation.selection.mlflow_selection - INFO - Found 0 benchmark runs with required metrics and grouping tags
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Found 0 benchmark runs with required metrics and grouping tags
2026-01-18 00:31:23,048 - evaluation.selection.mlflow_selection - WARNING - No valid benchmark runs found
2026-01-18 00:31:23 WARNI [evaluation.selection.mlflow_selection] No valid benchmark runs found
2026-01-18 00:31:23,051 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-18 00:31:23,051 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-18 00:31:23,051 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 2
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 2
2026-01-18 00:31:23,051 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-18 00:31:23,051 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.70, Latency=0.30
2026-01-18 00:31:23,051 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 00:31:23,051 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-18 00:31:23,051 - evaluation.selection.mlflow_selection - INFO - Found 0 finished benchmark runs
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Found 0 finished benchmark runs
2026-01-18 00:31:23,051 - evaluation.selection.mlflow_selection - INFO - Found 0 benchmark runs with required metrics and grouping tags
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Found 0 benchmark runs with required metrics and grouping tags
2026-01-18 00:31:23,051 - evaluation.selection.mlflow_selection - WARNING - No valid benchmark runs found
2026-01-18 00:31:23 WARNI [evaluation.selection.mlflow_selection] No valid benchmark runs found
2026-01-18 00:31:23,054 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-18 00:31:23,054 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-18 00:31:23,054 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 2
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 2
2026-01-18 00:31:23,054 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-18 00:31:23,054 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.70, Latency=0.30
2026-01-18 00:31:23,054 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 00:31:23,054 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-18 00:31:23,054 - evaluation.selection.mlflow_selection - INFO - Found 1 finished benchmark runs
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Found 1 finished benchmark runs
2026-01-18 00:31:23,054 - evaluation.selection.mlflow_selection - INFO - Found 1 benchmark runs with required metrics and grouping tags
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Found 1 benchmark runs with required metrics and grouping tags
2026-01-18 00:31:23,054 - evaluation.selection.mlflow_selection - INFO - Preloading trial and refit runs from HPO experiments...
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Preloading trial and refit runs from HPO experiments...
2026-01-18 00:31:23,054 - evaluation.selection.mlflow_selection - INFO - Built trial lookup with 0 unique (study_hash, trial_hash) pairs
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Built trial lookup with 0 unique (study_hash, trial_hash) pairs
2026-01-18 00:31:23,054 - evaluation.selection.mlflow_selection - INFO - Built refit lookup with 0 unique (study_hash, trial_hash) pairs
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Built refit lookup with 0 unique (study_hash, trial_hash) pairs
2026-01-18 00:31:23,054 - evaluation.selection.mlflow_selection - WARNING - No trial runs found in HPO experiments
2026-01-18 00:31:23 WARNI [evaluation.selection.mlflow_selection] No trial runs found in HPO experiments
2026-01-18 00:31:23,057 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-18 00:31:23,057 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-18 00:31:23,057 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 2
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 2
2026-01-18 00:31:23,057 - evaluation.selection.mlflow_selection - INFO -   Objective metric: accuracy
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Objective metric: accuracy
2026-01-18 00:31:23,057 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.50, Latency=0.50
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.50, Latency=0.50
2026-01-18 00:31:23,057 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 00:31:23,057 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-18 00:31:23,057 - evaluation.selection.mlflow_selection - INFO - Found 0 finished benchmark runs
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Found 0 finished benchmark runs
2026-01-18 00:31:23,057 - evaluation.selection.mlflow_selection - INFO - Found 0 benchmark runs with required metrics and grouping tags
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Found 0 benchmark runs with required metrics and grouping tags
2026-01-18 00:31:23,057 - evaluation.selection.mlflow_selection - WARNING - No valid benchmark runs found
2026-01-18 00:31:23 WARNI [evaluation.selection.mlflow_selection] No valid benchmark runs found
2026-01-18 00:31:23,061 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-18 00:31:23,061 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-18 00:31:23,061 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 2
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 2
2026-01-18 00:31:23,061 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-18 00:31:23,061 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.70, Latency=0.30
2026-01-18 00:31:23,061 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 00:31:23,062 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-18 00:31:23,062 - evaluation.selection.mlflow_selection - INFO - Found 1 finished benchmark runs
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Found 1 finished benchmark runs
2026-01-18 00:31:23,062 - evaluation.selection.mlflow_selection - INFO - Found 1 benchmark runs with required metrics and grouping tags
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Found 1 benchmark runs with required metrics and grouping tags
2026-01-18 00:31:23,062 - evaluation.selection.mlflow_selection - INFO - Preloading trial and refit runs from HPO experiments...
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Preloading trial and refit runs from HPO experiments...
2026-01-18 00:31:23,062 - evaluation.selection.mlflow_selection - INFO - Built trial lookup with 1 unique (study_hash, trial_hash) pairs
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Built trial lookup with 1 unique (study_hash, trial_hash) pairs
2026-01-18 00:31:23,062 - evaluation.selection.mlflow_selection - INFO - Built refit lookup with 1 unique (study_hash, trial_hash) pairs
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Built refit lookup with 1 unique (study_hash, trial_hash) pairs
2026-01-18 00:31:23,062 - evaluation.selection.mlflow_selection - INFO - Grouped 1 benchmark runs into 1 unique groups (by study_key_hash, trial_key_hash, benchmark_key)
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Grouped 1 benchmark runs into 1 unique groups (by study_key_hash, trial_key_hash, benchmark_key)
2026-01-18 00:31:23,062 - evaluation.selection.mlflow_selection - INFO - Found 1 candidate(s) with both benchmark and training metrics
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Found 1 candidate(s) with both benchmark and training metrics
2026-01-18 00:31:23,062 - evaluation.selection.mlflow_selection - INFO - Best model selected: backbone=distilbert, f1=0.7500, latency=5.00ms, composite=0.3000
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Best model selected: backbone=distilbert, f1=0.7500, latency=5.00ms, composite=0.3000
2026-01-18 00:31:23,065 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-18 00:31:23,065 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-18 00:31:23,065 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 2
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 2
2026-01-18 00:31:23,065 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-18 00:31:23,065 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.70, Latency=0.30
2026-01-18 00:31:23,065 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 00:31:23,065 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-18 00:31:23,065 - evaluation.selection.mlflow_selection - INFO - Found 0 finished benchmark runs
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Found 0 finished benchmark runs
2026-01-18 00:31:23,065 - evaluation.selection.mlflow_selection - INFO - Found 0 benchmark runs with required metrics and grouping tags
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Found 0 benchmark runs with required metrics and grouping tags
2026-01-18 00:31:23,065 - evaluation.selection.mlflow_selection - WARNING - No valid benchmark runs found
2026-01-18 00:31:23 WARNI [evaluation.selection.mlflow_selection] No valid benchmark runs found
2026-01-18 00:31:23,067 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-18 00:31:23,068 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-18 00:31:23,068 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 2
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 2
2026-01-18 00:31:23,068 - evaluation.selection.mlflow_selection - INFO -   Objective metric: accuracy
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Objective metric: accuracy
2026-01-18 00:31:23,068 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.50, Latency=0.50
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.50, Latency=0.50
2026-01-18 00:31:23,068 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 00:31:23,068 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-18 00:31:23,068 - evaluation.selection.mlflow_selection - INFO - Found 0 finished benchmark runs
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Found 0 finished benchmark runs
2026-01-18 00:31:23,068 - evaluation.selection.mlflow_selection - INFO - Found 0 benchmark runs with required metrics and grouping tags
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Found 0 benchmark runs with required metrics and grouping tags
2026-01-18 00:31:23,068 - evaluation.selection.mlflow_selection - WARNING - No valid benchmark runs found
2026-01-18 00:31:23 WARNI [evaluation.selection.mlflow_selection] No valid benchmark runs found
2026-01-18 00:31:23,077 - evaluation.selection.cache - INFO - Using cached best model selection: run_id=test_run_id_...
2026-01-18 00:31:23 INFO  [evaluation.selection.cache] Using cached best model selection: run_id=test_run_id_...
2026-01-18 00:31:23,085 - evaluation.selection.cache - INFO - Saved best model selection cache: latest.json
2026-01-18 00:31:23 INFO  [evaluation.selection.cache] Saved best model selection cache: latest.json
2026-01-18 00:31:23,090 - evaluation.selection.cache - INFO - Using cached best model selection: run_id=test_run_id_...
2026-01-18 00:31:23 INFO  [evaluation.selection.cache] Using cached best model selection: run_id=test_run_id_...
2026-01-18 00:31:23,112 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-18 00:31:23,112 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-18 00:31:23,112 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 2
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 2
2026-01-18 00:31:23,112 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-18 00:31:23,112 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.70, Latency=0.30
2026-01-18 00:31:23,112 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 00:31:23,112 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-18 00:31:23,112 - evaluation.selection.mlflow_selection - INFO - Found 1 finished benchmark runs
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Found 1 finished benchmark runs
2026-01-18 00:31:23,112 - evaluation.selection.mlflow_selection - INFO - Found 0 benchmark runs with required metrics and grouping tags
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Found 0 benchmark runs with required metrics and grouping tags
2026-01-18 00:31:23,113 - evaluation.selection.mlflow_selection - WARNING - No valid benchmark runs found
2026-01-18 00:31:23 WARNI [evaluation.selection.mlflow_selection] No valid benchmark runs found
2026-01-18 00:31:23,115 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-18 00:31:23,115 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-18 00:31:23,115 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 2
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 2
2026-01-18 00:31:23,115 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-18 00:31:23,115 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.70, Latency=0.30
2026-01-18 00:31:23,115 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 00:31:23,116 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-18 00:31:23,116 - evaluation.selection.mlflow_selection - INFO - Found 0 finished benchmark runs
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Found 0 finished benchmark runs
2026-01-18 00:31:23,116 - evaluation.selection.mlflow_selection - INFO - Found 0 benchmark runs with required metrics and grouping tags
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Found 0 benchmark runs with required metrics and grouping tags
2026-01-18 00:31:23,116 - evaluation.selection.mlflow_selection - WARNING - No valid benchmark runs found
2026-01-18 00:31:23 WARNI [evaluation.selection.mlflow_selection] No valid benchmark runs found
2026-01-18 00:31:23,123 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-18 00:31:23,124 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-18 00:31:23,124 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 2
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 2
2026-01-18 00:31:23,124 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-18 00:31:23,124 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.70, Latency=0.30
2026-01-18 00:31:23,124 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 00:31:23,124 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-18 00:31:23,124 - evaluation.selection.mlflow_selection - INFO - Found 0 finished benchmark runs
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Found 0 finished benchmark runs
2026-01-18 00:31:23,124 - evaluation.selection.mlflow_selection - INFO - Found 0 benchmark runs with required metrics and grouping tags
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Found 0 benchmark runs with required metrics and grouping tags
2026-01-18 00:31:23,124 - evaluation.selection.mlflow_selection - WARNING - No valid benchmark runs found
2026-01-18 00:31:23 WARNI [evaluation.selection.mlflow_selection] No valid benchmark runs found
2026-01-18 00:31:23,130 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-18 00:31:23,130 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-18 00:31:23,130 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 2
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 2
2026-01-18 00:31:23,130 - evaluation.selection.mlflow_selection - INFO -   Objective metric: accuracy
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Objective metric: accuracy
2026-01-18 00:31:23,130 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.50, Latency=0.50
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.50, Latency=0.50
2026-01-18 00:31:23,130 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-18 00:31:23,130 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-18 00:31:23,130 - evaluation.selection.mlflow_selection - INFO - Found 0 finished benchmark runs
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Found 0 finished benchmark runs
2026-01-18 00:31:23,130 - evaluation.selection.mlflow_selection - INFO - Found 0 benchmark runs with required metrics and grouping tags
2026-01-18 00:31:23 INFO  [evaluation.selection.mlflow_selection] Found 0 benchmark runs with required metrics and grouping tags
2026-01-18 00:31:23,130 - evaluation.selection.mlflow_selection - WARNING - No valid benchmark runs found
2026-01-18 00:31:23 WARNI [evaluation.selection.mlflow_selection] No valid benchmark runs found
2026-01-18 00:31:23,136 - evaluation.selection.cache - INFO - Using cached best model selection: run_id=test_run_id_...
2026-01-18 00:31:23 INFO  [evaluation.selection.cache] Using cached best model selection: run_id=test_run_id_...
2026-01-18 00:31:23 WARNI [infrastructure.paths.resolve] Pattern 'final_training_v2' not found in paths.yaml. Using fallback logic.
2026-01-18 00:31:23 WARNI [infrastructure.paths.resolve] Pattern 'final_training_v2' not found in paths.yaml. Using fallback logic.
2026-01-18 00:31:23 WARNI [infrastructure.paths.resolve] Pattern 'conversion_v2' not found in paths.yaml. Using fallback logic.
2026-01-18 00:31:23 WARNI [infrastructure.paths.resolve] Pattern 'final_training_v2' not found in paths.yaml. Using fallback logic.
2026-01-18 00:31:23 WARNI [infrastructure.paths.resolve] Pattern 'final_training_v2' not found in paths.yaml. Using fallback logic.
2026/01/18 00:31:23 INFO mlflow.tracking.fluent: Experiment with name 'test-experiment' does not exist. Creating a new experiment.
2026/01/18 00:31:23 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696283410, 1768696283410)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 350, in create_experiment
    eid = session.query(SqlExperiment).filter_by(name=name).first().experiment_id
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2759, in first
    return self.limit(1)._iter().first()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1233, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696283410, 1768696283410)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:23,439 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696283410, 1768696283410)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:23 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696283410, 1768696283410)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:23,439 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Continuing without MLflow tracking...
2026-01-18 00:31:23 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Continuing without MLflow tracking...
2026-01-18 00:31:23,442 - infrastructure.tracking.mlflow.trackers.benchmark_tracker - INFO - [START_BENCHMARK_RUN] Building tags: context=None, study_key_hash=None..., trial_key_hash=None..., context.model=None, context.process_type=None
2026-01-18 00:31:23 INFO  [infrastructure.tracking.mlflow.trackers.benchmark_tracker] [START_BENCHMARK_RUN] Building tags: context=None, study_key_hash=None..., trial_key_hash=None..., context.model=None, context.process_type=None
2026-01-18 00:31:23,442 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_benchmark_tracking_enable0/config/tags.yaml, using defaults
2026-01-18 00:31:23 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_benchmark_tracking_enable0/config/tags.yaml, using defaults
2026-01-18 00:31:23,442 - infrastructure.tracking.mlflow.trackers.benchmark_tracker - INFO - [START_BENCHMARK_RUN] Built tags, grouping tags present: study_key_hash=False, trial_key_hash=False, code.model=unknown, code.stage=unknown
2026-01-18 00:31:23 INFO  [infrastructure.tracking.mlflow.trackers.benchmark_tracker] [START_BENCHMARK_RUN] Built tags, grouping tags present: study_key_hash=False, trial_key_hash=False, code.model=unknown, code.stage=unknown
2026-01-18 00:31:23,442 - infrastructure.tracking.mlflow.trackers.benchmark_tracker - WARNING - [START_BENCHMARK_RUN] No valid parent run IDs found. Received: trial=None, refit=None, sweep=None. These may be timestamps or None - will query MLflow if trial_key_hash available.
2026-01-18 00:31:23 WARNI [infrastructure.tracking.mlflow.trackers.benchmark_tracker] [START_BENCHMARK_RUN] No valid parent run IDs found. Received: trial=None, refit=None, sweep=None. These may be timestamps or None - will query MLflow if trial_key_hash available.
2026/01/18 00:31:23 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('1f6ad882e9674bcdaf62bd4d3d3a3e5c', 'nebulous-lark-516', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696283447, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/1f6ad882e9674bcdaf62bd4d3d3a3e5c/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 586, in create_run
    inputs_list = self._get_run_inputs(session, [run_id])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 626, in _get_run_inputs
    ).all()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2704, in all
    return self._iter().all()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1048, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('1f6ad882e9674bcdaf62bd4d3d3a3e5c', 'nebulous-lark-516', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696283447, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/1f6ad882e9674bcdaf62bd4d3d3a3e5c/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:23,453 - infrastructure.tracking.mlflow.trackers.benchmark_tracker - WARNING - MLflow tracking failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('1f6ad882e9674bcdaf62bd4d3d3a3e5c', 'nebulous-lark-516', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696283447, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/1f6ad882e9674bcdaf62bd4d3d3a3e5c/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:23 WARNI [infrastructure.tracking.mlflow.trackers.benchmark_tracker] MLflow tracking failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('1f6ad882e9674bcdaf62bd4d3d3a3e5c', 'nebulous-lark-516', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696283447, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/1f6ad882e9674bcdaf62bd4d3d3a3e5c/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:23,454 - infrastructure.tracking.mlflow.trackers.benchmark_tracker - WARNING - Continuing benchmarking without MLflow tracking...
2026-01-18 00:31:23 WARNI [infrastructure.tracking.mlflow.trackers.benchmark_tracker] Continuing benchmarking without MLflow tracking...
2026/01/18 00:31:24 INFO mlflow.tracking.fluent: Experiment with name 'test-experiment' does not exist. Creating a new experiment.
2026/01/18 00:31:24 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696284655, 1768696284655)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 350, in create_experiment
    eid = session.query(SqlExperiment).filter_by(name=name).first().experiment_id
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2759, in first
    return self.limit(1)._iter().first()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1233, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696284655, 1768696284655)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:24,656 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696284655, 1768696284655)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:24 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696284655, 1768696284655)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:24,656 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Continuing without MLflow tracking...
2026-01-18 00:31:24 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Continuing without MLflow tracking...
2026-01-18 00:31:24,658 - infrastructure.tracking.mlflow.trackers.benchmark_tracker - INFO - [Benchmark Tracker] MLflow tracking disabled for benchmark stage (tracking.benchmark.enabled=false)
2026-01-18 00:31:24 INFO  [infrastructure.tracking.mlflow.trackers.benchmark_tracker] [Benchmark Tracker] MLflow tracking disabled for benchmark stage (tracking.benchmark.enabled=false)
2026/01/18 00:31:24 INFO mlflow.tracking.fluent: Experiment with name 'test-experiment' does not exist. Creating a new experiment.
2026/01/18 00:31:24 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696284663, 1768696284663)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 350, in create_experiment
    eid = session.query(SqlExperiment).filter_by(name=name).first().experiment_id
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2759, in first
    return self.limit(1)._iter().first()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1233, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696284663, 1768696284663)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:24,664 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696284663, 1768696284663)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:24 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696284663, 1768696284663)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:24,665 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Continuing without MLflow tracking...
2026-01-18 00:31:24 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Continuing without MLflow tracking...
2026-01-18 00:31:24,666 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_training_tracking_enabled0/config/tags.yaml, using defaults
2026-01-18 00:31:24 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_training_tracking_enabled0/config/tags.yaml, using defaults
2026/01/18 00:31:24 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('a394c0b04a5a4d78be1230c0b7d4ef3d', 'upbeat-kit-767', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696284671, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/a394c0b04a5a4d78be1230c0b7d4ef3d/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 586, in create_run
    inputs_list = self._get_run_inputs(session, [run_id])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 626, in _get_run_inputs
    ).all()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2704, in all
    return self._iter().all()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1048, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('a394c0b04a5a4d78be1230c0b7d4ef3d', 'upbeat-kit-767', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696284671, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/a394c0b04a5a4d78be1230c0b7d4ef3d/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:24,675 - infrastructure.tracking.mlflow.trackers.training_tracker - WARNING - MLflow tracking failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('a394c0b04a5a4d78be1230c0b7d4ef3d', 'upbeat-kit-767', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696284671, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/a394c0b04a5a4d78be1230c0b7d4ef3d/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:24 WARNI [infrastructure.tracking.mlflow.trackers.training_tracker] MLflow tracking failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('a394c0b04a5a4d78be1230c0b7d4ef3d', 'upbeat-kit-767', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696284671, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/a394c0b04a5a4d78be1230c0b7d4ef3d/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:24,676 - infrastructure.tracking.mlflow.trackers.training_tracker - WARNING - Continuing training without MLflow tracking...
2026-01-18 00:31:24 WARNI [infrastructure.tracking.mlflow.trackers.training_tracker] Continuing training without MLflow tracking...
2026/01/18 00:31:26 INFO mlflow.tracking.fluent: Experiment with name 'test-experiment' does not exist. Creating a new experiment.
2026/01/18 00:31:26 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696286029, 1768696286029)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 350, in create_experiment
    eid = session.query(SqlExperiment).filter_by(name=name).first().experiment_id
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2759, in first
    return self.limit(1)._iter().first()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1233, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696286029, 1768696286029)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:26,031 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696286029, 1768696286029)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:26 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696286029, 1768696286029)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:26,031 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Continuing without MLflow tracking...
2026-01-18 00:31:26 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Continuing without MLflow tracking...
2026-01-18 00:31:26,032 - infrastructure.tracking.mlflow.trackers.training_tracker - INFO - [Training Tracker] MLflow tracking disabled for training stage (tracking.training.enabled=false)
2026-01-18 00:31:26 INFO  [infrastructure.tracking.mlflow.trackers.training_tracker] [Training Tracker] MLflow tracking disabled for training stage (tracking.training.enabled=false)
2026/01/18 00:31:26 INFO mlflow.tracking.fluent: Experiment with name 'test-experiment' does not exist. Creating a new experiment.
2026/01/18 00:31:26 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696286038, 1768696286038)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 350, in create_experiment
    eid = session.query(SqlExperiment).filter_by(name=name).first().experiment_id
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2759, in first
    return self.limit(1)._iter().first()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1233, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696286038, 1768696286038)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:26,039 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696286038, 1768696286038)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:26 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696286038, 1768696286038)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:26,039 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Continuing without MLflow tracking...
2026-01-18 00:31:26 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Continuing without MLflow tracking...
2026-01-18 00:31:26,041 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_conversion_tracking_enabl0/config/tags.yaml, using defaults
2026-01-18 00:31:26 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_conversion_tracking_enabl0/config/tags.yaml, using defaults
2026/01/18 00:31:26 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('62e396c0710a44b3bcaba0cc27138242', 'big-kit-958', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696286045, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/62e396c0710a44b3bcaba0cc27138242/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 586, in create_run
    inputs_list = self._get_run_inputs(session, [run_id])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 626, in _get_run_inputs
    ).all()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2704, in all
    return self._iter().all()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1048, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('62e396c0710a44b3bcaba0cc27138242', 'big-kit-958', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696286045, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/62e396c0710a44b3bcaba0cc27138242/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:26,050 - infrastructure.tracking.mlflow.trackers.conversion_tracker - WARNING - MLflow tracking failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('62e396c0710a44b3bcaba0cc27138242', 'big-kit-958', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696286045, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/62e396c0710a44b3bcaba0cc27138242/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:26 WARNI [infrastructure.tracking.mlflow.trackers.conversion_tracker] MLflow tracking failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('62e396c0710a44b3bcaba0cc27138242', 'big-kit-958', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696286045, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/62e396c0710a44b3bcaba0cc27138242/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:26,050 - infrastructure.tracking.mlflow.trackers.conversion_tracker - WARNING - Continuing conversion without MLflow tracking...
2026-01-18 00:31:26 WARNI [infrastructure.tracking.mlflow.trackers.conversion_tracker] Continuing conversion without MLflow tracking...
2026/01/18 00:31:26 INFO mlflow.tracking.fluent: Experiment with name 'test-experiment' does not exist. Creating a new experiment.
2026/01/18 00:31:26 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696286942, 1768696286942)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 350, in create_experiment
    eid = session.query(SqlExperiment).filter_by(name=name).first().experiment_id
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2759, in first
    return self.limit(1)._iter().first()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1233, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696286942, 1768696286942)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:26,943 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696286942, 1768696286942)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:26 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696286942, 1768696286942)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:26,943 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Continuing without MLflow tracking...
2026-01-18 00:31:26 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Continuing without MLflow tracking...
2026-01-18 00:31:26,945 - infrastructure.tracking.mlflow.trackers.conversion_tracker - INFO - [Conversion Tracker] MLflow tracking disabled for conversion stage (tracking.conversion.enabled=false)
2026-01-18 00:31:26 INFO  [infrastructure.tracking.mlflow.trackers.conversion_tracker] [Conversion Tracker] MLflow tracking disabled for conversion stage (tracking.conversion.enabled=false)
2026/01/18 00:31:26 INFO mlflow.tracking.fluent: Experiment with name 'test-experiment' does not exist. Creating a new experiment.
2026/01/18 00:31:26 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696286951, 1768696286951)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 350, in create_experiment
    eid = session.query(SqlExperiment).filter_by(name=name).first().experiment_id
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2759, in first
    return self.limit(1)._iter().first()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1233, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696286951, 1768696286951)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:26,952 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696286951, 1768696286951)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:26 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696286951, 1768696286951)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:26,952 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Continuing without MLflow tracking...
2026-01-18 00:31:26 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Continuing without MLflow tracking...
2026-01-18 00:31:26,954 - infrastructure.tracking.mlflow.trackers.benchmark_tracker - INFO - [START_BENCHMARK_RUN] Building tags: context=None, study_key_hash=None..., trial_key_hash=None..., context.model=None, context.process_type=None
2026-01-18 00:31:26 INFO  [infrastructure.tracking.mlflow.trackers.benchmark_tracker] [START_BENCHMARK_RUN] Building tags: context=None, study_key_hash=None..., trial_key_hash=None..., context.model=None, context.process_type=None
2026-01-18 00:31:26,954 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_benchmark_log_artifacts_d0/config/tags.yaml, using defaults
2026-01-18 00:31:26 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_benchmark_log_artifacts_d0/config/tags.yaml, using defaults
2026-01-18 00:31:26,954 - infrastructure.tracking.mlflow.trackers.benchmark_tracker - INFO - [START_BENCHMARK_RUN] Built tags, grouping tags present: study_key_hash=False, trial_key_hash=False, code.model=unknown, code.stage=unknown
2026-01-18 00:31:26 INFO  [infrastructure.tracking.mlflow.trackers.benchmark_tracker] [START_BENCHMARK_RUN] Built tags, grouping tags present: study_key_hash=False, trial_key_hash=False, code.model=unknown, code.stage=unknown
2026-01-18 00:31:26,954 - infrastructure.tracking.mlflow.trackers.benchmark_tracker - WARNING - [START_BENCHMARK_RUN] No valid parent run IDs found. Received: trial=None, refit=None, sweep=None. These may be timestamps or None - will query MLflow if trial_key_hash available.
2026-01-18 00:31:26 WARNI [infrastructure.tracking.mlflow.trackers.benchmark_tracker] [START_BENCHMARK_RUN] No valid parent run IDs found. Received: trial=None, refit=None, sweep=None. These may be timestamps or None - will query MLflow if trial_key_hash available.
2026/01/18 00:31:26 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('2e959dc6d7914a3c98863c9703ef2454', 'unruly-goose-690', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696286959, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/2e959dc6d7914a3c98863c9703ef2454/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 586, in create_run
    inputs_list = self._get_run_inputs(session, [run_id])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 626, in _get_run_inputs
    ).all()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2704, in all
    return self._iter().all()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1048, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('2e959dc6d7914a3c98863c9703ef2454', 'unruly-goose-690', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696286959, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/2e959dc6d7914a3c98863c9703ef2454/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:26,963 - infrastructure.tracking.mlflow.trackers.benchmark_tracker - WARNING - MLflow tracking failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('2e959dc6d7914a3c98863c9703ef2454', 'unruly-goose-690', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696286959, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/2e959dc6d7914a3c98863c9703ef2454/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:26 WARNI [infrastructure.tracking.mlflow.trackers.benchmark_tracker] MLflow tracking failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('2e959dc6d7914a3c98863c9703ef2454', 'unruly-goose-690', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696286959, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/2e959dc6d7914a3c98863c9703ef2454/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:26,963 - infrastructure.tracking.mlflow.trackers.benchmark_tracker - WARNING - Continuing benchmarking without MLflow tracking...
2026-01-18 00:31:26 WARNI [infrastructure.tracking.mlflow.trackers.benchmark_tracker] Continuing benchmarking without MLflow tracking...
2026/01/18 00:31:26 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('dde6da4ef46d43bf884b7ca991fcbf66', 'sincere-elk-761', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696286967, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/dde6da4ef46d43bf884b7ca991fcbf66/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 586, in create_run
    inputs_list = self._get_run_inputs(session, [run_id])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 626, in _get_run_inputs
    ).all()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2704, in all
    return self._iter().all()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1048, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('dde6da4ef46d43bf884b7ca991fcbf66', 'sincere-elk-761', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696286967, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/dde6da4ef46d43bf884b7ca991fcbf66/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:26,971 - infrastructure.tracking.mlflow.trackers.benchmark_tracker - WARNING - Could not log benchmark results to MLflow: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('dde6da4ef46d43bf884b7ca991fcbf66', 'sincere-elk-761', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696286967, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/dde6da4ef46d43bf884b7ca991fcbf66/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:26 WARNI [infrastructure.tracking.mlflow.trackers.benchmark_tracker] Could not log benchmark results to MLflow: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('dde6da4ef46d43bf884b7ca991fcbf66', 'sincere-elk-761', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696286967, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/dde6da4ef46d43bf884b7ca991fcbf66/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026/01/18 00:31:26 INFO mlflow.tracking.fluent: Experiment with name 'test-experiment' does not exist. Creating a new experiment.
2026/01/18 00:31:26 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696286977, 1768696286977)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 350, in create_experiment
    eid = session.query(SqlExperiment).filter_by(name=name).first().experiment_id
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2759, in first
    return self.limit(1)._iter().first()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1233, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696286977, 1768696286977)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:26,978 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696286977, 1768696286977)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:26 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696286977, 1768696286977)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:26,978 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Continuing without MLflow tracking...
2026-01-18 00:31:26 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Continuing without MLflow tracking...
2026-01-18 00:31:26,980 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_training_log_checkpoint_d0/config/tags.yaml, using defaults
2026-01-18 00:31:26 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_training_log_checkpoint_d0/config/tags.yaml, using defaults
2026/01/18 00:31:26 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('4642a5794b2d4fe08e8858da4c159c0f', 'rumbling-whale-681', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696286984, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/4642a5794b2d4fe08e8858da4c159c0f/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 586, in create_run
    inputs_list = self._get_run_inputs(session, [run_id])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 626, in _get_run_inputs
    ).all()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2704, in all
    return self._iter().all()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1048, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('4642a5794b2d4fe08e8858da4c159c0f', 'rumbling-whale-681', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696286984, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/4642a5794b2d4fe08e8858da4c159c0f/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:26,988 - infrastructure.tracking.mlflow.trackers.training_tracker - WARNING - MLflow tracking failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('4642a5794b2d4fe08e8858da4c159c0f', 'rumbling-whale-681', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696286984, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/4642a5794b2d4fe08e8858da4c159c0f/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:26 WARNI [infrastructure.tracking.mlflow.trackers.training_tracker] MLflow tracking failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('4642a5794b2d4fe08e8858da4c159c0f', 'rumbling-whale-681', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696286984, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/4642a5794b2d4fe08e8858da4c159c0f/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:26,988 - infrastructure.tracking.mlflow.trackers.training_tracker - WARNING - Continuing training without MLflow tracking...
2026-01-18 00:31:26 WARNI [infrastructure.tracking.mlflow.trackers.training_tracker] Continuing training without MLflow tracking...
2026/01/18 00:31:26 INFO mlflow.tracking.fluent: Experiment with name 'test-experiment' does not exist. Creating a new experiment.
2026/01/18 00:31:26 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696286993, 1768696286993)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 350, in create_experiment
    eid = session.query(SqlExperiment).filter_by(name=name).first().experiment_id
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2759, in first
    return self.limit(1)._iter().first()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1233, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696286993, 1768696286993)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:26,994 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696286993, 1768696286993)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:26 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696286993, 1768696286993)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:26,995 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Continuing without MLflow tracking...
2026-01-18 00:31:26 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Continuing without MLflow tracking...
2026-01-18 00:31:26,996 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_training_log_metrics_json0/config/tags.yaml, using defaults
2026-01-18 00:31:26 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_training_log_metrics_json0/config/tags.yaml, using defaults
2026/01/18 00:31:27 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('aa8c02ab3d824ec8be286b2810fa6c28', 'overjoyed-perch-945', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696287001, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/aa8c02ab3d824ec8be286b2810fa6c28/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 586, in create_run
    inputs_list = self._get_run_inputs(session, [run_id])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 626, in _get_run_inputs
    ).all()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2704, in all
    return self._iter().all()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1048, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('aa8c02ab3d824ec8be286b2810fa6c28', 'overjoyed-perch-945', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696287001, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/aa8c02ab3d824ec8be286b2810fa6c28/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:27,005 - infrastructure.tracking.mlflow.trackers.training_tracker - WARNING - MLflow tracking failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('aa8c02ab3d824ec8be286b2810fa6c28', 'overjoyed-perch-945', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696287001, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/aa8c02ab3d824ec8be286b2810fa6c28/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:27 WARNI [infrastructure.tracking.mlflow.trackers.training_tracker] MLflow tracking failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('aa8c02ab3d824ec8be286b2810fa6c28', 'overjoyed-perch-945', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696287001, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/aa8c02ab3d824ec8be286b2810fa6c28/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:27,005 - infrastructure.tracking.mlflow.trackers.training_tracker - WARNING - Continuing training without MLflow tracking...
2026-01-18 00:31:27 WARNI [infrastructure.tracking.mlflow.trackers.training_tracker] Continuing training without MLflow tracking...
2026-01-18 00:31:27,005 - infrastructure.tracking.mlflow.artifacts.uploader - INFO - Creating checkpoint archive from /tmp/pytest-of-codespace/pytest-38/test_training_log_metrics_json0/outputs/final_training/checkpoint...
2026-01-18 00:31:27 INFO  [infrastructure.tracking.mlflow.artifacts.uploader] Creating checkpoint archive from /tmp/pytest-of-codespace/pytest-38/test_training_log_metrics_json0/outputs/final_training/checkpoint...
2026-01-18 00:31:27,005 - infrastructure.tracking.mlflow.artifacts.manager - INFO - Created checkpoint archive: /tmp/checkpoint_spl0g9up.tar.gz (0 files, 0.0MB)
2026-01-18 00:31:27 INFO  [infrastructure.tracking.mlflow.artifacts.manager] Created checkpoint archive: /tmp/checkpoint_spl0g9up.tar.gz (0 files, 0.0MB)
2026-01-18 00:31:27,006 - infrastructure.tracking.mlflow._artifacts_file - INFO - Uploading checkpoint archive (0.0MB)...
2026-01-18 00:31:27 INFO  [infrastructure.tracking.mlflow._artifacts_file] Uploading checkpoint archive (0.0MB)...
2026-01-18 00:31:27,006 - infrastructure.tracking.mlflow._artifacts_file - INFO - Successfully uploaded checkpoint archive: checkpoint_spl0g9up.tar.gz
2026-01-18 00:31:27 INFO  [infrastructure.tracking.mlflow._artifacts_file] Successfully uploaded checkpoint archive: checkpoint_spl0g9up.tar.gz
2026-01-18 00:31:27,006 - infrastructure.tracking.mlflow.artifacts.uploader - INFO - Successfully uploaded checkpoint archive: 0 files (0.0MB) for trial 0
2026-01-18 00:31:27 INFO  [infrastructure.tracking.mlflow.artifacts.uploader] Successfully uploaded checkpoint archive: 0 files (0.0MB) for trial 0
2026/01/18 00:31:27 INFO mlflow.tracking.fluent: Experiment with name 'test-experiment' does not exist. Creating a new experiment.
2026/01/18 00:31:27 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696287011, 1768696287011)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 350, in create_experiment
    eid = session.query(SqlExperiment).filter_by(name=name).first().experiment_id
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2759, in first
    return self.limit(1)._iter().first()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1233, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696287011, 1768696287011)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:27,012 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696287011, 1768696287011)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:27 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696287011, 1768696287011)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:27,013 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Continuing without MLflow tracking...
2026-01-18 00:31:27 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Continuing without MLflow tracking...
2026-01-18 00:31:27,014 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_conversion_log_onnx_model0/config/tags.yaml, using defaults
2026-01-18 00:31:27 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_conversion_log_onnx_model0/config/tags.yaml, using defaults
2026/01/18 00:31:27 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('bcfe9d1eac2e46919eae50ffb253ce64', 'amazing-seal-527', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696287019, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/bcfe9d1eac2e46919eae50ffb253ce64/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 586, in create_run
    inputs_list = self._get_run_inputs(session, [run_id])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 626, in _get_run_inputs
    ).all()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2704, in all
    return self._iter().all()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1048, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('bcfe9d1eac2e46919eae50ffb253ce64', 'amazing-seal-527', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696287019, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/bcfe9d1eac2e46919eae50ffb253ce64/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:27,023 - infrastructure.tracking.mlflow.trackers.conversion_tracker - WARNING - MLflow tracking failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('bcfe9d1eac2e46919eae50ffb253ce64', 'amazing-seal-527', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696287019, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/bcfe9d1eac2e46919eae50ffb253ce64/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:27 WARNI [infrastructure.tracking.mlflow.trackers.conversion_tracker] MLflow tracking failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('bcfe9d1eac2e46919eae50ffb253ce64', 'amazing-seal-527', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696287019, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/bcfe9d1eac2e46919eae50ffb253ce64/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:27,023 - infrastructure.tracking.mlflow.trackers.conversion_tracker - WARNING - Continuing conversion without MLflow tracking...
2026-01-18 00:31:27 WARNI [infrastructure.tracking.mlflow.trackers.conversion_tracker] Continuing conversion without MLflow tracking...
2026/01/18 00:31:27 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('10043a985b25463192071af7420ecf2b', 'legendary-smelt-734', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696287027, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/10043a985b25463192071af7420ecf2b/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 586, in create_run
    inputs_list = self._get_run_inputs(session, [run_id])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 626, in _get_run_inputs
    ).all()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2704, in all
    return self._iter().all()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1048, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('10043a985b25463192071af7420ecf2b', 'legendary-smelt-734', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696287027, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/10043a985b25463192071af7420ecf2b/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:27,031 - infrastructure.tracking.mlflow.trackers.conversion_tracker - WARNING - Could not log conversion results to MLflow: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('10043a985b25463192071af7420ecf2b', 'legendary-smelt-734', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696287027, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/10043a985b25463192071af7420ecf2b/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:27 WARNI [infrastructure.tracking.mlflow.trackers.conversion_tracker] Could not log conversion results to MLflow: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('10043a985b25463192071af7420ecf2b', 'legendary-smelt-734', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696287027, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/10043a985b25463192071af7420ecf2b/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026/01/18 00:31:27 INFO mlflow.tracking.fluent: Experiment with name 'test-experiment' does not exist. Creating a new experiment.
2026/01/18 00:31:27 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696287036, 1768696287036)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 350, in create_experiment
    eid = session.query(SqlExperiment).filter_by(name=name).first().experiment_id
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2759, in first
    return self.limit(1)._iter().first()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1233, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696287036, 1768696287036)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:27,037 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696287036, 1768696287036)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:27 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-experiment', None, 'active', 1768696287036, 1768696287036)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:27,037 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Continuing without MLflow tracking...
2026-01-18 00:31:27 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Continuing without MLflow tracking...
2026-01-18 00:31:27,039 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_conversion_log_conversion0/config/tags.yaml, using defaults
2026-01-18 00:31:27 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_conversion_log_conversion0/config/tags.yaml, using defaults
2026/01/18 00:31:27 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('7c9301c1ec09434582404fa0db6f41a2', 'enchanting-hen-606', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696287043, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/7c9301c1ec09434582404fa0db6f41a2/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 586, in create_run
    inputs_list = self._get_run_inputs(session, [run_id])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 626, in _get_run_inputs
    ).all()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2704, in all
    return self._iter().all()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1048, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('7c9301c1ec09434582404fa0db6f41a2', 'enchanting-hen-606', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696287043, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/7c9301c1ec09434582404fa0db6f41a2/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:27,047 - infrastructure.tracking.mlflow.trackers.conversion_tracker - WARNING - MLflow tracking failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('7c9301c1ec09434582404fa0db6f41a2', 'enchanting-hen-606', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696287043, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/7c9301c1ec09434582404fa0db6f41a2/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:27 WARNI [infrastructure.tracking.mlflow.trackers.conversion_tracker] MLflow tracking failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('7c9301c1ec09434582404fa0db6f41a2', 'enchanting-hen-606', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696287043, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/7c9301c1ec09434582404fa0db6f41a2/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:27,047 - infrastructure.tracking.mlflow.trackers.conversion_tracker - WARNING - Continuing conversion without MLflow tracking...
2026-01-18 00:31:27 WARNI [infrastructure.tracking.mlflow.trackers.conversion_tracker] Continuing conversion without MLflow tracking...
2026/01/18 00:31:27 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('a5ee516b412449fc970019a98f5426d4', 'abundant-jay-550', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696287051, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/a5ee516b412449fc970019a98f5426d4/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 586, in create_run
    inputs_list = self._get_run_inputs(session, [run_id])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 626, in _get_run_inputs
    ).all()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2704, in all
    return self._iter().all()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1048, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('a5ee516b412449fc970019a98f5426d4', 'abundant-jay-550', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696287051, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/a5ee516b412449fc970019a98f5426d4/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:27,057 - infrastructure.tracking.mlflow.trackers.conversion_tracker - WARNING - Could not log conversion results to MLflow: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('a5ee516b412449fc970019a98f5426d4', 'abundant-jay-550', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696287051, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/a5ee516b412449fc970019a98f5426d4/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:27 WARNI [infrastructure.tracking.mlflow.trackers.conversion_tracker] Could not log conversion results to MLflow: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('a5ee516b412449fc970019a98f5426d4', 'abundant-jay-550', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696287051, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/a5ee516b412449fc970019a98f5426d4/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:27 INFO  [tracking.mlflow.artifacts] Uploading checkpoint archive (0.0MB)...
2026-01-18 00:31:27 INFO  [tracking.mlflow.artifacts] Successfully uploaded checkpoint archive: tmpbkq42eg0.tar.gz
2026-01-18 00:31:27 INFO  [tracking.mlflow.artifacts] Uploading checkpoint archive (0.0MB)...
2026-01-18 00:31:27 INFO  [tracking.mlflow.artifacts] Successfully uploaded checkpoint archive: tmpqneq6alv.tar.gz
2026-01-18 00:31:27,075 - infrastructure.tracking.mlflow.lifecycle - INFO - Successfully terminated run refit-run-id... with status FINISHED
2026-01-18 00:31:27 INFO  [infrastructure.tracking.mlflow.lifecycle] Successfully terminated run refit-run-id... with status FINISHED
2026-01-18 00:31:27,078 - infrastructure.tracking.mlflow.lifecycle - INFO - Successfully terminated run refit-run-id... with status FAILED
2026-01-18 00:31:27 INFO  [infrastructure.tracking.mlflow.lifecycle] Successfully terminated run refit-run-id... with status FAILED
2026-01-18 00:31:27,081 - infrastructure.tracking.mlflow.lifecycle - INFO - Run refit-run-id... already has status FINISHED, skipping termination (expected RUNNING)
2026-01-18 00:31:27 INFO  [infrastructure.tracking.mlflow.lifecycle] Run refit-run-id... already has status FINISHED, skipping termination (expected RUNNING)
2026-01-18 00:31:27,112 - orchestration.jobs.tracking.config.loader - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-38/test_naming_run_name_config0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 00:31:27 INFO  [orchestration.jobs.tracking.config.loader] [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-38/test_naming_run_name_config0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 00:31:27,112 - orchestration.jobs.tracking.config.loader - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 00:31:27 INFO  [orchestration.jobs.tracking.config.loader] [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 00:31:27 WARNI [tracking.mlflow.artifacts] Failed to upload artifact tmpzq52peu3: Upload failed
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/artifacts.py", line 89, in log_artifact_safe
    retry_with_backoff(
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/utils.py", line 64, in retry_with_backoff
    return func()
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/artifacts.py", line 84, in upload_func
    mlflow.log_artifact(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: Upload failed
2026-01-18 00:31:27 INFO  [tracking.mlflow.artifacts] Uploading 2 files from /tmp/tmpybsuw6t_...
2026-01-18 00:31:27 INFO  [tracking.mlflow.artifacts] Successfully uploaded 2 files from tmpybsuw6t_ (run_id=active)
2026-01-18 00:31:27 INFO  [tracking.mlflow.artifacts] Uploading checkpoint archive (0.0MB)...
2026-01-18 00:31:27 INFO  [tracking.mlflow.artifacts] Successfully uploaded checkpoint archive: tmp6oc3bfcz.tar.gz
2026-01-18 00:31:27,154 - infrastructure.tracking.mlflow.lifecycle - INFO - Successfully terminated run test-run-id-... with status FINISHED
2026-01-18 00:31:27 INFO  [infrastructure.tracking.mlflow.lifecycle] Successfully terminated run test-run-id-... with status FINISHED
2026-01-18 00:31:27,158 - infrastructure.tracking.mlflow.lifecycle - INFO - Run test-run-id-... already has status FINISHED, skipping termination (expected RUNNING)
2026-01-18 00:31:27 INFO  [infrastructure.tracking.mlflow.lifecycle] Run test-run-id-... already has status FINISHED, skipping termination (expected RUNNING)
2026-01-18 00:31:27,166 - infrastructure.tracking.mlflow.lifecycle - INFO - Successfully terminated run test-run-id-... with status FINISHED
2026-01-18 00:31:27 INFO  [infrastructure.tracking.mlflow.lifecycle] Successfully terminated run test-run-id-... with status FINISHED
2026-01-18 00:31:27,179 - infrastructure.tracking.mlflow.lifecycle - INFO - Successfully terminated run test-run-id-... with status FINISHED
2026-01-18 00:31:27 INFO  [infrastructure.tracking.mlflow.lifecycle] Successfully terminated run test-run-id-... with status FINISHED
2026-01-18 00:31:27,182 - infrastructure.tracking.mlflow.lifecycle - INFO - Successfully terminated run test-run-id-... with status FINISHED
2026-01-18 00:31:27 INFO  [infrastructure.tracking.mlflow.lifecycle] Successfully terminated run test-run-id-... with status FINISHED
2026-01-18 00:31:27,197 - infrastructure.tracking.mlflow.runs - INFO - Created new experiment: new-experiment (exp-456)
2026-01-18 00:31:27 INFO  [infrastructure.tracking.mlflow.runs] Created new experiment: new-experiment (exp-456)
2026-01-18 00:31:27 WARNI [infrastructure.paths.resolve] Failed to load paths.yaml config: [Errno 13] Permission denied: '/root/config/paths.yaml'. Using fallback logic.
2026-01-18 00:31:27 WARNI [infrastructure.paths.resolve] Failed to load paths.yaml config: [Errno 13] Permission denied: '/root/config/paths.yaml'. Using fallback logic.
2026-01-18 00:31:27 WARNI [infrastructure.paths.resolve] Failed to load paths.yaml config: [Errno 13] Permission denied: '/root/config/paths.yaml'. Using fallback logic.
2026-01-18 00:31:27 WARNI [infrastructure.paths.resolve] Failed to load paths.yaml config: [Errno 13] Permission denied: '/root/config/paths.yaml'. Using fallback logic.
2026-01-18 00:31:27 WARNI [infrastructure.paths.resolve] Failed to load paths.yaml config: [Errno 13] Permission denied: '/root/config/paths.yaml'. Using fallback logic.
2026-01-18 00:31:27 WARNI [infrastructure.paths.resolve] Failed to load paths.yaml config: [Errno 13] Permission denied: '/root/config/paths.yaml'. Using fallback logic.
2026-01-18 00:31:27,237 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-38/test_hpo_trial_run_name0/config, raw_auto_inc_config={}
2026-01-18 00:31:27 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-38/test_hpo_trial_run_name0/config, raw_auto_inc_config={}
2026-01-18 00:31:27,237 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 00:31:27 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 00:31:27,241 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-38/test_hpo_trial_fold_run_name0/config, raw_auto_inc_config={}
2026-01-18 00:31:27 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-38/test_hpo_trial_fold_run_name0/config, raw_auto_inc_config={}
2026-01-18 00:31:27,241 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo_trial_fold
2026-01-18 00:31:27 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo_trial_fold
2026-01-18 00:31:27 WARNI [infrastructure.naming.display_policy] [Naming Policy] No pattern for process_type: hpo_sweep
2026-01-18 00:31:27,245 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-38/test_hpo_refit_run_name0/config, raw_auto_inc_config={}
2026-01-18 00:31:27 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-38/test_hpo_refit_run_name0/config, raw_auto_inc_config={}
2026-01-18 00:31:27,245 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 00:31:27 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 00:31:27,248 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-38/test_hpo_sweep_run_name0/config, raw_auto_inc_config={}
2026-01-18 00:31:27 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-38/test_hpo_sweep_run_name0/config, raw_auto_inc_config={}
2026-01-18 00:31:27,248 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 00:31:27 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 00:31:27 WARNI [infrastructure.naming.display_policy] [Naming Policy] Missing key in pattern substitution: 'study_hash'
2026-01-18 00:31:27,252 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-38/test_run_name_max_length0/config, raw_auto_inc_config={}
2026-01-18 00:31:27 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-38/test_run_name_max_length0/config, raw_auto_inc_config={}
2026-01-18 00:31:27,252 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 00:31:27 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 00:31:27 WARNI [infrastructure.naming.display_policy] [Naming Policy] Missing key in pattern substitution: 'study_hash'
2026-01-18 00:31:27,255 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-38/test_run_name_forbidden_chars_0/config, raw_auto_inc_config={}
2026-01-18 00:31:27 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-38/test_run_name_forbidden_chars_0/config, raw_auto_inc_config={}
2026-01-18 00:31:27,255 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 00:31:27 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 00:31:27,258 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 00:31:27 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 00:31:27,258 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=final_training
2026-01-18 00:31:27 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=final_training
2026-01-18 00:31:27 ERROR [infrastructure.naming.display_policy] [Naming Policy] Run name exceeds max length (256): aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa... (length: 300)
2026-01-18 00:31:27 ERROR [infrastructure.naming.display_policy] [Naming Policy] Run name contains forbidden characters ['/']: local/distilbert/hpo_trial...
2026-01-18 00:31:27 WARNI [infrastructure.naming.display_policy] [Naming Policy] Run name exceeds recommended length (150): aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa... (length: 200)
2026-01-18 00:31:27,332 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - Found best trial 0 (study_key_hash + trial_number + parentRunId): child-run-id...
2026-01-18 00:31:27 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] Found best trial 0 (study_key_hash + trial_number + parentRunId): child-run-id...
2026-01-18 00:31:27,342 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - Found best trial 0 (study_key_hash + trial_number + parentRunId): child-run-id...
2026-01-18 00:31:27 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] Found best trial 0 (study_key_hash + trial_number + parentRunId): child-run-id...
2026-01-18 00:31:27,351 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - Found best trial 0 (study_key_hash + trial_number + parentRunId): child-run-id...
2026-01-18 00:31:27 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] Found best trial 0 (study_key_hash + trial_number + parentRunId): child-run-id...
2026-01-18 00:31:27,353 - infrastructure.paths.repo - WARNING - Could not find repository root. Falling back to current working directory: /tmp/pytest-of-codespace/pytest-38/test_log_best_trial_id_falls_b0
2026-01-18 00:31:27 WARNI [infrastructure.paths.repo] Could not find repository root. Falling back to current working directory: /tmp/pytest-of-codespace/pytest-38/test_log_best_trial_id_falls_b0
2026-01-18 00:31:27,362 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=parent-run-i...
2026-01-18 00:31:27 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=parent-run-i...
2026-01-18 00:31:27,363 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Found 0 completed trials out of 1 total
2026-01-18 00:31:27 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Found 0 completed trials out of 1 total
2026-01-18 00:31:27,363 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging metrics: n_trials=1, n_completed_trials=0
2026-01-18 00:31:27 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Logging metrics: n_trials=1, n_completed_trials=0
2026-01-18 00:31:27,363 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=0.5
2026-01-18 00:31:27 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=0.5
2026-01-18 00:31:27,363 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging best hyperparameters: {'learning_rate': 0.0001}
2026-01-18 00:31:27 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Logging best hyperparameters: {'learning_rate': 0.0001}
2026-01-18 00:31:27,363 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Finding and logging best trial run ID...
2026-01-18 00:31:27 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Finding and logging best trial run ID...
2026-01-18 00:31:27,363 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - Found best trial 0 (study_key_hash + trial_number + parentRunId): child-run-id...
2026-01-18 00:31:27 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] Found best trial 0 (study_key_hash + trial_number + parentRunId): child-run-id...
2026-01-18 00:31:27,364 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Completed best trial ID logging
2026-01-18 00:31:27 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Completed best trial ID logging
2026-01-18 00:31:27,364 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Checkpoint logging enabled: None (upload_checkpoint=True)
2026-01-18 00:31:27 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Checkpoint logging enabled: None (upload_checkpoint=True)
2026-01-18 00:31:27,364 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
2026-01-18 00:31:27 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
2026-01-18 00:31:27,364 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Completed log_final_metrics successfully
2026-01-18 00:31:27 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Completed log_final_metrics successfully
2026-01-18 00:31:27,371 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=parent-run-i...
2026-01-18 00:31:27 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=parent-run-i...
2026-01-18 00:31:27,372 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Found 0 completed trials out of 1 total
2026-01-18 00:31:27 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Found 0 completed trials out of 1 total
2026-01-18 00:31:27,372 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging metrics: n_trials=1, n_completed_trials=0
2026-01-18 00:31:27 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Logging metrics: n_trials=1, n_completed_trials=0
2026-01-18 00:31:27,372 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=0.5
2026-01-18 00:31:27 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=0.5
2026-01-18 00:31:27,372 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging best hyperparameters: {'learning_rate': 0.0001}
2026-01-18 00:31:27 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Logging best hyperparameters: {'learning_rate': 0.0001}
2026-01-18 00:31:27,372 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Finding and logging best trial run ID...
2026-01-18 00:31:27 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Finding and logging best trial run ID...
2026-01-18 00:31:27,373 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - Found best trial 0 (study_key_hash + trial_number + parentRunId): child-run-id...
2026-01-18 00:31:27 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] Found best trial 0 (study_key_hash + trial_number + parentRunId): child-run-id...
2026/01/18 00:31:27 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('81d004895af1445498c04e53ed045423', 'serious-newt-16', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696287377, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/81d004895af1445498c04e53ed045423/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 586, in create_run
    inputs_list = self._get_run_inputs(session, [run_id])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 626, in _get_run_inputs
    ).all()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2704, in all
    return self._iter().all()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1048, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('81d004895af1445498c04e53ed045423', 'serious-newt-16', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696287377, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/81d004895af1445498c04e53ed045423/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:27,382 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Could not find MLflow run ID for best trial 0. Search failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('81d004895af1445498c04e53ed045423', 'serious-newt-16', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696287377, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/81d004895af1445498c04e53ed045423/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8). This may be a timing issue - trial runs may not be created/committed yet.
2026-01-18 00:31:27 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker] Could not find MLflow run ID for best trial 0. Search failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('81d004895af1445498c04e53ed045423', 'serious-newt-16', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696287377, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/81d004895af1445498c04e53ed045423/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8). This may be a timing issue - trial runs may not be created/committed yet.
2026-01-18 00:31:27,382 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Completed best trial ID logging
2026-01-18 00:31:27 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Completed best trial ID logging
2026-01-18 00:31:27,382 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Checkpoint logging enabled: None (upload_checkpoint=True)
2026-01-18 00:31:27 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Checkpoint logging enabled: None (upload_checkpoint=True)
2026-01-18 00:31:27,382 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
2026-01-18 00:31:27 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
2026-01-18 00:31:27,382 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Completed log_final_metrics successfully
2026-01-18 00:31:27 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Completed log_final_metrics successfully
2026-01-18 00:31:27,391 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - Found best trial 0 (study_key_hash + trial_number + parentRunId): child-run-id...
2026-01-18 00:31:27 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] Found best trial 0 (study_key_hash + trial_number + parentRunId): child-run-id...
2026/01/18 00:31:27 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('73420a438bb84ff1baee478b9735c666', 'nosy-mink-977', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696287396, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/73420a438bb84ff1baee478b9735c666/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 586, in create_run
    inputs_list = self._get_run_inputs(session, [run_id])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 626, in _get_run_inputs
    ).all()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2704, in all
    return self._iter().all()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1048, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('73420a438bb84ff1baee478b9735c666', 'nosy-mink-977', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696287396, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/73420a438bb84ff1baee478b9735c666/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:27,401 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Could not find MLflow run ID for best trial 0. Search failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('73420a438bb84ff1baee478b9735c666', 'nosy-mink-977', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696287396, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/73420a438bb84ff1baee478b9735c666/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8). This may be a timing issue - trial runs may not be created/committed yet.
2026-01-18 00:31:27 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker] Could not find MLflow run ID for best trial 0. Search failed: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO runs (run_uuid, name, source_type, source_name, entry_point_name, user_id, status, start_time, end_time, deleted_time, source_version, lifecycle_stage, artifact_uri, experiment_id) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]
[parameters: ('73420a438bb84ff1baee478b9735c666', 'nosy-mink-977', 'UNKNOWN', '', '', 'codespace', 'RUNNING', 1768696287396, None, None, '', 'active', '/workspaces/resume-ner-azureml/mlruns/1/73420a438bb84ff1baee478b9735c666/artifacts', '1')]
(Background on this error at: https://sqlalche.me/e/20/e3q8). This may be a timing issue - trial runs may not be created/committed yet.
2026-01-18 00:31:27,413 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_load_tags_registry_fallba0/config/tags.yaml, using defaults
2026-01-18 00:31:27 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_load_tags_registry_fallba0/config/tags.yaml, using defaults
2026-01-18 00:31:27,438 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_build_mlflow_tags_minimal0/config/tags.yaml, using defaults
2026-01-18 00:31:27 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_build_mlflow_tags_minimal0/config/tags.yaml, using defaults
2026-01-18 00:31:27,441 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_build_mlflow_tags_hpo_pro0/config/tags.yaml, using defaults
2026-01-18 00:31:27 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_build_mlflow_tags_hpo_pro0/config/tags.yaml, using defaults
2026-01-18 00:31:27,445 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_build_mlflow_tags_hpo_ref0/config/tags.yaml, using defaults
2026-01-18 00:31:27 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_build_mlflow_tags_hpo_ref0/config/tags.yaml, using defaults
2026-01-18 00:31:27,449 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_build_mlflow_tags_benchma0/config/tags.yaml, using defaults
2026-01-18 00:31:27 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_build_mlflow_tags_benchma0/config/tags.yaml, using defaults
2026-01-18 00:31:27,452 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_build_mlflow_tags_final_t0/config/tags.yaml, using defaults
2026-01-18 00:31:27 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_build_mlflow_tags_final_t0/config/tags.yaml, using defaults
2026-01-18 00:31:27,455 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_build_mlflow_tags_convers0/config/tags.yaml, using defaults
2026-01-18 00:31:27 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_build_mlflow_tags_convers0/config/tags.yaml, using defaults
2026-01-18 00:31:27,459 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_build_mlflow_tags_optiona0/config/tags.yaml, using defaults
2026-01-18 00:31:27 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_build_mlflow_tags_optiona0/config/tags.yaml, using defaults
2026-01-18 00:31:27,476 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_get_tag_key_falls_back_to0/config/tags.yaml, using defaults
2026-01-18 00:31:27 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_get_tag_key_falls_back_to0/config/tags.yaml, using defaults
2026-01-18 00:31:27,478 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_get_tag_key_raises_when_m0/config/tags.yaml, using defaults
2026-01-18 00:31:27 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_get_tag_key_raises_when_m0/config/tags.yaml, using defaults
2026-01-18 00:31:27,481 - infrastructure.naming.mlflow.tags_registry - WARNING - [Tags Registry] Failed to load config from /tmp/pytest-of-codespace/pytest-38/test_get_tag_key_handles_regis0/config/tags.yaml: mapping values are not allowed here
  in "/tmp/pytest-of-codespace/pytest-38/test_get_tag_key_handles_regis0/config/tags.yaml", line 1, column 14. Using defaults.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/naming/mlflow/tags_registry.py", line 215, in load_tags_registry
    loaded_data = load_yaml(config_path)
  File "/workspaces/resume-ner-azureml/src/common/shared/yaml_utils.py", line 42, in load_yaml
    return yaml.safe_load(handle)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/__init__.py", line 125, in safe_load
    return load(stream, SafeLoader)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/composer.py", line 36, in get_single_node
    document = self.compose_document()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/composer.py", line 55, in compose_document
    node = self.compose_node(None, None)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/composer.py", line 84, in compose_node
    node = self.compose_mapping_node(anchor)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/composer.py", line 127, in compose_mapping_node
    while not self.check_event(MappingEndEvent):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/parser.py", line 98, in check_event
    self.current_event = self.state()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/parser.py", line 428, in parse_block_mapping_key
    if self.check_token(KeyToken):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/scanner.py", line 116, in check_token
    self.fetch_more_tokens()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/scanner.py", line 223, in fetch_more_tokens
    return self.fetch_value()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/scanner.py", line 577, in fetch_value
    raise ScannerError(None, None,
yaml.scanner.ScannerError: mapping values are not allowed here
  in "/tmp/pytest-of-codespace/pytest-38/test_get_tag_key_handles_regis0/config/tags.yaml", line 1, column 14
2026-01-18 00:31:27 WARNI [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Failed to load config from /tmp/pytest-of-codespace/pytest-38/test_get_tag_key_handles_regis0/config/tags.yaml: mapping values are not allowed here
  in "/tmp/pytest-of-codespace/pytest-38/test_get_tag_key_handles_regis0/config/tags.yaml", line 1, column 14. Using defaults.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/naming/mlflow/tags_registry.py", line 215, in load_tags_registry
    loaded_data = load_yaml(config_path)
  File "/workspaces/resume-ner-azureml/src/common/shared/yaml_utils.py", line 42, in load_yaml
    return yaml.safe_load(handle)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/__init__.py", line 125, in safe_load
    return load(stream, SafeLoader)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/composer.py", line 36, in get_single_node
    document = self.compose_document()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/composer.py", line 55, in compose_document
    node = self.compose_node(None, None)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/composer.py", line 84, in compose_node
    node = self.compose_mapping_node(anchor)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/composer.py", line 127, in compose_mapping_node
    while not self.check_event(MappingEndEvent):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/parser.py", line 98, in check_event
    self.current_event = self.state()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/parser.py", line 428, in parse_block_mapping_key
    if self.check_token(KeyToken):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/scanner.py", line 116, in check_token
    self.fetch_more_tokens()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/scanner.py", line 223, in fetch_more_tokens
    return self.fetch_value()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/scanner.py", line 577, in fetch_value
    raise ScannerError(None, None,
yaml.scanner.ScannerError: mapping values are not allowed here
  in "/tmp/pytest-of-codespace/pytest-38/test_get_tag_key_handles_regis0/config/tags.yaml", line 1, column 14
2026-01-18 00:31:27,494 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_load_tags_registry_missin0/config/tags.yaml, using defaults
2026-01-18 00:31:27 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-38/test_load_tags_registry_missin0/config/tags.yaml, using defaults
2026-01-18 00:31:27,517 - training.hpo.execution.local.sweep - WARNING - Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
2026-01-18 00:31:27 WARNI [training.hpo.execution.local.sweep] Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
2026-01-18 00:31:27,517 - training.hpo.execution.local.sweep - WARNING - Could not set objective direction tag: 'Missing tag key: objective.direction'
2026-01-18 00:31:27 WARNI [training.hpo.execution.local.sweep] Could not set objective direction tag: 'Missing tag key: objective.direction'
2026-01-18 00:31:27,517 - training.hpo.execution.local.sweep - WARNING - Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
2026-01-18 00:31:27 WARNI [training.hpo.execution.local.sweep] Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
2026-01-18 00:31:27,517 - training.hpo.execution.local.sweep - WARNING - Could not set artifact.available tag: 'Missing tag key: artifact.available'
2026-01-18 00:31:27 WARNI [training.hpo.execution.local.sweep] Could not set artifact.available tag: 'Missing tag key: artifact.available'
2026-01-18 00:31:27,517 - training.hpo.execution.local.sweep - INFO -  Set Phase 2 tags on parent run parent-run-1...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-18 00:31:27 INFO  [training.hpo.execution.local.sweep]  Set Phase 2 tags on parent run parent-run-1...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-18 00:31:27 WARNI [infrastructure.naming.display_policy] [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-38/test_priority_1_use_provided_h0/workspace/config/naming.yaml, using empty policy
2026-01-18 00:31:27 WARNI [infrastructure.naming.display_policy] [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-38/test_priority_2_get_from_paren0/workspace/config/naming.yaml, using empty policy
2026-01-18 00:31:27 WARNI [infrastructure.naming.display_policy] [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-38/test_priority_3_compute_v2_fro0/workspace/config/naming.yaml, using empty policy
2026-01-18 00:31:27 WARNI [infrastructure.naming.display_policy] [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-38/test_hash_consistency_with_par0/workspace/config/naming.yaml, using empty policy
2026-01-18 00:31:27 WARNI [infrastructure.naming.display_policy] [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-38/test_missing_train_config_stil0/workspace/config/naming.yaml, using empty policy
2026-01-18 00:31:27 WARNI [infrastructure.naming.display_policy] [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-38/test_error_handling_parent_run0/workspace/config/naming.yaml, using empty policy
2026-01-18 00:31:27,582 - infrastructure.tracking.mlflow.hash_utils - WARNING - Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
2026-01-18 00:31:27 WARNI [infrastructure.tracking.mlflow.hash_utils] Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
2026-01-18 00:31:27,582 - training.hpo.execution.local.cv - WARNING - Could not compute trial_key_hash from configs
2026-01-18 00:31:27 WARNI [training.hpo.execution.local.cv] Could not compute trial_key_hash from configs
2026-01-18 00:31:27,594 - training.hpo.execution.local.sweep - INFO -  Set Phase 2 tags on parent run run123...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-18 00:31:27 INFO  [training.hpo.execution.local.sweep]  Set Phase 2 tags on parent run run123...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-18 00:31:27,597 - training.hpo.execution.local.sweep - WARNING - _set_phase2_hpo_tags: parent_run_id is empty, skipping
2026-01-18 00:31:27 WARNI [training.hpo.execution.local.sweep] _set_phase2_hpo_tags: parent_run_id is empty, skipping
2026-01-18 00:31:27,600 - training.hpo.execution.local.sweep - WARNING - _set_phase2_hpo_tags: parent_run_id is empty, skipping
2026-01-18 00:31:27 WARNI [training.hpo.execution.local.sweep] _set_phase2_hpo_tags: parent_run_id is empty, skipping
2026-01-18 00:31:27,607 - training.hpo.execution.local.sweep - INFO -  Set Phase 2 tags on parent run run123...: schema_version=1.0, data_fp=missing, eval_fp=set, objective_direction=<MagicMock name='mlflow.get().get()' id='140077220224432'>
2026-01-18 00:31:27 INFO  [training.hpo.execution.local.sweep]  Set Phase 2 tags on parent run run123...: schema_version=1.0, data_fp=missing, eval_fp=set, objective_direction=<MagicMock name='mlflow.get().get()' id='140077220224432'>
2026-01-18 00:31:27,612 - training.hpo.execution.local.sweep - INFO -  Set Phase 2 tags on parent run run123...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=minimize
2026-01-18 00:31:27 INFO  [training.hpo.execution.local.sweep]  Set Phase 2 tags on parent run run123...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=minimize
2026-01-18 00:31:27,618 - training.hpo.execution.local.sweep - INFO -  Set Phase 2 tags on parent run run123...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-18 00:31:27 INFO  [training.hpo.execution.local.sweep]  Set Phase 2 tags on parent run run123...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-18 00:31:27,625 - training.hpo.execution.local.sweep - INFO -  Set Phase 2 tags on parent run run123...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-18 00:31:27 INFO  [training.hpo.execution.local.sweep]  Set Phase 2 tags on parent run run123...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-18 00:31:27,630 - infrastructure.tracking.mlflow.hash_utils - WARNING - Could not compute v2 study_key_hash: Cannot build v2
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/hash_utils.py", line 297, in compute_study_key_hash_v2
    study_key_v2 = build_hpo_study_key_v2(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: Cannot build v2
2026-01-18 00:31:27 WARNI [infrastructure.tracking.mlflow.hash_utils] Could not compute v2 study_key_hash: Cannot build v2
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/hash_utils.py", line 297, in compute_study_key_hash_v2
    study_key_v2 = build_hpo_study_key_v2(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: Cannot build v2
2026-01-18 00:31:27,631 - training.hpo.execution.local.sweep - INFO -  Set Phase 2 tags on parent run run123...: schema_version=1.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-18 00:31:27 INFO  [training.hpo.execution.local.sweep]  Set Phase 2 tags on parent run run123...: schema_version=1.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-18 00:31:27,639 - evaluation.selection.trial_finder - WARNING - No v2 study folders found in /tmp/pytest-of-codespace/pytest-38/test_falls_back_to_resolve_sto0/outputs/hpo/colab/distilbert (resolved from /tmp/pytest-of-codespace/pytest-38/test_falls_back_to_resolve_sto0/outputs/hpo/colab/distilbert)
2026-01-18 00:31:27 WARNI [evaluation.selection.trial_finder] No v2 study folders found in /tmp/pytest-of-codespace/pytest-38/test_falls_back_to_resolve_sto0/outputs/hpo/colab/distilbert (resolved from /tmp/pytest-of-codespace/pytest-38/test_falls_back_to_resolve_sto0/outputs/hpo/colab/distilbert)
2026-01-18 00:31:27,642 - training.hpo.utils.helpers - INFO - Restored HPO checkpoint from Drive: /tmp/pytest-of-codespace/pytest-38/test_restore_from_drive_works_0/outputs/hpo/colab/distilbert/study-c3659fea/study.db
2026-01-18 00:31:27 INFO  [training.hpo.utils.helpers] Restored HPO checkpoint from Drive: /tmp/pytest-of-codespace/pytest-38/test_restore_from_drive_works_0/outputs/hpo/colab/distilbert/study-c3659fea/study.db
2026-01-18 00:31:37,945 - common.shared.mlflow_setup - INFO - Using local tracking: sqlite:///./mlruns/mlflow.db
2026-01-18 00:31:37 INFO  [common.shared.mlflow_setup] Using local tracking: sqlite:///./mlruns/mlflow.db
2026-01-18 00:31:37,949 - common.shared.mlflow_setup - INFO - Using Azure ML workspace tracking
2026-01-18 00:31:37 INFO  [common.shared.mlflow_setup] Using Azure ML workspace tracking
2026-01-18 00:31:37,952 - common.shared.mlflow_setup - WARNING - Azure ML tracking failed: Azure ML unavailable
2026-01-18 00:31:37 WARNI [common.shared.mlflow_setup] Azure ML tracking failed: Azure ML unavailable
2026-01-18 00:31:37,952 - common.shared.mlflow_setup - INFO - Falling back to local tracking...
2026-01-18 00:31:37 INFO  [common.shared.mlflow_setup] Falling back to local tracking...
2026-01-18 00:31:37,953 - common.shared.mlflow_setup - INFO - Using local tracking: sqlite:///./mlruns/mlflow.db
2026-01-18 00:31:37 INFO  [common.shared.mlflow_setup] Using local tracking: sqlite:///./mlruns/mlflow.db
2026-01-18 00:31:37,959 - common.shared.mlflow_setup - INFO - Using local tracking: sqlite:////content/drive/MyDrive/resume-ner-mlflow/mlflow.db
2026-01-18 00:31:37 INFO  [common.shared.mlflow_setup] Using local tracking: sqlite:////content/drive/MyDrive/resume-ner-mlflow/mlflow.db
2026-01-18 00:31:37,964 - common.shared.mlflow_setup - INFO - Using local tracking: sqlite:////kaggle/working/mlflow.db
2026-01-18 00:31:37 INFO  [common.shared.mlflow_setup] Using local tracking: sqlite:////kaggle/working/mlflow.db
2026-01-18 00:31:37,968 - common.shared.mlflow_setup - INFO - Using local tracking: sqlite:////workspaces/resume-ner-azureml/mlruns/mlflow.db
2026-01-18 00:31:37 INFO  [common.shared.mlflow_setup] Using local tracking: sqlite:////workspaces/resume-ner-azureml/mlruns/mlflow.db
2026-01-18 00:31:37,971 - common.shared.mlflow_setup - WARNING - [DEBUG] Initial env check - subscription_id: True, resource_group: True, client_id: False, client_secret: False, tenant_id: False
2026-01-18 00:31:37 WARNI [common.shared.mlflow_setup] [DEBUG] Initial env check - subscription_id: True, resource_group: True, client_id: False, client_secret: False, tenant_id: False
2026-01-18 00:31:37,971 - common.shared.mlflow_setup - INFO - Attempting to load credentials from config.env at: config.env
2026-01-18 00:31:37 INFO  [common.shared.mlflow_setup] Attempting to load credentials from config.env at: config.env
2026-01-18 00:31:37,972 - common.shared.mlflow_setup - WARNING - config.env not found at config.env. Looking for: /workspaces/resume-ner-azureml/config.env
2026-01-18 00:31:37 WARNI [common.shared.mlflow_setup] config.env not found at config.env. Looking for: /workspaces/resume-ner-azureml/config.env
2026-01-18 00:31:37,972 - common.shared.mlflow_setup - WARNING - [DEBUG] Platform detected: local
2026-01-18 00:31:37 WARNI [common.shared.mlflow_setup] [DEBUG] Platform detected: local
2026-01-18 00:31:37,972 - common.shared.mlflow_setup - WARNING - [DEBUG] Service Principal check - client_id present: False, client_secret present: False, tenant_id present: False, has_service_principal: False
2026-01-18 00:31:37 WARNI [common.shared.mlflow_setup] [DEBUG] Service Principal check - client_id present: False, client_secret present: False, tenant_id present: False, has_service_principal: False
2026-01-18 00:31:37,972 - common.shared.mlflow_setup - INFO - Using DefaultAzureCredential (trying multiple auth methods)
2026-01-18 00:31:37 INFO  [common.shared.mlflow_setup] Using DefaultAzureCredential (trying multiple auth methods)
2026-01-18 00:31:37,972 - common.shared.mlflow_setup - INFO - Successfully connected to Azure ML workspace: test-ws
2026-01-18 00:31:37 INFO  [common.shared.mlflow_setup] Successfully connected to Azure ML workspace: test-ws
2026-01-18 00:31:37,980 - common.shared.mlflow_setup - WARNING - azure-ai-ml and azure-identity are required for Azure ML tracking. Install with: pip install azure-ai-ml azure-identity. Falling back to local tracking.
2026-01-18 00:31:37 WARNI [common.shared.mlflow_setup] azure-ai-ml and azure-identity are required for Azure ML tracking. Install with: pip install azure-ai-ml azure-identity. Falling back to local tracking.
2026-01-18 00:31:37,982 - common.shared.mlflow_setup - INFO - Azure ML enabled in config, attempting to connect...
2026-01-18 00:31:37 INFO  [common.shared.mlflow_setup] Azure ML enabled in config, attempting to connect...
2026-01-18 00:31:37,984 - common.shared.mlflow_setup - INFO - Azure ML disabled in config, using local tracking
2026-01-18 00:31:37 INFO  [common.shared.mlflow_setup] Azure ML disabled in config, using local tracking
2026-01-18 00:31:37,986 - common.shared.mlflow_setup - WARNING - MLflow config not found, using local tracking
2026-01-18 00:31:37 WARNI [common.shared.mlflow_setup] MLflow config not found, using local tracking
  [Training]  No active MLflow run and no MLFLOW_RUN_ID - cannot log artifacts
  [Training]  No active MLflow run and no MLFLOW_RUN_ID - cannot log artifacts
  [Training]  No active MLflow run and no MLFLOW_RUN_ID - cannot log artifacts
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2026/01/18 00:31:39 INFO mlflow.tracking.fluent: Experiment with name 'resume_ner_baseline-training' does not exist. Creating a new experiment.
2026-01-18 00:31:39,428 - training.hpo.execution.local.sweep - INFO - [EARLY HASH] Computed v2 study_key_hash=79a646c77bb15106... for folder creation
2026-01-18 00:31:39 INFO  [training.hpo.execution.local.sweep] [EARLY HASH] Computed v2 study_key_hash=79a646c77bb15106... for folder creation
2026-01-18 00:31:39,429 - training.hpo.core.study - INFO - [HPO] Starting optimization for distilroberta with checkpointing...
2026-01-18 00:31:39 INFO  [training.hpo.core.study] [HPO] Starting optimization for distilroberta with checkpointing...
2026-01-18 00:31:39,429 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 00:31:39 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 00:31:39,429 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 00:31:39 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 00:31:39,429 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:7202413b67e3126708d0544fb08f79f5b50640effd450..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-18 00:31:39 INFO  [orchestration.jobs.tracking.index.version_counter] [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:7202413b67e3126708d0544fb08f79f5b50640effd450..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-18 00:31:39,431 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Loaded 25 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-18 00:31:39 INFO  [orchestration.jobs.tracking.index.version_counter] [Reserve Version] Loaded 25 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-18 00:31:39,431 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0
2026-01-18 00:31:39 INFO  [orchestration.jobs.tracking.index.version_counter] [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0
2026-01-18 00:31:39,431 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)
2026-01-18 00:31:39 INFO  [orchestration.jobs.tracking.index.version_counter] [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)
2026-01-18 00:31:39,432 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version]  Successfully reserved version 1 for counter_key resume-ner:hpo:7202413b67e3126708d0544fb08f79f5b50... (run_id: pending_2026...)
2026-01-18 00:31:39 INFO  [orchestration.jobs.tracking.index.version_counter] [Reserve Version]  Successfully reserved version 1 for counter_key resume-ner:hpo:7202413b67e3126708d0544fb08f79f5b50... (run_id: pending_2026...)
2026-01-18 00:31:39,432 - training.hpo.execution.local.sweep - INFO - [HPO Setup] k_folds=5, fold_splits_file=None, k_fold config: {'enabled': True, 'n_splits': 5, 'random_seed': 42, 'shuffle': True, 'stratified': True}
2026-01-18 00:31:39 INFO  [training.hpo.execution.local.sweep] [HPO Setup] k_folds=5, fold_splits_file=None, k_fold config: {'enabled': True, 'n_splits': 5, 'random_seed': 42, 'shuffle': True, 'stratified': True}
2026-01-18 00:31:39,434 - training.hpo.execution.local.sweep - INFO - [CV Setup]  Created 5 fold splits and saved to /workspaces/resume-ner-azureml/outputs/hpo/local/distilroberta/study-79a646c7/fold_splits.json
2026-01-18 00:31:39 INFO  [training.hpo.execution.local.sweep] [CV Setup]  Created 5 fold splits and saved to /workspaces/resume-ner-azureml/outputs/hpo/local/distilroberta/study-79a646c7/fold_splits.json
2026/01/18 00:31:39 INFO mlflow.tracking.fluent: Experiment with name 'resume_ner_baseline-hpo-distilroberta' does not exist. Creating a new experiment.
2026/01/18 00:31:39 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('resume_ner_baseline-hpo-distilroberta', None, 'active', 1768696299439, 1768696299439)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 350, in create_experiment
    eid = session.query(SqlExperiment).filter_by(name=name).first().experiment_id
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2759, in first
    return self.limit(1)._iter().first()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1233, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('resume_ner_baseline-hpo-distilroberta', None, 'active', 1768696299439, 1768696299439)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:39,441 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('resume_ner_baseline-hpo-distilroberta', None, 'active', 1768696299439, 1768696299439)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:39 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('resume_ner_baseline-hpo-distilroberta', None, 'active', 1768696299439, 1768696299439)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:39,441 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Continuing without MLflow tracking...
2026-01-18 00:31:39 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Continuing without MLflow tracking...
2026-01-18 00:31:39,441 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Using LOCAL MLflow tracking (not Azure ML)
2026-01-18 00:31:39 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker] Using LOCAL MLflow tracking (not Azure ML)
2026-01-18 00:31:39,441 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
2026-01-18 00:31:39 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
2026-01-18 00:31:39,448 - infrastructure.tracking.mlflow.trackers.sweep_tracker - ERROR - [START_SWEEP_RUN] MLflow tracking failed: No Experiment with id=546145851228734692 exists
2026-01-18 00:31:39 ERROR [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] MLflow tracking failed: No Experiment with id=546145851228734692 exists
2026-01-18 00:31:39,448 - infrastructure.tracking.mlflow.trackers.sweep_tracker - ERROR - [START_SWEEP_RUN] Traceback: Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/trackers/sweep_tracker.py", line 120, in start_sweep_run
    with mlflow.start_run(run_name=run_name) as parent_run:
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/fluent.py", line 475, in start_run
    active_run_obj = client.create_run(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 479, in create_run
    return self._tracking_client.create_run(experiment_id, start_time, tags, run_name)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/telemetry/track.py", line 30, in wrapper
    result = func(*args, **kwargs)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 183, in create_run
    return self.store.create_run(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 541, in create_run
    experiment = self.get_experiment(experiment_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 462, in get_experiment
    return self._get_experiment(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 440, in _get_experiment
    raise MlflowException(
mlflow.exceptions.MlflowException: No Experiment with id=546145851228734692 exists

2026-01-18 00:31:39 ERROR [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Traceback: Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/trackers/sweep_tracker.py", line 120, in start_sweep_run
    with mlflow.start_run(run_name=run_name) as parent_run:
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/fluent.py", line 475, in start_run
    active_run_obj = client.create_run(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 479, in create_run
    return self._tracking_client.create_run(experiment_id, start_time, tags, run_name)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/telemetry/track.py", line 30, in wrapper
    result = func(*args, **kwargs)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 183, in create_run
    return self.store.create_run(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 541, in create_run
    experiment = self.get_experiment(experiment_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 462, in get_experiment
    return self._get_experiment(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 440, in _get_experiment
    raise MlflowException(
mlflow.exceptions.MlflowException: No Experiment with id=546145851228734692 exists

2026-01-18 00:31:39,448 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Continuing HPO without MLflow tracking...
2026-01-18 00:31:39 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker] Continuing HPO without MLflow tracking...
2026-01-18 00:31:39,449 - training.hpo.execution.local.sweep - INFO - Setting Phase 2 tags on parent run None... (data_config=present, train_config=present)
2026-01-18 00:31:39 INFO  [training.hpo.execution.local.sweep] Setting Phase 2 tags on parent run None... (data_config=present, train_config=present)
2026-01-18 00:31:39,449 - training.hpo.execution.local.sweep - WARNING - _set_phase2_hpo_tags: parent_run_id is empty, skipping
2026-01-18 00:31:39 WARNI [training.hpo.execution.local.sweep] _set_phase2_hpo_tags: parent_run_id is empty, skipping
2026-01-18 00:31:39,449 - training.hpo.execution.local.sweep - INFO -  Successfully completed Phase 2 tag setting for parent run None...
2026-01-18 00:31:39 INFO  [training.hpo.execution.local.sweep]  Successfully completed Phase 2 tag setting for parent run None...
2026-01-18 00:31:39,449 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.
2026-01-18 00:31:39 INFO  [training.hpo.tracking.cleanup] [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.
2026-01-18 00:31:39,449 - training.hpo.execution.local.sweep - INFO - [REFIT] Starting refit training for best trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>
2026-01-18 00:31:39 INFO  [training.hpo.execution.local.sweep] [REFIT] Starting refit training for best trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>
2026-01-18 00:31:39,450 - training.hpo.execution.local.sweep - WARNING - [REFIT] Could not determine trial_key_hash from best trial run tags. Refit run may not match trial run.
2026-01-18 00:31:39 WARNI [training.hpo.execution.local.sweep] [REFIT] Could not determine trial_key_hash from best trial run tags. Refit run may not match trial run.
2026-01-18 00:31:39,450 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'> with hyperparameters: <MagicMock name='mock.create_study().best_trial.params' id='140077206865856'>
2026-01-18 00:31:39 INFO  [training.hpo.execution.local.refit] [REFIT] Starting refit training for trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'> with hyperparameters: <MagicMock name='mock.create_study().best_trial.params' id='140077206865856'>
2026-01-18 00:31:39,450 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id="trial_<MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>_20260118_003139", run_id='20260118_003139', trial_number=<MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>
2026-01-18 00:31:39 INFO  [training.hpo.execution.local.refit] [REFIT] Computed trial_id="trial_<MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>_20260118_003139", run_id='20260118_003139', trial_number=<MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>
2026-01-18 00:31:39,450 - infrastructure.tracking.mlflow.hash_utils - WARNING - Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
2026-01-18 00:31:39 WARNI [infrastructure.tracking.mlflow.hash_utils] Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
2026-01-18 00:31:39,450 - training.hpo.execution.local.sweep - WARNING - [REFIT] Refit training failed: Cannot create refit in v2 study folder study-79a646c7 without trial_key_hash. study_key_hash=present, trial_key_hash=missing, refit_params=missing. Hash computation from trial parameters may have failed.. Continuing without refit checkpoint.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py", line 1244, in run_local_hpo_sweep
    refit_metrics, refit_checkpoint_dir, refit_run_id = run_refit_training(
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 240, in run_refit_training
    raise RuntimeError(
RuntimeError: Cannot create refit in v2 study folder study-79a646c7 without trial_key_hash. study_key_hash=present, trial_key_hash=missing, refit_params=missing. Hash computation from trial parameters may have failed.
2026-01-18 00:31:39 WARNI [training.hpo.execution.local.sweep] [REFIT] Refit training failed: Cannot create refit in v2 study folder study-79a646c7 without trial_key_hash. study_key_hash=present, trial_key_hash=missing, refit_params=missing. Hash computation from trial parameters may have failed.. Continuing without refit checkpoint.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py", line 1244, in run_local_hpo_sweep
    refit_metrics, refit_checkpoint_dir, refit_run_id = run_refit_training(
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 240, in run_refit_training
    raise RuntimeError(
RuntimeError: Cannot create refit in v2 study folder study-79a646c7 without trial_key_hash. study_key_hash=present, trial_key_hash=missing, refit_params=missing. Hash computation from trial parameters may have failed.
2026-01-18 00:31:39,451 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /workspaces/resume-ner-azureml/outputs/hpo/config/tags.yaml, using defaults
2026-01-18 00:31:39 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /workspaces/resume-ner-azureml/outputs/hpo/config/tags.yaml, using defaults
2026-01-18 00:31:39,451 - training.hpo.execution.local.sweep - WARNING - [REFIT] No refit_run_id available to mark as FINISHED
2026-01-18 00:31:39 WARNI [training.hpo.execution.local.sweep] [REFIT] No refit_run_id available to mark as FINISHED
2026/01/18 00:31:39 INFO mlflow.tracking.fluent: Experiment with name 'test-experiment' does not exist. Creating a new experiment.
2026-01-18 00:31:39,530 - training.hpo.execution.local.sweep - INFO - [EARLY HASH] Computed v2 study_key_hash=c3659feaead8e1ec... for folder creation
2026-01-18 00:31:39 INFO  [training.hpo.execution.local.sweep] [EARLY HASH] Computed v2 study_key_hash=c3659feaead8e1ec... for folder creation
2026-01-18 00:31:39,531 - training.hpo.core.study - INFO - [HPO] Starting optimization for distilbert with checkpointing...
2026-01-18 00:31:39 INFO  [training.hpo.core.study] [HPO] Starting optimization for distilbert with checkpointing...
2026-01-18 00:31:39,532 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 00:31:39 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 00:31:39,532 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 00:31:39 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-18 00:31:39,532 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:3639b270abbc20a6f2b828357499e2f4d7eaa262dcff6..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-18 00:31:39 INFO  [orchestration.jobs.tracking.index.version_counter] [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:3639b270abbc20a6f2b828357499e2f4d7eaa262dcff6..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-18 00:31:39,532 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Loaded 26 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-18 00:31:39 INFO  [orchestration.jobs.tracking.index.version_counter] [Reserve Version] Loaded 26 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-18 00:31:39,532 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0
2026-01-18 00:31:39 INFO  [orchestration.jobs.tracking.index.version_counter] [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0
2026-01-18 00:31:39,533 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)
2026-01-18 00:31:39 INFO  [orchestration.jobs.tracking.index.version_counter] [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)
2026-01-18 00:31:39,533 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version]  Successfully reserved version 1 for counter_key resume-ner:hpo:3639b270abbc20a6f2b828357499e2f4d7e... (run_id: pending_2026...)
2026-01-18 00:31:39 INFO  [orchestration.jobs.tracking.index.version_counter] [Reserve Version]  Successfully reserved version 1 for counter_key resume-ner:hpo:3639b270abbc20a6f2b828357499e2f4d7e... (run_id: pending_2026...)
2026-01-18 00:31:39,534 - training.hpo.execution.local.sweep - INFO - [HPO Setup] k_folds=2, fold_splits_file=None, k_fold config: {'enabled': True, 'n_splits': 2, 'random_seed': 42, 'shuffle': True, 'stratified': True}
2026-01-18 00:31:39 INFO  [training.hpo.execution.local.sweep] [HPO Setup] k_folds=2, fold_splits_file=None, k_fold config: {'enabled': True, 'n_splits': 2, 'random_seed': 42, 'shuffle': True, 'stratified': True}
2026-01-18 00:31:39,535 - training.hpo.execution.local.sweep - INFO - [CV Setup]  Created 2 fold splits and saved to /workspaces/resume-ner-azureml/outputs/hpo/local/distilbert/study-c3659fea/fold_splits.json
2026-01-18 00:31:39 INFO  [training.hpo.execution.local.sweep] [CV Setup]  Created 2 fold splits and saved to /workspaces/resume-ner-azureml/outputs/hpo/local/distilbert/study-c3659fea/fold_splits.json
2026/01/18 00:31:39 INFO mlflow.tracking.fluent: Experiment with name 'resume_ner_baseline-hpo-distilbert' does not exist. Creating a new experiment.
2026/01/18 00:31:39 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('resume_ner_baseline-hpo-distilbert', None, 'active', 1768696299539, 1768696299539)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 350, in create_experiment
    eid = session.query(SqlExperiment).filter_by(name=name).first().experiment_id
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2759, in first
    return self.limit(1)._iter().first()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1233, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('resume_ner_baseline-hpo-distilbert', None, 'active', 1768696299539, 1768696299539)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:39,541 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('resume_ner_baseline-hpo-distilbert', None, 'active', 1768696299539, 1768696299539)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:39 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('resume_ner_baseline-hpo-distilbert', None, 'active', 1768696299539, 1768696299539)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:39,541 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Continuing without MLflow tracking...
2026-01-18 00:31:39 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Continuing without MLflow tracking...
2026-01-18 00:31:39,541 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Using LOCAL MLflow tracking (not Azure ML)
2026-01-18 00:31:39 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker] Using LOCAL MLflow tracking (not Azure ML)
2026-01-18 00:31:39,541 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
2026-01-18 00:31:39 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
2026-01-18 00:31:39,551 - infrastructure.tracking.mlflow.trackers.sweep_tracker - ERROR - [START_SWEEP_RUN] MLflow tracking failed: No Experiment with id=679667700195200030 exists
2026-01-18 00:31:39 ERROR [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] MLflow tracking failed: No Experiment with id=679667700195200030 exists
2026-01-18 00:31:39,551 - infrastructure.tracking.mlflow.trackers.sweep_tracker - ERROR - [START_SWEEP_RUN] Traceback: Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/trackers/sweep_tracker.py", line 120, in start_sweep_run
    with mlflow.start_run(run_name=run_name) as parent_run:
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/fluent.py", line 475, in start_run
    active_run_obj = client.create_run(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 479, in create_run
    return self._tracking_client.create_run(experiment_id, start_time, tags, run_name)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/telemetry/track.py", line 30, in wrapper
    result = func(*args, **kwargs)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 183, in create_run
    return self.store.create_run(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 541, in create_run
    experiment = self.get_experiment(experiment_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 462, in get_experiment
    return self._get_experiment(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 440, in _get_experiment
    raise MlflowException(
mlflow.exceptions.MlflowException: No Experiment with id=679667700195200030 exists

2026-01-18 00:31:39 ERROR [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Traceback: Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/trackers/sweep_tracker.py", line 120, in start_sweep_run
    with mlflow.start_run(run_name=run_name) as parent_run:
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/fluent.py", line 475, in start_run
    active_run_obj = client.create_run(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 479, in create_run
    return self._tracking_client.create_run(experiment_id, start_time, tags, run_name)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/telemetry/track.py", line 30, in wrapper
    result = func(*args, **kwargs)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 183, in create_run
    return self.store.create_run(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 541, in create_run
    experiment = self.get_experiment(experiment_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 462, in get_experiment
    return self._get_experiment(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 440, in _get_experiment
    raise MlflowException(
mlflow.exceptions.MlflowException: No Experiment with id=679667700195200030 exists

2026-01-18 00:31:39,551 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Continuing HPO without MLflow tracking...
2026-01-18 00:31:39 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker] Continuing HPO without MLflow tracking...
2026-01-18 00:31:39,552 - training.hpo.execution.local.sweep - INFO - Setting Phase 2 tags on parent run None... (data_config=present, train_config=present)
2026-01-18 00:31:39 INFO  [training.hpo.execution.local.sweep] Setting Phase 2 tags on parent run None... (data_config=present, train_config=present)
2026-01-18 00:31:39,552 - training.hpo.execution.local.sweep - WARNING - _set_phase2_hpo_tags: parent_run_id is empty, skipping
2026-01-18 00:31:39 WARNI [training.hpo.execution.local.sweep] _set_phase2_hpo_tags: parent_run_id is empty, skipping
2026-01-18 00:31:39,552 - training.hpo.execution.local.sweep - INFO -  Successfully completed Phase 2 tag setting for parent run None...
2026-01-18 00:31:39 INFO  [training.hpo.execution.local.sweep]  Successfully completed Phase 2 tag setting for parent run None...
2026-01-18 00:31:39,553 - training.hpo.execution.local.sweep - INFO - [REFIT] Starting refit training for best trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>
2026-01-18 00:31:39 INFO  [training.hpo.execution.local.sweep] [REFIT] Starting refit training for best trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>
2026-01-18 00:31:39,554 - training.hpo.execution.local.sweep - WARNING - [REFIT] Could not determine trial_key_hash from best trial run tags. Refit run may not match trial run.
2026-01-18 00:31:39 WARNI [training.hpo.execution.local.sweep] [REFIT] Could not determine trial_key_hash from best trial run tags. Refit run may not match trial run.
2026-01-18 00:31:39,554 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'> with hyperparameters: <MagicMock name='mock.create_study().best_trial.params' id='140077206865856'>
2026-01-18 00:31:39 INFO  [training.hpo.execution.local.refit] [REFIT] Starting refit training for trial <MagicMock name='mock.create_study().best_trial.number' id='140077206848368'> with hyperparameters: <MagicMock name='mock.create_study().best_trial.params' id='140077206865856'>
2026-01-18 00:31:39,554 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id="trial_<MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>_20260118_003139", run_id='20260118_003139', trial_number=<MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>
2026-01-18 00:31:39 INFO  [training.hpo.execution.local.refit] [REFIT] Computed trial_id="trial_<MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>_20260118_003139", run_id='20260118_003139', trial_number=<MagicMock name='mock.create_study().best_trial.number' id='140077206848368'>
2026-01-18 00:31:39,555 - infrastructure.tracking.mlflow.hash_utils - WARNING - Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
2026-01-18 00:31:39 WARNI [infrastructure.tracking.mlflow.hash_utils] Could not get or compute trial_key_hash. trial_key_hash=missing, trial_run_id=missing, study_key_hash=available, hyperparameters=missing
2026-01-18 00:31:39,555 - training.hpo.execution.local.sweep - WARNING - [REFIT] Refit training failed: Cannot create refit in v2 study folder study-c3659fea without trial_key_hash. study_key_hash=present, trial_key_hash=missing, refit_params=missing. Hash computation from trial parameters may have failed.. Continuing without refit checkpoint.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py", line 1244, in run_local_hpo_sweep
    refit_metrics, refit_checkpoint_dir, refit_run_id = run_refit_training(
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 240, in run_refit_training
    raise RuntimeError(
RuntimeError: Cannot create refit in v2 study folder study-c3659fea without trial_key_hash. study_key_hash=present, trial_key_hash=missing, refit_params=missing. Hash computation from trial parameters may have failed.
2026-01-18 00:31:39 WARNI [training.hpo.execution.local.sweep] [REFIT] Refit training failed: Cannot create refit in v2 study folder study-c3659fea without trial_key_hash. study_key_hash=present, trial_key_hash=missing, refit_params=missing. Hash computation from trial parameters may have failed.. Continuing without refit checkpoint.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py", line 1244, in run_local_hpo_sweep
    refit_metrics, refit_checkpoint_dir, refit_run_id = run_refit_training(
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 240, in run_refit_training
    raise RuntimeError(
RuntimeError: Cannot create refit in v2 study folder study-c3659fea without trial_key_hash. study_key_hash=present, trial_key_hash=missing, refit_params=missing. Hash computation from trial parameters may have failed.
2026-01-18 00:31:39,555 - training.hpo.execution.local.sweep - WARNING - [REFIT] No refit_run_id available to mark as FINISHED
2026-01-18 00:31:39 WARNI [training.hpo.execution.local.sweep] [REFIT] No refit_run_id available to mark as FINISHED
2026/01/18 00:31:39 INFO mlflow.tracking.fluent: Experiment with name 'test-benchmark-experiment' does not exist. Creating a new experiment.
2026/01/18 00:31:39 ERROR mlflow.store.db.utils: SQLAlchemy database error. The following exception is caught.
(raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-benchmark-experiment', None, 'active', 1768696299593, 1768696299593)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: attempt to write a readonly database

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/db/utils.py", line 160, in make_managed_session
    yield session
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 350, in create_experiment
    eid = session.query(SqlExperiment).filter_by(name=name).first().experiment_id
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2759, in first
    return self.limit(1)._iter().first()  # type: ignore
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2857, in _iter
    result: Union[ScalarResult[_T], Result[_T]] = self.session.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2351, in execute
    return self._execute_internal(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2228, in _execute_internal
    ) = compile_state_cls.orm_pre_session_exec(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/context.py", line 577, in orm_pre_session_exec
    session._autoflush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3051, in _autoflush
    raise e.with_traceback(sys.exc_info()[2])
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3040, in _autoflush
    self.flush()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4331, in flush
    self._flush(objects)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4466, in _flush
    with util.safe_reraise():
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 224, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 4427, in _flush
    flush_context.execute()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 466, in execute
    rec.execute(self)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/unitofwork.py", line 642, in execute
    util.preloaded.orm_persistence.save_obj(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 93, in save_obj
    _emit_insert_statements(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/orm/persistence.py", line 1233, in _emit_insert_statements
    result = connection.execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1419, in execute
    return meth(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/sql/elements.py", line 527, in _execute_on_connection
    return connection._execute_clauseelement(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1641, in _execute_clauseelement
    ret = self._execute_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1846, in _execute_context
    return self._exec_single_context(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1986, in _exec_single_context
    self._handle_dbapi_exception(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2363, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 1967, in _exec_single_context
    self.dialect.do_execute(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 952, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-benchmark-experiment', None, 'active', 1768696299593, 1768696299593)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:39,595 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-benchmark-experiment', None, 'active', 1768696299593, 1768696299593)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:39 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Could not set MLflow experiment: (raised as a result of Query-invoked autoflush; consider using a session.no_autoflush block if this flush is occurring prematurely)
(sqlite3.OperationalError) attempt to write a readonly database
[SQL: INSERT INTO experiments (name, artifact_location, lifecycle_stage, creation_time, last_update_time) VALUES (?, ?, ?, ?, ?)]
[parameters: ('test-benchmark-experiment', None, 'active', 1768696299593, 1768696299593)]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2026-01-18 00:31:39,595 - infrastructure.tracking.mlflow.trackers.base_tracker - WARNING - Continuing without MLflow tracking...
2026-01-18 00:31:39 WARNI [infrastructure.tracking.mlflow.trackers.base_tracker] Continuing without MLflow tracking...
2026-01-18 00:31:39,682 - evaluation.benchmarking.orchestrator - INFO - Benchmarking distilbert (d7c6e3fbd7c6e3fb)...
2026-01-18 00:31:39 INFO  [evaluation.benchmarking.orchestrator] Benchmarking distilbert (d7c6e3fbd7c6e3fb)...
2026-01-18 00:31:39,682 - evaluation.benchmarking.orchestrator - INFO - [BENCHMARK] Final run IDs: trial=None..., refit=trial-run-12..., sweep=None...
2026-01-18 00:31:39 INFO  [evaluation.benchmarking.orchestrator] [BENCHMARK] Final run IDs: trial=None..., refit=trial-run-12..., sweep=None...
2026-01-18 00:31:39,682 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /workspaces/resume-ner-azureml/src/evaluation/benchmarking/cli.py --checkpoint /workspaces/resume-ner-azureml/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-d7c6e3fb/refit/checkpoint --test-data /tmp/pytest-of-codespace/pytest-38/test_benchmarking_execution_mo0/dataset_tiny/seed0/test.json --batch-sizes 1 --iterations 10 --warmup 10 --max-length 512 --output /workspaces/resume-ner-azureml/outputs/benchmarking/local/distilbert/study-df9d920c/trial-d7c6e3fb/bench-fcfe0227/benchmark.json
2026-01-18 00:31:39 INFO  [evaluation.benchmarking.utils] Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /workspaces/resume-ner-azureml/src/evaluation/benchmarking/cli.py --checkpoint /workspaces/resume-ner-azureml/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-d7c6e3fb/refit/checkpoint --test-data /tmp/pytest-of-codespace/pytest-38/test_benchmarking_execution_mo0/dataset_tiny/seed0/test.json --batch-sizes 1 --iterations 10 --warmup 10 --max-length 512 --output /workspaces/resume-ner-azureml/outputs/benchmarking/local/distilbert/study-df9d920c/trial-d7c6e3fb/bench-fcfe0227/benchmark.json
2026-01-18 00:31:39,683 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Building run name: trial_id=d7c6e3fbd7c6e3fb, root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config
2026-01-18 00:31:39 INFO  [evaluation.benchmarking.utils] [Benchmark Run Name] Building run name: trial_id=d7c6e3fbd7c6e3fb, root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config
2026-01-18 00:31:39,683 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 00:31:39 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 00:31:39,683 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=benchmarking
2026-01-18 00:31:39 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=benchmarking
2026-01-18 00:31:39,683 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Starting reservation: counter_key=resume-ner:benchmarking:ff128a5db1208df8b16a45f57c39ad83a138..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-18 00:31:39 INFO  [orchestration.jobs.tracking.index.version_counter] [Reserve Version] Starting reservation: counter_key=resume-ner:benchmarking:ff128a5db1208df8b16a45f57c39ad83a138..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-18 00:31:39,684 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Loaded 27 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-18 00:31:39 INFO  [orchestration.jobs.tracking.index.version_counter] [Reserve Version] Loaded 27 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-18 00:31:39,684 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0
2026-01-18 00:31:39 INFO  [orchestration.jobs.tracking.index.version_counter] [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0
2026-01-18 00:31:39,684 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)
2026-01-18 00:31:39 INFO  [orchestration.jobs.tracking.index.version_counter] [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)
2026-01-18 00:31:39,684 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version]  Successfully reserved version 1 for counter_key resume-ner:benchmarking:ff128a5db1208df8b16a45f57c... (run_id: pending_2026...)
2026-01-18 00:31:39 INFO  [orchestration.jobs.tracking.index.version_counter] [Reserve Version]  Successfully reserved version 1 for counter_key resume-ner:benchmarking:ff128a5db1208df8b16a45f57c... (run_id: pending_2026...)
2026-01-18 00:31:39,685 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Generated run name: local_distilbert_benchmark_study-df9d920c_trial-d7c6e3fb_bench-fcfe0227_1
2026-01-18 00:31:39 INFO  [evaluation.benchmarking.utils] [Benchmark Run Name] Generated run name: local_distilbert_benchmark_study-df9d920c_trial-d7c6e3fb_bench-fcfe0227_1
2026-01-18 00:31:39,693 - infrastructure.tracking.mlflow.trackers.benchmark_tracker - WARNING - MLflow tracking failed: No Experiment with id=679667700195200030 exists
2026-01-18 00:31:39 WARNI [infrastructure.tracking.mlflow.trackers.benchmark_tracker] MLflow tracking failed: No Experiment with id=679667700195200030 exists
2026-01-18 00:31:39,693 - infrastructure.tracking.mlflow.trackers.benchmark_tracker - WARNING - Continuing benchmarking without MLflow tracking...
2026-01-18 00:31:39 WARNI [infrastructure.tracking.mlflow.trackers.benchmark_tracker] Continuing benchmarking without MLflow tracking...
2026-01-18 00:31:39,700 - infrastructure.tracking.mlflow.trackers.benchmark_tracker - WARNING - Could not log benchmark results to MLflow: No Experiment with id=679667700195200030 exists
2026-01-18 00:31:39 WARNI [infrastructure.tracking.mlflow.trackers.benchmark_tracker] Could not log benchmark results to MLflow: No Experiment with id=679667700195200030 exists
2026-01-18 00:31:39,700 - evaluation.benchmarking.orchestrator - INFO - Benchmark completed: /workspaces/resume-ner-azureml/outputs/benchmarking/local/distilbert/study-df9d920c/trial-d7c6e3fb/bench-fcfe0227/benchmark.json
2026-01-18 00:31:39 INFO  [evaluation.benchmarking.orchestrator] Benchmark completed: /workspaces/resume-ner-azureml/outputs/benchmarking/local/distilbert/study-df9d920c/trial-d7c6e3fb/bench-fcfe0227/benchmark.json
2026-01-18 00:31:39,700 - evaluation.benchmarking.orchestrator - INFO - Benchmarking complete. 1/1 trials benchmarked.
2026-01-18 00:31:39 INFO  [evaluation.benchmarking.orchestrator] Benchmarking complete. 1/1 trials benchmarked.
2026-01-18 00:31:39,798 - deployment.conversion.orchestration - INFO - Output directory: /workspaces/resume-ner-azureml/outputs/conversion/local/local/spec-aaaaaaaa_exec-bbbbbbbb/v1/conv-cd2379f5
2026-01-18 00:31:39 INFO  [deployment.conversion.orchestration] Output directory: /workspaces/resume-ner-azureml/outputs/conversion/local/local/spec-aaaaaaaa_exec-bbbbbbbb/v1/conv-cd2379f5
2026/01/18 00:31:39 INFO mlflow.tracking.fluent: Experiment with name 'resume_ner_baseline-conversion' does not exist. Creating a new experiment.
2026-01-18 00:31:39,804 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 00:31:39 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-18 00:31:39,804 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=conversion
2026-01-18 00:31:39 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=conversion
2026-01-18 00:31:39,818 - deployment.conversion.orchestration - INFO - Created MLflow run: local_local_conversion_spec-aaaaaaaa_exec-bbbbbbbb_v1_conv-cd2379f5 (92fec1f3a586...)
2026-01-18 00:31:39 INFO  [deployment.conversion.orchestration] Created MLflow run: local_local_conversion_spec-aaaaaaaa_exec-bbbbbbbb_v1_conv-cd2379f5 (92fec1f3a586...)
2026-01-18 00:31:39,818 - deployment.conversion.orchestration - INFO - Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /tmp/pytest-of-codespace/pytest-38/test_best_config_selection_e2e0/final_training/local/distilbert/spec-aaaaaaaa_exec-bbbbbbbb/v1/checkpoint --config-dir /workspaces/resume-ner-azureml/config --backbone local --output-dir /workspaces/resume-ner-azureml/outputs/conversion/local/local/spec-aaaaaaaa_exec-bbbbbbbb/v1/conv-cd2379f5 --opset-version 18 --run-smoke-test
2026-01-18 00:31:39 INFO  [deployment.conversion.orchestration] Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /tmp/pytest-of-codespace/pytest-38/test_best_config_selection_e2e0/final_training/local/distilbert/spec-aaaaaaaa_exec-bbbbbbbb/v1/checkpoint --config-dir /workspaces/resume-ner-azureml/config --backbone local --output-dir /workspaces/resume-ner-azureml/outputs/conversion/local/local/spec-aaaaaaaa_exec-bbbbbbbb/v1/conv-cd2379f5 --opset-version 18 --run-smoke-test
2026-01-18 00:31:39,819 - infrastructure.tracking.mlflow.lifecycle - INFO - Run 92fec1f3a586... already terminated with status <Mock name='mock.get_run().info.status' id='140077279695808'> (expected FINISHED)
2026-01-18 00:31:39 INFO  [infrastructure.tracking.mlflow.lifecycle] Run 92fec1f3a586... already terminated with status <Mock name='mock.get_run().info.status' id='140077279695808'> (expected FINISHED)
2026-01-18 00:31:39,820 - deployment.conversion.orchestration - INFO - Conversion completed. ONNX model: /workspaces/resume-ner-azureml/outputs/conversion/local/local/spec-aaaaaaaa_exec-bbbbbbbb/v1/conv-cd2379f5/model_fp32.onnx
2026-01-18 00:31:39 INFO  [deployment.conversion.orchestration] Conversion completed. ONNX model: /workspaces/resume-ner-azureml/outputs/conversion/local/local/spec-aaaaaaaa_exec-bbbbbbbb/v1/conv-cd2379f5/model_fp32.onnx

ERROR conda.cli.main_run:execute(127): `conda run pytest tests/ -v --tb=short` failed. (See above for error)

