
============================================================
[INFO] Pytest log file: /workspaces/resume-ner-azureml/outputs/pytest_logs/pytest_20260115_000546.log
============================================================

============================= test session starts ==============================
platform linux -- Python 3.10.19, pytest-9.0.2, pluggy-1.6.0 -- /opt/conda/envs/resume-ner-training/bin/python
cachedir: .pytest_cache
rootdir: /workspaces/resume-ner-azureml
configfile: pytest.ini
plugins: xdist-3.8.0, cov-7.0.0, mock-3.15.1, anyio-4.12.1
collecting ... 
----------------------------- live log collection ------------------------------
2026-01-15 00:05:46 INFO     infrastructure.tracking.mlflow.compatibility: Applied Azure ML artifact compatibility patch
collected 1386 items

tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_missing_benchmark_yaml_uses_defaults PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_empty_batch_sizes_handled_gracefully PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_non_integer_batch_sizes_type_check PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_negative_iterations_handled PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_zero_iterations_handled PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_non_integer_iterations_type_check PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_negative_warmup_iterations_handled PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_non_integer_warmup_type_check PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_negative_max_length_handled PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_zero_max_length_handled PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_non_integer_max_length_type_check PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_invalid_device_value_handled PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_test_data_nonexistent_path_handled PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_test_data_resolution_fallback_logic PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_output_filename_with_path_separators PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_missing_benchmarking_section_uses_defaults PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_missing_output_section_uses_defaults PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_benchmark_best_trials_handles_missing_test_data 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Skipping benchmarking (test data not available)
PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_run_benchmarking_handles_missing_benchmark_script Warning: Benchmark script not found: /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_handles_0/src/benchmarking/cli.py
PASSED
tests/benchmarking/integration/test_benchmark_edge_cases.py::TestBenchmarkConfigEdgeCases::test_run_benchmarking_handles_subprocess_failure 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_handles_1/src/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_handles_1/checkpoint --test-data /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_handles_1/test.json --batch-sizes 1 8 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_handles_1/benchmark.json
2026-01-15 00:05:51 ERROR    evaluation.benchmarking.utils: Benchmarking failed with return code 1
PASSED
tests/benchmarking/integration/test_benchmark_mlflow_tracking.py::TestBenchmarkMlflowTrackingWithTrialId::test_benchmark_passes_trial_id_to_run_benchmarking 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Benchmarking distilbert (trial-25d03eeb)...
2026-01-15 00:05:51 WARNING  evaluation.benchmarking.orchestrator: [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=f26f08fe24516179...
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Benchmark completed: /tmp/pytest-of-codespace/pytest-28/test_benchmark_passes_trial_id0/outputs/benchmarking/test/benchmark.json
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Benchmarking complete. 1/1 trials benchmarked.
PASSED
tests/benchmarking/integration/test_benchmark_mlflow_tracking.py::TestBenchmarkMlflowTrackingWithTrialId::test_benchmark_passes_trial_id_old_format 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Benchmarking distilbert (trial_1_20251231_161745)...
2026-01-15 00:05:51 WARNING  evaluation.benchmarking.orchestrator: [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=f26f08fe24516179...
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Benchmark completed: /tmp/pytest-of-codespace/pytest-28/test_benchmark_passes_trial_id1/outputs/benchmarking/test/benchmark.json
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Benchmarking complete. 1/1 trials benchmarked.
PASSED
tests/benchmarking/integration/test_benchmark_mlflow_tracking.py::TestBenchmarkMlflowTrackingWithTrialId::test_run_benchmarking_mlflow_tracking_with_trial_id 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_mlflow_t0/src/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_mlflow_t0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_mlflow_t0/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_mlflow_t0/benchmark.json
2026-01-15 00:05:51 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Building run name: trial_id=trial-25d03eeb, root_dir=/tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_mlflow_t0, config_dir=/tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_mlflow_t0/config
2026-01-15 00:05:51 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_mlflow_t0/config/naming.yaml, using empty policy
2026-01-15 00:05:51 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Generated run name: <Mock name='create_naming_context().storage_env' id='124894361544368'>_<Mock name='create_naming_context().model' id='124894361544416'>_<Mock name='create_naming_context().process_type' id='124894361544320'>_legacy
PASSED
tests/benchmarking/integration/test_benchmark_mlflow_tracking.py::TestBenchmarkMlflowTrackingWithTrialId::test_run_benchmarking_mlflow_tracking_fallback_to_trial_key_hash 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_mlflow_t1/src/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_mlflow_t1/checkpoint --test-data /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_mlflow_t1/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_mlflow_t1/benchmark.json
2026-01-15 00:05:51 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Constructed trial_id from trial_key_hash: trial-25d03eeb
2026-01-15 00:05:51 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Building run name: trial_id=trial-25d03eeb, root_dir=/tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_mlflow_t1, config_dir=/tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_mlflow_t1/config
2026-01-15 00:05:51 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_mlflow_t1/config/naming.yaml, using empty policy
2026-01-15 00:05:51 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Generated run name: <Mock name='create_naming_context().storage_env' id='124894361593568'>_<Mock name='create_naming_context().model' id='124894361593616'>_<Mock name='create_naming_context().process_type' id='124894361593520'>_legacy
PASSED
tests/benchmarking/integration/test_benchmark_orchestrator.py::TestBenchmarkOrchestratorConfigUsage::test_benchmark_best_trials_uses_config_batch_sizes 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Benchmarking distilbert (trial_0)...
2026-01-15 00:05:51 WARNING  evaluation.benchmarking.orchestrator: [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=9ab6cf5dc0566079...
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Benchmark completed: /tmp/pytest-of-codespace/pytest-28/test_benchmark_best_trials_use0/outputs/benchmarking/test/benchmark.json
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Benchmarking complete. 1/1 trials benchmarked.
PASSED
tests/benchmarking/integration/test_benchmark_orchestrator.py::TestBenchmarkOrchestratorConfigUsage::test_benchmark_best_trials_uses_config_iterations 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Benchmarking distilbert (trial_0)...
2026-01-15 00:05:51 WARNING  evaluation.benchmarking.orchestrator: [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=9ab6cf5dc0566079...
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Benchmark completed: /tmp/pytest-of-codespace/pytest-28/test_benchmark_best_trials_use1/outputs/benchmarking/test/benchmark.json
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Benchmarking complete. 1/1 trials benchmarked.
PASSED
tests/benchmarking/integration/test_benchmark_orchestrator.py::TestBenchmarkOrchestratorConfigUsage::test_benchmark_best_trials_uses_config_warmup 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Benchmarking distilbert (trial_0)...
2026-01-15 00:05:51 WARNING  evaluation.benchmarking.orchestrator: [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=9ab6cf5dc0566079...
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Benchmark completed: /tmp/pytest-of-codespace/pytest-28/test_benchmark_best_trials_use2/outputs/benchmarking/test/benchmark.json
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Benchmarking complete. 1/1 trials benchmarked.
PASSED
tests/benchmarking/integration/test_benchmark_orchestrator.py::TestBenchmarkOrchestratorConfigUsage::test_benchmark_best_trials_uses_config_max_length 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Benchmarking distilbert (trial_0)...
2026-01-15 00:05:51 WARNING  evaluation.benchmarking.orchestrator: [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=9ab6cf5dc0566079...
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Benchmark completed: /tmp/pytest-of-codespace/pytest-28/test_benchmark_best_trials_use3/outputs/benchmarking/test/benchmark.json
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Benchmarking complete. 1/1 trials benchmarked.
PASSED
tests/benchmarking/integration/test_benchmark_orchestrator.py::TestBenchmarkOrchestratorConfigUsage::test_benchmark_best_trials_uses_config_device 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Benchmarking distilbert (trial_0)...
2026-01-15 00:05:51 WARNING  evaluation.benchmarking.orchestrator: [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=b8f03b32dbe8202f...
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Benchmark completed: /tmp/pytest-of-codespace/pytest-28/test_benchmark_best_trials_use4/outputs/benchmarking/test/custom_benchmark.json
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Benchmarking complete. 1/1 trials benchmarked.
PASSED
tests/benchmarking/integration/test_benchmark_orchestrator.py::TestBenchmarkOrchestratorConfigUsage::test_benchmark_best_trials_uses_output_filename 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Benchmarking distilbert (trial_0)...
2026-01-15 00:05:51 WARNING  evaluation.benchmarking.orchestrator: [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=9ab6cf5dc0566079...
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Benchmark completed: /tmp/pytest-of-codespace/pytest-28/test_benchmark_best_trials_use5/outputs/benchmarking/test/benchmark.json
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Benchmarking complete. 1/1 trials benchmarked.
PASSED
tests/benchmarking/integration/test_benchmark_orchestrator.py::TestBenchmarkOrchestratorConfigUsage::test_benchmark_best_trials_uses_custom_output_filename 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Benchmarking distilbert (trial_0)...
2026-01-15 00:05:51 WARNING  evaluation.benchmarking.orchestrator: [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=b8f03b32dbe8202f...
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Benchmark completed: /tmp/pytest-of-codespace/pytest-28/test_benchmark_best_trials_use6/outputs/benchmarking/test/custom_benchmark.json
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Benchmarking complete. 1/1 trials benchmarked.
PASSED
tests/benchmarking/integration/test_benchmark_orchestrator.py::TestBenchmarkOrchestratorConfigUsage::test_benchmark_best_trials_all_config_options_together 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Benchmarking distilbert (trial_0)...
2026-01-15 00:05:51 WARNING  evaluation.benchmarking.orchestrator: [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=b8f03b32dbe8202f...
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Benchmark completed: /tmp/pytest-of-codespace/pytest-28/test_benchmark_best_trials_all0/outputs/benchmarking/test/custom_benchmark.json
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Benchmarking complete. 1/1 trials benchmarked.
PASSED
tests/benchmarking/integration/test_benchmark_orchestrator.py::TestBenchmarkOrchestratorConfigUsage::test_benchmark_best_trials_defaults_when_config_missing 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 WARNING  infrastructure.paths.resolve: Pattern 'benchmarking_v2' not found in paths.yaml. Using fallback logic.
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Benchmarking distilbert (trial_0)...
2026-01-15 00:05:51 WARNING  evaluation.benchmarking.orchestrator: [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=4f4d383480ad8038...
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
Warning: Benchmark script not found: /tmp/pytest-of-codespace/pytest-28/test_benchmark_best_trials_def0/outputs/src/benchmarking/cli.py
2026-01-15 00:05:51 ERROR    evaluation.benchmarking.orchestrator: Benchmark failed for distilbert
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Benchmarking complete. 0/1 trials benchmarked.
PASSED
tests/benchmarking/integration/test_benchmark_run_mode_idempotency.py::TestRunModeIntegration::test_force_new_returns_all_champions 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Run mode is 'force_new' - skipping idempotency check, will benchmark all champions
PASSED
tests/benchmarking/integration/test_benchmark_run_mode_idempotency.py::TestRunModeIntegration::test_reuse_if_exists_filters_existing_benchmarks 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Skipping distilbert - benchmark already exists (trial_key_hash=trial_hash_456...)
PASSED
tests/benchmarking/integration/test_benchmark_run_mode_idempotency.py::TestRunModeIntegration::test_get_benchmark_run_mode_from_config PASSED
tests/benchmarking/integration/test_benchmark_run_mode_idempotency.py::TestBenchmarkKeyIdempotencyIntegration::test_benchmark_key_primary_check_succeeds PASSED
tests/benchmarking/integration/test_benchmark_run_mode_idempotency.py::TestBenchmarkKeyIdempotencyIntegration::test_benchmark_key_primary_check_fails_then_fallback 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 WARNING  evaluation.benchmarking.orchestrator: Found 1 finished benchmark run(s) by hash (fallback). Consider setting benchmark_key tag for more reliable idempotency. trial_key_hash=trial_hash_456...
PASSED
tests/benchmarking/integration/test_benchmark_run_mode_idempotency.py::TestBenchmarkKeyIdempotencyIntegration::test_benchmark_key_changes_with_config_creates_new_benchmark PASSED
tests/benchmarking/integration/test_benchmark_run_mode_idempotency.py::TestBenchmarkKeyIdempotencyIntegration::test_benchmark_key_same_config_reuses_benchmark PASSED
tests/benchmarking/integration/test_benchmark_run_mode_idempotency.py::TestBackwardCompatibilityFallback::test_fallback_to_hash_when_benchmark_key_tag_missing 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 WARNING  evaluation.benchmarking.orchestrator: Found 1 finished benchmark run(s) by hash (fallback). Consider setting benchmark_key tag for more reliable idempotency. trial_key_hash=trial_hash_456...
PASSED
tests/benchmarking/integration/test_benchmark_run_mode_idempotency.py::TestBackwardCompatibilityFallback::test_fallback_requires_both_hashes PASSED
tests/benchmarking/integration/test_benchmark_run_mode_idempotency.py::TestCompleteWorkflowScenarios::test_scenario_reuse_if_exists_with_existing_benchmark 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Skipping distilbert - benchmark already exists (trial_key_hash=trial_hash_456...)
PASSED
tests/benchmarking/integration/test_benchmark_run_mode_idempotency.py::TestCompleteWorkflowScenarios::test_scenario_reuse_if_exists_without_existing_benchmark PASSED
tests/benchmarking/integration/test_benchmark_run_mode_idempotency.py::TestCompleteWorkflowScenarios::test_scenario_force_new_always_creates 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Run mode is 'force_new' - skipping idempotency check, will benchmark all champions
PASSED
tests/benchmarking/integration/test_benchmark_run_mode_idempotency.py::TestCompleteWorkflowScenarios::test_scenario_config_change_creates_new_benchmark PASSED
tests/benchmarking/integration/test_benchmark_utils.py::TestBenchmarkUtilsConfigUsage::test_run_benchmarking_uses_batch_sizes 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_bat0/src/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_bat0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_bat0/test.json --batch-sizes 1 8 16 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_bat0/benchmark.json
PASSED
tests/benchmarking/integration/test_benchmark_utils.py::TestBenchmarkUtilsConfigUsage::test_run_benchmarking_uses_iterations 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_ite0/src/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_ite0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_ite0/test.json --batch-sizes 1 8 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_ite0/benchmark.json
PASSED
tests/benchmarking/integration/test_benchmark_utils.py::TestBenchmarkUtilsConfigUsage::test_run_benchmarking_uses_warmup_iterations 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_war0/src/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_war0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_war0/test.json --batch-sizes 1 8 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_war0/benchmark.json
PASSED
tests/benchmarking/integration/test_benchmark_utils.py::TestBenchmarkUtilsConfigUsage::test_run_benchmarking_uses_max_length 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_max0/src/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_max0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_max0/test.json --batch-sizes 1 8 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_max0/benchmark.json
PASSED
tests/benchmarking/integration/test_benchmark_utils.py::TestBenchmarkUtilsConfigUsage::test_run_benchmarking_uses_device_when_provided 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_dev0/src/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_dev0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_dev0/test.json --batch-sizes 1 8 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_dev0/benchmark.json --device cuda
PASSED
tests/benchmarking/integration/test_benchmark_utils.py::TestBenchmarkUtilsConfigUsage::test_run_benchmarking_skips_device_when_null 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_skips_de0/src/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_skips_de0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_skips_de0/test.json --batch-sizes 1 8 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_skips_de0/benchmark.json
PASSED
tests/benchmarking/integration/test_benchmark_utils.py::TestBenchmarkUtilsConfigUsage::test_run_benchmarking_uses_output_path 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_out0/src/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_out0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_out0/test.json --batch-sizes 1 8 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_out0/benchmark.json
PASSED
tests/benchmarking/integration/test_benchmark_utils.py::TestBenchmarkUtilsConfigUsage::test_run_benchmarking_all_config_options_together 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_all_conf0/src/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_all_conf0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_all_conf0/test.json --batch-sizes 2 4 32 --iterations 200 --warmup 20 --max-length 256 --output /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_all_conf0/custom_benchmark.json --device cuda
PASSED
tests/benchmarking/integration/test_benchmark_workflow.py::TestBenchmarkWorkflow::test_workflow_loads_config_and_uses_all_options 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Benchmarking distilbert (trial_0)...
2026-01-15 00:05:51 WARNING  evaluation.benchmarking.orchestrator: [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=9ab6cf5dc0566079...
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Benchmark completed: /tmp/pytest-of-codespace/pytest-28/test_workflow_loads_config_and0/outputs/benchmarking/test/benchmark.json
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Benchmarking complete. 1/1 trials benchmarked.
PASSED
tests/benchmarking/integration/test_benchmark_workflow.py::TestBenchmarkWorkflow::test_workflow_custom_config_values 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-28/test_workflow_custom_config_va0/src/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-28/test_workflow_custom_config_va0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-28/test_workflow_custom_config_va0/test.json --batch-sizes 2 4 32 --iterations 200 --warmup 20 --max-length 256 --output /tmp/pytest-of-codespace/pytest-28/test_workflow_custom_config_va0/custom_benchmark.json --device cuda
PASSED
tests/benchmarking/integration/test_benchmark_workflow.py::TestBenchmarkWorkflow::test_workflow_defaults_when_config_missing 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Benchmarking distilbert (trial_0)...
2026-01-15 00:05:51 WARNING  evaluation.benchmarking.orchestrator: [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=4f4d383480ad8038...
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Benchmark completed: /tmp/pytest-of-codespace/pytest-28/test_workflow_defaults_when_co0/outputs/benchmarking/test/benchmark.json
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Benchmarking complete. 1/1 trials benchmarked.
PASSED
tests/benchmarking/unit/test_benchmark_config.py::TestBenchmarkConfigLoading::test_load_experiment_config_includes_benchmark PASSED
tests/benchmarking/unit/test_benchmark_config.py::TestBenchmarkConfigLoading::test_load_experiment_config_defaults_to_benchmark_yaml PASSED
tests/benchmarking/unit/test_benchmark_config.py::TestBenchmarkConfigLoading::test_load_all_configs_loads_benchmark_if_exists PASSED
tests/benchmarking/unit/test_benchmark_config.py::TestBenchmarkConfigLoading::test_load_all_configs_skips_benchmark_if_not_exists PASSED
tests/benchmarking/unit/test_benchmark_config.py::TestBenchmarkConfigLoading::test_load_benchmark_config_structure PASSED
tests/benchmarking/unit/test_benchmark_config.py::TestBenchmarkConfigLoading::test_load_benchmark_config_with_custom_values PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_batch_sizes_extraction PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_batch_sizes_default PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_batch_sizes_custom_values PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_iterations_extraction PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_iterations_default PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_iterations_custom_value PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_warmup_iterations_extraction PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_warmup_iterations_default PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_warmup_iterations_custom_value PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_max_length_extraction PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_max_length_default PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_max_length_custom_value PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_device_extraction_null PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_device_extraction_cuda PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_device_extraction_cpu PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_device_default PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_test_data_extraction_null PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_test_data_extraction_relative_path PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_test_data_extraction_absolute_path PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_test_data_default PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_output_filename_extraction PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_output_filename_default PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_output_filename_custom_value PASSED
tests/benchmarking/unit/test_benchmark_config_options.py::TestBenchmarkConfigOptions::test_all_options_together PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestRunModeConfig::test_run_mode_extraction_reuse_if_exists PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestRunModeConfig::test_run_mode_extraction_force_new PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestRunModeConfig::test_run_mode_default_when_missing PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestRunModeConfig::test_run_mode_default_when_run_section_missing PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestRunModeConfig::test_get_benchmark_run_mode_uses_config PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestRunModeConfig::test_get_benchmark_run_mode_default PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestRunModeBehavior::test_force_new_skips_filtering 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Run mode is 'force_new' - skipping idempotency check, will benchmark all champions
PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestRunModeBehavior::test_reuse_if_exists_filters_existing 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Skipping distilbert - benchmark already exists (trial_key_hash=trial_hash_456...)
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Skipping deberta - benchmark already exists (trial_key_hash=trial_hash_789...)
PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestBenchmarkKeyIdempotency::test_build_benchmark_key_includes_config_hash PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestBenchmarkKeyIdempotency::test_benchmark_key_changes_with_config_change PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestBenchmarkKeyIdempotency::test_benchmark_key_same_with_same_config PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestBenchmarkKeyIdempotency::test_benchmark_key_used_as_primary_check PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestBenchmarkKeyFallback::test_fallback_to_hash_when_benchmark_key_not_found PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestBenchmarkKeyFallback::test_fallback_requires_both_hashes PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestIdempotencySuccessCases::test_success_benchmark_exists_by_benchmark_key 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Skipping distilbert - benchmark already exists (trial_key_hash=trial_hash_456...)
PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestIdempotencySuccessCases::test_success_benchmark_exists_by_hash_fallback 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     evaluation.benchmarking.orchestrator: Skipping distilbert - benchmark already exists (trial_key_hash=trial_hash_456...)
PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestIdempotencySuccessCases::test_success_benchmark_not_exists_creates_new PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestIdempotencyFailureCases::test_failure_missing_champion_run_id 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 WARNING  evaluation.benchmarking.orchestrator: No run_id found for champion distilbert, skipping idempotency check
PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestIdempotencyFailureCases::test_failure_mlflow_client_unavailable PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestIdempotencyFailureCases::test_failure_mlflow_check_raises_exception PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestConfigDocumentationCoverage::test_run_mode_documentation_covered PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestConfigDocumentationCoverage::test_idempotency_documentation_covered PASSED
tests/benchmarking/unit/test_benchmark_run_mode_config.py::TestConfigDocumentationCoverage::test_independence_from_hpo_config PASSED
tests/benchmarking/unit/test_benchmark_trial_id_extraction.py::TestTrialIdExtraction::test_trial_id_from_parameter_old_format 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-28/test_trial_id_from_parameter_o0/src/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-28/test_trial_id_from_parameter_o0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-28/test_trial_id_from_parameter_o0/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-28/test_trial_id_from_parameter_o0/benchmark.json
2026-01-15 00:05:51 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Building run name: trial_id=1_20251231_161745, root_dir=/tmp/pytest-of-codespace/pytest-28/test_trial_id_from_parameter_o0, config_dir=/tmp/pytest-of-codespace/pytest-28/test_trial_id_from_parameter_o0/config
2026-01-15 00:05:51 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-28/test_trial_id_from_parameter_o0/config/naming.yaml, using empty policy
2026-01-15 00:05:51 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Generated run name: <Mock name='create_naming_context().storage_env' id='124894361420752'>_<Mock name='create_naming_context().model' id='124894361420608'>_<Mock name='create_naming_context().process_type' id='124894361420512'>_legacy
PASSED
tests/benchmarking/unit/test_benchmark_trial_id_extraction.py::TestTrialIdExtraction::test_trial_id_from_parameter_new_format 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-28/test_trial_id_from_parameter_n0/src/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-28/test_trial_id_from_parameter_n0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-28/test_trial_id_from_parameter_n0/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-28/test_trial_id_from_parameter_n0/benchmark.json
2026-01-15 00:05:51 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Building run name: trial_id=trial-25d03eeb, root_dir=/tmp/pytest-of-codespace/pytest-28/test_trial_id_from_parameter_n0, config_dir=/tmp/pytest-of-codespace/pytest-28/test_trial_id_from_parameter_n0/config
2026-01-15 00:05:51 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-28/test_trial_id_from_parameter_n0/config/naming.yaml, using empty policy
2026-01-15 00:05:51 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Generated run name: <Mock name='create_naming_context().storage_env' id='124894361595680'>_<Mock name='create_naming_context().model' id='124894361595536'>_<Mock name='create_naming_context().process_type' id='124894361595440'>_legacy
PASSED
tests/benchmarking/unit/test_benchmark_trial_id_extraction.py::TestTrialIdExtraction::test_trial_id_extraction_from_path_old_format 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-28/test_trial_id_extraction_from_0/src/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-28/test_trial_id_extraction_from_0/hpo/local/distilbert/study-abc123/trial_1_20251231_161745/checkpoint --test-data /tmp/pytest-of-codespace/pytest-28/test_trial_id_extraction_from_0/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-28/test_trial_id_extraction_from_0/benchmark.json
2026-01-15 00:05:51 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Extracted trial_id from checkpoint path: trial_1_20251231_161745 (at level 1)
2026-01-15 00:05:51 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Building run name: trial_id=trial_1_20251231_161745, root_dir=/tmp/pytest-of-codespace/pytest-28/test_trial_id_extraction_from_0, config_dir=/tmp/pytest-of-codespace/pytest-28/test_trial_id_extraction_from_0/config
2026-01-15 00:05:51 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-28/test_trial_id_extraction_from_0/config/naming.yaml, using empty policy
2026-01-15 00:05:51 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Generated run name: <Mock name='create_naming_context().storage_env' id='124894361593568'>_<Mock name='create_naming_context().model' id='124894361593904'>_<Mock name='create_naming_context().process_type' id='124894361593616'>_legacy
PASSED
tests/benchmarking/unit/test_benchmark_trial_id_extraction.py::TestTrialIdExtraction::test_trial_id_extraction_from_path_new_format 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-28/test_trial_id_extraction_from_1/src/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-28/test_trial_id_extraction_from_1/hpo/local/distilbert/study-abc123/trial-25d03eeb/checkpoint --test-data /tmp/pytest-of-codespace/pytest-28/test_trial_id_extraction_from_1/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-28/test_trial_id_extraction_from_1/benchmark.json
2026-01-15 00:05:51 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Extracted trial_id from checkpoint path: trial-25d03eeb (at level 1)
2026-01-15 00:05:51 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Building run name: trial_id=trial-25d03eeb, root_dir=/tmp/pytest-of-codespace/pytest-28/test_trial_id_extraction_from_1, config_dir=/tmp/pytest-of-codespace/pytest-28/test_trial_id_extraction_from_1/config
2026-01-15 00:05:51 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-28/test_trial_id_extraction_from_1/config/naming.yaml, using empty policy
2026-01-15 00:05:51 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Generated run name: <Mock name='create_naming_context().storage_env' id='124894361590160'>_<Mock name='create_naming_context().model' id='124894361590208'>_<Mock name='create_naming_context().process_type' id='124894361590112'>_legacy
PASSED
tests/benchmarking/unit/test_benchmark_trial_id_extraction.py::TestTrialIdExtraction::test_trial_id_extraction_from_refit_checkpoint_path 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-28/test_trial_id_extraction_from_2/src/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-28/test_trial_id_extraction_from_2/hpo/local/distilbert/study-abc123/trial-25d03eeb/refit/checkpoint --test-data /tmp/pytest-of-codespace/pytest-28/test_trial_id_extraction_from_2/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-28/test_trial_id_extraction_from_2/benchmark.json
2026-01-15 00:05:51 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Extracted trial_id from checkpoint path: trial-25d03eeb (at level 2)
2026-01-15 00:05:51 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Building run name: trial_id=trial-25d03eeb, root_dir=/tmp/pytest-of-codespace/pytest-28/test_trial_id_extraction_from_2, config_dir=/tmp/pytest-of-codespace/pytest-28/test_trial_id_extraction_from_2/config
2026-01-15 00:05:51 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-28/test_trial_id_extraction_from_2/config/naming.yaml, using empty policy
2026-01-15 00:05:51 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Generated run name: <Mock name='create_naming_context().storage_env' id='124894361127616'>_<Mock name='create_naming_context().model' id='124894361127664'>_<Mock name='create_naming_context().process_type' id='124894361127568'>_legacy
PASSED
tests/benchmarking/unit/test_benchmark_trial_id_extraction.py::TestTrialIdExtraction::test_trial_id_fallback_to_trial_key_hash 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-28/test_trial_id_fallback_to_tria0/src/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-28/test_trial_id_fallback_to_tria0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-28/test_trial_id_fallback_to_tria0/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-28/test_trial_id_fallback_to_tria0/benchmark.json
2026-01-15 00:05:51 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Constructed trial_id from trial_key_hash: trial-25d03eeb
2026-01-15 00:05:51 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Building run name: trial_id=trial-25d03eeb, root_dir=/tmp/pytest-of-codespace/pytest-28/test_trial_id_fallback_to_tria0, config_dir=/tmp/pytest-of-codespace/pytest-28/test_trial_id_fallback_to_tria0/config
2026-01-15 00:05:51 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-28/test_trial_id_fallback_to_tria0/config/naming.yaml, using empty policy
2026-01-15 00:05:51 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Generated run name: <Mock name='create_naming_context().storage_env' id='124894490717840'>_<Mock name='create_naming_context().model' id='124894490715488'>_<Mock name='create_naming_context().process_type' id='124894490718416'>_legacy
PASSED
tests/benchmarking/unit/test_benchmark_trial_id_extraction.py::TestTrialIdExtraction::test_trial_id_parameter_overrides_path_extraction 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     evaluation.benchmarking.utils: Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-28/test_trial_id_parameter_overri0/src/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-28/test_trial_id_parameter_overri0/hpo/local/distilbert/study-abc123/trial-25d03eeb/checkpoint --test-data /tmp/pytest-of-codespace/pytest-28/test_trial_id_parameter_overri0/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-28/test_trial_id_parameter_overri0/benchmark.json
2026-01-15 00:05:51 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Building run name: trial_id=trial-custom123, root_dir=/tmp/pytest-of-codespace/pytest-28/test_trial_id_parameter_overri0, config_dir=/tmp/pytest-of-codespace/pytest-28/test_trial_id_parameter_overri0/config
2026-01-15 00:05:51 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-28/test_trial_id_parameter_overri0/config/naming.yaml, using empty policy
2026-01-15 00:05:51 INFO     evaluation.benchmarking.utils: [Benchmark Run Name] Generated run name: <Mock name='create_naming_context().storage_env' id='124894361098976'>_<Mock name='create_naming_context().model' id='124894361099024'>_<Mock name='create_naming_context().process_type' id='124894361098928'>_legacy
PASSED
tests/config/integration/test_config_integration.py::TestEndToEndScenarios::test_hpo_trial_workflow 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_hpo_trial_workflow0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-15 00:05:51 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-15 00:05:51 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:e0fbae020bc656b7e6425344765f3df149e5f6831d2b7..., root_dir=/tmp/pytest-of-codespace/pytest-28/test_hpo_trial_workflow0, config_dir=/tmp/pytest-of-codespace/pytest-28/test_hpo_trial_workflow0/config, counter_path=/tmp/pytest-of-codespace/pytest-28/test_hpo_trial_workflow0/outputs/cache/run_name_counter.json
2026-01-15 00:05:51 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Loaded 0 existing allocations from /tmp/pytest-of-codespace/pytest-28/test_hpo_trial_workflow0/outputs/cache/run_name_counter.json
2026-01-15 00:05:51 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0
2026-01-15 00:05:51 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)
2026-01-15 00:05:51 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version]  Successfully reserved version 1 for counter_key resume-ner:hpo:e0fbae020bc656b7e6425344765f3df149e... (run_id: pending_2026...)
PASSED
tests/config/integration/test_config_integration.py::TestEndToEndScenarios::test_final_training_workflow 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_final_training_workflow0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-15 00:05:51 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=final_training
PASSED
tests/config/integration/test_config_integration.py::TestEndToEndScenarios::test_benchmarking_workflow 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 WARNING  infrastructure.naming.display_policy: [Naming Policy] No pattern for process_type: benchmarking
2026-01-15 00:05:51 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_benchmarking_workflow0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-15 00:05:51 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=benchmarking
2026-01-15 00:05:51 WARNING  infrastructure.naming.mlflow.run_names: Could not reserve version for run name: Benchmarking requires trial_id for run_key, using base name without version
PASSED
tests/config/integration/test_config_integration.py::TestConfigurationConsistency::test_paths_match_naming_patterns 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_paths_match_naming_patter0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-15 00:05:51 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-15 00:05:51 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:fa924e9d2ac5675b719f2281bdcff80badd52b78fb7ef..., root_dir=/tmp/pytest-of-codespace/pytest-28/test_paths_match_naming_patter0, config_dir=/tmp/pytest-of-codespace/pytest-28/test_paths_match_naming_patter0/config, counter_path=/tmp/pytest-of-codespace/pytest-28/test_paths_match_naming_patter0/outputs/cache/run_name_counter.json
2026-01-15 00:05:51 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Loaded 0 existing allocations from /tmp/pytest-of-codespace/pytest-28/test_paths_match_naming_patter0/outputs/cache/run_name_counter.json
2026-01-15 00:05:51 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0
2026-01-15 00:05:51 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)
2026-01-15 00:05:51 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version]  Successfully reserved version 1 for counter_key resume-ner:hpo:fa924e9d2ac5675b719f2281bdcff80badd... (run_id: pending_2026...)
PASSED
tests/config/integration/test_config_integration.py::TestConfigurationConsistency::test_tag_keys_from_tags_yaml PASSED
tests/config/integration/test_config_integration.py::TestConfigurationConsistency::test_naming_patterns_from_naming_yaml 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_naming_patterns_from_nami0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-15 00:05:51 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-15 00:05:51 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:e0fbae020bc656b7e6425344765f3df149e5f6831d2b7..., root_dir=/tmp/pytest-of-codespace/pytest-28/test_naming_patterns_from_nami0, config_dir=/tmp/pytest-of-codespace/pytest-28/test_naming_patterns_from_nami0/config, counter_path=/tmp/pytest-of-codespace/pytest-28/test_naming_patterns_from_nami0/outputs/cache/run_name_counter.json
2026-01-15 00:05:51 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Loaded 0 existing allocations from /tmp/pytest-of-codespace/pytest-28/test_naming_patterns_from_nami0/outputs/cache/run_name_counter.json
2026-01-15 00:05:51 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0
2026-01-15 00:05:51 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)
2026-01-15 00:05:51 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version]  Successfully reserved version 1 for counter_key resume-ner:hpo:e0fbae020bc656b7e6425344765f3df149e... (run_id: pending_2026...)
PASSED
tests/config/integration/test_config_integration.py::TestCrossProcessConsistency::test_same_model_environment_consistent_paths PASSED
tests/config/integration/test_config_integration.py::TestCrossProcessConsistency::test_tag_keys_consistent_across_processes PASSED
tests/config/integration/test_config_integration.py::TestCrossProcessConsistency::test_naming_conventions_consistent 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_naming_conventions_consis0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-15 00:05:51 INFO     infrastructure.naming.mlflow.config: [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-15 00:05:51 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:e0fbae020bc656b7e6425344765f3df149e5f6831d2b7..., root_dir=/tmp/pytest-of-codespace/pytest-28/test_naming_conventions_consis0, config_dir=/tmp/pytest-of-codespace/pytest-28/test_naming_conventions_consis0/config, counter_path=/tmp/pytest-of-codespace/pytest-28/test_naming_conventions_consis0/outputs/cache/run_name_counter.json
2026-01-15 00:05:51 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Loaded 0 existing allocations from /tmp/pytest-of-codespace/pytest-28/test_naming_conventions_consis0/outputs/cache/run_name_counter.json
2026-01-15 00:05:51 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0
2026-01-15 00:05:51 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)
2026-01-15 00:05:51 INFO     orchestration.jobs.tracking.index.version_counter: [Reserve Version]  Successfully reserved version 1 for counter_key resume-ner:hpo:e0fbae020bc656b7e6425344765f3df149e... (run_id: pending_2026...)
PASSED
tests/config/unit/test_config_loader.py::TestConfigHashComputation::test_compute_config_hash_length PASSED
tests/config/unit/test_config_loader.py::TestConfigHashComputation::test_compute_config_hash_deterministic PASSED
tests/config/unit/test_config_loader.py::TestConfigHashComputation::test_compute_config_hash_different_configs PASSED
tests/config/unit/test_config_loader.py::TestConfigHashComputation::test_compute_config_hash_no_randomness PASSED
tests/config/unit/test_config_loader.py::TestConfigHashComputation::test_compute_config_hash_order_independent PASSED
tests/config/unit/test_config_loader.py::TestConfigHashComputation::test_compute_config_hashes_all_domains PASSED
tests/config/unit/test_config_loader.py::TestConfigHashComputation::test_compute_config_hashes_deterministic PASSED
tests/config/unit/test_config_loader.py::TestConfigLoading::test_load_experiment_config PASSED
tests/config/unit/test_config_loader.py::TestConfigLoading::test_load_experiment_config_defaults PASSED
tests/config/unit/test_config_loader.py::TestConfigLoading::test_load_all_configs PASSED
tests/config/unit/test_config_loader.py::TestConfigLoading::test_load_all_configs_with_benchmark PASSED
tests/config/unit/test_config_loader.py::TestConfigMetadata::test_create_config_metadata PASSED
tests/config/unit/test_config_loader.py::TestConfigImmutability::test_snapshot_configs PASSED
tests/config/unit/test_config_loader.py::TestConfigImmutability::test_validate_config_immutability_unchanged PASSED
tests/config/unit/test_config_loader.py::TestConfigImmutability::test_validate_config_immutability_changed PASSED
tests/config/unit/test_config_loader.py::TestConfigImmutability::test_validate_config_immutability_multiple_changes PASSED
tests/config/unit/test_data_config.py::TestDataConfigLoading::test_load_complete_data_config PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_name_option PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_version_option PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_description_option PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_local_path_option PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_seed_option PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_splitting_train_test_ratio PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_splitting_stratified_true PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_splitting_stratified_false PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_splitting_random_seed PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_schema_format PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_schema_annotation_format PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_schema_entity_types PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_schema_stats_median_sentence_length PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_schema_stats_mean_sentence_length PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_schema_stats_p95_sentence_length PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_schema_stats_suggested_sequence_length PASSED
tests/config/unit/test_data_config.py::TestDataConfigOptions::test_schema_stats_entity_density PASSED
tests/config/unit/test_data_config.py::TestDataConfigIntegration::test_data_config_via_experiment_config PASSED
tests/config/unit/test_data_config.py::TestDataConfigIntegration::test_build_label_list_from_data_config PASSED
tests/config/unit/test_data_config.py::TestDataConfigValidation::test_data_config_missing_optional_sections PASSED
tests/config/unit/test_data_config.py::TestDataConfigValidation::test_data_config_partial_splitting PASSED
tests/config/unit/test_data_config.py::TestDataConfigValidation::test_data_config_partial_schema PASSED
tests/config/unit/test_data_config.py::TestDataConfigValidation::test_data_config_partial_stats PASSED
tests/config/unit/test_data_config.py::TestDataConfigValidation::test_data_config_numeric_types PASSED
tests/config/unit/test_data_config.py::TestDataConfigValidation::test_data_config_boolean_types PASSED
tests/config/unit/test_data_config.py::TestDataConfigValidation::test_data_config_entity_types_list PASSED
tests/config/unit/test_data_config.py::TestDataConfigRealFiles::test_load_real_resume_tiny_config PASSED
tests/config/unit/test_data_config.py::TestDataConfigRealFiles::test_load_real_resume_v1_config PASSED
tests/config/unit/test_data_config.py::TestDataConfigRealFiles::test_all_real_data_configs_have_required_sections SKIPPED
tests/config/unit/test_experiment_config.py::TestExperimentConfigLoading::test_load_complete_experiment_config PASSED
tests/config/unit/test_experiment_config.py::TestExperimentConfigLoading::test_load_experiment_config_with_defaults PASSED
tests/config/unit/test_experiment_config.py::TestExperimentConfigOptions::test_experiment_name_option PASSED
tests/config/unit/test_experiment_config.py::TestExperimentConfigOptions::test_experiment_name_fallback PASSED
tests/config/unit/test_experiment_config.py::TestExperimentConfigOptions::test_data_config_option PASSED
tests/config/unit/test_experiment_config.py::TestExperimentConfigOptions::test_model_config_option PASSED
tests/config/unit/test_experiment_config.py::TestExperimentConfigOptions::test_train_config_option PASSED
tests/config/unit/test_experiment_config.py::TestExperimentConfigOptions::test_hpo_config_option PASSED
tests/config/unit/test_experiment_config.py::TestExperimentConfigOptions::test_env_config_option PASSED
tests/config/unit/test_experiment_config.py::TestExperimentConfigOptions::test_benchmark_config_option PASSED
tests/config/unit/test_experiment_config.py::TestExperimentConfigOptions::test_benchmark_config_default PASSED
tests/config/unit/test_experiment_config.py::TestStagesConfiguration::test_stages_smoke_aml_experiment PASSED
tests/config/unit/test_experiment_config.py::TestStagesConfiguration::test_stages_smoke_hpo_config PASSED
tests/config/unit/test_experiment_config.py::TestStagesConfiguration::test_stages_smoke_both_options PASSED
tests/config/unit/test_experiment_config.py::TestStagesConfiguration::test_stages_hpo_aml_experiment PASSED
tests/config/unit/test_experiment_config.py::TestStagesConfiguration::test_stages_hpo_hpo_config PASSED
tests/config/unit/test_experiment_config.py::TestStagesConfiguration::test_stages_hpo_both_options PASSED
tests/config/unit/test_experiment_config.py::TestStagesConfiguration::test_stages_training_aml_experiment PASSED
tests/config/unit/test_experiment_config.py::TestStagesConfiguration::test_stages_training_backbones_single PASSED
tests/config/unit/test_experiment_config.py::TestStagesConfiguration::test_stages_training_backbones_multiple PASSED
tests/config/unit/test_experiment_config.py::TestStagesConfiguration::test_stages_training_both_options PASSED
tests/config/unit/test_experiment_config.py::TestStagesConfiguration::test_stages_all_stages_together PASSED
tests/config/unit/test_experiment_config.py::TestNamingConfiguration::test_naming_include_backbone_in_experiment_true PASSED
tests/config/unit/test_experiment_config.py::TestNamingConfiguration::test_naming_include_backbone_in_experiment_false PASSED
tests/config/unit/test_experiment_config.py::TestNamingConfiguration::test_naming_missing_defaults_to_empty PASSED
tests/config/unit/test_experiment_config.py::TestExperimentConfigIntegration::test_load_all_configs_with_experiment_config PASSED
tests/config/unit/test_experiment_config.py::TestExperimentConfigIntegration::test_experiment_config_stages_preserved PASSED
tests/config/unit/test_experiment_config.py::TestExperimentConfigIntegration::test_experiment_config_naming_preserved PASSED
tests/config/unit/test_experiment_config.py::TestExperimentConfigRealFile::test_load_real_resume_ner_baseline_config SKIPPED
tests/config/unit/test_experiment_config.py::TestExperimentConfigRealFile::test_real_resume_ner_baseline_has_all_sections SKIPPED
tests/config/unit/test_fingerprint_placeholder_fallback.py::TestFingerprintPlaceholderFallback::test_compute_fingerprints_returns_placeholders_on_import_error PASSED
tests/config/unit/test_fingerprint_placeholder_fallback.py::TestFingerprintPlaceholderFallback::test_compute_fingerprints_import_error_handling PASSED
tests/config/unit/test_fingerprint_placeholder_fallback.py::TestFingerprintPlaceholderFallback::test_placeholder_values_are_short_enough_for_naming PASSED
tests/config/unit/test_fingerprints.py::test_compute_spec_fp_deterministic PASSED
tests/config/unit/test_fingerprints.py::test_compute_spec_fp_without_seed PASSED
tests/config/unit/test_fingerprints.py::test_compute_spec_fp_different_seeds PASSED
tests/config/unit/test_fingerprints.py::test_compute_exec_fp_basic PASSED
tests/config/unit/test_fingerprints.py::test_compute_exec_fp_auto_detect PASSED
tests/config/unit/test_fingerprints.py::test_compute_exec_fp_different_precision PASSED
tests/config/unit/test_fingerprints.py::test_compute_conv_fp PASSED
tests/config/unit/test_fingerprints.py::test_compute_conv_fp_different_parents PASSED
tests/config/unit/test_fingerprints.py::test_compute_bench_fp PASSED
tests/config/unit/test_fingerprints.py::test_compute_hardware_fp PASSED
tests/config/unit/test_fingerprints.py::test_compute_hardware_fp_empty PASSED
tests/config/unit/test_mlflow_yaml.py::TestAzureMLConfiguration::test_azure_ml_enabled PASSED
tests/config/unit/test_mlflow_yaml.py::TestAzureMLConfiguration::test_azure_ml_disabled PASSED
tests/config/unit/test_mlflow_yaml.py::TestAzureMLConfiguration::test_azure_ml_workspace_name PASSED
tests/config/unit/test_mlflow_yaml.py::TestTrackingConfiguration::test_tracking_benchmark_enabled PASSED
tests/config/unit/test_mlflow_yaml.py::TestTrackingConfiguration::test_tracking_benchmark_disabled PASSED
tests/config/unit/test_mlflow_yaml.py::TestTrackingConfiguration::test_tracking_benchmark_log_artifacts PASSED
tests/config/unit/test_mlflow_yaml.py::TestTrackingConfiguration::test_tracking_training_enabled PASSED
tests/config/unit/test_mlflow_yaml.py::TestTrackingConfiguration::test_tracking_training_disabled PASSED
tests/config/unit/test_mlflow_yaml.py::TestTrackingConfiguration::test_tracking_training_log_checkpoint PASSED
tests/config/unit/test_mlflow_yaml.py::TestTrackingConfiguration::test_tracking_training_log_metrics_json PASSED
tests/config/unit/test_mlflow_yaml.py::TestTrackingConfiguration::test_tracking_conversion_enabled PASSED
tests/config/unit/test_mlflow_yaml.py::TestTrackingConfiguration::test_tracking_conversion_disabled PASSED
tests/config/unit/test_mlflow_yaml.py::TestTrackingConfiguration::test_tracking_conversion_log_onnx_model PASSED
tests/config/unit/test_mlflow_yaml.py::TestTrackingConfiguration::test_tracking_conversion_log_conversion_log PASSED
tests/config/unit/test_mlflow_yaml.py::TestNamingConfiguration::test_naming_project_name PASSED
tests/config/unit/test_mlflow_yaml.py::TestNamingConfiguration::test_naming_tags_max_length PASSED
tests/config/unit/test_mlflow_yaml.py::TestNamingConfiguration::test_naming_tags_sanitize PASSED
tests/config/unit/test_mlflow_yaml.py::TestNamingConfiguration::test_naming_run_name_max_length PASSED
tests/config/unit/test_mlflow_yaml.py::TestNamingConfiguration::test_naming_run_name_shorten_fingerprints PASSED
tests/config/unit/test_mlflow_yaml.py::TestNamingConfiguration::test_naming_run_name_auto_increment_enabled 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     orchestration.jobs.tracking.config.loader: [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_naming_run_name_auto_incr0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-15 00:05:51 INFO     orchestration.jobs.tracking.config.loader: [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
PASSED
tests/config/unit/test_mlflow_yaml.py::TestNamingConfiguration::test_naming_run_name_auto_increment_processes_hpo 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     orchestration.jobs.tracking.config.loader: [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_naming_run_name_auto_incr1/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-15 00:05:51 INFO     orchestration.jobs.tracking.config.loader: [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
PASSED
tests/config/unit/test_mlflow_yaml.py::TestNamingConfiguration::test_naming_run_name_auto_increment_processes_benchmarking 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     orchestration.jobs.tracking.config.loader: [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_naming_run_name_auto_incr2/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-15 00:05:51 INFO     orchestration.jobs.tracking.config.loader: [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=benchmarking
PASSED
tests/config/unit/test_mlflow_yaml.py::TestNamingConfiguration::test_naming_run_name_auto_increment_format 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     orchestration.jobs.tracking.config.loader: [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_naming_run_name_auto_incr3/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-15 00:05:51 INFO     orchestration.jobs.tracking.config.loader: [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
PASSED
tests/config/unit/test_mlflow_yaml.py::TestIndexConfiguration::test_index_enabled PASSED
tests/config/unit/test_mlflow_yaml.py::TestIndexConfiguration::test_index_disabled PASSED
tests/config/unit/test_mlflow_yaml.py::TestIndexConfiguration::test_index_max_entries PASSED
tests/config/unit/test_mlflow_yaml.py::TestIndexConfiguration::test_index_file_name PASSED
tests/config/unit/test_mlflow_yaml.py::TestRunFinderConfiguration::test_run_finder_strict_mode_default_true PASSED
tests/config/unit/test_mlflow_yaml.py::TestRunFinderConfiguration::test_run_finder_strict_mode_default_false PASSED
tests/config/unit/test_model_config.py::TestModelConfigLoading::test_load_distilbert_config PASSED
tests/config/unit/test_model_config.py::TestModelConfigLoading::test_load_distilroberta_config PASSED
tests/config/unit/test_model_config.py::TestModelConfigLoading::test_load_deberta_config PASSED
tests/config/unit/test_model_config.py::TestModelConfigOptions::test_backbone_option PASSED
tests/config/unit/test_model_config.py::TestModelConfigOptions::test_tokenizer_option PASSED
tests/config/unit/test_model_config.py::TestModelConfigOptions::test_preprocessing_sequence_length PASSED
tests/config/unit/test_model_config.py::TestModelConfigOptions::test_preprocessing_max_length PASSED
tests/config/unit/test_model_config.py::TestModelConfigOptions::test_preprocessing_tokenization PASSED
tests/config/unit/test_model_config.py::TestModelConfigOptions::test_preprocessing_replace_rare_with_unk PASSED
tests/config/unit/test_model_config.py::TestModelConfigOptions::test_preprocessing_unk_frequency_threshold PASSED
tests/config/unit/test_model_config.py::TestModelConfigOptions::test_preprocessing_keep_stopwords PASSED
tests/config/unit/test_model_config.py::TestModelConfigOptions::test_decoding_use_crf PASSED
tests/config/unit/test_model_config.py::TestModelConfigOptions::test_decoding_crf_learning_rate PASSED
tests/config/unit/test_model_config.py::TestModelConfigOptions::test_loss_use_class_weights PASSED
tests/config/unit/test_model_config.py::TestModelConfigOptions::test_loss_class_weight_smoothing PASSED
tests/config/unit/test_model_config.py::TestModelConfigOptions::test_loss_ignore_index PASSED
tests/config/unit/test_model_config.py::TestModelConfigIntegration::test_model_config_via_experiment_config PASSED
tests/config/unit/test_model_config.py::TestModelConfigIntegration::test_model_config_in_training_config_building PASSED
tests/config/unit/test_model_config.py::TestModelConfigValidation::test_model_config_missing_sections PASSED
tests/config/unit/test_model_config.py::TestModelConfigValidation::test_model_config_partial_preprocessing PASSED
tests/config/unit/test_model_config.py::TestModelConfigValidation::test_model_config_partial_decoding PASSED
tests/config/unit/test_model_config.py::TestModelConfigValidation::test_model_config_partial_loss PASSED
tests/config/unit/test_model_config.py::TestModelConfigValidation::test_model_config_numeric_types PASSED
tests/config/unit/test_model_config.py::TestModelConfigValidation::test_model_config_boolean_types PASSED
tests/config/unit/test_model_config.py::TestModelConfigRealFiles::test_load_real_distilbert_config PASSED
tests/config/unit/test_model_config.py::TestModelConfigRealFiles::test_load_real_distilroberta_config PASSED
tests/config/unit/test_model_config.py::TestModelConfigRealFiles::test_load_real_deberta_config PASSED
tests/config/unit/test_model_config.py::TestModelConfigRealFiles::test_all_real_model_configs_have_required_sections SKIPPED
tests/config/unit/test_naming_yaml.py::TestSchemaVersion::test_schema_version_is_loaded PASSED
tests/config/unit/test_naming_yaml.py::TestSeparatorsExplicit::test_separator_field PASSED
tests/config/unit/test_naming_yaml.py::TestSeparatorsExplicit::test_separator_component PASSED
tests/config/unit/test_naming_yaml.py::TestSeparatorsExplicit::test_separator_version PASSED
tests/config/unit/test_naming_yaml.py::TestVersionFormatExplicit::test_version_format PASSED
tests/config/unit/test_naming_yaml.py::TestVersionFormatExplicit::test_version_separator PASSED
tests/config/unit/test_naming_yaml.py::TestNormalizeExplicit::test_normalize_env_replace PASSED
tests/config/unit/test_naming_yaml.py::TestNormalizeExplicit::test_normalize_env_lowercase PASSED
tests/config/unit/test_naming_yaml.py::TestNormalizeExplicit::test_normalize_model_replace PASSED
tests/config/unit/test_naming_yaml.py::TestNormalizeExplicit::test_normalize_model_lowercase PASSED
tests/config/unit/test_naming_yaml.py::TestValidateExplicit::test_validate_max_length PASSED
tests/config/unit/test_naming_yaml.py::TestValidateExplicit::test_validate_forbidden_chars PASSED
tests/config/unit/test_naming_yaml.py::TestValidateExplicit::test_validate_warn_length PASSED
tests/config/unit/test_naming_yaml.py::TestRunNamesComponentOptions::test_hpo_trial_component_options PASSED
tests/config/unit/test_naming_yaml.py::TestRunNamesComponentOptions::test_hpo_trial_fold_component_options PASSED
tests/config/unit/test_naming_yaml.py::TestRunNamesComponentOptions::test_hpo_refit_component_options PASSED
tests/config/unit/test_naming_yaml.py::TestRunNamesComponentOptions::test_hpo_sweep_semantic_suffix_options PASSED
tests/config/unit/test_naming_yaml.py::TestRunNamesComponentOptions::test_final_training_component_options PASSED
tests/config/unit/test_naming_yaml.py::TestRunNamesComponentOptions::test_benchmarking_component_options PASSED
tests/config/unit/test_naming_yaml.py::TestRunNamesComponentOptions::test_conversion_component_options PASSED
tests/config/unit/test_paths.py::TestLoadPathsConfig::test_load_paths_config_with_file PASSED
tests/config/unit/test_paths.py::TestLoadPathsConfig::test_load_paths_config_without_file PASSED
tests/config/unit/test_paths.py::TestResolveOutputPath::test_resolve_simple_path PASSED
tests/config/unit/test_paths.py::TestResolveOutputPath::test_resolve_cache_subdirectory PASSED
tests/config/unit/test_paths.py::TestResolveOutputPath::test_resolve_path_with_pattern PASSED
tests/config/unit/test_paths.py::TestGetCacheFilePath::test_get_latest_cache_file PASSED
tests/config/unit/test_paths.py::TestGetCacheFilePath::test_get_index_cache_file PASSED
tests/config/unit/test_paths.py::TestGetTimestampedCacheFilename::test_generate_best_config_filename PASSED
tests/config/unit/test_paths.py::TestGetTimestampedCacheFilename::test_generate_final_training_filename PASSED
tests/config/unit/test_paths.py::TestGetCacheStrategyConfig::test_get_strategy_config PASSED
tests/config/unit/test_paths.py::TestSaveCacheWithDualStrategy::test_save_cache_creates_all_files PASSED
tests/config/unit/test_paths.py::TestLoadCacheFile::test_load_latest_cache PASSED
tests/config/unit/test_paths.py::TestLoadCacheFile::test_load_specific_timestamp PASSED
tests/config/unit/test_paths.py::TestLoadCacheFile::test_load_returns_none_when_not_found PASSED
tests/config/unit/test_paths.py::TestPathBuildingV2::test_path_building_v2_hpo PASSED
tests/config/unit/test_paths.py::TestPathBuildingV2::test_path_building_v2_normalized PASSED
tests/config/unit/test_paths.py::TestPathBuildingV2::test_path_building_v2_all_storage_envs PASSED
tests/config/unit/test_paths.py::TestPathBuildingV2::test_path_building_v2_study8_trial8_format PASSED
tests/config/unit/test_paths_yaml.py::TestBaseDirectories::test_base_outputs PASSED
tests/config/unit/test_paths_yaml.py::TestBaseDirectories::test_base_notebooks PASSED
tests/config/unit/test_paths_yaml.py::TestBaseDirectories::test_base_config PASSED
tests/config/unit/test_paths_yaml.py::TestBaseDirectories::test_base_src PASSED
tests/config/unit/test_paths_yaml.py::TestBaseDirectories::test_base_tests PASSED
tests/config/unit/test_paths_yaml.py::TestBaseDirectories::test_base_mlruns PASSED
tests/config/unit/test_paths_yaml.py::TestOutputsSubdirectories::test_outputs_hpo_tests PASSED
tests/config/unit/test_paths_yaml.py::TestOutputsSubdirectories::test_outputs_dry_run PASSED
tests/config/unit/test_paths_yaml.py::TestOutputsSubdirectories::test_outputs_e2e_test PASSED
tests/config/unit/test_paths_yaml.py::TestOutputsSubdirectories::test_outputs_pytest_logs PASSED
tests/config/unit/test_paths_yaml.py::TestFilesConfiguration::test_files_metrics PASSED
tests/config/unit/test_paths_yaml.py::TestFilesConfiguration::test_files_benchmark PASSED
tests/config/unit/test_paths_yaml.py::TestFilesConfiguration::test_files_checkpoint_dir PASSED
tests/config/unit/test_paths_yaml.py::TestFilesConfiguration::test_files_cache_best_config_latest PASSED
tests/config/unit/test_paths_yaml.py::TestFilesConfiguration::test_files_cache_best_config_index PASSED
tests/config/unit/test_paths_yaml.py::TestFilesConfiguration::test_files_cache_final_training_latest PASSED
tests/config/unit/test_paths_yaml.py::TestFilesConfiguration::test_files_cache_final_training_index PASSED
tests/config/unit/test_paths_yaml.py::TestFilesConfiguration::test_files_cache_best_model_selection_latest PASSED
tests/config/unit/test_paths_yaml.py::TestFilesConfiguration::test_files_cache_best_model_selection_index PASSED
tests/config/unit/test_paths_yaml.py::TestFilesConfiguration::test_files_cache_conversion_cache PASSED
tests/config/unit/test_paths_yaml.py::TestCacheStrategiesConfiguration::test_cache_strategies_best_configurations_timestamped_enabled PASSED
tests/config/unit/test_paths_yaml.py::TestCacheStrategiesConfiguration::test_cache_strategies_best_configurations_latest_include_timestamped_ref PASSED
tests/config/unit/test_paths_yaml.py::TestCacheStrategiesConfiguration::test_cache_strategies_best_configurations_index_max_entries PASSED
tests/config/unit/test_paths_yaml.py::TestCacheStrategiesConfiguration::test_cache_strategies_final_training_all_options PASSED
tests/config/unit/test_paths_yaml.py::TestCacheStrategiesConfiguration::test_cache_strategies_best_model_selection_all_options PASSED
tests/config/unit/test_paths_yaml.py::TestDriveConfiguration::test_drive_mount_point PASSED
tests/config/unit/test_paths_yaml.py::TestDriveConfiguration::test_drive_backup_base_dir PASSED
tests/config/unit/test_paths_yaml.py::TestDriveConfiguration::test_drive_auto_restore_on_startup PASSED
tests/config/unit/test_paths_yaml.py::TestDriveConfiguration::test_drive_auto_restore_on_startup_true PASSED
tests/config/unit/test_paths_yaml.py::TestNormalizePathsConfiguration::test_normalize_paths_replace PASSED
tests/config/unit/test_paths_yaml.py::TestNormalizePathsConfiguration::test_normalize_paths_lowercase PASSED
tests/config/unit/test_paths_yaml.py::TestNormalizePathsConfiguration::test_normalize_paths_lowercase_true PASSED
tests/config/unit/test_paths_yaml.py::TestNormalizePathsConfiguration::test_normalize_paths_max_component_length PASSED
tests/config/unit/test_paths_yaml.py::TestNormalizePathsConfiguration::test_normalize_paths_max_path_length PASSED
tests/config/unit/test_paths_yaml.py::TestPatternsConfiguration::test_patterns_best_config_file PASSED
tests/config/unit/test_paths_yaml.py::TestPatternsConfiguration::test_patterns_final_training_cache_file PASSED
tests/config/unit/test_paths_yaml.py::TestPatternsConfiguration::test_patterns_best_model_selection_cache_file PASSED
tests/config/unit/test_run_mode.py::TestGetRunMode::test_get_run_mode_explicit_force_new PASSED
tests/config/unit/test_run_mode.py::TestGetRunMode::test_get_run_mode_explicit_reuse_if_exists PASSED
tests/config/unit/test_run_mode.py::TestGetRunMode::test_get_run_mode_explicit_resume_if_incomplete PASSED
tests/config/unit/test_run_mode.py::TestGetRunMode::test_get_run_mode_default_when_missing PASSED
tests/config/unit/test_run_mode.py::TestGetRunMode::test_get_run_mode_default_when_run_section_missing PASSED
tests/config/unit/test_run_mode.py::TestGetRunMode::test_get_run_mode_custom_default PASSED
tests/config/unit/test_run_mode.py::TestGetRunMode::test_get_run_mode_nested_config PASSED
tests/config/unit/test_run_mode.py::TestGetRunMode::test_get_run_mode_combined_config PASSED
tests/config/unit/test_run_mode.py::TestIsForceNew::test_is_force_new_true PASSED
tests/config/unit/test_run_mode.py::TestIsForceNew::test_is_force_new_false PASSED
tests/config/unit/test_run_mode.py::TestIsForceNew::test_is_force_new_default PASSED
tests/config/unit/test_run_mode.py::TestIsReuseIfExists::test_is_reuse_if_exists_true PASSED
tests/config/unit/test_run_mode.py::TestIsReuseIfExists::test_is_reuse_if_exists_false PASSED
tests/config/unit/test_run_mode.py::TestIsReuseIfExists::test_is_reuse_if_exists_default PASSED
tests/config/unit/test_run_mode.py::TestIsResumeIfIncomplete::test_is_resume_if_incomplete_true PASSED
tests/config/unit/test_run_mode.py::TestIsResumeIfIncomplete::test_is_resume_if_incomplete_false PASSED
tests/config/unit/test_run_mode.py::TestIsResumeIfIncomplete::test_is_resume_if_incomplete_default PASSED
tests/config/unit/test_run_mode.py::TestRunModeIntegration::test_hpo_config_with_run_mode PASSED
tests/config/unit/test_run_mode.py::TestRunModeIntegration::test_combined_hpo_checkpoint_config PASSED
tests/config/unit/test_run_mode.py::TestRunModeIntegration::test_final_training_config_with_run_mode PASSED
tests/config/unit/test_variants.py::TestComputeNextVariant::test_compute_next_variant_hpo_no_existing PASSED
tests/config/unit/test_variants.py::TestComputeNextVariant::test_compute_next_variant_hpo_with_existing PASSED
tests/config/unit/test_variants.py::TestComputeNextVariant::test_compute_next_variant_hpo_custom_base_name PASSED
tests/config/unit/test_variants.py::TestComputeNextVariant::test_compute_next_variant_final_training_no_existing PASSED
tests/config/unit/test_variants.py::TestComputeNextVariant::test_compute_next_variant_final_training_with_existing PASSED
tests/config/unit/test_variants.py::TestFindExistingVariants::test_find_existing_variants_hpo_none PASSED
tests/config/unit/test_variants.py::TestFindExistingVariants::test_find_existing_variants_hpo_implicit_variant_1 PASSED
tests/config/unit/test_variants.py::TestFindExistingVariants::test_find_existing_variants_hpo_explicit_variants PASSED
tests/config/unit/test_variants.py::TestFindExistingVariants::test_find_existing_variants_hpo_mixed_patterns PASSED
tests/config/unit/test_variants.py::TestFindExistingVariants::test_find_existing_variants_final_training PASSED
tests/config/unit/test_variants.py::TestFindExistingVariants::test_find_existing_variants_invalid_process_type PASSED
tests/config/unit/test_variants.py::TestVariantsIntegration::test_hpo_variant_sequence PASSED
tests/config/unit/test_variants.py::TestVariantsIntegration::test_hpo_variant_with_custom_study_name PASSED
tests/conversion/integration/test_conversion_config.py::TestConversionConfig::test_opset_version_passed_to_subprocess 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     script.conversion.orchestration: Output directory: /tmp/pytest-of-codespace/pytest-28/test_opset_version_passed_to_s0/outputs/conversion/test
2026-01-15 00:05:51 INFO     script.conversion.orchestration: Created MLflow run: test_run_name (test_run_id...)
2026-01-15 00:05:51 INFO     script.conversion.orchestration: Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /path/to/checkpoint --config-dir /tmp/pytest-of-codespace/pytest-28/test_opset_version_passed_to_s0/config --backbone distilbert-base-uncased --output-dir /tmp/pytest-of-codespace/pytest-28/test_opset_version_passed_to_s0/outputs/conversion/test --opset-version 19 --run-smoke-test
2026-01-15 00:05:51 WARNING  infrastructure.tracking.mlflow.lifecycle: Could not ensure run test_run_id... is terminated: Run 'test_run_id' not found
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/lifecycle.py", line 130, in ensure_run_terminated
    run = client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'test_run_id' not found
2026-01-15 00:05:51 INFO     script.conversion.orchestration: Conversion completed. ONNX model: /tmp/pytest-of-codespace/pytest-28/test_opset_version_passed_to_s0/outputs/conversion/test/model.onnx
PASSED
tests/conversion/integration/test_conversion_config.py::TestConversionConfig::test_quantization_int8_adds_flag 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     script.conversion.orchestration: Output directory: /tmp/pytest-of-codespace/pytest-28/test_quantization_int8_adds_fl0/outputs/conversion/test
2026-01-15 00:05:51 INFO     script.conversion.orchestration: Created MLflow run: test_run_name (test_run_id...)
2026-01-15 00:05:51 INFO     script.conversion.orchestration: Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /path/to/checkpoint --config-dir /tmp/pytest-of-codespace/pytest-28/test_quantization_int8_adds_fl0/config --backbone distilbert-base-uncased --output-dir /tmp/pytest-of-codespace/pytest-28/test_quantization_int8_adds_fl0/outputs/conversion/test --opset-version 18 --quantize-int8 --run-smoke-test
2026-01-15 00:05:51 WARNING  infrastructure.tracking.mlflow.lifecycle: Could not ensure run test_run_id... is terminated: Run 'test_run_id' not found
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/lifecycle.py", line 130, in ensure_run_terminated
    run = client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'test_run_id' not found
2026-01-15 00:05:51 INFO     script.conversion.orchestration: Conversion completed. ONNX model: /tmp/pytest-of-codespace/pytest-28/test_quantization_int8_adds_fl0/outputs/conversion/test/model_int8.onnx
PASSED
tests/conversion/integration/test_conversion_config.py::TestConversionConfig::test_quantization_none_no_flag 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     script.conversion.orchestration: Output directory: /tmp/pytest-of-codespace/pytest-28/test_quantization_none_no_flag0/outputs/conversion/test
2026-01-15 00:05:51 INFO     script.conversion.orchestration: Created MLflow run: test_run_name (test_run_id...)
2026-01-15 00:05:51 INFO     script.conversion.orchestration: Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /path/to/checkpoint --config-dir /tmp/pytest-of-codespace/pytest-28/test_quantization_none_no_flag0/config --backbone distilbert-base-uncased --output-dir /tmp/pytest-of-codespace/pytest-28/test_quantization_none_no_flag0/outputs/conversion/test --opset-version 18 --run-smoke-test
2026-01-15 00:05:51 WARNING  infrastructure.tracking.mlflow.lifecycle: Could not ensure run test_run_id... is terminated: Run 'test_run_id' not found
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/lifecycle.py", line 130, in ensure_run_terminated
    run = client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'test_run_id' not found
2026-01-15 00:05:51 INFO     script.conversion.orchestration: Conversion completed. ONNX model: /tmp/pytest-of-codespace/pytest-28/test_quantization_none_no_flag0/outputs/conversion/test/model.onnx
PASSED
tests/conversion/integration/test_conversion_config.py::TestConversionConfig::test_run_smoke_test_true_adds_flag 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     script.conversion.orchestration: Output directory: /tmp/pytest-of-codespace/pytest-28/test_run_smoke_test_true_adds_0/outputs/conversion/test
2026-01-15 00:05:51 INFO     script.conversion.orchestration: Created MLflow run: test_run_name (test_run_id...)
2026-01-15 00:05:51 INFO     script.conversion.orchestration: Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /path/to/checkpoint --config-dir /tmp/pytest-of-codespace/pytest-28/test_run_smoke_test_true_adds_0/config --backbone distilbert-base-uncased --output-dir /tmp/pytest-of-codespace/pytest-28/test_run_smoke_test_true_adds_0/outputs/conversion/test --opset-version 18 --run-smoke-test
2026-01-15 00:05:51 WARNING  infrastructure.tracking.mlflow.lifecycle: Could not ensure run test_run_id... is terminated: Run 'test_run_id' not found
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/lifecycle.py", line 130, in ensure_run_terminated
    run = client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'test_run_id' not found
2026-01-15 00:05:51 INFO     script.conversion.orchestration: Conversion completed. ONNX model: /tmp/pytest-of-codespace/pytest-28/test_run_smoke_test_true_adds_0/outputs/conversion/test/model.onnx
PASSED
tests/conversion/integration/test_conversion_config.py::TestConversionConfig::test_run_smoke_test_false_no_flag 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     script.conversion.orchestration: Output directory: /tmp/pytest-of-codespace/pytest-28/test_run_smoke_test_false_no_f0/outputs/conversion/test
2026-01-15 00:05:51 INFO     script.conversion.orchestration: Created MLflow run: test_run_name (test_run_id...)
2026-01-15 00:05:51 INFO     script.conversion.orchestration: Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /path/to/checkpoint --config-dir /tmp/pytest-of-codespace/pytest-28/test_run_smoke_test_false_no_f0/config --backbone distilbert-base-uncased --output-dir /tmp/pytest-of-codespace/pytest-28/test_run_smoke_test_false_no_f0/outputs/conversion/test --opset-version 18
2026-01-15 00:05:51 WARNING  infrastructure.tracking.mlflow.lifecycle: Could not ensure run test_run_id... is terminated: Run 'test_run_id' not found
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/lifecycle.py", line 130, in ensure_run_terminated
    run = client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'test_run_id' not found
2026-01-15 00:05:51 INFO     script.conversion.orchestration: Conversion completed. ONNX model: /tmp/pytest-of-codespace/pytest-28/test_run_smoke_test_false_no_f0/outputs/conversion/test/model.onnx
PASSED
tests/conversion/integration/test_conversion_config.py::TestConversionConfig::test_filename_pattern_used_in_find_onnx_model 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:51 INFO     script.conversion.orchestration: Output directory: /tmp/pytest-of-codespace/pytest-28/test_filename_pattern_used_in_0/outputs/conversion/test
2026-01-15 00:05:51 INFO     script.conversion.orchestration: Created MLflow run: test_run_name (test_run_id...)
2026-01-15 00:05:51 INFO     script.conversion.orchestration: Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /path/to/checkpoint --config-dir /tmp/pytest-of-codespace/pytest-28/test_filename_pattern_used_in_0/config --backbone distilbert-base-uncased --output-dir /tmp/pytest-of-codespace/pytest-28/test_filename_pattern_used_in_0/outputs/conversion/test --opset-version 18 --run-smoke-test
2026-01-15 00:05:51 WARNING  infrastructure.tracking.mlflow.lifecycle: Could not ensure run test_run_id... is terminated: Run 'test_run_id' not found
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/lifecycle.py", line 130, in ensure_run_terminated
    run = client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'test_run_id' not found
2026-01-15 00:05:51 INFO     script.conversion.orchestration: Conversion completed. ONNX model: /tmp/pytest-of-codespace/pytest-28/test_filename_pattern_used_in_0/outputs/conversion/test/custom_fp32_model.onnx
PASSED
tests/conversion/unit/test_conversion_config.py::TestConversionConfigLoading::test_load_yaml_loads_conversion_config PASSED
tests/conversion/unit/test_conversion_config.py::TestConversionConfigLoading::test_conversion_config_structure PASSED
tests/conversion/unit/test_conversion_config.py::TestConversionConfigLoading::test_conversion_config_default_values PASSED
tests/conversion/unit/test_conversion_config.py::TestConversionConfigLoading::test_conversion_config_custom_values PASSED
tests/conversion/unit/test_conversion_config.py::TestConversionConfigLoading::test_conversion_config_missing_sections PASSED
tests/conversion/unit/test_conversion_config.py::TestConversionConfigLoading::test_conversion_config_types PASSED
tests/conversion/unit/test_conversion_config.py::TestConversionConfigLoading::test_load_actual_conversion_yaml PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_target_format_extraction PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_target_format_default PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_onnx_opset_version_extraction PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_onnx_opset_version_custom PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_onnx_opset_version_default PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_onnx_quantization_extraction PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_onnx_quantization_int8 PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_onnx_quantization_dynamic PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_onnx_quantization_default PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_onnx_run_smoke_test_extraction PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_onnx_run_smoke_test_false PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_onnx_run_smoke_test_default PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_output_filename_pattern_extraction PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_output_filename_pattern_custom PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_output_filename_pattern_default PASSED
tests/conversion/unit/test_conversion_options.py::TestConversionOptions::test_all_options_together PASSED
tests/evaluation/selection/integration/test_benchmarking_checkpoint_resolution.py::TestCheckpointResolutionFromLocalDisk::test_resolve_checkpoint_from_champion_checkpoint_path PASSED
tests/evaluation/selection/integration/test_benchmarking_checkpoint_resolution.py::TestCheckpointResolutionFromLocalDisk::test_resolve_checkpoint_from_trial_dir PASSED
tests/evaluation/selection/integration/test_benchmarking_checkpoint_resolution.py::TestCheckpointResolutionFromLocalDisk::test_resolve_checkpoint_from_legacy_trial_structure PASSED
tests/evaluation/selection/integration/test_benchmarking_checkpoint_resolution.py::TestCheckpointResolutionFromMLflow::test_resolve_from_mlflow_refit_run PASSED
tests/evaluation/selection/integration/test_benchmarking_checkpoint_resolution.py::TestCheckpointResolutionFromMLflow::test_resolve_from_mlflow_parent_run PASSED
tests/evaluation/selection/integration/test_benchmarking_checkpoint_resolution.py::TestBenchmarkingWorkflowIntegration::test_benchmarking_with_local_checkpoint PASSED
tests/evaluation/selection/integration/test_benchmarking_checkpoint_resolution.py::TestBenchmarkingWorkflowIntegration::test_benchmarking_with_mlflow_checkpoint PASSED
tests/evaluation/selection/integration/test_checkpoint_acquisition_for_benchmarking.py::TestRefitRunDiscovery::test_find_refit_run_by_hash_match PASSED
tests/evaluation/selection/integration/test_checkpoint_acquisition_for_benchmarking.py::TestRefitRunDiscovery::test_find_refit_run_by_study_hash_only PASSED
tests/evaluation/selection/integration/test_checkpoint_acquisition_for_benchmarking.py::TestRefitRunDiscovery::test_find_refit_run_via_parent_relationship PASSED
tests/evaluation/selection/integration/test_checkpoint_acquisition_for_benchmarking.py::TestRefitRunDiscovery::test_find_any_refit_run_as_last_resort PASSED
tests/evaluation/selection/integration/test_checkpoint_acquisition_for_benchmarking.py::TestCheckpointArtifactDiscovery::test_checkpoint_in_refit_run PASSED
tests/evaluation/selection/integration/test_checkpoint_acquisition_for_benchmarking.py::TestCheckpointArtifactDiscovery::test_checkpoint_in_parent_hpo_run PASSED
tests/evaluation/selection/integration/test_checkpoint_acquisition_for_benchmarking.py::TestCheckpointArtifactDiscovery::test_no_checkpoint_in_trial_run PASSED
tests/evaluation/selection/integration/test_checkpoint_acquisition_for_benchmarking.py::TestCheckpointArtifactDiscovery::test_checkpoint_artifact_paths PASSED
tests/evaluation/selection/integration/test_checkpoint_acquisition_for_benchmarking.py::TestCheckpointAcquisitionWorkflow::test_acquire_from_refit_run_success PASSED
tests/evaluation/selection/integration/test_checkpoint_acquisition_for_benchmarking.py::TestCheckpointAcquisitionWorkflow::test_acquire_from_parent_hpo_run_fallback PASSED
tests/evaluation/selection/integration/test_checkpoint_acquisition_for_benchmarking.py::TestCheckpointAcquisitionWorkflow::test_no_checkpoint_found_anywhere PASSED
tests/evaluation/selection/integration/test_checkpoint_acquisition_for_benchmarking.py::TestEdgeCases::test_parent_run_id_not_available PASSED
tests/evaluation/selection/integration/test_checkpoint_acquisition_for_benchmarking.py::TestEdgeCases::test_refit_run_without_trial_key_hash PASSED
tests/evaluation/selection/integration/test_checkpoint_acquisition_for_benchmarking.py::TestEdgeCases::test_multiple_refit_runs PASSED
tests/evaluation/selection/test_champion_selection.py::TestSelectChampionPerBackbone::test_successful_selection_v2 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: Found 3 runs with stage tag for distilbert (backbone=distilbert)
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: Grouped runs for distilbert: 0 v1 group(s), 1 v2 group(s)
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: Found 1 eligible group(s) for distilbert (0 skipped due to min_trials requirement)
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: Found refit run refit-run-12... for champion trial run2... (selected latest from 1 refit run(s))
PASSED
tests/evaluation/selection/test_champion_selection.py::TestSelectChampionPerBackbone::test_successful_selection_v1 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: Found 3 runs with stage tag for distilbert (backbone=distilbert)
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: Found 1 eligible group(s) for distilbert (0 skipped due to min_trials requirement)
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: Found refit run refit-run-12... for champion trial run2... (selected latest from 1 refit run(s))
PASSED
tests/evaluation/selection/test_champion_selection.py::TestSelectChampionPerBackbone::test_no_runs_returns_none 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: No runs found with stage='hpo_trial' for distilbert, trying legacy stage='hpo'
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: Found 0 runs with stage tag for distilbert (backbone=distilbert)
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: Grouped runs for distilbert: 0 v1 group(s), 0 v2 group(s)
2026-01-15 00:05:52 WARNING  evaluation.selection.trial_finder: No valid groups found for distilbert. No trial runs found in HPO experiment 'test_hpo_experiment'. This may indicate:
  - HPO was not run for this backbone
  - Runs exist but don't have required tags (stage='hpo_trial' or 'hpo', backbone tag)
  - Runs exist but were filtered out (missing metrics, artifacts, or grouping tags)
Skipping champion selection for distilbert.
PASSED
tests/evaluation/selection/test_champion_selection.py::TestSelectChampionPerBackbone::test_insufficient_trials_returns_none 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: Found 2 runs with stage tag for distilbert (backbone=distilbert)
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)
2026-01-15 00:05:52 WARNING  evaluation.selection.trial_finder: Skipping group hash1... - only 2 valid trials (minimum: 3)
2026-01-15 00:05:52 WARNING  evaluation.selection.trial_finder: No eligible groups for distilbert. Processed 1 group(s), but 1 were skipped due to insufficient trials (minimum: 3). Remaining groups: 0
PASSED
tests/evaluation/selection/test_champion_selection.py::TestSelectChampionPerBackbone::test_missing_metrics_filtered 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: Found 3 runs with stage tag for distilbert (backbone=distilbert)
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)
2026-01-15 00:05:52 WARNING  evaluation.selection.trial_finder: Group hash1...: 2 valid metrics, 1 missing/invalid. Missing metric runs: ['run3']
2026-01-15 00:05:52 WARNING  evaluation.selection.trial_finder: Skipping group hash1... - only 2 valid trials (minimum: 3)
2026-01-15 00:05:52 WARNING  evaluation.selection.trial_finder: No eligible groups for distilbert. Processed 1 group(s), but 1 were skipped due to insufficient trials (minimum: 3). Remaining groups: 0
PASSED
tests/evaluation/selection/test_champion_selection.py::TestSelectChampionPerBackbone::test_nan_metrics_filtered 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: Found 3 runs with stage tag for distilbert (backbone=distilbert)
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)
2026-01-15 00:05:52 WARNING  evaluation.selection.trial_finder: Group hash1...: 2 valid metrics, 1 missing/invalid. Missing metric runs: []
2026-01-15 00:05:52 WARNING  evaluation.selection.trial_finder: Skipping group hash1... - only 2 valid trials (minimum: 3)
2026-01-15 00:05:52 WARNING  evaluation.selection.trial_finder: No eligible groups for distilbert. Processed 1 group(s), but 1 were skipped due to insufficient trials (minimum: 3). Remaining groups: 0
PASSED
tests/evaluation/selection/test_champion_selection.py::TestSelectChampionPerBackbone::test_artifact_availability_filter 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: Found 3 runs with stage tag for distilbert (backbone=distilbert)
2026-01-15 00:05:52 WARNING  evaluation.selection.trial_finder: Artifact filter: 1 run(s) have code.artifact.available='false' (explicitly marked as unavailable)
2026-01-15 00:05:52 WARNING  evaluation.selection.trial_finder: Artifact filter: 1 run(s) excluded (1 explicitly false, 0 missing/legacy allowed)
2026-01-15 00:05:52 WARNING  evaluation.selection.trial_finder: Artifact filter removed 1 runs for distilbert (2 remaining). Check that runs have 'code.artifact.available' tag set to 'true'.
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)
2026-01-15 00:05:52 WARNING  evaluation.selection.trial_finder: Skipping group hash1... - only 2 valid trials (minimum: 3)
2026-01-15 00:05:52 WARNING  evaluation.selection.trial_finder: No eligible groups for distilbert. Processed 1 group(s), but 1 were skipped due to insufficient trials (minimum: 3). Remaining groups: 0
PASSED
tests/evaluation/selection/test_champion_selection.py::TestSelectChampionPerBackbone::test_no_artifact_requirement 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: Found 3 runs with stage tag for distilbert (backbone=distilbert)
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: Found 1 eligible group(s) for distilbert (0 skipped due to min_trials requirement)
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: Found refit run refit-run-12... for champion trial run2... (selected latest from 1 refit run(s))
PASSED
tests/evaluation/selection/test_champion_selection.py::TestSelectChampionPerBackbone::test_never_mix_v1_v2_when_disabled 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: Found 5 runs with stage tag for distilbert (backbone=distilbert)
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: Grouped runs for distilbert: 1 v1 group(s), 1 v2 group(s)
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: Found both v1 and v2 runs for distilbert. Using 2.0 groups only (never mixing versions).
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: Found 1 eligible group(s) for distilbert (0 skipped due to min_trials requirement)
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: Found refit run refit-run-12... for champion trial run3... (selected latest from 1 refit run(s))
PASSED
tests/evaluation/selection/test_champion_selection.py::TestSelectChampionPerBackbone::test_minimize_objective 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: Found 3 runs with stage tag for distilbert (backbone=distilbert)
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: Found 1 eligible group(s) for distilbert (0 skipped due to min_trials requirement)
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: Found refit run refit-run-12... for champion trial run3... (selected latest from 1 refit run(s))
PASSED
tests/evaluation/selection/test_champion_selection.py::TestSelectChampionPerBackbone::test_legacy_goal_key_migration 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: Found 3 runs with stage tag for distilbert (backbone=distilbert)
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: Found 1 eligible group(s) for distilbert (0 skipped due to min_trials requirement)
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: Found refit run refit-run-12... for champion trial run2... (selected latest from 1 refit run(s))
PASSED
tests/evaluation/selection/test_champion_selection.py::TestSelectChampionPerBackbone::test_multiple_groups_selects_best 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: Found 6 runs with stage tag for distilbert (backbone=distilbert)
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: Grouped runs for distilbert: 2 v1 group(s), 0 v2 group(s)
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: Found 2 eligible group(s) for distilbert (0 skipped due to min_trials requirement)
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: Found refit run refit-run-12... for champion trial run6... (selected latest from 1 refit run(s))
PASSED
tests/evaluation/selection/test_champion_selection.py::TestSelectChampionPerBackbone::test_stable_score_computation 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: Found 5 runs with stage tag for distilbert (backbone=distilbert)
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: Found 1 eligible group(s) for distilbert (0 skipped due to min_trials requirement)
2026-01-15 00:05:52 INFO     evaluation.selection.trial_finder: Found refit run refit-run-12... for champion trial run3... (selected latest from 1 refit run(s))
PASSED
tests/final_training/integration/test_final_training_component.py::test_execute_final_training_reuse_if_exists_skips_training  Final training config loaded from final_training.yaml
 Output directory: /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_re0/outputs/v1
 Found existing completed final training run
  Output directory: /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_re0/outputs/v1
  Checkpoint: /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_re0/outputs/v1/checkpoint
  Reusing existing checkpoint (run.mode: reuse_if_exists)
PASSED
tests/final_training/integration/test_final_training_component.py::test_execute_final_training_force_new_runs_training  Final training config loaded from final_training.yaml
 Output directory: /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_fo0/outputs/v1

-------------------------------- live log call ---------------------------------
2026-01-15 00:05:52 WARNING  root: Malformed experiment '3'. Detailed error Yaml file '/workspaces/resume-ner-azureml/mlruns/3/meta.yaml' does not exist.
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 368, in search_experiments
    exp = self._get_experiment(exp_id, view_type)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 466, in _get_experiment
    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 1636, in _read_yaml
    return _read_helper(root, file_name, attempts_remaining=retries)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 1629, in _read_helper
    result = read_yaml(root, file_name)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/utils/yaml_utils.py", line 107, in read_yaml
    raise MissingConfigException(f"Yaml file '{file_path}' does not exist.")
mlflow.exceptions.MissingConfigException: Yaml file '/workspaces/resume-ner-azureml/mlruns/3/meta.yaml' does not exist.
2026-01-15 00:05:52 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_fo0/config/tags.yaml, using defaults
2026-01-15 00:05:52 INFO     training.execution.mlflow_setup:  View run test_run_name at: file:///tmp/mlflow/#/experiments/314291386468007766/runs/509af58e16c246fba3d96307c6832ba4
2026-01-15 00:05:52 INFO     training.execution.mlflow_setup: Created MLflow run: test_run_name (509af58e16c2...)
 Created MLflow run: test_run_name (509af58e16c2...)
 Running final training...
2026-01-15 00:05:52 INFO     infrastructure.tracking.mlflow.lifecycle: Run 509af58e16c2... already terminated with status <Mock name='mock.get_run().info.status' id='124894361456016'> (expected FINISHED)
2026-01-15 00:05:52 WARNING  infrastructure.paths.resolve: Pattern 'final_training_v2' not found in paths.yaml. Using fallback logic.
 Saved metadata to: /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_fo0/outputs/final_training/local/distilbert/spec_spec123_exec_exec456/v1/metadata.json
 Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_fo0/outputs/v1/checkpoint
 MLflow run: 509af58e16c2...
PASSED
tests/final_training/integration/test_final_training_component.py::test_execute_final_training_resume_if_incomplete_continues  Final training config loaded from final_training.yaml
 Output directory: /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_re1/outputs/v1

-------------------------------- live log call ---------------------------------
2026-01-15 00:05:52 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_re1/config/tags.yaml, using defaults
2026-01-15 00:05:52 INFO     training.execution.mlflow_setup:  View run test_run_name at: file:///tmp/mlflow/#/experiments/314291386468007766/runs/f8c16e4c3d5c416595e2bb89699a68a0
2026-01-15 00:05:52 INFO     training.execution.mlflow_setup: Created MLflow run: test_run_name (f8c16e4c3d5c...)
 Created MLflow run: test_run_name (f8c16e4c3d5c...)
 Running final training...
2026-01-15 00:05:52 INFO     infrastructure.tracking.mlflow.lifecycle: Run f8c16e4c3d5c... already terminated with status <Mock name='mock.get_run().info.status' id='124894361700144'> (expected FINISHED)
2026-01-15 00:05:52 WARNING  infrastructure.paths.resolve: Pattern 'final_training_v2' not found in paths.yaml. Using fallback logic.
 Saved metadata to: /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_re1/outputs/final_training/local/distilbert/spec_spec123_exec_exec456/v1/metadata.json
 Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_re1/outputs/v1/checkpoint
 MLflow run: f8c16e4c3d5c...
PASSED
tests/final_training/integration/test_final_training_component.py::test_execute_final_training_missing_dataset_raises_error  Final training config loaded from final_training.yaml
 Output directory: /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_mi0/outputs/v1
PASSED
tests/final_training/integration/test_final_training_component.py::test_execute_final_training_local_path_override  Final training config loaded from final_training.yaml
 Output directory: /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_lo0/outputs/v1

-------------------------------- live log call ---------------------------------
2026-01-15 00:05:52 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_lo0/config/tags.yaml, using defaults
2026-01-15 00:05:52 INFO     training.execution.mlflow_setup:  View run test_run_name at: file:///tmp/mlflow/#/experiments/314291386468007766/runs/b353d126578349999acc8a39ed05e413
2026-01-15 00:05:52 INFO     training.execution.mlflow_setup: Created MLflow run: test_run_name (b353d1265783...)
 Created MLflow run: test_run_name (b353d1265783...)
 Running final training...
2026-01-15 00:05:52 INFO     infrastructure.tracking.mlflow.lifecycle: Run b353d1265783... already terminated with status <Mock name='mock.get_run().info.status' id='124894361705232'> (expected FINISHED)
2026-01-15 00:05:52 WARNING  infrastructure.paths.resolve: Pattern 'final_training_v2' not found in paths.yaml. Using fallback logic.
 Saved metadata to: /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_lo0/outputs/final_training/local/distilbert/spec_spec123_exec_exec456/v1/metadata.json
 Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_lo0/outputs/v1/checkpoint
 MLflow run: b353d1265783...
PASSED
tests/final_training/integration/test_final_training_component.py::test_execute_final_training_training_failure_marks_run_failed  Final training config loaded from final_training.yaml
 Output directory: /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_tr0/outputs/v1

-------------------------------- live log call ---------------------------------
2026-01-15 00:05:52 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_tr0/config/tags.yaml, using defaults
2026-01-15 00:05:52 INFO     training.execution.mlflow_setup:  View run test_run_name at: file:///tmp/mlflow/#/experiments/314291386468007766/runs/61243bd7c32340e9b4e0b2c851de03ca
2026-01-15 00:05:52 INFO     training.execution.mlflow_setup: Created MLflow run: test_run_name (61243bd7c323...)
 Created MLflow run: test_run_name (61243bd7c323...)
 Running final training...
2026-01-15 00:05:52 INFO     infrastructure.tracking.mlflow.lifecycle: Run 61243bd7c323... already has status <Mock name='mock.get_run().info.status' id='124894361713968'>, skipping termination (expected RUNNING)
PASSED
tests/final_training/integration/test_final_training_component.py::test_execute_final_training_mlflow_disabled_skips_tracking  Final training config loaded from final_training.yaml
 Output directory: /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_ml0/outputs/v1

-------------------------------- live log call ---------------------------------
2026-01-15 00:05:52 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_ml0/config/tags.yaml, using defaults
2026-01-15 00:05:52 INFO     training.execution.mlflow_setup:  View run test_run_name at: file:///tmp/mlflow/#/experiments/314291386468007766/runs/0af4a2761b074926838a53bbf33cb064
2026-01-15 00:05:52 INFO     training.execution.mlflow_setup: Created MLflow run: test_run_name (0af4a2761b07...)
 Created MLflow run: test_run_name (0af4a2761b07...)
 Running final training...
2026-01-15 00:05:52 INFO     infrastructure.tracking.mlflow.lifecycle: Run 0af4a2761b07... already terminated with status <Mock name='mock.get_run().info.status' id='124894361698560'> (expected FINISHED)
2026-01-15 00:05:52 WARNING  infrastructure.paths.resolve: Pattern 'final_training_v2' not found in paths.yaml. Using fallback logic.
 Saved metadata to: /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_ml0/outputs/final_training/local/distilbert/spec_spec123_exec_exec456/v1/metadata.json
 Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_ml0/outputs/v1/checkpoint
 MLflow run: 0af4a2761b07...
PASSED
tests/final_training/integration/test_final_training_component.py::test_execute_final_training_source_scratch_no_checkpoint  Final training config loaded from final_training.yaml
 Output directory: /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_so0/outputs/v1

-------------------------------- live log call ---------------------------------
2026-01-15 00:05:52 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_so0/config/tags.yaml, using defaults
2026-01-15 00:05:52 INFO     training.execution.mlflow_setup:  View run test_run_name at: file:///tmp/mlflow/#/experiments/314291386468007766/runs/0eea0fa7cbf6486cabb5287adfa34d34
2026-01-15 00:05:52 INFO     training.execution.mlflow_setup: Created MLflow run: test_run_name (0eea0fa7cbf6...)
 Created MLflow run: test_run_name (0eea0fa7cbf6...)
 Running final training...
2026-01-15 00:05:52 INFO     infrastructure.tracking.mlflow.lifecycle: Run 0eea0fa7cbf6... already terminated with status <Mock name='mock.get_run().info.status' id='124894361705376'> (expected FINISHED)
2026-01-15 00:05:52 WARNING  infrastructure.paths.resolve: Pattern 'final_training_v2' not found in paths.yaml. Using fallback logic.
 Saved metadata to: /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_so0/outputs/final_training/local/distilbert/spec_spec123_exec_exec456/v1/metadata.json
 Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_so0/outputs/v1/checkpoint
 MLflow run: 0eea0fa7cbf6...
PASSED
tests/final_training/integration/test_final_training_component.py::test_execute_final_training_source_final_training_with_checkpoint  Final training config loaded from final_training.yaml
 Output directory: /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_so1/outputs/v1

-------------------------------- live log call ---------------------------------
2026-01-15 00:05:52 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_so1/config/tags.yaml, using defaults
2026-01-15 00:05:52 INFO     training.execution.mlflow_setup:  View run test_run_name at: file:///tmp/mlflow/#/experiments/314291386468007766/runs/42cc110841cb4e9c8e9b646ad99d825f
2026-01-15 00:05:52 INFO     training.execution.mlflow_setup: Created MLflow run: test_run_name (42cc110841cb...)
 Created MLflow run: test_run_name (42cc110841cb...)
 Running final training...
2026-01-15 00:05:52 INFO     infrastructure.tracking.mlflow.lifecycle: Run 42cc110841cb... already terminated with status <Mock name='mock.get_run().info.status' id='124894361493488'> (expected FINISHED)
2026-01-15 00:05:52 WARNING  infrastructure.paths.resolve: Pattern 'final_training_v2' not found in paths.yaml. Using fallback logic.
 Saved metadata to: /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_so1/outputs/final_training/local/distilbert/spec_spec123_exec_exec456/v1/metadata.json
 Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_so1/outputs/v1/checkpoint
 MLflow run: 42cc110841cb...
PASSED
tests/final_training/integration/test_final_training_component.py::test_execute_final_training_hyperparameter_precedence  Final training config loaded from final_training.yaml
 Output directory: /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_hy0/outputs/v1

-------------------------------- live log call ---------------------------------
2026-01-15 00:05:52 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_hy0/config/tags.yaml, using defaults
2026-01-15 00:05:52 INFO     training.execution.mlflow_setup:  View run test_run_name at: file:///tmp/mlflow/#/experiments/314291386468007766/runs/98d57ceda9ff41b8bdcf5a779088abab
2026-01-15 00:05:52 INFO     training.execution.mlflow_setup: Created MLflow run: test_run_name (98d57ceda9ff...)
 Created MLflow run: test_run_name (98d57ceda9ff...)
 Running final training...
2026-01-15 00:05:52 INFO     infrastructure.tracking.mlflow.lifecycle: Run 98d57ceda9ff... already terminated with status <Mock name='mock.get_run().info.status' id='124894361714496'> (expected FINISHED)
2026-01-15 00:05:52 WARNING  infrastructure.paths.resolve: Pattern 'final_training_v2' not found in paths.yaml. Using fallback logic.
 Saved metadata to: /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_hy0/outputs/final_training/local/distilbert/spec_spec123_exec_exec456/v1/metadata.json
 Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_hy0/outputs/v1/checkpoint
 MLflow run: 98d57ceda9ff...
PASSED
tests/final_training/integration/test_final_training_component.py::test_execute_final_training_mlflow_overrides  Final training config loaded from final_training.yaml
 Output directory: /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_ml1/outputs/v1

-------------------------------- live log call ---------------------------------
2026-01-15 00:05:52 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_ml1/config/tags.yaml, using defaults
2026-01-15 00:05:52 INFO     training.execution.mlflow_setup:  View run default_run_name at: file:///tmp/mlflow/#/experiments/805178406443610137/runs/fa407fdfa98f4e69bc3e69d1cbc7f34a
2026-01-15 00:05:52 INFO     training.execution.mlflow_setup: Created MLflow run: default_run_name (fa407fdfa98f...)
 Created MLflow run: default_run_name (fa407fdfa98f...)
 Running final training...
2026-01-15 00:05:52 INFO     infrastructure.tracking.mlflow.lifecycle: Run fa407fdfa98f... already terminated with status <Mock name='mock.get_run().info.status' id='124894361495456'> (expected FINISHED)
2026-01-15 00:05:52 WARNING  infrastructure.paths.resolve: Pattern 'final_training_v2' not found in paths.yaml. Using fallback logic.
 Saved metadata to: /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_ml1/outputs/final_training/local/distilbert/spec_spec123_exec_exec456/v1/metadata.json
 Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_ml1/outputs/v1/checkpoint
 MLflow run: fa407fdfa98f...
PASSED
tests/final_training/integration/test_final_training_logging_intervals.py::test_logging_eval_interval_loaded_from_config  Final training config loaded from final_training.yaml
 Output directory: /tmp/pytest-of-codespace/pytest-28/test_logging_eval_interval_loa0/outputs/v1

-------------------------------- live log call ---------------------------------
2026-01-15 00:05:52 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_logging_eval_interval_loa0/config/tags.yaml, using defaults
2026-01-15 00:05:52 INFO     training.execution.mlflow_setup:  View run test_run_name at: file:///tmp/mlflow/#/experiments/314291386468007766/runs/3efa65780ee543e18a53804bc86ddd23
2026-01-15 00:05:52 INFO     training.execution.mlflow_setup: Created MLflow run: test_run_name (3efa65780ee5...)
 Created MLflow run: test_run_name (3efa65780ee5...)
 Running final training...
2026-01-15 00:05:52 INFO     infrastructure.tracking.mlflow.lifecycle: Run 3efa65780ee5... already terminated with status <Mock name='mock.get_run().info.status' id='124894361706000'> (expected FINISHED)
2026-01-15 00:05:52 WARNING  infrastructure.paths.resolve: Pattern 'final_training_v2' not found in paths.yaml. Using fallback logic.
 Saved metadata to: /tmp/pytest-of-codespace/pytest-28/test_logging_eval_interval_loa0/outputs/final_training/local/distilbert/spec_spec123_exec_exec456/v1/metadata.json
 Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-28/test_logging_eval_interval_loa0/outputs/v1/checkpoint
 MLflow run: 3efa65780ee5...
PASSED
tests/final_training/integration/test_final_training_logging_intervals.py::test_logging_save_interval_loaded_from_config  Final training config loaded from final_training.yaml
 Output directory: /tmp/pytest-of-codespace/pytest-28/test_logging_save_interval_loa0/outputs/v1

-------------------------------- live log call ---------------------------------
2026-01-15 00:05:52 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_logging_save_interval_loa0/config/tags.yaml, using defaults
2026-01-15 00:05:52 INFO     training.execution.mlflow_setup:  View run test_run_name at: file:///tmp/mlflow/#/experiments/314291386468007766/runs/5ca51071baa64e2a8ead5acffeca1c88
2026-01-15 00:05:52 INFO     training.execution.mlflow_setup: Created MLflow run: test_run_name (5ca51071baa6...)
 Created MLflow run: test_run_name (5ca51071baa6...)
 Running final training...
2026-01-15 00:05:52 INFO     infrastructure.tracking.mlflow.lifecycle: Run 5ca51071baa6... already terminated with status <Mock name='mock.get_run().info.status' id='124894361496512'> (expected FINISHED)
2026-01-15 00:05:52 WARNING  infrastructure.paths.resolve: Pattern 'final_training_v2' not found in paths.yaml. Using fallback logic.
 Saved metadata to: /tmp/pytest-of-codespace/pytest-28/test_logging_save_interval_loa0/outputs/final_training/local/distilbert/spec_spec123_exec_exec456/v1/metadata.json
 Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-28/test_logging_save_interval_loa0/outputs/v1/checkpoint
 MLflow run: 5ca51071baa6...
PASSED
tests/final_training/integration/test_final_training_logging_intervals.py::test_logging_intervals_both_loaded_from_config  Final training config loaded from final_training.yaml
 Output directory: /tmp/pytest-of-codespace/pytest-28/test_logging_intervals_both_lo0/outputs/v1

-------------------------------- live log call ---------------------------------
2026-01-15 00:05:52 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_logging_intervals_both_lo0/config/tags.yaml, using defaults
2026-01-15 00:05:52 INFO     training.execution.mlflow_setup:  View run test_run_name at: file:///tmp/mlflow/#/experiments/314291386468007766/runs/4999f8f3962e4729b64753c51e15d13b
2026-01-15 00:05:52 INFO     training.execution.mlflow_setup: Created MLflow run: test_run_name (4999f8f3962e...)
 Created MLflow run: test_run_name (4999f8f3962e...)
 Running final training...
2026-01-15 00:05:52 INFO     infrastructure.tracking.mlflow.lifecycle: Run 4999f8f3962e... already terminated with status <Mock name='mock.get_run().info.status' id='124894361489216'> (expected FINISHED)
2026-01-15 00:05:52 WARNING  infrastructure.paths.resolve: Pattern 'final_training_v2' not found in paths.yaml. Using fallback logic.
 Saved metadata to: /tmp/pytest-of-codespace/pytest-28/test_logging_intervals_both_lo0/outputs/final_training/local/distilbert/spec_spec123_exec_exec456/v1/metadata.json
 Final training completed. Checkpoint: /tmp/pytest-of-codespace/pytest-28/test_logging_intervals_both_lo0/outputs/v1/checkpoint
 MLflow run: 4999f8f3962e...
PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestSourceParentDictFormat::test_source_parent_dict_format_resolves_checkpoint PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestSourceParentDictFormat::test_source_parent_dict_format_with_validation_false PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestCheckpointSource::test_checkpoint_source_string_path PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestCheckpointSource::test_checkpoint_source_relative_path PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestCheckpointSource::test_checkpoint_source_dict_format PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestCheckpointSource::test_checkpoint_source_validation_fails PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestCheckpointSource::test_checkpoint_source_validation_false_allows_invalid PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestSeedRandomSeed::test_seed_random_seed_override PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestSeedRandomSeed::test_seed_falls_back_to_train_config PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestSeedRandomSeed::test_seed_falls_back_to_default PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestSeedRandomSeed::test_seed_precedence_final_training_over_best_config PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestVariantNumber::test_variant_number_explicit_override PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestVariantNumber::test_variant_number_ignored_when_force_new PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestVariantNumber::test_variant_number_none_auto_increments PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestEarlyStopping::test_early_stopping_enabled_override PASSED
tests/final_training/unit/test_final_training_config_critical.py::TestEarlyStopping::test_early_stopping_patience_override PASSED
tests/hpo/integration/test_best_trial_selection_component.py::TestBestTrialSelection::test_best_trial_selected_by_optuna PASSED
tests/hpo/integration/test_best_trial_selection_component.py::TestBestTrialSelection::test_best_trial_extraction PASSED
tests/hpo/integration/test_best_trial_selection_component.py::TestBestTrialSelection::test_best_trial_with_cv_statistics PASSED
tests/hpo/integration/test_best_trial_selection_component.py::TestBestTrialSelection::test_best_trial_minimization_direction PASSED
tests/hpo/integration/test_best_trial_selection_component.py::TestBestTrialSelectionWithCriteria::test_selection_with_accuracy_threshold PASSED
tests/hpo/integration/test_best_trial_selection_component.py::TestBestTrialSelectionWithCriteria::test_selection_with_min_accuracy_gain PASSED
tests/hpo/integration/test_best_trial_selection_component.py::TestBestTrialSelectionWithCriteria::test_selection_accuracy_only_when_outside_threshold PASSED
tests/hpo/integration/test_best_trial_selection_component.py::TestBestTrialSelectionWithCriteria::test_selection_smoke_yaml_parameters PASSED
tests/hpo/integration/test_best_trial_selection_component.py::TestBestTrialIntegration::test_best_trial_used_for_refit PASSED
tests/hpo/integration/test_best_trial_selection_component.py::TestBestTrialIntegration::test_best_trial_with_no_completed_trials PASSED
tests/hpo/integration/test_best_trial_selection_component.py::TestBestTrialIntegration::test_best_trial_preserves_hyperparameters PASSED
tests/hpo/integration/test_early_termination.py::TestPrunerCreation::test_create_pruner_bandit_policy PASSED
tests/hpo/integration/test_early_termination.py::TestPrunerCreation::test_create_pruner_median_policy PASSED
tests/hpo/integration/test_early_termination.py::TestPrunerCreation::test_create_pruner_no_early_termination PASSED
tests/hpo/integration/test_early_termination.py::TestPrunerCreation::test_create_pruner_smoke_yaml_params PASSED
tests/hpo/integration/test_early_termination.py::TestPruningBehavior::test_pruner_delays_evaluation PASSED
tests/hpo/integration/test_early_termination.py::TestPruningBehavior::test_pruner_prunes_poor_trials PASSED
tests/hpo/integration/test_early_termination.py::TestPruningBehavior::test_pruner_evaluation_interval PASSED
tests/hpo/integration/test_early_termination.py::TestPruningBehavior::test_pruner_with_study_manager 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:53 INFO     training.hpo.core.study: [HPO] Starting optimization for distilbert with checkpointing...
PASSED
tests/hpo/integration/test_early_termination.py::TestPruningIntegration::test_pruning_preserves_best_trials PASSED
tests/hpo/integration/test_early_termination.py::TestPruningIntegration::test_pruning_with_checkpoint_resume 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:54 INFO     training.hpo.core.study: [HPO] Starting optimization for distilbert with checkpointing...
2026-01-15 00:05:54 INFO     training.hpo.core.study: [HPO] Resuming optimization for distilbert from checkpoint...
2026-01-15 00:05:54 INFO     training.hpo.core.study: Loaded 1 existing trials (1 completed, 0 marked as failed)
PASSED
tests/hpo/integration/test_early_termination.py::TestPruningIntegration::test_pruning_disabled_behavior PASSED
tests/hpo/integration/test_early_termination.py::TestPruningSmokeYaml::test_pruning_smoke_yaml_config PASSED
tests/hpo/integration/test_error_handling.py::TestTrialExecutionErrors::test_training_subprocess_failure PASSED
tests/hpo/integration/test_error_handling.py::TestTrialExecutionErrors::test_training_module_not_found 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:54 WARNING  infrastructure.paths.utils: Could not find project root with src/training/ directory after 5 levels. Using /tmp/pytest-of-codespace/pytest-28/test_training_module_not_found0 as root_dir. Config_dir: /tmp/pytest-of-codespace/pytest-28/test_training_module_not_found0/config
PASSED
tests/hpo/integration/test_error_handling.py::TestTrialExecutionErrors::test_missing_metrics_file 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:54 ERROR    training.hpo.trial.metrics: metrics.json not found at expected location: /tmp/pytest-of-codespace/pytest-28/test_missing_metrics_file0/outputs/hpo/local/distilbert/study-abc12345/trial-def67890/metrics.json. Trial output dir: /tmp/pytest-of-codespace/pytest-28/test_missing_metrics_file0/outputs/hpo/local/distilbert/study-abc12345/trial-def67890, Root dir: /tmp/pytest-of-codespace/pytest-28/test_missing_metrics_file0
PASSED
tests/hpo/integration/test_error_handling.py::TestTrialExecutionErrors::test_missing_objective_metric_in_metrics PASSED
tests/hpo/integration/test_error_handling.py::TestCVOrchestratorErrors::test_cv_trial_failure_propagates_error 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:54 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-28/test_cv_trial_failure_propagat0/config/naming.yaml, using empty policy
2026-01-15 00:05:54 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_cv_trial_failure_propagat0/config/tags.yaml, using defaults
2026-01-15 00:05:54 WARNING  infrastructure.paths.resolve: Pattern 'hpo_v2' not found in paths.yaml. Using fallback logic.
2026-01-15 00:05:54 INFO     training.hpo.execution.local.cv: [CV] Created trial folder: /tmp/pytest-of-codespace/pytest-28/test_cv_trial_failure_propagat0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-d7c6e3fb (trial 0)
PASSED
tests/hpo/integration/test_error_handling.py::TestCVOrchestratorErrors::test_cv_missing_trial_key_hash_raises_error 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:54 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-28/test_cv_missing_trial_key_hash0/config/naming.yaml, using empty policy
2026-01-15 00:05:54 WARNING  training.hpo.execution.local.cv: Could not build systematic run name and tags: HPO trial run name built without study_key_hash; check study identity propagation., using fallback
2026-01-15 00:05:54 ERROR    training.hpo.execution.local.cv: In v2 study folder study-abc12345 but trial_key_hash is None. study_key_hash=NO. Will attempt to compute hash.
2026-01-15 00:05:54 WARNING  training.hpo.execution.local.cv: In v2 study folder but missing hashes: study_key_hash=NO, trial_key_hash=NO
2026-01-15 00:05:54 ERROR    training.hpo.execution.local.cv: ERROR: In v2 study folder study-abc12345 but computed_trial_key_hash is None. study_key_hash=None, trial_params={'learning_rate': 3e-05, 'batch_size': 4, 'backbone': 'distilbert', 'trial_number': 0}. Attempting to compute trial_key_hash from trial_params...
2026-01-15 00:05:54 ERROR    training.hpo.execution.local.cv: CRITICAL: Failed to compute trial_key_hash for v2 study folder study-abc12345. study_key_hash=NO, trial_params keys: ['learning_rate', 'batch_size', 'backbone', 'trial_number']. Error: Cannot compute trial_key_hash without study_key_hash
2026-01-15 00:05:54 ERROR    training.hpo.execution.local.cv: Traceback: Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/cv.py", line 292, in run_training_trial_with_cv
    raise ValueError("Cannot compute trial_key_hash without study_key_hash")
ValueError: Cannot compute trial_key_hash without study_key_hash

PASSED
tests/hpo/integration/test_error_handling.py::TestRefitExecutionErrors::test_refit_subprocess_failure 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:54 INFO     training.hpo.execution.local.refit: [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'batch_size': 4, 'dropout': 0.2, 'weight_decay': 0.05, 'backbone': 'distilbert'}
2026-01-15 00:05:54 INFO     training.hpo.execution.local.refit: [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-15 00:05:54 WARNING  infrastructure.paths.utils: Could not find project root with src/training/ directory after 5 levels. Using /tmp/pytest-of-codespace/pytest-28/test_refit_subprocess_failure0 as root_dir. Config_dir: /tmp/pytest-of-codespace/pytest-28/test_refit_subprocess_failure0/config
2026-01-15 00:05:54 WARNING  infrastructure.paths.resolve: Pattern 'hpo_v2' not found in paths.yaml. Using fallback logic.
2026-01-15 00:05:54 WARNING  infrastructure.naming.display_policy: [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-28/test_refit_subprocess_failure0/config/naming.yaml, using empty policy
2026-01-15 00:05:54 INFO     infrastructure.naming.mlflow.tags_registry: [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_refit_subprocess_failure0/config/tags.yaml, using defaults
2026-01-15 00:05:54 WARNING  training.execution.mlflow_setup: Could not get parent run parent_123, using experiment directly: Run 'parent_123' not found
2026-01-15 00:05:54 INFO     training.execution.mlflow_setup:  View run local_distilbert_hpo_refit_legacy at: file:///tmp/mlflow/#/experiments/989272176793454112/runs/9da9b6307d3845ed9f011aa5f1080c41
2026-01-15 00:05:54 INFO     training.execution.mlflow_setup: Created MLflow run: local_distilbert_hpo_refit_legacy (9da9b6307d38...)
2026-01-15 00:05:54 INFO     training.hpo.execution.local.refit: [REFIT] Attempting to link refit run 9da9b6307d38... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:05:54 ERROR    training.hpo.execution.local.refit: [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
PASSED
tests/hpo/integration/test_error_handling.py::TestRefitExecutionErrors::test_refit_non_v2_study_folder_raises_error 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:54 INFO     training.hpo.execution.local.refit: [REFIT] Starting refit training for trial 0 with hyperparameters: {'backbone': 'distilbert'}
2026-01-15 00:05:54 INFO     training.hpo.execution.local.refit: [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-15 00:05:54 WARNING  training.hpo.execution.local.refit: Could not construct v2 refit folder, falling back to legacy: 'NoneType' object has no attribute 'mkdir'
PASSED
tests/hpo/integration/test_error_handling.py::TestStudyManagerErrors::test_study_creation_with_invalid_storage_uri 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:54 INFO     training.hpo.core.study: [HPO] Starting optimization for distilbert with checkpointing...
PASSED
tests/hpo/integration/test_error_handling.py::TestBestTrialSelectionErrors::test_extract_best_config_with_no_completed_trials PASSED
tests/hpo/integration/test_error_handling.py::TestBestTrialSelectionErrors::test_selection_logic_with_empty_candidates PASSED
tests/hpo/integration/test_error_handling.py::TestSearchSpaceErrors::test_invalid_search_space_type PASSED
tests/hpo/integration/test_error_handling.py::TestSearchSpaceErrors::test_invalid_float_range PASSED
tests/hpo/integration/test_error_handling.py::TestMLflowErrors::test_mlflow_run_creation_failure_handled_gracefully PASSED
tests/hpo/integration/test_error_handling.py::TestConfigurationErrors::test_missing_objective_metric_in_config PASSED
tests/hpo/integration/test_error_handling.py::TestConfigurationErrors::test_invalid_sampling_algorithm PASSED
tests/hpo/integration/test_error_handling.py::TestPathResolutionErrors::test_missing_config_dir_handled_gracefully PASSED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointCreation::test_checkpoint_storage_path_resolution PASSED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointCreation::test_checkpoint_storage_path_with_backbone_placeholder PASSED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointCreation::test_checkpoint_disabled_returns_none PASSED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointCreation::test_storage_uri_conversion PASSED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointResume::test_resume_from_existing_checkpoint 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:55 INFO     training.hpo.core.study: [HPO] Resuming optimization for distilbert from checkpoint...
2026-01-15 00:05:55 INFO     training.hpo.core.study: Loaded 1 existing trials (1 completed, 0 marked as failed)
PASSED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointResume::test_resume_preserves_trials 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:55 INFO     training.hpo.core.study: [HPO] Resuming optimization for distilbert from checkpoint...
2026-01-15 00:05:55 INFO     training.hpo.core.study: Loaded 3 existing trials (3 completed, 0 marked as failed)
PASSED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointResume::test_resume_marks_running_trials_as_failed 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:55 INFO     training.hpo.core.study: [HPO] Resuming optimization for distilbert from checkpoint...
2026-01-15 00:05:55 WARNING  training.hpo.core.study: Found 1 RUNNING trials from previous session. Marking them as FAILED (interrupted).
2026-01-15 00:05:55 INFO     training.hpo.core.study: Loaded 2 existing trials (1 completed, 0 marked as failed)
PASSED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointResume::test_resume_with_auto_resume_false_raises_error 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:55 INFO     training.hpo.core.study: [HPO] auto_resume=false: Creating new study 'hpo_distilbert_no_resume_test' (ignoring existing study)
2026-01-15 00:05:55 INFO     training.hpo.core.study: [HPO] Starting optimization for distilbert with checkpointing...
PASSED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointResume::test_resume_continues_trial_numbering 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:55 INFO     training.hpo.core.study: [HPO] Resuming optimization for distilbert from checkpoint...
2026-01-15 00:05:55 INFO     training.hpo.core.study: Loaded 2 existing trials (2 completed, 0 marked as failed)
PASSED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointSmokeYaml::test_checkpoint_smoke_yaml_study_name_template PASSED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointSmokeYaml::test_checkpoint_smoke_yaml_storage_path_template PASSED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointSmokeYaml::test_checkpoint_smoke_yaml_auto_resume_true 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:56 INFO     training.hpo.core.study: [HPO] Resuming optimization for distilbert from checkpoint...
2026-01-15 00:05:56 INFO     training.hpo.core.study: Loaded 1 existing trials (1 completed, 0 marked as failed)
PASSED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointFileIO::test_checkpoint_file_exists_after_study_creation PASSED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointFileIO::test_checkpoint_file_persists_trials PASSED
tests/hpo/integration/test_hpo_checkpoint_resume.py::TestCheckpointFileIO::test_checkpoint_file_can_be_moved_and_loaded PASSED
tests/hpo/integration/test_hpo_full_workflow.py::TestFullHPOWorkflow::test_full_hpo_workflow_with_cv_and_refit 
-------------------------------- live log call ---------------------------------
2026-01-15 00:05:56 INFO     training.hpo.execution.local.sweep: [EARLY HASH] Computed v2 study_key_hash=01b6aaaa11c2ca02... for folder creation
2026-01-15 00:05:56 INFO     training.hpo.core.study: [HPO] Starting optimization for distilbert with checkpointing...
2026-01-15 00:05:56 WARNING  infrastructure.naming.display_policy: [naming.yaml] Missing required 'run_names' section (/tmp/pytest-of-codespace/pytest-28/test_full_hpo_workflow_with_cv0/config/naming.yaml)
2026-01-15 00:05:56 INFO     training.hpo.execution.local.sweep: [HPO Setup] k_folds=2, fold_splits_file=None, k_fold config: {'enabled': True, 'n_splits': 2, 'random_seed': 42, 'shuffle': True, 'stratified': True}
2026-01-15 00:05:56 INFO     training.hpo.execution.local.sweep: [CV Setup]  Created 2 fold splits and saved to /tmp/pytest-of-codespace/pytest-28/test_full_hpo_workflow_with_cv0/outputs/hpo/local/distilbert/study-01b6aaaa/fold_splits.json
PASSED
tests/hpo/integration/test_hpo_full_workflow.py::TestFullHPOWorkflow::test_full_hpo_workflow_no_cv_no_refit PASSED
tests/hpo/integration/test_hpo_full_workflow.py::TestFullHPOWorkflow::test_full_hpo_workflow_creates_correct_path_structure PASSED
tests/hpo/integration/test_hpo_resume_workflow.py::TestHPOResumeWorkflow::test_resume_workflow_preserves_trials PASSED
tests/hpo/integration/test_hpo_resume_workflow.py::TestHPOResumeWorkflow::test_resume_workflow_with_different_run_id [W 2026-01-15 00:05:59,377] Trial 0 failed with parameters: {'learning_rate': 2.9947757458691608e-05} because of the following error: TypeError("'Mock' object is not iterable").
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py", line 347, in objective
    metric_value = run_training_trial(
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/trial.py", line 212, in run_training_trial
    return executor.execute(
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/trial.py", line 149, in execute
    result = execute_training_subprocess(
  File "/workspaces/resume-ner-azureml/src/training/execution/subprocess_runner.py", line 360, in execute_training_subprocess
    filtered_stdout = _filter_debug_messages(result.stdout)
  File "/workspaces/resume-ner-azureml/src/training/execution/subprocess_runner.py", line 388, in _filter_debug_messages
    for line in lines:
TypeError: 'Mock' object is not iterable
[W 2026-01-15 00:05:59,377] Trial 0 failed with value None.
[W 2026-01-15 00:05:59,574] Trial 1 failed with parameters: {'learning_rate': 2.100128358306658e-05} because of the following error: TypeError("'Mock' object is not iterable").
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py", line 347, in objective
    metric_value = run_training_trial(
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/trial.py", line 212, in run_training_trial
    return executor.execute(
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/trial.py", line 149, in execute
    result = execute_training_subprocess(
  File "/workspaces/resume-ner-azureml/src/training/execution/subprocess_runner.py", line 360, in execute_training_subprocess
    filtered_stdout = _filter_debug_messages(result.stdout)
  File "/workspaces/resume-ner-azureml/src/training/execution/subprocess_runner.py", line 388, in _filter_debug_messages
    for line in lines:
TypeError: 'Mock' object is not iterable
[W 2026-01-15 00:05:59,575] Trial 1 failed with value None.
PASSED
tests/hpo/integration/test_hpo_resume_workflow.py::TestHPOResumeWorkflow::test_resume_workflow_with_cv [W 2026-01-15 00:05:59,824] Trial 0 failed with parameters: {'learning_rate': 1.3372159440586654e-05} because of the following error: RuntimeError('In v2 study folder study-dc0c421a but trial_base_dir uses legacy naming: study-aaaaaaaa. This indicates a bug in the path construction logic. trial_base_dir=/tmp/pytest-of-codespace/pytest-28/test_resume_workflow_with_cv0/outputs/hpo/local/distilbert/study-aaaaaaaa').
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py", line 240, in objective
    average_metric, fold_metrics = run_training_trial_with_cv(
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/cv.py", line 323, in run_training_trial_with_cv
    raise RuntimeError(
RuntimeError: In v2 study folder study-dc0c421a but trial_base_dir uses legacy naming: study-aaaaaaaa. This indicates a bug in the path construction logic. trial_base_dir=/tmp/pytest-of-codespace/pytest-28/test_resume_workflow_with_cv0/outputs/hpo/local/distilbert/study-aaaaaaaa
[W 2026-01-15 00:05:59,825] Trial 0 failed with value None.
[W 2026-01-15 00:06:00,021] Trial 1 failed with parameters: {'learning_rate': 1.2963166586618282e-05} because of the following error: RuntimeError('In v2 study folder study-dc0c421a but trial_base_dir uses legacy naming: study-aaaaaaaa. This indicates a bug in the path construction logic. trial_base_dir=/tmp/pytest-of-codespace/pytest-28/test_resume_workflow_with_cv0/outputs/hpo/local/distilbert/study-aaaaaaaa').
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py", line 240, in objective
    average_metric, fold_metrics = run_training_trial_with_cv(
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/cv.py", line 323, in run_training_trial_with_cv
    raise RuntimeError(
RuntimeError: In v2 study folder study-dc0c421a but trial_base_dir uses legacy naming: study-aaaaaaaa. This indicates a bug in the path construction logic. trial_base_dir=/tmp/pytest-of-codespace/pytest-28/test_resume_workflow_with_cv0/outputs/hpo/local/distilbert/study-aaaaaaaa
[W 2026-01-15 00:06:00,021] Trial 1 failed with value None.
PASSED
tests/hpo/integration/test_hpo_run_mode_variants.py::TestHPORunModeForceNew::test_force_new_creates_variant_1_on_first_run PASSED
tests/hpo/integration/test_hpo_run_mode_variants.py::TestHPORunModeForceNew::test_force_new_creates_variant_2_on_second_run PASSED
tests/hpo/integration/test_hpo_run_mode_variants.py::TestHPORunModeForceNew::test_force_new_creates_variant_3_on_third_run PASSED
tests/hpo/integration/test_hpo_run_mode_variants.py::TestHPORunModeForceNew::test_force_new_with_custom_study_name PASSED
tests/hpo/integration/test_hpo_run_mode_variants.py::TestHPORunModeReuseIfExists::test_reuse_if_exists_uses_base_name PASSED
tests/hpo/integration/test_hpo_run_mode_variants.py::TestHPORunModeReuseIfExists::test_reuse_if_exists_even_with_existing_variants PASSED
tests/hpo/integration/test_hpo_run_mode_variants.py::TestStudyManagerWithRunMode::test_study_manager_extracts_run_mode PASSED
tests/hpo/integration/test_hpo_run_mode_variants.py::TestStudyManagerWithRunMode::test_study_manager_passes_run_mode_to_create_study_name PASSED
tests/hpo/integration/test_hpo_run_mode_variants.py::TestSmokeYamlRunModeConfig::test_smoke_yaml_run_mode_default PASSED
tests/hpo/integration/test_hpo_run_mode_variants.py::TestSmokeYamlRunModeConfig::test_smoke_yaml_run_mode_force_new PASSED
tests/hpo/integration/test_hpo_run_mode_variants.py::TestSmokeYamlRunModeConfig::test_smoke_yaml_study_name_null_auto_generate PASSED
tests/hpo/integration/test_hpo_run_mode_variants.py::TestSmokeYamlRunModeConfig::test_smoke_yaml_variant_sequence PASSED
tests/hpo/integration/test_hpo_studies_dict_storage.py::TestHPOStudiesDictStorage::test_notebook_loop_stores_all_backbones PASSED
tests/hpo/integration/test_hpo_studies_dict_storage.py::TestHPOStudiesDictStorage::test_validate_hpo_studies_dict_helper PASSED
tests/hpo/integration/test_hpo_sweep_setup.py::TestHPOSweepSetup::test_load_configs_from_smoke_yaml PASSED
tests/hpo/integration/test_hpo_sweep_setup.py::TestHPOSweepSetup::test_setup_hpo_mlflow_run_creates_parent_run PASSED
tests/hpo/integration/test_hpo_sweep_setup.py::TestHPOSweepSetup::test_setup_hpo_mlflow_run_computes_study_key_hash PASSED
tests/hpo/integration/test_hpo_sweep_setup.py::TestHPOSweepSetup::test_create_study_name_from_checkpoint_config PASSED
tests/hpo/integration/test_hpo_sweep_setup.py::TestHPOSweepSetup::test_create_study_name_without_checkpoint PASSED
tests/hpo/integration/test_hpo_sweep_setup.py::TestHPOSweepSetup::test_setup_hpo_mlflow_run_tags PASSED
tests/hpo/integration/test_hpo_sweep_setup.py::TestHPOSweepSetup::test_checkpoint_file_created PASSED
tests/hpo/integration/test_hpo_sweep_setup.py::TestHPOSweepSetup::test_study_key_hash_and_family_hash_computed PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunHierarchy::test_hpo_parent_run_has_correct_tags PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunHierarchy::test_trial_run_is_child_of_hpo_parent PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunHierarchy::test_fold_run_is_child_of_trial_run PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunHierarchy::test_refit_run_is_child_of_hpo_parent PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunTags::test_hpo_parent_run_tags PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunTags::test_trial_run_tags PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunTags::test_refit_run_tags PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunTags::test_trial_run_inherits_study_key_hash_from_parent PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunMetrics::test_trial_run_logs_metrics PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunMetrics::test_trial_run_logs_hyperparameters PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunMetrics::test_cv_trial_run_logs_aggregated_metrics PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunMetrics::test_refit_run_logs_metrics PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunStructure::test_mlflow_structure_no_cv PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunStructure::test_mlflow_structure_with_cv PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunStructure::test_mlflow_runs_have_grouping_tags PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunStatus::test_trial_run_status_transitions PASSED
tests/hpo/integration/test_mlflow_structure.py::TestMLflowRunStatus::test_refit_run_status_finished_after_upload PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureV2::test_study_folder_naming PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureV2::test_trial_folder_naming PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureV2::test_refit_folder_structure PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureV2::test_cv_fold_folder_structure PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureV2::test_checkpoint_location_in_trial PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureV2::test_checkpoint_location_in_refit PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureV2::test_checkpoint_location_in_cv_fold PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureValidation::test_is_v2_path_detects_trial_folder PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureValidation::test_find_study_folder PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureValidation::test_find_trial_folder PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureWithBuildOutputPath::test_build_output_path_hpo_sweep PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureWithBuildOutputPath::test_build_output_path_hpo_trial PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureWithBuildOutputPath::test_build_output_path_hpo_refit PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureFiles::test_study_db_location PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureFiles::test_trial_meta_json_location PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureFiles::test_fold_splits_json_location PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureFiles::test_metrics_json_location_in_trial PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureFiles::test_metrics_json_location_in_refit PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureSmokeYaml::test_path_structure_matches_smoke_yaml PASSED
tests/hpo/integration/test_path_structure.py::TestPathStructureSmokeYaml::test_path_structure_study8_trial8_format PASSED
tests/hpo/integration/test_refit_training.py::TestRefitTrainingSetup::test_refit_uses_best_trial_hyperparameters PASSED
tests/hpo/integration/test_refit_training.py::TestRefitTrainingSetup::test_refit_creates_mlflow_run PASSED
tests/hpo/integration/test_refit_training.py::TestRefitTrainingSetup::test_refit_creates_v2_output_directory PASSED
tests/hpo/integration/test_refit_training.py::TestRefitTrainingExecution::test_refit_reads_metrics_from_file PASSED
tests/hpo/integration/test_refit_training.py::TestRefitTrainingExecution::test_refit_logs_metrics_to_mlflow PASSED
tests/hpo/integration/test_refit_training.py::TestRefitTrainingExecution::test_refit_creates_checkpoint_directory PASSED
tests/hpo/integration/test_refit_training.py::TestRefitTrainingSmokeYaml::test_refit_enabled_in_smoke_yaml PASSED
tests/hpo/integration/test_refit_training.py::TestRefitTrainingSmokeYaml::test_refit_uses_full_epochs PASSED
tests/hpo/integration/test_refit_training.py::TestRefitCheckpointDuplicationPrevention::test_refit_skips_checkpoint_folder_logging PASSED
tests/hpo/integration/test_refit_training.py::TestRefitCheckpointDuplicationPrevention::test_refit_prevents_duplication_only_archive_uploaded FAILED
tests/hpo/integration/test_smoke_yaml_options.py::TestTimeoutMinutes::test_timeout_minutes_stops_study_after_timeout PASSED
tests/hpo/integration/test_smoke_yaml_options.py::TestTimeoutMinutes::test_timeout_minutes_conversion_to_seconds PASSED
tests/hpo/integration/test_smoke_yaml_options.py::TestSaveOnlyBest::test_save_only_best_deletes_non_best_checkpoints PASSED
tests/hpo/integration/test_smoke_yaml_options.py::TestSaveOnlyBest::test_save_only_best_false_preserves_all_checkpoints PASSED
tests/hpo/integration/test_smoke_yaml_options.py::TestLogBestCheckpoint::test_log_best_checkpoint_config_enabled PASSED
tests/hpo/integration/test_smoke_yaml_options.py::TestLogBestCheckpoint::test_log_best_checkpoint_config_disabled PASSED
tests/hpo/integration/test_smoke_yaml_options.py::TestLogBestCheckpoint::test_log_best_checkpoint_conditional_call PASSED
tests/hpo/integration/test_smoke_yaml_options.py::TestDisableAutoCleanup::test_disable_auto_cleanup_false_enables_cleanup PASSED
tests/hpo/integration/test_smoke_yaml_options.py::TestDisableAutoCleanup::test_disable_auto_cleanup_true_disables_cleanup PASSED
tests/hpo/integration/test_smoke_yaml_options.py::TestDisableAutoCleanup::test_disable_auto_cleanup_default_is_disabled PASSED
tests/hpo/integration/test_smoke_yaml_options.py::TestDisableAutoOptunaMark::test_disable_auto_optuna_mark_false_enables_marking PASSED
tests/hpo/integration/test_smoke_yaml_options.py::TestDisableAutoOptunaMark::test_disable_auto_optuna_mark_true_skips_marking PASSED
tests/hpo/integration/test_trial_execution.py::TestTrialExecutionNoCV::test_trial_execution_no_cv_creates_single_run PASSED
tests/hpo/integration/test_trial_execution.py::TestTrialExecutionNoCV::test_trial_execution_no_cv_output_path PASSED
tests/hpo/integration/test_trial_execution.py::TestTrialExecutionNoCV::test_trial_execution_no_cv_metrics_file PASSED
tests/hpo/integration/test_trial_execution.py::TestTrialExecutionWithCV::test_trial_execution_with_cv_creates_nested_runs PASSED
tests/hpo/integration/test_trial_execution.py::TestTrialExecutionWithCV::test_trial_execution_with_cv_creates_fold_runs PASSED
tests/hpo/integration/test_trial_execution.py::TestTrialExecutionWithCV::test_trial_execution_with_cv_aggregates_metrics PASSED
tests/hpo/integration/test_trial_execution.py::TestTrialExecutionWithCV::test_trial_execution_with_cv_output_paths PASSED
tests/hpo/integration/test_trial_execution.py::TestTrialExecutionWithCV::test_trial_execution_with_cv_smoke_yaml_params PASSED
tests/hpo/unit/test_hpo_variant_generation.py::TestCreateStudyNameWithVariants::test_create_study_name_force_new_no_existing PASSED
tests/hpo/unit/test_hpo_variant_generation.py::TestCreateStudyNameWithVariants::test_create_study_name_force_new_with_existing PASSED
tests/hpo/unit/test_hpo_variant_generation.py::TestCreateStudyNameWithVariants::test_create_study_name_reuse_if_exists PASSED
tests/hpo/unit/test_hpo_variant_generation.py::TestCreateStudyNameWithVariants::test_create_study_name_custom_study_name_force_new PASSED
tests/hpo/unit/test_hpo_variant_generation.py::TestCreateStudyNameWithVariants::test_create_study_name_custom_study_name_reuse_if_exists PASSED
tests/hpo/unit/test_hpo_variant_generation.py::TestCreateStudyNameWithVariants::test_create_study_name_checkpoint_disabled_force_new PASSED
tests/hpo/unit/test_hpo_variant_generation.py::TestFindStudyVariants::test_find_study_variants_none PASSED
tests/hpo/unit/test_hpo_variant_generation.py::TestFindStudyVariants::test_find_study_variants_implicit_variant_1 PASSED
tests/hpo/unit/test_hpo_variant_generation.py::TestFindStudyVariants::test_find_study_variants_explicit_variants PASSED
tests/hpo/unit/test_hpo_variant_generation.py::TestFindStudyVariants::test_find_study_variants_ignores_other_folders PASSED
tests/hpo/unit/test_hpo_variant_generation.py::TestFindStudyVariants::test_find_study_variants_different_backbone PASSED
tests/hpo/unit/test_search_space.py::TestSearchSpaceTranslation::test_translate_smoke_yaml_search_space PASSED
tests/hpo/unit/test_search_space.py::TestSearchSpaceTranslation::test_backbone_choice_values PASSED
tests/hpo/unit/test_search_space.py::TestSearchSpaceTranslation::test_learning_rate_loguniform_range PASSED
tests/hpo/unit/test_search_space.py::TestSearchSpaceTranslation::test_batch_size_choice_values PASSED
tests/hpo/unit/test_search_space.py::TestSearchSpaceTranslation::test_dropout_uniform_range PASSED
tests/hpo/unit/test_search_space.py::TestSearchSpaceTranslation::test_weight_decay_loguniform_range PASSED
tests/hpo/unit/test_search_space.py::TestSearchSpaceTranslation::test_exclude_params PASSED
tests/hpo/unit/test_search_space.py::TestSearchSpaceTranslation::test_unsupported_search_space_type PASSED
tests/hpo/unit/test_search_space.py::TestSearchSpaceTranslation::test_search_space_translator_class PASSED
tests/hpo/unit/test_trial_selection.py::TestSelectionCriteria::test_selection_faster_model_within_threshold PASSED
tests/hpo/unit/test_trial_selection.py::TestSelectionCriteria::test_selection_slower_model_when_accuracy_better PASSED
tests/hpo/unit/test_trial_selection.py::TestSelectionCriteria::test_selection_min_accuracy_gain_respected PASSED
tests/hpo/unit/test_trial_selection.py::TestSelectionCriteria::test_selection_tie_breaking_deterministic PASSED
tests/hpo/unit/test_trial_selection.py::TestSelectionCriteria::test_selection_relative_threshold_calculation PASSED
tests/hpo/unit/test_trial_selection.py::TestSelectionCriteria::test_selection_smoke_yaml_parameters PASSED
tests/hpo/unit/test_trial_selection.py::TestSelectionCriteria::test_selection_no_candidates_raises_error PASSED
tests/hpo/unit/test_trial_selection.py::TestSelectionCriteria::test_selection_single_candidate PASSED
tests/hpo/unit/test_trial_selection.py::TestSelectionCriteria::test_selection_no_threshold_accuracy_only PASSED
tests/hpo/unit/test_trial_selection.py::TestSelectionCriteria::test_selection_normalize_speed_scores PASSED
tests/hpo/unit/test_trial_selection.py::TestSelectionCriteria::test_selection_apply_threshold_logic PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_force_new_with_exists_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_force_new_without_exists_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_force_new_with_exists_and_complete_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_force_new_with_exists_and_incomplete_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_reuse_if_exists_hpo_with_exists_returns_true PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_reuse_if_exists_hpo_without_exists_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_reuse_if_exists_final_training_with_exists_and_complete_returns_true PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_reuse_if_exists_final_training_with_exists_and_incomplete_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_reuse_if_exists_final_training_without_exists_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_resume_if_incomplete_with_exists_and_incomplete_returns_true PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_resume_if_incomplete_with_exists_and_complete_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_resume_if_incomplete_without_exists_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_resume_if_incomplete_hpo_treats_as_reuse_if_exists PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_default_with_exists_returns_true PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_default_without_exists_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestShouldReuseExisting::test_not_exists_always_returns_false_regardless_of_mode PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestGetLoadIfExistsFlag::test_force_new_with_checkpoint_enabled_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestGetLoadIfExistsFlag::test_force_new_with_checkpoint_disabled_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestGetLoadIfExistsFlag::test_reuse_if_exists_with_checkpoint_enabled_returns_true PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestGetLoadIfExistsFlag::test_reuse_if_exists_with_checkpoint_disabled_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestGetLoadIfExistsFlag::test_default_with_checkpoint_enabled_returns_true PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestGetLoadIfExistsFlag::test_default_with_checkpoint_disabled_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestGetLoadIfExistsFlag::test_resume_if_incomplete_with_checkpoint_enabled_returns_true PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestGetLoadIfExistsFlag::test_resume_if_incomplete_with_checkpoint_disabled_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestGetLoadIfExistsFlag::test_checkpoint_disabled_always_returns_false PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestProcessTypeIntegration::test_hpo_process_type_ignores_completeness PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestProcessTypeIntegration::test_final_training_process_type_checks_completeness PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestProcessTypeIntegration::test_selection_process_type_ignores_completeness PASSED
tests/infrastructure/config/unit/test_run_decision.py::TestProcessTypeIntegration::test_benchmarking_process_type_ignores_completeness PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetObjectiveDirection::test_direction_key_maximize PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetObjectiveDirection::test_direction_key_minimize PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetObjectiveDirection::test_goal_key_maximize_legacy PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetObjectiveDirection::test_goal_key_minimize_legacy PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetObjectiveDirection::test_goal_key_max_legacy PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetObjectiveDirection::test_goal_key_min_legacy PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetObjectiveDirection::test_goal_key_case_insensitive PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetObjectiveDirection::test_direction_takes_precedence_over_goal PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetObjectiveDirection::test_default_when_missing PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetObjectiveDirection::test_default_when_objective_empty PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetObjectiveDirection::test_goal_passthrough_when_already_correct_format PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetChampionSelectionConfig::test_default_values PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetChampionSelectionConfig::test_custom_values PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetChampionSelectionConfig::test_top_k_clamping_when_greater_than_min_trials PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetChampionSelectionConfig::test_top_k_equal_to_min_trials PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetChampionSelectionConfig::test_top_k_less_than_min_trials PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetChampionSelectionConfig::test_require_artifact_available_false PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetChampionSelectionConfig::test_artifact_check_source_disk PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetChampionSelectionConfig::test_prefer_schema_version_1_0 PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetChampionSelectionConfig::test_prefer_schema_version_2_0 PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetChampionSelectionConfig::test_allow_mixed_schema_groups_true PASSED
tests/infrastructure/config/unit/test_selection.py::TestGetChampionSelectionConfig::test_complete_config PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestComputeDataFingerprint::test_content_hash_priority PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestComputeDataFingerprint::test_manifest_hash_priority PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestComputeDataFingerprint::test_content_hash_over_manifest_hash PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestComputeDataFingerprint::test_semantic_fallback PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestComputeDataFingerprint::test_semantic_fallback_deterministic PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestComputeDataFingerprint::test_empty_config PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestComputeDataFingerprint::test_minimal_config PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestComputeEvalFingerprint::test_with_eval_config PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestComputeEvalFingerprint::test_deterministic PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestComputeEvalFingerprint::test_empty_config PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestComputeEvalFingerprint::test_with_different_configs PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestBuildHpoStudyKeyV2::test_basic_structure PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestBuildHpoStudyKeyV2::test_includes_fingerprints PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestBuildHpoStudyKeyV2::test_deterministic PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestBuildHpoStudyKeyV2::test_different_models_produce_different_keys PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestBuildHpoStudyKeyV2::test_with_benchmark_config PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestBuildHpoStudyKeyV2::test_without_benchmark_config PASSED
tests/infrastructure/naming/test_hpo_keys_v2.py::TestBuildHpoStudyKeyV2::test_hash_function PASSED
tests/infrastructure/naming/test_semantic_suffix.py::TestSemanticSuffixBehavior::test_auto_generated_study_name_returns_empty_semantic_suffix PASSED
tests/infrastructure/naming/test_semantic_suffix.py::TestSemanticSuffixBehavior::test_custom_study_name_includes_semantic_suffix PASSED
tests/infrastructure/naming/test_semantic_suffix.py::TestSemanticSuffixBehavior::test_auto_generated_with_variant_returns_empty_semantic_suffix PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestQueryRunsByTags::test_filters_finished_runs PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestQueryRunsByTags::test_filters_by_required_tags PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestQueryRunsByTags::test_handles_missing_tags PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestQueryRunsByTags::test_passes_filter_string PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestQueryRunsByTags::test_passes_max_results PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestQueryRunsByTags::test_empty_result PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestFindBestRunByMetric::test_maximize_metric PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestFindBestRunByMetric::test_minimize_metric PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestFindBestRunByMetric::test_filters_runs_without_metric PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestFindBestRunByMetric::test_returns_none_when_no_runs_have_metric PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestFindBestRunByMetric::test_empty_runs_list PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestFindBestRunByMetric::test_single_run PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestGroupRunsByVariant::test_groups_by_variant_tag PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestGroupRunsByVariant::test_default_variant_for_missing_tag PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestGroupRunsByVariant::test_custom_variant_tag PASSED
tests/infrastructure/tracking/unit/test_mlflow_queries.py::TestGroupRunsByVariant::test_empty_runs_list PASSED
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerV2HashComputation::test_v2_hash_computation_success PASSED
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerV2HashComputation::test_v2_hash_computation_missing_train_config PASSED
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerV2HashComputation::test_v2_hash_computation_empty_eval_config PASSED
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerBestTrialSearch::test_best_trial_search_by_study_key_hash_success PASSED
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerBestTrialSearch::test_best_trial_search_filters_by_parent_run_id PASSED
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerBestTrialSearch::test_best_trial_search_no_parent_study_key_hash PASSED
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerBestTrialSearch::test_best_trial_search_no_matching_runs PASSED
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerBestTrialSearch::test_best_trial_search_exception_handling PASSED
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerTrialNumberExtraction::test_extract_trial_number_from_tag PASSED
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerTrialNumberExtraction::test_extract_trial_number_from_run_name PASSED
tests/infrastructure/tracking/unit/test_sweep_tracker_hash_and_search.py::TestSweepTrackerTrialNumberExtraction::test_extract_trial_number_returns_none_when_not_found PASSED
tests/integration/api/test_api.py::TestHealthEndpoint::test_health_check PASSED
tests/integration/api/test_api.py::TestHealthEndpoint::test_model_info_not_loaded PASSED
tests/integration/api/test_api.py::TestHealthEndpoint::test_model_info_loaded FAILED
tests/integration/api/test_api.py::TestPredictEndpoint::test_predict_not_loaded PASSED
tests/integration/api/test_api.py::TestPredictEndpoint::test_predict_success FAILED
tests/integration/api/test_api.py::TestPredictEndpoint::test_predict_batch_success FAILED
tests/integration/api/test_api.py::TestPredictEndpoint::test_predict_batch_size_exceeded FAILED
tests/integration/api/test_api.py::TestFileEndpoints::test_predict_file_not_loaded PASSED
tests/integration/api/test_api.py::TestFileEndpoints::test_predict_file_pdf FAILED
tests/integration/api/test_api.py::TestFileEndpoints::test_predict_file_image FAILED
tests/integration/api/test_api_local_server.py::TestServerLifecycle::test_server_startup_with_valid_model SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestServerLifecycle::test_server_startup_with_invalid_model_path PASSED
tests/integration/api/test_api_local_server.py::TestServerLifecycle::test_server_graceful_shutdown SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestHealthEndpoints::test_health_check_model_loaded SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestHealthEndpoints::test_model_info_loaded SKIPPEDraining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestSingleTextPrediction::test_predict_valid_text SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestSingleTextPrediction::test_predict_empty_text SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestSingleTextPrediction::test_predict_unicode_text SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestSingleTextPrediction::test_predict_long_text SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestSingleTextPrediction::test_predict_special_characters SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestSingleTextPrediction::test_predict_whitespace_only SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestSingleTextPrediction::test_predict_non_string_value SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestBatchTextPrediction::test_predict_batch_small SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestBatchTextPrediction::test_predict_batch_medium SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestBatchTextPrediction::test_predict_batch_empty SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestBatchTextPrediction::test_predict_batch_mixed SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestBatchTextPrediction::test_predict_batch_size_exceeded SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestBatchTextPrediction::test_predict_batch_with_empty_text SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestBatchTextPrediction::test_predict_batch_missing_texts_field SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestBatchTextPrediction::test_predict_batch_non_list_value SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestFileUpload::test_predict_file_pdf SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestFileUpload::test_predict_file_png SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestFileUpload::test_predict_file_larger_pdf SKIPPEDraining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestFileUpload::test_predict_file_small_pdf SKIPPEDraining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestFileUpload::test_predict_file_missing_file SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestBatchFileUpload::test_predict_file_batch_small SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestBatchFileUpload::test_predict_file_batch_mixed_types SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestBatchFileUpload::test_predict_file_batch_medium SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestBatchFileUpload::test_predict_file_batch_empty SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestBatchFileUpload::test_predict_file_batch_size_exceeded SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestDebugEndpoint::test_predict_debug SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestErrorHandling::test_invalid_json SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestErrorHandling::test_missing_required_fields SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestErrorHandling::test_invalid_file_type SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestPerformance::test_predict_latency SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestPerformance::test_predict_batch_latency SKIPPEDraining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestStability::test_repeated_predictions_consistency SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestStability::test_repeated_file_processing SKIPPEDraining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_api_local_server.py::TestStability::test_comprehensive_multi_file_multi_iteration SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_inference_direct.py::test_direct_inference SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_inference_performance.py::TestInferencePerformanceIntegration::test_real_inference_performance SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_inference_performance.py::TestInferencePerformanceIntegration::test_tokenization_consistency_mock PASSED
tests/integration/api/test_onnx_inference.py::test_onnx_inference_speed SKIPPEDaining/distilroberta/distilroberta_model.onnx.
Provide via --onnx-model or ONNX_MODEL_PATH)
tests/integration/api/test_tokenization_speed.py::test_tokenization_speed SKIPPEDs/final_training/distilroberta/checkpoint)
tests/selection/integration/test_artifact_acquisition_edge_cases.py::TestArtifactAcquisitionEdgeCases::test_empty_priority_list PASSED
tests/selection/integration/test_artifact_acquisition_edge_cases.py::TestArtifactAcquisitionEdgeCases::test_invalid_priority_values PASSED
tests/selection/integration/test_artifact_acquisition_edge_cases.py::TestArtifactAcquisitionEdgeCases::test_missing_local_section PASSED
tests/selection/integration/test_artifact_acquisition_edge_cases.py::TestArtifactAcquisitionEdgeCases::test_missing_drive_section PASSED
tests/selection/integration/test_artifact_acquisition_edge_cases.py::TestArtifactAcquisitionEdgeCases::test_missing_mlflow_section PASSED
tests/selection/integration/test_artifact_acquisition_edge_cases.py::TestArtifactAcquisitionEdgeCases::test_validation_false_allows_invalid_checkpoints FAILED
tests/selection/integration/test_artifact_acquisition_edge_cases.py::TestArtifactAcquisitionEdgeCases::test_validation_true_rejects_invalid_checkpoints FAILED
tests/selection/integration/test_artifact_acquisition_edge_cases.py::TestArtifactAcquisitionEdgeCases::test_priority_order_affects_strategy_selection PASSED
tests/selection/integration/test_artifact_acquisition_edge_cases.py::TestArtifactAcquisitionEdgeCases::test_duplicate_priority_values PASSED
tests/selection/integration/test_artifact_acquisition_edge_cases.py::TestArtifactAcquisitionEdgeCases::test_priority_with_only_one_source PASSED
tests/selection/integration/test_artifact_acquisition_edge_cases.py::TestArtifactAcquisitionEdgeCases::test_missing_study_trial_hashes_skips_local PASSED
tests/selection/integration/test_artifact_acquisition_edge_cases.py::TestArtifactAcquisitionEdgeCases::test_config_with_all_optional_fields PASSED
tests/selection/integration/test_artifact_acquisition_logic.py::TestArtifactAcquisitionConfig::test_priority_order_local_first FAILED
tests/selection/integration/test_artifact_acquisition_logic.py::TestArtifactAcquisitionConfig::test_priority_order_mlflow_first FAILED
tests/selection/integration/test_artifact_acquisition_logic.py::TestArtifactAcquisitionConfig::test_local_validate_controls_validation FAILED
tests/selection/integration/test_artifact_acquisition_logic.py::TestArtifactAcquisitionConfig::test_drive_enabled_controls_drive_strategy PASSED
tests/selection/integration/test_artifact_acquisition_logic.py::TestArtifactAcquisitionConfig::test_drive_validate_controls_validation PASSED
tests/selection/integration/test_artifact_acquisition_logic.py::TestArtifactAcquisitionConfig::test_mlflow_enabled_controls_mlflow_strategy PASSED
tests/selection/integration/test_artifact_acquisition_logic.py::TestArtifactAcquisitionConfig::test_mlflow_validate_controls_validation PASSED
tests/selection/integration/test_artifact_acquisition_logic.py::TestArtifactAcquisitionConfig::test_priority_order_with_some_sources_disabled PASSED
tests/selection/integration/test_artifact_acquisition_logic.py::TestArtifactAcquisitionConfig::test_all_strategies_fail_gracefully_when_disabled PASSED
tests/selection/integration/test_artifact_acquisition_unified_config.py::TestSearchRootsIntegration::test_search_roots_used_in_local_discovery PASSED
tests/selection/integration/test_artifact_acquisition_unified_config.py::TestSearchRootsIntegration::test_search_roots_default_when_missing PASSED
tests/selection/integration/test_artifact_acquisition_unified_config.py::TestArtifactKindsPriorityIntegration::test_artifact_kinds_priority_overrides_global PASSED
tests/selection/integration/test_artifact_acquisition_unified_config.py::TestArtifactKindsPriorityIntegration::test_artifact_kinds_fallback_to_global_priority PASSED
tests/selection/integration/test_artifact_acquisition_unified_config.py::TestMlflowRequireArtifactTagIntegration::test_require_artifact_tag_config_extracted PASSED
tests/selection/integration/test_artifact_acquisition_unified_config.py::TestConfigOptionCombinations::test_all_config_options_together PASSED
tests/selection/integration/test_artifact_acquisition_unified_config.py::TestConfigOptionCombinations::test_config_with_disabled_sources FAILED
tests/selection/integration/test_artifact_acquisition_workflow.py::TestArtifactAcquisitionWorkflow::test_complete_workflow_with_default_config FAILED
tests/selection/integration/test_artifact_acquisition_workflow.py::TestArtifactAcquisitionWorkflow::test_workflow_with_custom_priority_order FAILED
tests/selection/integration/test_artifact_acquisition_workflow.py::TestArtifactAcquisitionWorkflow::test_workflow_with_validation_disabled FAILED
tests/selection/integration/test_artifact_acquisition_workflow.py::TestArtifactAcquisitionWorkflow::test_workflow_with_all_sources_enabled FAILED
tests/selection/integration/test_artifact_acquisition_workflow.py::TestArtifactAcquisitionWorkflow::test_workflow_with_all_sources_disabled PASSED
tests/selection/integration/test_artifact_acquisition_workflow.py::TestArtifactAcquisitionWorkflow::test_workflow_mlflow_fallback_to_manual FAILED
tests/selection/integration/test_best_model_cache.py::test_cache_dual_file_strategy_creates_all_files PASSED
tests/selection/integration/test_best_model_cache.py::test_cache_load_valid_cache_with_mlflow_validation  Checking for cached best model selection...
  Cache file: /tmp/pytest-of-codespace/pytest-28/test_cache_load_valid_cache_wi0/outputs/cache/best_model_selection/latest_best_model_selection.json
   Cache file found
   Cache key matches
   Schema version valid
   Validating MLflow run: run-123...
   MLflow run exists and is FINISHED

 Cache validation successful - reusing cached best model selection
  Cache timestamp: None
  Run ID: run-123...
  Backbone: distilbert-base-uncased
PASSED
tests/selection/integration/test_best_model_cache.py::test_cache_load_cache_key_mismatch_returns_none  Checking for cached best model selection...
  Cache file: /tmp/pytest-of-codespace/pytest-28/test_cache_load_cache_key_mism0/outputs/cache/best_model_selection/latest_best_model_selection.json
   Cache file found
   Cache key mismatch - config changed since cache was created
    Cached key: old_cach... (from unknown)
    Current key: 54c8d4d3...
    Reason: Selection config, tags config, experiment, or benchmark experiment changed
PASSED
tests/selection/integration/test_best_model_cache.py::test_cache_partial_write_recovery  Checking for cached best model selection...
  Cache file: /tmp/pytest-of-codespace/pytest-28/test_cache_partial_write_recov0/outputs/cache/best_model_selection/latest_best_model_selection.json
   Cache file does not exist (first run or cache was cleared)
PASSED
tests/selection/integration/test_best_model_cache.py::test_cache_missing_metrics_handling  Checking for cached best model selection...
  Cache file: /tmp/pytest-of-codespace/pytest-28/test_cache_missing_metrics_han0/outputs/cache/best_model_selection/latest_best_model_selection.json
   Cache file found
   Cache key matches
   Schema version valid
   Validating MLflow run: run-123...
   MLflow run exists and is FINISHED

 Cache validation successful - reusing cached best model selection
  Cache timestamp: None
  Run ID: run-123...
  Backbone: distilbert-base-uncased
PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestLatencyAggregationIntegration::test_latency_aggregation_latest_strategy PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestLatencyAggregationIntegration::test_latency_aggregation_median_strategy PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestLatencyAggregationIntegration::test_latency_aggregation_mean_strategy PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestLatencyAggregationIntegration::test_latency_aggregation_default_when_missing PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestObjectiveDirectionMigrationIntegration::test_objective_direction_preferred_over_goal PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestObjectiveDirectionMigrationIntegration::test_objective_goal_fallback_when_direction_missing PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestObjectiveDirectionMigrationIntegration::test_objective_direction_default_when_both_missing PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestChampionSelectionConfigIntegration::test_min_trials_per_group_extraction PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestChampionSelectionConfigIntegration::test_top_k_for_stable_score_extraction PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestChampionSelectionConfigIntegration::test_require_artifact_available_extraction PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestChampionSelectionConfigIntegration::test_artifact_check_source_extraction PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestChampionSelectionConfigIntegration::test_prefer_schema_version_extraction PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestChampionSelectionConfigIntegration::test_allow_mixed_schema_groups_extraction PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestScoringConfigIntegration::test_scoring_weights_used_in_composite_score PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestScoringConfigIntegration::test_normalize_weights_controls_normalization PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestBenchmarkRequiredMetricsIntegration::test_required_metrics_filters_benchmark_runs PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestCompleteConfigWorkflow::test_all_config_options_used_together PASSED
tests/selection/integration/test_best_model_selection_config_integration.py::TestCompleteConfigWorkflow::test_config_with_missing_sections_uses_defaults PASSED
tests/selection/integration/test_mlflow_selection_config.py::TestMLflowSelectionConfigUsage::test_find_best_model_uses_objective_metric PASSED
tests/selection/integration/test_mlflow_selection_config.py::TestMLflowSelectionConfigUsage::test_find_best_model_uses_scoring_weights PASSED
tests/selection/integration/test_mlflow_selection_config.py::TestMLflowSelectionConfigUsage::test_find_best_model_uses_benchmark_required_metrics PASSED
tests/selection/integration/test_mlflow_selection_config.py::TestMLflowSelectionConfigUsage::test_find_best_model_weight_normalization PASSED
tests/selection/integration/test_mlflow_selection_config.py::TestMLflowSelectionConfigUsage::test_find_best_model_composite_score_calculation 
 Best model selected:
   Run ID: refit_run_id_123
   Experiment: test_experiment-hpo-distilbert
   Backbone: distilbert
   F1 Score: 0.7500
   Latency: 5.00 ms
   Composite Score: 0.3000
PASSED
tests/selection/integration/test_mlflow_selection_config.py::TestMLflowSelectionConfigUsage::test_find_best_model_all_config_options_together PASSED
tests/selection/integration/test_mlflow_selection_config.py::TestMLflowSelectionConfigUsage::test_find_best_model_custom_config_values PASSED
tests/selection/integration/test_selection_cache_config.py::TestSelectionCacheConfig::test_compute_selection_cache_key_includes_selection_config PASSED
tests/selection/integration/test_selection_cache_config.py::TestSelectionCacheConfig::test_compute_selection_cache_key_deterministic PASSED
tests/selection/integration/test_selection_cache_config.py::TestSelectionCacheConfig::test_load_cached_best_model_validates_cache_key  Checking for cached best model selection...
  Cache file: /tmp/pytest-of-codespace/pytest-28/test_load_cached_best_model_va0/outputs/cache.json
   Cache file found
   Cache key matches
   Schema version valid
   Validating MLflow run: test_run_id_...
   MLflow run exists and is FINISHED

 Cache validation successful - reusing cached best model selection
  Cache timestamp: 2026-01-08T20:00:00Z
  Run ID: test_run_id_...
  Backbone: distilbert
PASSED
tests/selection/integration/test_selection_cache_config.py::TestSelectionCacheConfig::test_load_cached_best_model_cache_key_mismatch   Mode is 'force_new' - skipping cache
PASSED
tests/selection/integration/test_selection_cache_config.py::TestSelectionCacheConfig::test_save_best_model_cache_includes_selection_config PASSED
tests/selection/integration/test_selection_cache_config.py::TestSelectionCacheConfig::test_run_mode_reuse_if_exists_behavior  Checking for cached best model selection...
  Cache file: /tmp/pytest-of-codespace/pytest-28/test_run_mode_reuse_if_exists_0/outputs/cache.json
   Cache file found
   Cache key matches
   Schema version valid
   Validating MLflow run: test_run_id_...
   MLflow run exists and is FINISHED

 Cache validation successful - reusing cached best model selection
  Cache timestamp: 2026-01-08T20:00:00Z
  Run ID: test_run_id_...
  Backbone: distilbert
PASSED
tests/selection/integration/test_selection_cache_config.py::TestSelectionCacheConfig::test_run_mode_force_new_behavior   Mode is 'force_new' - skipping cache
PASSED
tests/selection/integration/test_selection_edge_cases.py::TestSelectionConfigEdgeCases::test_missing_run_section_uses_defaults PASSED
tests/selection/integration/test_selection_edge_cases.py::TestSelectionConfigEdgeCases::test_missing_objective_section_handled PASSED
tests/selection/integration/test_selection_edge_cases.py::TestSelectionConfigEdgeCases::test_missing_scoring_section_uses_defaults PASSED
tests/selection/integration/test_selection_edge_cases.py::TestSelectionConfigEdgeCases::test_negative_weights_handled PASSED
tests/selection/integration/test_selection_edge_cases.py::TestSelectionConfigEdgeCases::test_zero_weights_handled PASSED
tests/selection/integration/test_selection_edge_cases.py::TestSelectionConfigEdgeCases::test_weight_normalization_zero_sum PASSED
tests/selection/integration/test_selection_edge_cases.py::TestSelectionConfigEdgeCases::test_empty_required_metrics_list PASSED
tests/selection/integration/test_selection_edge_cases.py::TestSelectionConfigEdgeCases::test_missing_benchmark_section_uses_defaults PASSED
tests/selection/integration/test_selection_edge_cases.py::TestSelectionConfigEdgeCases::test_cache_key_with_missing_sections PASSED
tests/selection/integration/test_selection_edge_cases.py::TestSelectionConfigEdgeCases::test_find_best_model_missing_required_metrics PASSED
tests/selection/integration/test_selection_edge_cases.py::TestSelectionConfigEdgeCases::test_find_best_model_empty_required_metrics PASSED
tests/selection/integration/test_selection_edge_cases.py::TestSelectionConfigEdgeCases::test_invalid_run_mode_value PASSED
tests/selection/integration/test_selection_workflow.py::TestSelectionWorkflow::test_workflow_loads_config_and_uses_all_options PASSED
tests/selection/integration/test_selection_workflow.py::TestSelectionWorkflow::test_workflow_custom_config_values PASSED
tests/selection/integration/test_selection_workflow.py::TestSelectionWorkflow::test_workflow_cache_key_computation PASSED
tests/selection/integration/test_selection_workflow.py::TestSelectionWorkflow::test_workflow_cache_loading_with_config  Checking for cached best model selection...
  Cache file: /tmp/pytest-of-codespace/pytest-28/test_workflow_cache_loading_wi0/outputs/cache.json
   Cache file found
   Cache key matches
   Schema version valid
   Validating MLflow run: test_run_id_...
   MLflow run exists and is FINISHED

 Cache validation successful - reusing cached best model selection
  Cache timestamp: 2026-01-08T20:00:00Z
  Run ID: test_run_id_...
  Backbone: distilbert
PASSED
tests/selection/unit/test_artifact_acquisition_config.py::TestArtifactAcquisitionConfigLoading::test_load_yaml_loads_artifact_acquisition_config PASSED
tests/selection/unit/test_artifact_acquisition_config.py::TestArtifactAcquisitionConfigLoading::test_artifact_acquisition_config_structure PASSED
tests/selection/unit/test_artifact_acquisition_config.py::TestArtifactAcquisitionConfigLoading::test_artifact_acquisition_config_default_values PASSED
tests/selection/unit/test_artifact_acquisition_config.py::TestArtifactAcquisitionConfigLoading::test_artifact_acquisition_config_custom_values PASSED
tests/selection/unit/test_artifact_acquisition_config.py::TestArtifactAcquisitionConfigLoading::test_artifact_acquisition_config_missing_sections PASSED
tests/selection/unit/test_artifact_acquisition_config.py::TestArtifactAcquisitionConfigLoading::test_artifact_acquisition_config_types PASSED
tests/selection/unit/test_artifact_acquisition_config.py::TestArtifactAcquisitionConfigLoading::test_load_actual_artifact_acquisition_yaml PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestSearchRootsConfig::test_search_roots_extraction PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestSearchRootsConfig::test_search_roots_custom_order PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestSearchRootsConfig::test_search_roots_default PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestSearchRootsConfig::test_search_roots_empty_list PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestSearchRootsConfig::test_search_roots_passed_to_discovery PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestArtifactKindsConfig::test_artifact_kinds_extraction PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestArtifactKindsConfig::test_artifact_kinds_priority_overrides_global PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestArtifactKindsConfig::test_artifact_kinds_fallback_to_global_priority PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestArtifactKindsConfig::test_artifact_kinds_all_artifact_types PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestArtifactKindsConfig::test_artifact_kinds_missing_priority PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestMlflowRequireArtifactTag::test_require_artifact_tag_extraction PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestMlflowRequireArtifactTag::test_require_artifact_tag_true PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestMlflowRequireArtifactTag::test_require_artifact_tag_default PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestConfigIntegrationSuccessCases::test_success_with_all_config_options PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestConfigIntegrationSuccessCases::test_success_with_custom_artifact_kinds_priority PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestConfigIntegrationSuccessCases::test_success_with_custom_search_roots PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestConfigIntegrationFailureCases::test_failure_missing_search_roots_uses_defaults PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestConfigIntegrationFailureCases::test_failure_missing_artifact_kinds_uses_global_priority PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestConfigIntegrationFailureCases::test_failure_empty_priority_list PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestConfigIntegrationFailureCases::test_failure_all_sources_disabled PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestConfigIntegrationFailureCases::test_failure_invalid_priority_source PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestConfigOptionTypes::test_all_config_option_types PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestConfigDefaults::test_all_defaults_match_config_file PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestConfigCompleteCoverage::test_all_config_sections_present PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestConfigCompleteCoverage::test_all_local_options_present PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestConfigCompleteCoverage::test_all_drive_options_present PASSED
tests/selection/unit/test_artifact_acquisition_config_comprehensive.py::TestConfigCompleteCoverage::test_all_mlflow_options_present PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_priority_extraction PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_priority_custom_order PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_priority_default PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_local_validate_extraction PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_local_validate_false PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_local_validate_default PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_local_match_strategy_extraction PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_local_match_strategy_metadata_run_id PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_local_match_strategy_spec_fp PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_local_require_exact_match_extraction PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_local_require_exact_match_false PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_drive_enabled_extraction PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_drive_enabled_false PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_drive_enabled_default PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_drive_validate_extraction PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_drive_validate_false PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_drive_validate_default PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_drive_folder_path_extraction PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_drive_folder_path_custom PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_mlflow_enabled_extraction PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_mlflow_enabled_false PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_mlflow_enabled_default PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_mlflow_validate_extraction PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_mlflow_validate_false PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_mlflow_validate_default PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_mlflow_download_timeout_extraction PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_mlflow_download_timeout_custom PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_mlflow_download_timeout_default PASSED
tests/selection/unit/test_artifact_acquisition_options.py::TestArtifactAcquisitionOptions::test_all_options_together PASSED
tests/selection/unit/test_best_model_selection_config.py::TestBestModelSelectionConfigLoading::test_load_best_model_selection_config PASSED
tests/selection/unit/test_best_model_selection_config.py::TestBestModelSelectionConfigLoading::test_load_config_with_custom_values PASSED
tests/selection/unit/test_best_model_selection_config.py::TestBestModelSelectionConfigLoading::test_load_config_structure_validation PASSED
tests/selection/unit/test_best_model_selection_config.py::TestBestModelSelectionConfigLoading::test_load_config_matches_actual_file PASSED
tests/selection/unit/test_best_model_selection_config.py::TestBestModelSelectionConfigLoading::test_load_config_with_champion_selection_section PASSED
tests/selection/unit/test_best_model_selection_config.py::TestBestModelSelectionConfigLoading::test_load_config_champion_selection_custom_values PASSED
tests/selection/unit/test_best_model_selection_config.py::TestBestModelSelectionConfigLoading::test_load_config_champion_selection_structure_validation PASSED
tests/selection/unit/test_best_model_selection_config.py::TestBestModelSelectionConfigLoading::test_load_config_champion_selection_with_objective_direction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestRunModeConfig::test_run_mode_extraction_force_new PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestRunModeConfig::test_run_mode_extraction_reuse_if_exists PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestRunModeConfig::test_run_mode_default PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestRunModeConfig::test_run_mode_invalid_value PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestObjectiveConfig::test_objective_metric_extraction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestObjectiveConfig::test_objective_metric_default PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestObjectiveConfig::test_objective_direction_extraction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestObjectiveConfig::test_objective_direction_minimize PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestObjectiveConfig::test_objective_goal_extraction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestObjectiveConfig::test_objective_direction_and_goal_both_present PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestObjectiveConfig::test_objective_direction_preferred_over_goal PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_min_trials_per_group_extraction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_min_trials_per_group_default PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_min_trials_per_group_custom_value PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_top_k_for_stable_score_extraction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_top_k_for_stable_score_default PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_require_artifact_available_extraction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_require_artifact_available_false PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_artifact_check_source_extraction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_artifact_check_source_disk PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_prefer_schema_version_extraction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_prefer_schema_version_2_0 PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_prefer_schema_version_1_0 PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_allow_mixed_schema_groups_extraction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_allow_mixed_schema_groups_true PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestChampionSelectionConfig::test_all_champion_selection_options_together PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestScoringConfig::test_f1_weight_extraction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestScoringConfig::test_f1_weight_default PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestScoringConfig::test_latency_weight_extraction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestScoringConfig::test_latency_weight_default PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestScoringConfig::test_normalize_weights_extraction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestScoringConfig::test_normalize_weights_false PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestScoringConfig::test_normalize_weights_default PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestBenchmarkConfig::test_required_metrics_extraction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestBenchmarkConfig::test_required_metrics_multiple PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestBenchmarkConfig::test_required_metrics_default PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestBenchmarkConfig::test_latency_aggregation_extraction PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestBenchmarkConfig::test_latency_aggregation_median PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestBenchmarkConfig::test_latency_aggregation_mean PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestBenchmarkConfig::test_latency_aggregation_default PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestBenchmarkConfig::test_latency_aggregation_invalid_value PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigIntegrationSuccessCases::test_all_config_options_together PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigIntegrationSuccessCases::test_config_with_custom_latency_aggregation PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigIntegrationSuccessCases::test_config_with_all_champion_selection_options PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigIntegrationFailureCases::test_failure_missing_run_section PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigIntegrationFailureCases::test_failure_missing_objective_section PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigIntegrationFailureCases::test_failure_missing_champion_selection_section PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigIntegrationFailureCases::test_failure_missing_scoring_section PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigIntegrationFailureCases::test_failure_missing_benchmark_section PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigIntegrationFailureCases::test_failure_empty_required_metrics PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigIntegrationFailureCases::test_failure_zero_weights PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigTypeValidation::test_all_config_option_types PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigDefaults::test_all_defaults_match_config_file PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigCompleteCoverage::test_all_config_sections_present PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigCompleteCoverage::test_all_run_options_present PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigCompleteCoverage::test_all_objective_options_present PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigCompleteCoverage::test_all_champion_selection_options_present PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigCompleteCoverage::test_all_scoring_options_present PASSED
tests/selection/unit/test_best_model_selection_config_comprehensive.py::TestConfigCompleteCoverage::test_all_benchmark_options_present PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_run_mode_extraction PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_run_mode_default PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_run_mode_reuse_if_exists PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_objective_metric_extraction PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_objective_metric_default PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_objective_goal_extraction PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_objective_goal_minimize PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_objective_goal_default PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_scoring_f1_weight_extraction PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_scoring_f1_weight_default PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_scoring_latency_weight_extraction PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_scoring_latency_weight_default PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_scoring_normalize_weights_extraction_true PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_scoring_normalize_weights_extraction_false PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_scoring_normalize_weights_default PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_benchmark_required_metrics_extraction PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_benchmark_required_metrics_multiple PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_benchmark_required_metrics_default PASSED
tests/selection/unit/test_best_model_selection_options.py::TestBestModelSelectionConfigOptions::test_all_options_together PASSED
tests/selection/unit/test_study_summary.py::TestStudySummaryMultipleBackbones::test_print_study_summaries_with_multiple_backbones PASSED
tests/selection/unit/test_study_summary.py::TestStudySummaryMultipleBackbones::test_print_study_summaries_with_missing_from_dict PASSED
tests/selection/unit/test_study_summary.py::TestStudySummaryMultipleBackbones::test_print_study_summaries_skips_already_printed PASSED
tests/selection/unit/test_study_summary.py::TestStudySummaryMultipleBackbones::test_format_study_summary_line_with_cv_stats PASSED
tests/selection/unit/test_study_summary.py::TestStudySummaryMultipleBackbones::test_format_study_summary_line_without_cv_stats PASSED
tests/selection/unit/test_study_summary.py::TestStudySummaryMultipleBackbones::test_extract_cv_statistics PASSED
tests/selection/unit/test_study_summary.py::TestStudySummaryMultipleBackbones::test_extract_cv_statistics_missing PASSED
tests/shared/unit/test_drive_backup.py::TestBackupResult::test_backup_result_str_success PASSED
tests/shared/unit/test_drive_backup.py::TestBackupResult::test_backup_result_str_error PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_init_validation PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_drive_path_for_valid_path PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_drive_path_for_outside_root PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_drive_path_for_outside_outputs PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_drive_path_for_allows_outputs_when_disabled PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_backup_file PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_backup_directory PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_backup_nonexistent_file PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_backup_type_mismatch PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_backup_type_inference PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_backup_dry_run PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_restore_file PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_restore_directory PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_restore_nonexistent_backup PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_restore_overwrites_existing PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_ensure_local_exists PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_ensure_local_restores_if_missing PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_backup_exists_true PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_backup_exists_false PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_as_restore_callback PASSED
tests/shared/unit/test_drive_backup.py::TestDriveBackupStore::test_as_backup_callback PASSED
tests/shared/unit/test_drive_backup.py::TestEnsureLocalOptions::test_default_options PASSED
tests/shared/unit/test_drive_backup.py::TestColabMounting::test_mount_colab_drive_success FAILED
tests/shared/unit/test_drive_backup.py::TestColabMounting::test_mount_colab_drive_import_error PASSED
tests/shared/unit/test_drive_backup.py::TestColabMounting::test_create_colab_store_success FAILED
tests/shared/unit/test_drive_backup.py::TestColabMounting::test_create_colab_store_mount_fails FAILED
tests/shared/unit/test_drive_backup.py::TestColabMounting::test_create_colab_store_default_path FAILED
tests/tracking/integration/test_azureml_artifact_upload_integration.py::TestAzureMLArtifactUploadIntegration::test_artifact_upload_to_refit_run_with_monkey_patch SKIPPED
tests/tracking/integration/test_azureml_artifact_upload_integration.py::TestAzureMLArtifactUploadIntegration::test_refit_run_completion_after_upload SKIPPED
tests/tracking/integration/test_azureml_artifact_upload_integration.py::TestMonkeyPatchBehavior::test_patch_handles_tracking_uri_error PASSED
tests/tracking/integration/test_naming_integration.py::test_end_to_end_final_training PASSED
tests/tracking/integration/test_naming_integration.py::test_end_to_end_conversion PASSED
tests/tracking/integration/test_naming_integration.py::test_cross_platform_same_spec_fp PASSED
tests/tracking/integration/test_tracking_config_enabled.py::TestBenchmarkTrackingEnabled::test_benchmark_tracking_enabled_creates_run PASSED
tests/tracking/integration/test_tracking_config_enabled.py::TestBenchmarkTrackingEnabled::test_benchmark_tracking_disabled_skips_run PASSED
tests/tracking/integration/test_tracking_config_enabled.py::TestTrainingTrackingEnabled::test_training_tracking_enabled_creates_run PASSED
tests/tracking/integration/test_tracking_config_enabled.py::TestTrainingTrackingEnabled::test_training_tracking_disabled_skips_run PASSED
tests/tracking/integration/test_tracking_config_enabled.py::TestConversionTrackingEnabled::test_conversion_tracking_enabled_creates_run PASSED
tests/tracking/integration/test_tracking_config_enabled.py::TestConversionTrackingEnabled::test_conversion_tracking_disabled_skips_run PASSED
tests/tracking/integration/test_tracking_config_enabled.py::TestLogArtifactsOptions::test_benchmark_log_artifacts_disabled_skips_logging PASSED
tests/tracking/integration/test_tracking_config_enabled.py::TestLogArtifactsOptions::test_training_log_checkpoint_disabled_skips_logging PASSED
tests/tracking/integration/test_tracking_config_enabled.py::TestLogArtifactsOptions::test_training_log_metrics_json_disabled_skips_logging PASSED
tests/tracking/integration/test_tracking_config_enabled.py::TestLogArtifactsOptions::test_conversion_log_onnx_model_disabled_skips_logging PASSED
tests/tracking/integration/test_tracking_config_enabled.py::TestLogArtifactsOptions::test_conversion_log_conversion_log_disabled_skips_logging PASSED
tests/tracking/scripts/test_artifact_upload_manual.py::test_monkey_patch_registration Testing monkey-patch registration...
 Azure ML builder registered: <function azureml_artifacts_builder at 0x71974c858ca0>
 Builder is patched (has __wrapped__ attribute)
PASSED
tests/tracking/scripts/test_artifact_upload_manual.py::test_artifact_upload_to_child_run 
Testing artifact upload to child run...
Tracking URI: sqlite:////workspaces/resume-ner-azureml/mlruns/mlflow.db
Active parent run: 1eb673147e1a443faf92e853fdb86126
Created child run: 03228e5f820d44cb9d70a4f56f0bc8a3
Attempting to upload artifact to child run 03228e5f820d44cb9d70a4f56f0bc8a3...
 Successfully uploaded artifact to child run!
 Marked child run 03228e5f820d44cb9d70a4f56f0bc8a3 as FINISHED
PASSED
tests/tracking/scripts/test_artifact_upload_manual.py::test_refit_run_completion 
Testing refit run completion logic...
Created refit run: 9831b7d19ede42caabe0cb8551229fb1
Refit run status: RUNNING
 Marked refit run 9831b7d19ede42caabe0cb8551229fb1 as FINISHED
 Verified refit run is FINISHED
PASSED
tests/tracking/unit/test_azureml_artifact_upload.py::TestAzureMLArtifactBuilderPatch::test_patch_registered_on_import PASSED
tests/tracking/unit/test_azureml_artifact_upload.py::TestAzureMLArtifactBuilderPatch::test_patch_handles_tracking_uri_parameter PASSED
tests/tracking/unit/test_azureml_artifact_upload.py::TestAzureMLArtifactBuilderPatch::test_patch_auto_applies_on_module_import PASSED
tests/tracking/unit/test_azureml_artifact_upload.py::TestArtifactUploadToChildRun::test_upload_to_refit_run_when_available PASSED
tests/tracking/unit/test_azureml_artifact_upload.py::TestArtifactUploadToChildRun::test_upload_to_parent_run_when_refit_not_available PASSED
tests/tracking/unit/test_azureml_artifact_upload.py::TestRefitRunFinishedStatus::test_refit_run_marked_finished_after_successful_upload PASSED
tests/tracking/unit/test_azureml_artifact_upload.py::TestRefitRunFinishedStatus::test_refit_run_marked_failed_after_upload_failure PASSED
tests/tracking/unit/test_azureml_artifact_upload.py::TestRefitRunFinishedStatus::test_refit_run_not_terminated_if_already_finished PASSED
tests/tracking/unit/test_azureml_artifact_upload.py::TestAzureMLCompatibility::test_azureml_mlflow_imported PASSED
tests/tracking/unit/test_azureml_artifact_upload.py::TestAzureMLCompatibility::test_artifact_repository_registry_has_azureml PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestAzureMLConfiguration::test_azure_ml_enabled PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestAzureMLConfiguration::test_azure_ml_disabled PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestExperimentNaming::test_build_mlflow_experiment_name_format PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestExperimentNaming::test_build_mlflow_experiment_name_all_stages PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestExperimentNaming::test_build_mlflow_experiment_name_special_characters PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestStageSpecificTrackingConfiguration::test_tracking_benchmark_enabled PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestStageSpecificTrackingConfiguration::test_tracking_training_config PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestStageSpecificTrackingConfiguration::test_tracking_conversion_config PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestNamingConfigurationDetails::test_naming_project_name PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestNamingConfigurationDetails::test_naming_project_name_default PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestNamingConfigurationDetails::test_naming_tags_config PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestNamingConfigurationDetails::test_naming_run_name_config PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestIndexCacheConfiguration::test_index_enabled PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestIndexCacheConfiguration::test_index_disabled PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestRunFinderConfiguration::test_run_finder_strict_mode_default_true PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestRunFinderConfiguration::test_run_finder_strict_mode_default_false PASSED
tests/tracking/unit/test_mlflow_config_comprehensive.py::TestRunFinderConfiguration::test_run_finder_default PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestArtifactUploadUtilities::test_log_artifact_safe_with_active_run PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestArtifactUploadUtilities::test_log_artifact_safe_with_explicit_run_id PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestArtifactUploadUtilities::test_log_artifact_safe_handles_errors PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestArtifactUploadUtilities::test_log_artifacts_safe_with_directory PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestArtifactUploadUtilities::test_upload_checkpoint_archive PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestRunLifecycleUtilities::test_terminate_run_safe_with_running_run PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestRunLifecycleUtilities::test_terminate_run_safe_with_already_terminated PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestRunLifecycleUtilities::test_terminate_run_safe_with_tags PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestRunLifecycleUtilities::test_terminate_run_with_tags PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestRunLifecycleUtilities::test_ensure_run_terminated PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestRunLifecycleUtilities::test_ensure_run_terminated_already_finished PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestRunCreationUtilities::test_get_or_create_experiment_existing PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestRunCreationUtilities::test_get_or_create_experiment_new PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestRunCreationUtilities::test_resolve_experiment_id_from_parent PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestRunCreationUtilities::test_resolve_experiment_id_from_name PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestURLUtilities::test_get_mlflow_run_url_azureml PASSED
tests/tracking/unit/test_mlflow_utilities.py::TestURLUtilities::test_get_mlflow_run_url_standard PASSED
tests/tracking/unit/test_mlflow_utils_config_inference.py::TestInferConfigDirFromPath::test_finds_config_in_parent_chain PASSED
tests/tracking/unit/test_mlflow_utils_config_inference.py::TestInferConfigDirFromPath::test_finds_config_at_root_level PASSED
tests/tracking/unit/test_mlflow_utils_config_inference.py::TestInferConfigDirFromPath::test_falls_back_to_cwd_when_not_found PASSED
tests/tracking/unit/test_mlflow_utils_config_inference.py::TestInferConfigDirFromPath::test_handles_none_path PASSED
tests/tracking/unit/test_mlflow_utils_config_inference.py::TestInferConfigDirFromPath::test_finds_first_config_in_parent_chain PASSED
tests/tracking/unit/test_mlflow_utils_config_inference.py::TestInferConfigDirFromPath::test_not_in_outputs_subdirectory PASSED
tests/tracking/unit/test_naming_centralized.py::test_naming_context_validation PASSED
tests/tracking/unit/test_naming_centralized.py::test_naming_context_final_training_requires_fingerprints PASSED
tests/tracking/unit/test_naming_centralized.py::test_naming_context_conversion_requires_parent_and_conv_fp PASSED
tests/tracking/unit/test_naming_centralized.py::test_create_naming_context_auto_detect PASSED
tests/tracking/unit/test_naming_centralized.py::test_build_output_path_hpo PASSED
tests/tracking/unit/test_naming_centralized.py::test_build_output_path_benchmarking PASSED
tests/tracking/unit/test_naming_centralized.py::test_build_output_path_final_training PASSED
tests/tracking/unit/test_naming_centralized.py::test_build_output_path_final_training_variant PASSED
tests/tracking/unit/test_naming_centralized.py::test_build_output_path_conversion PASSED
tests/tracking/unit/test_naming_centralized.py::test_build_output_path_best_configurations PASSED
tests/tracking/unit/test_naming_centralized.py::test_build_parent_training_id PASSED
tests/tracking/unit/test_naming_centralized.py::TestRunNameGeneration::test_hpo_trial_run_name PASSED
tests/tracking/unit/test_naming_centralized.py::TestRunNameGeneration::test_hpo_trial_fold_run_name PASSED
tests/tracking/unit/test_naming_centralized.py::TestRunNameGeneration::test_hpo_refit_run_name PASSED
tests/tracking/unit/test_naming_centralized.py::TestRunNameGeneration::test_hpo_sweep_run_name PASSED
tests/tracking/unit/test_naming_centralized.py::TestRunNameGeneration::test_run_name_max_length PASSED
tests/tracking/unit/test_naming_centralized.py::TestRunNameGeneration::test_run_name_forbidden_chars_removed PASSED
tests/tracking/unit/test_naming_placeholder_truncation.py::TestNamingPlaceholderTruncation::test_final_training_naming_with_unknown_placeholders PASSED
tests/tracking/unit/test_naming_placeholder_truncation.py::TestNamingPlaceholderTruncation::test_final_training_naming_pattern_with_unknown_placeholders PASSED
tests/tracking/unit/test_naming_placeholder_truncation.py::TestNamingPlaceholderTruncation::test_placeholder_truncation_to_8_chars PASSED
tests/tracking/unit/test_naming_placeholder_truncation.py::TestNamingPlaceholderTruncation::test_old_placeholder_behavior_would_truncate PASSED
tests/tracking/unit/test_naming_placeholder_truncation.py::TestNamingPlaceholderTruncation::test_naming_context_accepts_unknown_placeholders PASSED
tests/tracking/unit/test_naming_placeholder_truncation.py::TestNamingPlaceholderTruncation::test_token_values_with_unknown_placeholders PASSED
tests/tracking/unit/test_naming_policy.py::TestStudyKeyHashComputation::test_build_hpo_study_key PASSED
tests/tracking/unit/test_naming_policy.py::TestStudyKeyHashComputation::test_build_hpo_study_key_hash_length PASSED
tests/tracking/unit/test_naming_policy.py::TestStudyKeyHashComputation::test_build_hpo_study_key_hash_deterministic PASSED
tests/tracking/unit/test_naming_policy.py::TestStudyKeyHashComputation::test_build_hpo_study_key_hash_different_inputs PASSED
tests/tracking/unit/test_naming_policy.py::TestStudyKeyHashComputation::test_build_hpo_study_key_includes_all_config_components PASSED
tests/tracking/unit/test_naming_policy.py::TestStudyKeyHashComputation::test_build_hpo_study_key_without_benchmark PASSED
tests/tracking/unit/test_naming_policy.py::TestStudyKeyHashComputation::test_build_hpo_study_family_key PASSED
tests/tracking/unit/test_naming_policy.py::TestStudyKeyHashComputation::test_build_hpo_study_family_hash PASSED
tests/tracking/unit/test_naming_policy.py::TestTrialKeyHashComputation::test_build_hpo_trial_key PASSED
tests/tracking/unit/test_naming_policy.py::TestTrialKeyHashComputation::test_build_hpo_trial_key_hash_length PASSED
tests/tracking/unit/test_naming_policy.py::TestTrialKeyHashComputation::test_build_hpo_trial_key_hash_includes_study_hash PASSED
tests/tracking/unit/test_naming_policy.py::TestTrialKeyHashComputation::test_build_hpo_trial_key_same_params_same_hash PASSED
tests/tracking/unit/test_naming_policy.py::TestTrialKeyHashComputation::test_build_hpo_trial_key_different_params_different_hash PASSED
tests/tracking/unit/test_naming_policy.py::TestTrialKeyHashComputation::test_build_hpo_trial_key_normalizes_hyperparameters PASSED
tests/tracking/unit/test_naming_policy.py::TestTrialKeyHashComputation::test_build_hpo_trial_key_normalizes_strings PASSED
tests/tracking/unit/test_naming_policy.py::TestTrialKeyHashComputation::test_build_hpo_trial_key_with_smoke_yaml_params PASSED
tests/tracking/unit/test_naming_policy_details.py::TestComponentConfiguration::test_zero_pad_trial_number PASSED
tests/tracking/unit/test_naming_policy_details.py::TestComponentConfiguration::test_component_default_values PASSED
tests/tracking/unit/test_naming_policy_details.py::TestComponentConfiguration::test_component_length_truncation PASSED
tests/tracking/unit/test_naming_policy_details.py::TestSemanticSuffix::test_semantic_suffix_enabled PASSED
tests/tracking/unit/test_naming_policy_details.py::TestSemanticSuffix::test_semantic_suffix_max_length PASSED
tests/tracking/unit/test_naming_policy_details.py::TestSemanticSuffix::test_semantic_suffix_sanitization PASSED
tests/tracking/unit/test_naming_policy_details.py::TestVersionFormat::test_version_format_parsing PASSED
tests/tracking/unit/test_naming_policy_details.py::TestSeparatorPolicy::test_separator_field PASSED
tests/tracking/unit/test_naming_policy_details.py::TestNormalizationRules::test_normalization_env_replace PASSED
tests/tracking/unit/test_naming_policy_details.py::TestNormalizationRules::test_normalization_model_replace PASSED
tests/tracking/unit/test_naming_policy_details.py::TestValidationRules::test_validate_max_length PASSED
tests/tracking/unit/test_naming_policy_details.py::TestValidationRules::test_validate_forbidden_chars PASSED
tests/tracking/unit/test_naming_policy_details.py::TestValidationRules::test_validate_warn_length PASSED
tests/tracking/unit/test_sweep_tracker_config_inference.py::TestSweepTrackerConfigInference::test_log_best_trial_id_infers_config_from_output_dir FAILED
tests/tracking/unit/test_sweep_tracker_config_inference.py::TestSweepTrackerConfigInference::test_log_best_trial_id_finds_config_in_parent_chain FAILED
tests/tracking/unit/test_sweep_tracker_config_inference.py::TestSweepTrackerConfigInference::test_log_best_trial_id_falls_back_to_cwd_config_when_not_found FAILED
tests/tracking/unit/test_sweep_tracker_config_inference.py::TestSweepTrackerConfigInference::test_log_final_metrics_infers_config_correctly FAILED
tests/tracking/unit/test_sweep_tracker_config_inference.py::TestSweepTrackerConfigInference::test_log_final_metrics_uses_hpo_output_dir_when_output_dir_none FAILED
tests/tracking/unit/test_sweep_tracker_config_inference.py::TestSweepTrackerConfigInference::test_config_dir_not_in_outputs_directory FAILED
tests/tracking/unit/test_tags_comprehensive.py::TestTagsRegistry::test_load_tags_registry_from_file PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagsRegistry::test_load_tags_registry_fallback_to_defaults PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagsRegistry::test_load_tags_registry_merges_with_defaults PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagsRegistry::test_load_tags_registry_module_level_caching PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagsRegistry::test_load_tags_registry_schema_version_defaults_to_0 PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagsRegistry::test_tags_registry_key_access_all_sections PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagsRegistry::test_tags_registry_raises_tagkeyerror_for_missing_keys PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagsRegistry::test_tags_registry_handles_invalid_section_types PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagsRegistry::test_tags_registry_handles_invalid_key_value_types PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagBuilding::test_build_mlflow_tags_minimal_tags PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagBuilding::test_build_mlflow_tags_hpo_process PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagBuilding::test_build_mlflow_tags_hpo_refit_process PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagBuilding::test_build_mlflow_tags_benchmarking_process PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagBuilding::test_build_mlflow_tags_final_training_process PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagBuilding::test_build_mlflow_tags_conversion_process PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagBuilding::test_build_mlflow_tags_optional_tags PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagSanitization::test_sanitize_tag_value_truncates_exceeding_max_length PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagSanitization::test_sanitize_tag_value_adds_indicator_when_truncated PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagSanitization::test_sanitize_tag_value_preserves_values_within_limit PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagSanitization::test_sanitize_tag_value_handles_empty_strings PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagSanitization::test_sanitize_tag_value_uses_max_length_from_config PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagKeyResolution::test_get_tag_key_loads_from_registry PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagKeyResolution::test_get_tag_key_falls_back_to_fallback PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagKeyResolution::test_get_tag_key_raises_when_missing_and_no_fallback PASSED
tests/tracking/unit/test_tags_comprehensive.py::TestTagKeyResolution::test_get_tag_key_handles_registry_loading_failures PASSED
tests/tracking/unit/test_tags_registry.py::test_tags_registry_key_access PASSED
tests/tracking/unit/test_tags_registry.py::test_tags_registry_missing_key PASSED
tests/tracking/unit/test_tags_registry.py::test_tags_registry_validation_required_keys PASSED
tests/tracking/unit/test_tags_registry.py::test_load_tags_registry_from_file PASSED
tests/tracking/unit/test_tags_registry.py::test_load_tags_registry_missing_file PASSED
tests/tracking/unit/test_tags_registry.py::test_load_tags_registry_caching PASSED
tests/tracking/unit/test_tags_registry.py::test_load_tags_registry_merge_with_defaults PASSED
tests/tracking/unit/test_tags_registry.py::test_load_tags_registry_default_schema_version PASSED
tests/tracking/unit/test_tags_registry.py::test_tags_registry_invalid_section_type PASSED
tests/tracking/unit/test_tags_registry.py::test_tags_registry_invalid_key_type PASSED
tests/training/hpo/integration/test_hash_consistency.py::TestHashConsistency::test_v2_hash_computation_always_succeeds PASSED
tests/training/hpo/integration/test_hash_consistency.py::TestHashConsistency::test_v2_hash_with_empty_configs PASSED
tests/training/hpo/integration/test_hash_consistency.py::TestHashConsistency::test_trial_key_hash_consistency PASSED
tests/training/hpo/integration/test_hash_consistency.py::TestHashConsistency::test_hash_mismatch_detection PASSED
tests/training/hpo/integration/test_hash_consistency.py::TestRunTagConsistency::test_parent_run_has_v2_tags PASSED
tests/training/hpo/integration/test_hash_consistency.py::TestRefitLinking::test_refit_uses_parent_study_key_hash PASSED
tests/training/hpo/integration/test_hash_consistency.py::TestRefitLinking::test_refit_linking_tag_format PASSED
tests/training/hpo/integration/test_hash_consistency.py::TestParentTrialHashMismatch::test_parent_v2_trial_v1_mismatch_detection PASSED
tests/training/hpo/integration/test_hash_consistency.py::TestParentTrialHashMismatch::test_same_configs_produce_same_v2_hash PASSED
tests/training/hpo/unit/test_cv_hash_computation.py::TestCVTrialRunHashComputation::test_priority_1_use_provided_hashes PASSED
tests/training/hpo/unit/test_cv_hash_computation.py::TestCVTrialRunHashComputation::test_priority_2_get_from_parent_tags PASSED
tests/training/hpo/unit/test_cv_hash_computation.py::TestCVTrialRunHashComputation::test_priority_3_compute_v2_from_configs PASSED
tests/training/hpo/unit/test_cv_hash_computation.py::TestCVTrialRunHashComputation::test_hash_consistency_with_parent PASSED
tests/training/hpo/unit/test_cv_hash_computation.py::TestCVTrialRunHashComputation::test_missing_train_config_still_computes_hash PASSED
tests/training/hpo/unit/test_cv_hash_computation.py::TestCVTrialRunHashComputation::test_error_handling_parent_run_not_found PASSED
tests/training/hpo/unit/test_cv_hash_computation.py::TestCVTrialRunHashComputation::test_no_hpo_parent_run_id_returns_none PASSED
tests/training/hpo/unit/test_phase2_tags.py::TestSetPhase2HpoTags::test_sets_all_tags_successfully FAILED
tests/training/hpo/unit/test_phase2_tags.py::TestSetPhase2HpoTags::test_handles_missing_parent_run_id PASSED
tests/training/hpo/unit/test_phase2_tags.py::TestSetPhase2HpoTags::test_handles_none_parent_run_id PASSED
tests/training/hpo/unit/test_phase2_tags.py::TestSetPhase2HpoTags::test_handles_missing_data_config FAILED
tests/training/hpo/unit/test_phase2_tags.py::TestSetPhase2HpoTags::test_minimize_objective_direction FAILED
tests/training/hpo/unit/test_phase2_tags.py::TestSetPhase2HpoTags::test_legacy_goal_key_migration FAILED
tests/training/hpo/unit/test_phase2_tags.py::TestSetPhase2HpoTags::test_handles_exception_gracefully FAILED
tests/training/hpo/unit/test_phase2_tags.py::TestSetPhase2HpoTags::test_schema_version_1_0_fallback FAILED
tests/unit/api/test_extractors.py::TestFileTypeDetection::test_detect_pdf PASSED
tests/unit/api/test_extractors.py::TestFileTypeDetection::test_detect_png PASSED
tests/unit/api/test_extractors.py::TestFileTypeDetection::test_detect_jpeg PASSED
tests/unit/api/test_extractors.py::TestFileTypeDetection::test_invalid_file_type PASSED
tests/unit/api/test_extractors.py::TestPDFExtraction::test_extract_pdf_pymupdf FAILED
tests/unit/api/test_extractors.py::TestPDFExtraction::test_extract_pdf_pdfplumber FAILED
tests/unit/api/test_extractors.py::TestPDFExtraction::test_extract_pdf_invalid_extractor PASSED
tests/unit/api/test_extractors.py::TestImageExtraction::test_extract_image_easyocr FAILED
tests/unit/api/test_extractors.py::TestImageExtraction::test_extract_image_pytesseract FAILED
tests/unit/api/test_extractors.py::TestImageExtraction::test_extract_image_invalid_extractor PASSED
tests/unit/api/test_extractors.py::TestFileValidation::test_validate_file_success FAILED
tests/unit/api/test_extractors.py::TestFileValidation::test_validate_file_size_exceeded FAILED
tests/unit/api/test_inference.py::TestONNXInferenceEngine::test_load_model_success FAILED
tests/unit/api/test_inference.py::TestONNXInferenceEngine::test_load_model_file_not_found PASSED
tests/unit/api/test_inference.py::TestONNXInferenceEngine::test_predict_tokens FAILED
tests/unit/api/test_inference.py::TestONNXInferenceEngine::test_model_not_loaded_error PASSED
tests/unit/api/test_inference_fixes.py::TestInferenceFixes::test_tokenization_returns_numpy_arrays FAILED
tests/unit/api/test_inference_fixes.py::TestInferenceFixes::test_offset_mapping_extraction FAILED
tests/unit/api/test_inference_fixes.py::TestInferenceFixes::test_entity_extraction_with_offsets FAILED
tests/unit/api/test_inference_fixes.py::TestInferenceFixes::test_no_hanging_on_special_tokens FAILED
tests/unit/api/test_inference_performance.py::TestInferencePerformance::test_tokenization_does_not_hang FAILED
tests/unit/api/test_inference_performance.py::TestInferencePerformance::test_entity_extraction_with_offset_mapping FAILED
tests/unit/api/test_inference_performance.py::TestInferencePerformance::test_empty_text_handling FAILED
tests/unit/api/test_inference_performance.py::TestInferencePerformance::test_special_characters_handling FAILED
tests/unit/api/test_inference_performance.py::TestInferencePerformance::test_tokenization_consistency FAILED
tests/unit/shared/test_mlflow_setup.py::TestGetLocalTrackingUri::test_local_platform FAILED
tests/unit/shared/test_mlflow_setup.py::TestGetLocalTrackingUri::test_colab_with_drive FAILED
tests/unit/shared/test_mlflow_setup.py::TestGetLocalTrackingUri::test_colab_without_drive FAILED
tests/unit/shared/test_mlflow_setup.py::TestGetLocalTrackingUri::test_kaggle_platform FAILED
tests/unit/shared/test_mlflow_setup.py::TestGetAzureMlTrackingUri::test_success FAILED
tests/unit/shared/test_mlflow_setup.py::TestGetAzureMlTrackingUri::test_import_error FAILED
tests/unit/shared/test_mlflow_setup.py::TestGetAzureMlTrackingUri::test_workspace_access_error FAILED
tests/unit/shared/test_mlflow_setup.py::TestSetupMlflowCrossPlatform::test_local_fallback_no_ml_client FAILED
tests/unit/shared/test_mlflow_setup.py::TestSetupMlflowCrossPlatform::test_azure_ml_success FAILED
tests/unit/shared/test_mlflow_setup.py::TestSetupMlflowCrossPlatform::test_azure_ml_failure_with_fallback FAILED
tests/unit/shared/test_mlflow_setup.py::TestSetupMlflowCrossPlatform::test_azure_ml_failure_no_fallback FAILED
tests/unit/shared/test_mlflow_setup.py::TestSetupMlflowCrossPlatform::test_mlflow_not_installed FAILED
tests/unit/shared/test_mlflow_setup.py::TestPlatformSpecificBehavior::test_colab_drive_mounted FAILED
tests/unit/shared/test_mlflow_setup.py::TestPlatformSpecificBehavior::test_kaggle_platform FAILED
tests/unit/shared/test_mlflow_setup.py::TestPlatformSpecificBehavior::test_local_platform FAILED
tests/unit/shared/test_mlflow_setup.py::TestCreateMlClientFromConfig::test_success_with_env_vars FAILED
tests/unit/shared/test_mlflow_setup.py::TestCreateMlClientFromConfig::test_azure_ml_disabled FAILED
tests/unit/shared/test_mlflow_setup.py::TestCreateMlClientFromConfig::test_config_missing_azure_ml_section FAILED
tests/unit/shared/test_mlflow_setup.py::TestCreateMlClientFromConfig::test_missing_credentials FAILED
tests/unit/shared/test_mlflow_setup.py::TestCreateMlClientFromConfig::test_import_error FAILED
tests/unit/shared/test_mlflow_setup.py::TestSetupMlflowFromConfig::test_config_file_exists_azure_enabled FAILED
tests/unit/shared/test_mlflow_setup.py::TestSetupMlflowFromConfig::test_config_file_exists_azure_disabled FAILED
tests/unit/shared/test_mlflow_setup.py::TestSetupMlflowFromConfig::test_config_file_missing FAILED
tests/unit/shared/test_mlflow_setup.py::TestAzureMlNamespaceCollision::test_import_with_local_azureml_shadowing SKIPPEDe
'ensure_data_asset_uploaded' from 'azureml' (/opt/conda/envs/resume-ner-
training/lib/python3.10/site-packages/azureml/__init__.py))
tests/unit/shared/test_mlflow_setup.py::TestAzureMlNamespaceCollision::test_check_azureml_mlflow_available_with_shadowing FAILED
tests/unit/shared/test_mlflow_setup.py::TestAzureMlNamespaceCollision::test_local_azureml_functions_still_work_after_import SKIPPEDe
'ensure_data_asset_uploaded' from 'azureml' (/opt/conda/envs/resume-ner-
training/lib/python3.10/site-packages/azureml/__init__.py))
tests/unit/training/test_checkpoint_loader.py::TestValidateCheckpoint::test_validate_checkpoint_valid PASSED
tests/unit/training/test_checkpoint_loader.py::TestValidateCheckpoint::test_validate_checkpoint_with_safetensors PASSED
tests/unit/training/test_checkpoint_loader.py::TestValidateCheckpoint::test_validate_checkpoint_missing_config PASSED
tests/unit/training/test_checkpoint_loader.py::TestValidateCheckpoint::test_validate_checkpoint_missing_model PASSED
tests/unit/training/test_checkpoint_loader.py::TestValidateCheckpoint::test_validate_checkpoint_nonexistent PASSED
tests/unit/training/test_checkpoint_loader.py::TestResolveCheckpointPath::test_resolve_from_env_var PASSED
tests/unit/training/test_checkpoint_loader.py::TestResolveCheckpointPath::test_resolve_from_config PASSED
tests/unit/training/test_checkpoint_loader.py::TestResolveCheckpointPath::test_resolve_from_cache PASSED
tests/unit/training/test_checkpoint_loader.py::TestResolveCheckpointPath::test_resolve_with_pattern PASSED
tests/unit/training/test_checkpoint_loader.py::TestResolveCheckpointPath::test_resolve_none_when_invalid PASSED
tests/unit/training/test_checkpoint_loader.py::TestResolveCheckpointPath::test_resolve_none_when_not_configured PASSED
tests/unit/training/test_checkpoint_loader.py::TestResolveCheckpointPath::test_resolve_priority_env_over_config PASSED
tests/unit/training/test_cv_utils.py::TestKfoldSplitCreation::test_create_kfold_splits_returns_k_folds PASSED
tests/unit/training/test_cv_utils.py::TestKfoldSplitCreation::test_create_kfold_splits_all_samples_in_one_fold PASSED
tests/unit/training/test_cv_utils.py::TestKfoldSplitCreation::test_create_kfold_splits_stratified PASSED
tests/unit/training/test_cv_utils.py::TestKfoldSplitCreation::test_create_kfold_splits_same_seed_same_splits PASSED
tests/unit/training/test_cv_utils.py::TestKfoldSplitCreation::test_create_kfold_splits_different_seed_different_splits PASSED
tests/unit/training/test_cv_utils.py::TestKfoldSplitCreation::test_create_kfold_splits_smoke_yaml_params PASSED
tests/unit/training/test_cv_utils.py::TestKfoldSplitCreation::test_create_kfold_splits_no_shuffle PASSED
tests/unit/training/test_cv_utils.py::TestKfoldSplitCreation::test_create_kfold_splits_insufficient_samples PASSED
tests/unit/training/test_cv_utils.py::TestSaveLoadFoldSplits::test_save_fold_splits PASSED
tests/unit/training/test_cv_utils.py::TestSaveLoadFoldSplits::test_load_fold_splits PASSED
tests/unit/training/test_cv_utils.py::TestSaveLoadFoldSplits::test_load_fold_splits_file_not_found PASSED
tests/unit/training/test_cv_utils.py::TestSaveLoadFoldSplits::test_save_and_load_roundtrip PASSED
tests/unit/training/test_cv_utils.py::TestGetFoldData::test_get_fold_data PASSED
tests/unit/training/test_cv_utils.py::TestValidateSplits::test_validate_splits [CV] Fold 0: {'PERSON': 1, 'ORG': 1} | Missing: []
[CV] Fold 1: {'PERSON': 1, 'ORG': 1} | Missing: []
PASSED
tests/unit/training/test_data_combiner.py::TestCombineDatasets::test_new_only_strategy PASSED
tests/unit/training/test_data_combiner.py::TestCombineDatasets::test_combined_strategy PASSED
tests/unit/training/test_data_combiner.py::TestCombineDatasets::test_append_strategy PASSED
tests/unit/training/test_data_combiner.py::TestCombineDatasets::test_invalid_strategy PASSED
tests/unit/training/test_data_combiner.py::TestCombineDatasets::test_combined_requires_old_dataset PASSED
tests/unit/training/test_data_combiner.py::TestCombineDatasets::test_append_requires_old_dataset PASSED
tests/unit/training/test_data_combiner.py::TestCombineDatasets::test_new_dataset_not_found PASSED
tests/unit/training/test_data_combiner.py::TestCombineDatasets::test_old_dataset_not_found PASSED
tests/unit/training/test_data_combiner.py::TestCombineDatasets::test_no_validation_in_new_dataset PASSED
tests/unit/training/test_data_combiner.py::TestCombineDatasets::test_create_validation_split PASSED
tests/unit/training/test_train_config_defaults.py::test_core_training_defaults PASSED
tests/unit/training/test_train_config_defaults.py::test_metric_defaults PASSED
tests/unit/training/test_train_config_defaults.py::test_early_stopping_defaults PASSED
tests/unit/training/test_trainer.py::TestPrepareDataLoaders::test_prepare_data_loaders_basic PASSED
tests/unit/training/test_trainer.py::TestPrepareDataLoaders::test_prepare_data_loaders_with_indices PASSED
tests/unit/training/test_trainer.py::TestPrepareDataLoaders::test_prepare_data_loaders_use_all_data PASSED
tests/unit/training/test_trainer.py::TestPrepareDataLoaders::test_prepare_data_loaders_deberta_batch_size_cap PASSED
tests/unit/training/test_trainer.py::TestPrepareDataLoaders::test_prepare_data_loaders_val_split_fallback PASSED
tests/unit/training/test_trainer.py::TestPrepareDataLoaders::test_prepare_data_loaders_kfold_cv_val_from_train_data PASSED
tests/unit/training/test_trainer.py::TestCreateOptimizerAndScheduler::test_create_optimizer_and_scheduler_defaults PASSED
tests/unit/training/test_trainer.py::TestCreateOptimizerAndScheduler::test_create_optimizer_and_scheduler_custom PASSED
tests/unit/training/test_trainer.py::TestCreateOptimizerAndScheduler::test_create_optimizer_and_scheduler_warmup_capped PASSED
tests/unit/training/test_trainer.py::TestRunTrainingLoop::test_run_training_loop_basic PASSED
tests/unit/training/test_trainer.py::TestRunTrainingLoop::test_run_training_loop_multiple_epochs PASSED
tests/unit/training/test_trainer.py::TestSaveCheckpoint::test_save_checkpoint_success PASSED
tests/unit/training/test_trainer.py::TestSaveCheckpoint::test_save_checkpoint_creates_directory PASSED
tests/unit/training/test_trainer.py::TestTrainModel::test_train_model_basic PASSED
tests/unit/training/test_trainer.py::TestTrainModel::test_train_model_with_fold_splits PASSED
tests/unit/training/test_trainer.py::TestTrainModel::test_train_model_use_all_data PASSED
tests/unit/training/test_trainer.py::TestTrainModel::test_train_model_invalid_fold_idx PASSED
tests/workflows/test_full_workflow_e2e.py::test_full_workflow_e2e FAILED
tests/workflows/test_notebook_01_e2e.py::TestNotebookE2E_Core::test_environment_detection PASSED
tests/workflows/test_notebook_01_e2e.py::TestNotebookE2E_Core::test_path_setup PASSED
tests/workflows/test_notebook_01_e2e.py::TestNotebookE2E_Core::test_config_loading PASSED
tests/workflows/test_notebook_01_e2e.py::TestNotebookE2E_Core::test_dataset_verification PASSED
tests/workflows/test_notebook_01_e2e.py::TestNotebookE2E_Core::test_mlflow_setup PASSED
tests/workflows/test_notebook_01_e2e.py::TestNotebookE2E_Core::test_hpo_sweep_execution_mocked FAILED
tests/workflows/test_notebook_01_e2e.py::TestNotebookE2E_Core::test_benchmarking_execution_mocked PASSED
tests/workflows/test_notebook_01_e2e.py::TestNotebookE2E_Core::test_output_validation PASSED
tests/workflows/test_notebook_01_e2e.py::TestNotebookE2E_Full::test_repository_setup SKIPPED
tests/workflows/test_notebook_01_e2e.py::TestNotebookE2E_Full::test_dependency_check SKIPPED
tests/workflows/test_notebook_02_e2e.py::test_best_config_selection_e2e FAILED

=================================== FAILURES ===================================
_ TestRefitCheckpointDuplicationPrevention.test_refit_prevents_duplication_only_archive_uploaded _
tests/hpo/integration/test_refit_training.py:740: in test_refit_prevents_duplication_only_archive_uploaded
    assert len(checkpoint_archive_uploads) > 0, \
E   AssertionError: Checkpoint archive SHOULD be uploaded. Found: []
E   assert 0 > 0
E    +  where 0 = len([])
------------------------------ Captured log call -------------------------------
INFO     training.hpo.execution.local.refit:refit.py:107 [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
INFO     training.hpo.execution.local.refit:refit.py:130 [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
WARNING  infrastructure.paths.resolve:resolve.py:330 Pattern 'hpo_v2' not found in paths.yaml. Using fallback logic.
WARNING  infrastructure.naming.display_policy:display_policy.py:80 [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-28/test_refit_prevents_duplicatio0/config/naming.yaml, using empty policy
INFO     infrastructure.naming.mlflow.tags_registry:tags_registry.py:215 [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_refit_prevents_duplicatio0/config/tags.yaml, using defaults
WARNING  training.execution.mlflow_setup:mlflow_setup.py:128 Could not get parent run parent_123, using experiment directly: Run with id=parent_123 not found
INFO     training.execution.mlflow_setup:mlflow_setup.py:168  View run local_distilbert_hpo_refit_legacy at: sqlite:////workspaces/resume-ner-azureml/mlruns/#/experiments/2/runs/b76e01e4f71a434191d3892cb3c50ef3
INFO     training.execution.mlflow_setup:mlflow_setup.py:172 Created MLflow run: local_distilbert_hpo_refit_legacy (b76e01e4f71a...)
INFO     training.hpo.execution.local.refit:refit.py:522 [REFIT] Attempting to link refit run b76e01e4f71a... to trial run. trial_key_hash=present, hpo_parent_run_id=present
ERROR    training.hpo.execution.local.refit:refit.py:709 [REFIT]  Error linking refit run to trial run: Run with id=parent_123 not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=parent_123 not found
INFO     training.hpo.execution.local.refit:refit.py:522 [REFIT] Attempting to link refit run b76e01e4f71a... to trial run. trial_key_hash=present, hpo_parent_run_id=present
ERROR    training.hpo.execution.local.refit:refit.py:709 [REFIT]  Error linking refit run to trial run: Run with id=parent_123 not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=parent_123 not found
INFO     training.hpo.execution.local.refit:refit.py:498 [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
INFO     training.hpo.execution.local.refit:refit.py:416 [REFIT] Refit training completed. Metrics: 0.8, Checkpoint: /tmp/pytest-of-codespace/pytest-28/test_refit_prevents_duplicatio0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:935 Could not set refit_completed tag: Run with id=refit_run_123 not found
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:941 [LOG_BEST_CHECKPOINT] Using preferred checkpoint directory: /tmp/pytest-of-codespace/pytest-28/test_refit_prevents_duplicatio0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:962 Uploading checkpoint archive to MLflow...
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:967 Uploading checkpoint to refit run refit_run_12 (child of parent parent_123)
INFO     infrastructure.tracking.mlflow.artifacts.uploader:uploader.py:206 Creating checkpoint archive from /tmp/pytest-of-codespace/pytest-28/test_refit_prevents_duplicatio0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint...
INFO     infrastructure.tracking.mlflow.artifacts.manager:manager.py:156 Created checkpoint archive: /tmp/checkpoint_qlqi2wu1.tar.gz (1 files, 0.0MB)
INFO     infrastructure.tracking.mlflow._artifacts_file:artifacts.py:213 Uploading checkpoint archive (0.0MB)...
WARNING  infrastructure.tracking.mlflow._artifacts_file:artifacts.py:103 Failed to upload artifact checkpoint_qlqi2wu1.tar.gz: Run with id=refit_run_123 not found
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/artifacts.py", line 89, in log_artifact_safe
    retry_with_backoff(
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/utils.py", line 64, in retry_with_backoff
    return func()
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/artifacts.py", line 77, in upload_func
    client.log_artifact(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 2533, in log_artifact
    self._tracking_client.log_artifact(run_id, local_path, artifact_path)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 667, in log_artifact
    artifact_repo = self._get_artifact_repo(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 639, in _get_artifact_repo
    run = self.get_run(resource_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=refit_run_123 not found
WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:1022 Checkpoint upload returned False (may have been skipped)
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:1033 Marked study as complete with checkpoint uploaded (best trial: 0)
__________________ TestHealthEndpoint.test_model_info_loaded ___________________
tests/integration/api/test_api.py:99: in test_model_info_loaded
    assert response.status_code == 200
E   assert 503 == 200
E    +  where 503 = <Response [503 Service Unavailable]>.status_code
___________________ TestPredictEndpoint.test_predict_success ___________________
tests/integration/api/test_api.py:123: in test_predict_success
    assert response.status_code == 200
E   assert 503 == 200
E    +  where 503 = <Response [503 Service Unavailable]>.status_code
________________ TestPredictEndpoint.test_predict_batch_success ________________
tests/integration/api/test_api.py:135: in test_predict_batch_success
    assert response.status_code == 200
E   assert 503 == 200
E    +  where 503 = <Response [503 Service Unavailable]>.status_code
_____________ TestPredictEndpoint.test_predict_batch_size_exceeded _____________
tests/integration/api/test_api.py:147: in test_predict_batch_size_exceeded
    assert response.status_code == 400
E   assert 503 == 400
E    +  where 503 = <Response [503 Service Unavailable]>.status_code
___________________ TestFileEndpoints.test_predict_file_pdf ____________________
tests/integration/api/test_api.py:171: in test_predict_file_pdf
    assert response.status_code == 200
E   assert 503 == 200
E    +  where 503 = <Response [503 Service Unavailable]>.status_code
__________________ TestFileEndpoints.test_predict_file_image ___________________
tests/integration/api/test_api.py:185: in test_predict_file_image
    assert response.status_code == 200
E   assert 503 == 200
E    +  where 503 = <Response [503 Service Unavailable]>.status_code
_ TestArtifactAcquisitionEdgeCases.test_validation_false_allows_invalid_checkpoints _
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1447: in __enter__
    original, local = self.get_original()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1420: in get_original
    raise AttributeError(
E   AttributeError: <module 'evaluation.selection.artifact_unified.compat' from '/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/compat.py'> does not have the attribute '_validate_checkpoint'
_ TestArtifactAcquisitionEdgeCases.test_validation_true_rejects_invalid_checkpoints _
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1447: in __enter__
    original, local = self.get_original()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1420: in get_original
    raise AttributeError(
E   AttributeError: <module 'evaluation.selection.artifact_unified.compat' from '/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/compat.py'> does not have the attribute '_validate_checkpoint'
________ TestArtifactAcquisitionConfig.test_priority_order_local_first _________
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1447: in __enter__
    original, local = self.get_original()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1420: in get_original
    raise AttributeError(
E   AttributeError: <module 'evaluation.selection.artifact_unified.compat' from '/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/compat.py'> does not have the attribute '_validate_checkpoint'
________ TestArtifactAcquisitionConfig.test_priority_order_mlflow_first ________
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1447: in __enter__
    original, local = self.get_original()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1420: in get_original
    raise AttributeError(
E   AttributeError: <module 'evaluation.selection.artifact_unified.compat' from '/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/compat.py'> does not have the attribute '_validate_checkpoint'
____ TestArtifactAcquisitionConfig.test_local_validate_controls_validation _____
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1447: in __enter__
    original, local = self.get_original()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1420: in get_original
    raise AttributeError(
E   AttributeError: <module 'evaluation.selection.artifact_unified.compat' from '/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/compat.py'> does not have the attribute '_validate_checkpoint'
________ TestConfigOptionCombinations.test_config_with_disabled_sources ________
tests/selection/integration/test_artifact_acquisition_unified_config.py:386: in test_config_with_disabled_sources
    assert result.success is True
E   assert False is True
E    +  where False = ArtifactResult(request=ArtifactRequest(artifact_kind=<ArtifactKind.CHECKPOINT: 'checkpoint'>, run_id='test_run_123', backbone='distilbert', study_key_hash='study12345678', trial_key_hash='trial87654321', refit_run_id=None, experiment_name=None, metadata={}), success=False, path=None, source=None, status=None, error="Artifact not found in any configured source. Checked sources in priority order: ['local', 'drive', 'mlflow']. Request: artifact_kind=checkpoint, run_id=test_run_123..., backbone=distilbert", metadata={}).success
------------------------------ Captured log call -------------------------------
INFO     evaluation.selection.artifact_unified.acquisition:acquisition.py:101 Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_123..., study_key_hash=study123..., trial_key_hash=trial876...
INFO     infrastructure.naming.mlflow.tags_registry:tags_registry.py:215 [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_config_with_disabled_sour0/config/tags.yaml, using defaults
INFO     evaluation.selection.artifact_unified.acquisition:acquisition.py:148 Selected artifact run: artifact_run_id=test_run_123..., trial_run_id=test_run_123...
INFO     evaluation.selection.artifact_unified.acquisition:acquisition.py:168 Artifact acquisition config for checkpoint: priority=['local', 'drive', 'mlflow'], local.enabled=True, local.validate=True, drive.enabled=False, mlflow.enabled=False
ERROR    evaluation.selection.artifact_unified.acquisition:acquisition.py:227 Artifact not found in any configured source. Checked sources in priority order: ['local', 'drive', 'mlflow']. Request: artifact_kind=checkpoint, run_id=test_run_123..., backbone=distilbert
__ TestArtifactAcquisitionWorkflow.test_complete_workflow_with_default_config __
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1447: in __enter__
    original, local = self.get_original()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1420: in get_original
    raise AttributeError(
E   AttributeError: <module 'evaluation.selection.artifact_unified.compat' from '/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/compat.py'> does not have the attribute '_validate_checkpoint'
___ TestArtifactAcquisitionWorkflow.test_workflow_with_custom_priority_order ___
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1447: in __enter__
    original, local = self.get_original()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1420: in get_original
    raise AttributeError(
E   AttributeError: <module 'evaluation.selection.artifact_unified.compat' from '/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/compat.py'> does not have the attribute '_validate_checkpoint'
____ TestArtifactAcquisitionWorkflow.test_workflow_with_validation_disabled ____
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1447: in __enter__
    original, local = self.get_original()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1420: in get_original
    raise AttributeError(
E   AttributeError: <module 'evaluation.selection.artifact_unified.compat' from '/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/compat.py'> does not have the attribute '_validate_checkpoint'
____ TestArtifactAcquisitionWorkflow.test_workflow_with_all_sources_enabled ____
tests/selection/integration/test_artifact_acquisition_workflow.py:185: in test_workflow_with_all_sources_enabled
    result = acquire_best_model_checkpoint(
src/evaluation/selection/artifact_acquisition.py:390: in acquire_best_model_checkpoint
    return _acquire_best_model_checkpoint_unified(
src/evaluation/selection/artifact_unified/compat.py:182: in acquire_best_model_checkpoint
    raise ValueError(error_msg)
E   ValueError: 
E   [ERROR] Could not acquire checkpoint for run test_run...
E      Experiment: test_experiment-hpo-distilbert
E      Backbone: distilbert
E   
E   [TRIED] Strategies attempted:
E      1. Local
E      2. Drive
E      3. Mlflow
E   
E   [ERROR] Could not determine experiment ID: Run with id=test_run_id_123 not found
------------------------------ Captured log call -------------------------------
INFO     evaluation.selection.artifact_unified.compat:compat.py:104 Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
INFO     evaluation.selection.artifact_unified.acquisition:acquisition.py:101 Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_id_..., study_key_hash=study_ha..., trial_key_hash=trial_ha...
___ TestArtifactAcquisitionWorkflow.test_workflow_mlflow_fallback_to_manual ____
tests/selection/integration/test_artifact_acquisition_workflow.py:283: in test_workflow_mlflow_fallback_to_manual
    result = acquire_best_model_checkpoint(
src/evaluation/selection/artifact_acquisition.py:390: in acquire_best_model_checkpoint
    return _acquire_best_model_checkpoint_unified(
src/evaluation/selection/artifact_unified/compat.py:182: in acquire_best_model_checkpoint
    raise ValueError(error_msg)
E   ValueError: 
E   [ERROR] Could not acquire checkpoint for run test_run...
E      Experiment: test_experiment-hpo-distilbert
E      Backbone: distilbert
E   
E   [TRIED] Strategies attempted:
E      1. Mlflow
E   
E   [ERROR] Could not determine experiment ID: Run with id=test_run_id_123 not found
------------------------------ Captured log call -------------------------------
INFO     evaluation.selection.artifact_unified.compat:compat.py:104 Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
INFO     evaluation.selection.artifact_unified.acquisition:acquisition.py:101 Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_id_..., study_key_hash=study_ha..., trial_key_hash=trial_ha...
_______________ TestColabMounting.test_mount_colab_drive_success _______________
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1431: in __enter__
    self.target = self.getter()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1618: in <lambda>
    getter = lambda: _importer(target)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1257: in _importer
    thing = __import__(import_path)
E   ModuleNotFoundError: No module named 'storage'
______________ TestColabMounting.test_create_colab_store_success _______________
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1431: in __enter__
    self.target = self.getter()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1618: in <lambda>
    getter = lambda: _importer(target)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1257: in _importer
    thing = __import__(import_path)
E   ModuleNotFoundError: No module named 'paths'
____________ TestColabMounting.test_create_colab_store_mount_fails _____________
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1431: in __enter__
    self.target = self.getter()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1618: in <lambda>
    getter = lambda: _importer(target)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1257: in _importer
    thing = __import__(import_path)
E   ModuleNotFoundError: No module named 'storage'
____________ TestColabMounting.test_create_colab_store_default_path ____________
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1431: in __enter__
    self.target = self.getter()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1618: in <lambda>
    getter = lambda: _importer(target)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1257: in _importer
    thing = __import__(import_path)
E   ModuleNotFoundError: No module named 'paths'
_ TestSweepTrackerConfigInference.test_log_best_trial_id_infers_config_from_output_dir _
tests/tracking/unit/test_sweep_tracker_config_inference.py:75: in test_log_best_trial_id_infers_config_from_output_dir
    assert mock_set_tag.called
E   AssertionError: assert False
E    +  where False = <MagicMock name='set_tag' id='124894341639296'>.called
------------------------------ Captured log call -------------------------------
WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:595 Could not find MLflow run ID for best trial 0. Found 1 runs with matching study_key_hash and trial_number, but none belong to parent parent-run-i... This may be a timing issue - trial runs may not be created/committed yet.
_ TestSweepTrackerConfigInference.test_log_best_trial_id_finds_config_in_parent_chain _
tests/tracking/unit/test_sweep_tracker_config_inference.py:124: in test_log_best_trial_id_finds_config_in_parent_chain
    assert mock_set_tag.called
E   AssertionError: assert False
E    +  where False = <MagicMock name='set_tag' id='124894308895040'>.called
------------------------------ Captured log call -------------------------------
WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:595 Could not find MLflow run ID for best trial 0. Found 1 runs with matching study_key_hash and trial_number, but none belong to parent parent-run-i... This may be a timing issue - trial runs may not be created/committed yet.
_ TestSweepTrackerConfigInference.test_log_best_trial_id_falls_back_to_cwd_config_when_not_found _
/workspaces/resume-ner-azureml/tests/tracking/unit/test_sweep_tracker_config_inference.py:167: in test_log_best_trial_id_falls_back_to_cwd_config_when_not_found
    assert mock_set_tag.called
E   AssertionError: assert False
E    +  where False = <MagicMock name='set_tag' id='124894298178032'>.called
------------------------------ Captured log call -------------------------------
WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:595 Could not find MLflow run ID for best trial 0. Found 1 runs with matching study_key_hash and trial_number, but none belong to parent parent-run-i... This may be a timing issue - trial runs may not be created/committed yet.
_ TestSweepTrackerConfigInference.test_log_final_metrics_infers_config_correctly _
tests/tracking/unit/test_sweep_tracker_config_inference.py:207: in test_log_final_metrics_infers_config_correctly
    assert mock_set_tag.called
E   AssertionError: assert False
E    +  where False = <MagicMock name='set_tag' id='124894341652416'>.called
------------------------------ Captured log call -------------------------------
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:316 [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=parent-run-i...
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:340 [LOG_FINAL_METRICS] Found 0 completed trials out of 1 total
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:351 [LOG_FINAL_METRICS] Logging metrics: n_trials=1, n_completed_trials=0
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:360 [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=0.5
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:369 [LOG_FINAL_METRICS] Logging best hyperparameters: {'learning_rate': 0.0001}
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:376 [LOG_FINAL_METRICS] Finding and logging best trial run ID...
WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:595 Could not find MLflow run ID for best trial 0. Found 1 runs with matching study_key_hash and trial_number, but none belong to parent parent-run-i... This may be a timing issue - trial runs may not be created/committed yet.
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:387 [LOG_FINAL_METRICS] Completed best trial ID logging
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:397 [LOG_FINAL_METRICS] Checkpoint logging enabled: None (upload_checkpoint=True)
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:430 [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:447 [LOG_FINAL_METRICS] Completed log_final_metrics successfully
_ TestSweepTrackerConfigInference.test_log_final_metrics_uses_hpo_output_dir_when_output_dir_none _
tests/tracking/unit/test_sweep_tracker_config_inference.py:255: in test_log_final_metrics_uses_hpo_output_dir_when_output_dir_none
    assert len(config_dirs_passed) > 0
E   assert 0 > 0
E    +  where 0 = len([])
------------------------------ Captured log call -------------------------------
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:316 [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=parent-run-i...
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:340 [LOG_FINAL_METRICS] Found 0 completed trials out of 1 total
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:351 [LOG_FINAL_METRICS] Logging metrics: n_trials=1, n_completed_trials=0
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:360 [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=0.5
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:369 [LOG_FINAL_METRICS] Logging best hyperparameters: {'learning_rate': 0.0001}
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:376 [LOG_FINAL_METRICS] Finding and logging best trial run ID...
WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:595 Could not find MLflow run ID for best trial 0. Found 1 runs with matching study_key_hash and trial_number, but none belong to parent parent-run-i... This may be a timing issue - trial runs may not be created/committed yet.
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:387 [LOG_FINAL_METRICS] Completed best trial ID logging
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:397 [LOG_FINAL_METRICS] Checkpoint logging enabled: None (upload_checkpoint=True)
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:430 [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:447 [LOG_FINAL_METRICS] Completed log_final_metrics successfully
___ TestSweepTrackerConfigInference.test_config_dir_not_in_outputs_directory ___
tests/tracking/unit/test_sweep_tracker_config_inference.py:309: in test_config_dir_not_in_outputs_directory
    assert len(config_dirs_passed) > 0
E   assert 0 > 0
E    +  where 0 = len([])
------------------------------ Captured log call -------------------------------
WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:595 Could not find MLflow run ID for best trial 0. Found 1 runs with matching study_key_hash and trial_number, but none belong to parent parent-run-i... This may be a timing issue - trial runs may not be created/committed yet.
_____________ TestSetPhase2HpoTags.test_sets_all_tags_successfully _____________
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1447: in __enter__
    original, local = self.get_original()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1420: in get_original
    raise AttributeError(
E   AttributeError: <module 'training.hpo.execution.local.sweep' from '/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py'> does not have the attribute 'TagsRegistry'
____________ TestSetPhase2HpoTags.test_handles_missing_data_config _____________
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1447: in __enter__
    original, local = self.get_original()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1420: in get_original
    raise AttributeError(
E   AttributeError: <module 'training.hpo.execution.local.sweep' from '/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py'> does not have the attribute 'TagsRegistry'
____________ TestSetPhase2HpoTags.test_minimize_objective_direction ____________
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1447: in __enter__
    original, local = self.get_original()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1420: in get_original
    raise AttributeError(
E   AttributeError: <module 'training.hpo.execution.local.sweep' from '/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py'> does not have the attribute 'TagsRegistry'
_____________ TestSetPhase2HpoTags.test_legacy_goal_key_migration ______________
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1447: in __enter__
    original, local = self.get_original()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1420: in get_original
    raise AttributeError(
E   AttributeError: <module 'training.hpo.execution.local.sweep' from '/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py'> does not have the attribute 'TagsRegistry'
____________ TestSetPhase2HpoTags.test_handles_exception_gracefully ____________
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1447: in __enter__
    original, local = self.get_original()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1420: in get_original
    raise AttributeError(
E   AttributeError: <module 'training.hpo.execution.local.sweep' from '/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py'> does not have the attribute 'TagsRegistry'
____________ TestSetPhase2HpoTags.test_schema_version_1_0_fallback _____________
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1447: in __enter__
    original, local = self.get_original()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1420: in get_original
    raise AttributeError(
E   AttributeError: <module 'training.hpo.execution.local.sweep' from '/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py'> does not have the attribute 'TagsRegistry'
__________________ TestPDFExtraction.test_extract_pdf_pymupdf __________________
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/pymupdf/__init__.py:2986: in __init__
    doc = mupdf.fz_open_document_with_stream(filetype if filetype else '', stream2)
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/pymupdf/mupdf.py:49180: in fz_open_document_with_stream
    return _mupdf.fz_open_document_with_stream(magic, stream)
E   pymupdf.mupdf.FzErrorFormat: code=7: no objects found

The above exception was the direct cause of the following exception:
src/deployment/api/extractors.py:126: in extract_text_from_pdf
    return _extract_pdf_pymupdf(pdf_bytes)
src/deployment/api/extractors.py:145: in _extract_pdf_pymupdf
    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/pymupdf/__init__.py:2989: in __init__
    raise FileDataError('Failed to open stream') from e
E   pymupdf.FileDataError: Failed to open stream

The above exception was the direct cause of the following exception:
tests/unit/api/test_extractors.py:54: in test_extract_pdf_pymupdf
    text = extract_text_from_pdf(pdf_content, extractor="pymupdf")
src/deployment/api/extractors.py:132: in extract_text_from_pdf
    raise TextExtractionError(
E   src.deployment.api.exceptions.TextExtractionError: Failed to extract text from PDF: Failed to open stream
________________ TestPDFExtraction.test_extract_pdf_pdfplumber _________________
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/pdfplumber/pdf.py:50: in __init__
    self.doc = PDFDocument(PDFParser(stream), password=password or "")
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/pdfminer/pdfdocument.py:741: in __init__
    raise PDFSyntaxError("No /Root object! - Is this really a PDF?")
E   pdfminer.pdfparser.PDFSyntaxError: No /Root object! - Is this really a PDF?

During handling of the above exception, another exception occurred:
src/deployment/api/extractors.py:128: in extract_text_from_pdf
    return _extract_pdf_pdfplumber(pdf_bytes)
src/deployment/api/extractors.py:170: in _extract_pdf_pdfplumber
    with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/pdfplumber/pdf.py:107: in open
    return cls(
/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/pdfplumber/pdf.py:52: in __init__
    raise PdfminerException(e)
E   pdfplumber.utils.exceptions.PdfminerException: No /Root object! - Is this really a PDF?

The above exception was the direct cause of the following exception:
tests/unit/api/test_extractors.py:65: in test_extract_pdf_pdfplumber
    text = extract_text_from_pdf(pdf_content, extractor="pdfplumber")
src/deployment/api/extractors.py:132: in extract_text_from_pdf
    raise TextExtractionError(
E   src.deployment.api.exceptions.TextExtractionError: Failed to extract text from PDF: No /Root object! - Is this really a PDF?
________________ TestImageExtraction.test_extract_image_easyocr ________________
src/deployment/api/extractors.py:222: in _get_easyocr_reader
    import easyocr
E   ModuleNotFoundError: No module named 'easyocr'

During handling of the above exception, another exception occurred:
src/deployment/api/extractors.py:201: in extract_text_from_image
    return _extract_image_easyocr(image_bytes)
src/deployment/api/extractors.py:250: in _extract_image_easyocr
    reader = _get_easyocr_reader(gpu=False)  # Set gpu=True if GPU available
src/deployment/api/extractors.py:232: in _get_easyocr_reader
    raise TextExtractionError(
E   src.deployment.api.exceptions.TextExtractionError: EasyOCR or Pillow not installed. Install with: pip install easyocr pillow

The above exception was the direct cause of the following exception:
tests/unit/api/test_extractors.py:86: in test_extract_image_easyocr
    text = extract_text_from_image(image_content, extractor="easyocr")
src/deployment/api/extractors.py:207: in extract_text_from_image
    raise TextExtractionError(
E   src.deployment.api.exceptions.TextExtractionError: Failed to extract text from image: EasyOCR or Pillow not installed. Install with: pip install easyocr pillow
______________ TestImageExtraction.test_extract_image_pytesseract ______________
src/deployment/api/extractors.py:284: in _extract_image_pytesseract
    import pytesseract
E   ModuleNotFoundError: No module named 'pytesseract'

During handling of the above exception, another exception occurred:
src/deployment/api/extractors.py:203: in extract_text_from_image
    return _extract_image_pytesseract(image_bytes)
src/deployment/api/extractors.py:287: in _extract_image_pytesseract
    raise TextExtractionError(
E   src.deployment.api.exceptions.TextExtractionError: pytesseract or Pillow not installed. Install with: pip install pytesseract pillow

The above exception was the direct cause of the following exception:
tests/unit/api/test_extractors.py:98: in test_extract_image_pytesseract
    text = extract_text_from_image(image_content, extractor="pytesseract")
src/deployment/api/extractors.py:207: in extract_text_from_image
    raise TextExtractionError(
E   src.deployment.api.exceptions.TextExtractionError: Failed to extract text from image: pytesseract or Pillow not installed. Install with: pip install pytesseract pillow
________________ TestFileValidation.test_validate_file_success _________________
async def functions are not natively supported.
You need to install a suitable plugin for your async framework, for example:
  - anyio
  - pytest-asyncio
  - pytest-tornasync
  - pytest-trio
  - pytest-twisted
_____________ TestFileValidation.test_validate_file_size_exceeded ______________
async def functions are not natively supported.
You need to install a suitable plugin for your async framework, for example:
  - anyio
  - pytest-asyncio
  - pytest-tornasync
  - pytest-trio
  - pytest-twisted
_______________ TestONNXInferenceEngine.test_load_model_success ________________
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1447: in __enter__
    original, local = self.get_original()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1420: in get_original
    raise AttributeError(
E   AttributeError: <module 'deployment.api.inference.engine' from '/workspaces/resume-ner-azureml/src/deployment/api/inference/engine.py'> does not have the attribute 'AutoConfig'
_________________ TestONNXInferenceEngine.test_predict_tokens __________________
src/deployment/api/inference/engine.py:273: in predict_tokens
    input_ids = tokenizer_output["input_ids"][0]
E   KeyError: 'input_ids'

The above exception was the direct cause of the following exception:
src/deployment/api/inference/engine.py:301: in predict_tokens
    raise InferenceError(f"Token decoding failed: {e}") from e
E   src.deployment.api.exceptions.InferenceError: Token decoding failed: 'input_ids'

During handling of the above exception, another exception occurred:
tests/unit/api/test_inference.py:127: in test_predict_tokens
    logits, tokens, tokenizer_output, offset_mapping = engine.predict_tokens("hello world")
src/deployment/api/inference/__init__.py:74: in predict_tokens
    return self._inference_runner.predict_tokens(text, max_length)
src/deployment/api/inference/engine.py:304: in predict_tokens
    del input_ids
E   UnboundLocalError: local variable 'input_ids' referenced before assignment
------------------------------ Captured log call -------------------------------
ERROR    src.deployment.api.inference.engine:engine.py:298 Token decoding failed: 'input_ids'
__________ TestInferenceFixes.test_tokenization_returns_numpy_arrays ___________
src/deployment/api/inference/engine.py:273: in predict_tokens
    input_ids = tokenizer_output["input_ids"][0]
E   KeyError: 'input_ids'

The above exception was the direct cause of the following exception:
src/deployment/api/inference/engine.py:301: in predict_tokens
    raise InferenceError(f"Token decoding failed: {e}") from e
E   src.deployment.api.exceptions.InferenceError: Token decoding failed: 'input_ids'

During handling of the above exception, another exception occurred:
tests/unit/api/test_inference_fixes.py:129: in test_tokenization_returns_numpy_arrays
    logits, tokens, tokenizer_output, offset_mapping = engine.predict_tokens("John Doe")
src/deployment/api/inference/__init__.py:74: in predict_tokens
    return self._inference_runner.predict_tokens(text, max_length)
src/deployment/api/inference/engine.py:304: in predict_tokens
    del input_ids
E   UnboundLocalError: local variable 'input_ids' referenced before assignment
------------------------------ Captured log call -------------------------------
ERROR    src.deployment.api.inference.engine:engine.py:298 Token decoding failed: 'input_ids'
______________ TestInferenceFixes.test_offset_mapping_extraction _______________
src/deployment/api/inference/engine.py:273: in predict_tokens
    input_ids = tokenizer_output["input_ids"][0]
E   KeyError: 'input_ids'

The above exception was the direct cause of the following exception:
src/deployment/api/inference/engine.py:301: in predict_tokens
    raise InferenceError(f"Token decoding failed: {e}") from e
E   src.deployment.api.exceptions.InferenceError: Token decoding failed: 'input_ids'

During handling of the above exception, another exception occurred:
tests/unit/api/test_inference_fixes.py:193: in test_offset_mapping_extraction
    logits, tokens, tokenizer_output, offset_mapping = engine.predict_tokens("John Doe")
src/deployment/api/inference/__init__.py:74: in predict_tokens
    return self._inference_runner.predict_tokens(text, max_length)
src/deployment/api/inference/engine.py:304: in predict_tokens
    del input_ids
E   UnboundLocalError: local variable 'input_ids' referenced before assignment
------------------------------ Captured log call -------------------------------
ERROR    src.deployment.api.inference.engine:engine.py:298 Token decoding failed: 'input_ids'
____________ TestInferenceFixes.test_entity_extraction_with_offsets ____________
src/deployment/api/inference/engine.py:273: in predict_tokens
    input_ids = tokenizer_output["input_ids"][0]
E   KeyError: 'input_ids'

The above exception was the direct cause of the following exception:
src/deployment/api/inference/engine.py:301: in predict_tokens
    raise InferenceError(f"Token decoding failed: {e}") from e
E   src.deployment.api.exceptions.InferenceError: Token decoding failed: 'input_ids'

During handling of the above exception, another exception occurred:
tests/unit/api/test_inference_fixes.py:299: in test_entity_extraction_with_offsets
    entities = engine.predict(text, return_confidence=False)
src/deployment/api/inference/__init__.py:125: in predict
    logits, tokens, tokenizer_output, offset_mapping = self.predict_tokens(
src/deployment/api/inference/__init__.py:74: in predict_tokens
    return self._inference_runner.predict_tokens(text, max_length)
src/deployment/api/inference/engine.py:304: in predict_tokens
    del input_ids
E   UnboundLocalError: local variable 'input_ids' referenced before assignment
------------------------------ Captured log call -------------------------------
ERROR    src.deployment.api.inference.engine:engine.py:298 Token decoding failed: 'input_ids'
_____________ TestInferenceFixes.test_no_hanging_on_special_tokens _____________
src/deployment/api/inference/engine.py:273: in predict_tokens
    input_ids = tokenizer_output["input_ids"][0]
E   KeyError: 'input_ids'

The above exception was the direct cause of the following exception:
src/deployment/api/inference/engine.py:301: in predict_tokens
    raise InferenceError(f"Token decoding failed: {e}") from e
E   src.deployment.api.exceptions.InferenceError: Token decoding failed: 'input_ids'

During handling of the above exception, another exception occurred:
tests/unit/api/test_inference_fixes.py:366: in test_no_hanging_on_special_tokens
    entities = engine.predict("", return_confidence=False)
src/deployment/api/inference/__init__.py:125: in predict
    logits, tokens, tokenizer_output, offset_mapping = self.predict_tokens(
src/deployment/api/inference/__init__.py:74: in predict_tokens
    return self._inference_runner.predict_tokens(text, max_length)
src/deployment/api/inference/engine.py:304: in predict_tokens
    del input_ids
E   UnboundLocalError: local variable 'input_ids' referenced before assignment
------------------------------ Captured log call -------------------------------
ERROR    src.deployment.api.inference.engine:engine.py:298 Token decoding failed: 'input_ids'
___________ TestInferencePerformance.test_tokenization_does_not_hang ___________
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1447: in __enter__
    original, local = self.get_original()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1420: in get_original
    raise AttributeError(
E   AttributeError: <module 'deployment.api.inference.engine' from '/workspaces/resume-ner-azureml/src/deployment/api/inference/engine.py'> does not have the attribute 'AutoTokenizer'
_____ TestInferencePerformance.test_entity_extraction_with_offset_mapping ______
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1447: in __enter__
    original, local = self.get_original()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1420: in get_original
    raise AttributeError(
E   AttributeError: <module 'deployment.api.inference.engine' from '/workspaces/resume-ner-azureml/src/deployment/api/inference/engine.py'> does not have the attribute 'AutoTokenizer'
______________ TestInferencePerformance.test_empty_text_handling _______________
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1447: in __enter__
    original, local = self.get_original()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1420: in get_original
    raise AttributeError(
E   AttributeError: <module 'deployment.api.inference.engine' from '/workspaces/resume-ner-azureml/src/deployment/api/inference/engine.py'> does not have the attribute 'AutoTokenizer'
__________ TestInferencePerformance.test_special_characters_handling ___________
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1447: in __enter__
    original, local = self.get_original()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1420: in get_original
    raise AttributeError(
E   AttributeError: <module 'deployment.api.inference.engine' from '/workspaces/resume-ner-azureml/src/deployment/api/inference/engine.py'> does not have the attribute 'AutoTokenizer'
____________ TestInferencePerformance.test_tokenization_consistency ____________
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1447: in __enter__
    original, local = self.get_original()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1420: in get_original
    raise AttributeError(
E   AttributeError: <module 'deployment.api.inference.engine' from '/workspaces/resume-ner-azureml/src/deployment/api/inference/engine.py'> does not have the attribute 'AutoTokenizer'
_________________ TestGetLocalTrackingUri.test_local_platform __________________
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1248: in _dot_lookup
    return getattr(thing, comp)
E   AttributeError: module 'shared' has no attribute 'mlflow_setup'

During handling of the above exception, another exception occurred:
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1431: in __enter__
    self.target = self.getter()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1618: in <lambda>
    getter = lambda: _importer(target)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1261: in _importer
    thing = _dot_lookup(thing, comp, import_path)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1250: in _dot_lookup
    __import__(import_path)
E   ModuleNotFoundError: No module named 'shared.mlflow_setup'
________________ TestGetLocalTrackingUri.test_colab_with_drive _________________
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1248: in _dot_lookup
    return getattr(thing, comp)
E   AttributeError: module 'shared' has no attribute 'mlflow_setup'

During handling of the above exception, another exception occurred:
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1431: in __enter__
    self.target = self.getter()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1618: in <lambda>
    getter = lambda: _importer(target)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1261: in _importer
    thing = _dot_lookup(thing, comp, import_path)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1250: in _dot_lookup
    __import__(import_path)
E   ModuleNotFoundError: No module named 'shared.mlflow_setup'
_______________ TestGetLocalTrackingUri.test_colab_without_drive _______________
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1248: in _dot_lookup
    return getattr(thing, comp)
E   AttributeError: module 'shared' has no attribute 'mlflow_setup'

During handling of the above exception, another exception occurred:
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1431: in __enter__
    self.target = self.getter()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1618: in <lambda>
    getter = lambda: _importer(target)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1261: in _importer
    thing = _dot_lookup(thing, comp, import_path)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1250: in _dot_lookup
    __import__(import_path)
E   ModuleNotFoundError: No module named 'shared.mlflow_setup'
_________________ TestGetLocalTrackingUri.test_kaggle_platform _________________
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1248: in _dot_lookup
    return getattr(thing, comp)
E   AttributeError: module 'shared' has no attribute 'mlflow_setup'

During handling of the above exception, another exception occurred:
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1431: in __enter__
    self.target = self.getter()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1618: in <lambda>
    getter = lambda: _importer(target)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1261: in _importer
    thing = _dot_lookup(thing, comp, import_path)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1250: in _dot_lookup
    __import__(import_path)
E   ModuleNotFoundError: No module named 'shared.mlflow_setup'
____________________ TestGetAzureMlTrackingUri.test_success ____________________
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1248: in _dot_lookup
    return getattr(thing, comp)
E   AttributeError: module 'shared' has no attribute 'mlflow_setup'

During handling of the above exception, another exception occurred:
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1431: in __enter__
    self.target = self.getter()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1618: in <lambda>
    getter = lambda: _importer(target)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1261: in _importer
    thing = _dot_lookup(thing, comp, import_path)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1250: in _dot_lookup
    __import__(import_path)
E   ModuleNotFoundError: No module named 'shared.mlflow_setup'
_________________ TestGetAzureMlTrackingUri.test_import_error __________________
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1248: in _dot_lookup
    return getattr(thing, comp)
E   AttributeError: module 'shared' has no attribute 'mlflow_setup'

During handling of the above exception, another exception occurred:
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1431: in __enter__
    self.target = self.getter()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1618: in <lambda>
    getter = lambda: _importer(target)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1261: in _importer
    thing = _dot_lookup(thing, comp, import_path)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1250: in _dot_lookup
    __import__(import_path)
E   ModuleNotFoundError: No module named 'shared.mlflow_setup'
____________ TestGetAzureMlTrackingUri.test_workspace_access_error _____________
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1248: in _dot_lookup
    return getattr(thing, comp)
E   AttributeError: module 'shared' has no attribute 'mlflow_setup'

During handling of the above exception, another exception occurred:
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1431: in __enter__
    self.target = self.getter()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1618: in <lambda>
    getter = lambda: _importer(target)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1261: in _importer
    thing = _dot_lookup(thing, comp, import_path)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1250: in _dot_lookup
    __import__(import_path)
E   ModuleNotFoundError: No module named 'shared.mlflow_setup'
________ TestSetupMlflowCrossPlatform.test_local_fallback_no_ml_client _________
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1248: in _dot_lookup
    return getattr(thing, comp)
E   AttributeError: module 'shared' has no attribute 'mlflow_setup'

During handling of the above exception, another exception occurred:
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1431: in __enter__
    self.target = self.getter()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1618: in <lambda>
    getter = lambda: _importer(target)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1261: in _importer
    thing = _dot_lookup(thing, comp, import_path)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1250: in _dot_lookup
    __import__(import_path)
E   ModuleNotFoundError: No module named 'shared.mlflow_setup'
______________ TestSetupMlflowCrossPlatform.test_azure_ml_success ______________
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1248: in _dot_lookup
    return getattr(thing, comp)
E   AttributeError: module 'shared' has no attribute 'mlflow_setup'

During handling of the above exception, another exception occurred:
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1431: in __enter__
    self.target = self.getter()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1618: in <lambda>
    getter = lambda: _importer(target)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1261: in _importer
    thing = _dot_lookup(thing, comp, import_path)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1250: in _dot_lookup
    __import__(import_path)
E   ModuleNotFoundError: No module named 'shared.mlflow_setup'
_______ TestSetupMlflowCrossPlatform.test_azure_ml_failure_with_fallback _______
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1248: in _dot_lookup
    return getattr(thing, comp)
E   AttributeError: module 'shared' has no attribute 'mlflow_setup'

During handling of the above exception, another exception occurred:
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1431: in __enter__
    self.target = self.getter()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1618: in <lambda>
    getter = lambda: _importer(target)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1261: in _importer
    thing = _dot_lookup(thing, comp, import_path)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1250: in _dot_lookup
    __import__(import_path)
E   ModuleNotFoundError: No module named 'shared.mlflow_setup'
________ TestSetupMlflowCrossPlatform.test_azure_ml_failure_no_fallback ________
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1248: in _dot_lookup
    return getattr(thing, comp)
E   AttributeError: module 'shared' has no attribute 'mlflow_setup'

During handling of the above exception, another exception occurred:
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1431: in __enter__
    self.target = self.getter()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1618: in <lambda>
    getter = lambda: _importer(target)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1261: in _importer
    thing = _dot_lookup(thing, comp, import_path)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1250: in _dot_lookup
    __import__(import_path)
E   ModuleNotFoundError: No module named 'shared.mlflow_setup'
____________ TestSetupMlflowCrossPlatform.test_mlflow_not_installed ____________
tests/unit/shared/test_mlflow_setup.py:234: in test_mlflow_not_installed
    import shared.mlflow_setup
E   ModuleNotFoundError: No module named 'shared.mlflow_setup'
____________ TestPlatformSpecificBehavior.test_colab_drive_mounted _____________
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1248: in _dot_lookup
    return getattr(thing, comp)
E   AttributeError: module 'shared' has no attribute 'mlflow_setup'

During handling of the above exception, another exception occurred:
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1833: in _inner
    return f(*args, **kw)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1431: in __enter__
    self.target = self.getter()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1618: in <lambda>
    getter = lambda: _importer(target)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1261: in _importer
    thing = _dot_lookup(thing, comp, import_path)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1250: in _dot_lookup
    __import__(import_path)
E   ModuleNotFoundError: No module named 'shared.mlflow_setup'
______________ TestPlatformSpecificBehavior.test_kaggle_platform _______________
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1248: in _dot_lookup
    return getattr(thing, comp)
E   AttributeError: module 'shared' has no attribute 'mlflow_setup'

During handling of the above exception, another exception occurred:
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1833: in _inner
    return f(*args, **kw)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1431: in __enter__
    self.target = self.getter()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1618: in <lambda>
    getter = lambda: _importer(target)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1261: in _importer
    thing = _dot_lookup(thing, comp, import_path)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1250: in _dot_lookup
    __import__(import_path)
E   ModuleNotFoundError: No module named 'shared.mlflow_setup'
_______________ TestPlatformSpecificBehavior.test_local_platform _______________
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1248: in _dot_lookup
    return getattr(thing, comp)
E   AttributeError: module 'shared' has no attribute 'mlflow_setup'

During handling of the above exception, another exception occurred:
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1431: in __enter__
    self.target = self.getter()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1618: in <lambda>
    getter = lambda: _importer(target)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1261: in _importer
    thing = _dot_lookup(thing, comp, import_path)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1250: in _dot_lookup
    __import__(import_path)
E   ModuleNotFoundError: No module named 'shared.mlflow_setup'
___________ TestCreateMlClientFromConfig.test_success_with_env_vars ____________
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1248: in _dot_lookup
    return getattr(thing, comp)
E   AttributeError: module 'shared' has no attribute 'mlflow_setup'

During handling of the above exception, another exception occurred:
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1431: in __enter__
    self.target = self.getter()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1618: in <lambda>
    getter = lambda: _importer(target)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1261: in _importer
    thing = _dot_lookup(thing, comp, import_path)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1250: in _dot_lookup
    __import__(import_path)
E   ModuleNotFoundError: No module named 'shared.mlflow_setup'
_____________ TestCreateMlClientFromConfig.test_azure_ml_disabled ______________
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1248: in _dot_lookup
    return getattr(thing, comp)
E   AttributeError: module 'shared' has no attribute 'mlflow_setup'

During handling of the above exception, another exception occurred:
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1431: in __enter__
    self.target = self.getter()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1618: in <lambda>
    getter = lambda: _importer(target)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1261: in _importer
    thing = _dot_lookup(thing, comp, import_path)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1250: in _dot_lookup
    __import__(import_path)
E   ModuleNotFoundError: No module named 'shared.mlflow_setup'
______ TestCreateMlClientFromConfig.test_config_missing_azure_ml_section _______
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1248: in _dot_lookup
    return getattr(thing, comp)
E   AttributeError: module 'shared' has no attribute 'mlflow_setup'

During handling of the above exception, another exception occurred:
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1431: in __enter__
    self.target = self.getter()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1618: in <lambda>
    getter = lambda: _importer(target)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1261: in _importer
    thing = _dot_lookup(thing, comp, import_path)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1250: in _dot_lookup
    __import__(import_path)
E   ModuleNotFoundError: No module named 'shared.mlflow_setup'
____________ TestCreateMlClientFromConfig.test_missing_credentials _____________
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1248: in _dot_lookup
    return getattr(thing, comp)
E   AttributeError: module 'shared' has no attribute 'mlflow_setup'

During handling of the above exception, another exception occurred:
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1431: in __enter__
    self.target = self.getter()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1618: in <lambda>
    getter = lambda: _importer(target)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1261: in _importer
    thing = _dot_lookup(thing, comp, import_path)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1250: in _dot_lookup
    __import__(import_path)
E   ModuleNotFoundError: No module named 'shared.mlflow_setup'
________________ TestCreateMlClientFromConfig.test_import_error ________________
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1248: in _dot_lookup
    return getattr(thing, comp)
E   AttributeError: module 'shared' has no attribute 'mlflow_setup'

During handling of the above exception, another exception occurred:
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1431: in __enter__
    self.target = self.getter()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1618: in <lambda>
    getter = lambda: _importer(target)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1261: in _importer
    thing = _dot_lookup(thing, comp, import_path)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1250: in _dot_lookup
    __import__(import_path)
E   ModuleNotFoundError: No module named 'shared.mlflow_setup'
_______ TestSetupMlflowFromConfig.test_config_file_exists_azure_enabled ________
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1248: in _dot_lookup
    return getattr(thing, comp)
E   AttributeError: module 'shared' has no attribute 'mlflow_setup'

During handling of the above exception, another exception occurred:
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1431: in __enter__
    self.target = self.getter()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1618: in <lambda>
    getter = lambda: _importer(target)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1261: in _importer
    thing = _dot_lookup(thing, comp, import_path)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1250: in _dot_lookup
    __import__(import_path)
E   ModuleNotFoundError: No module named 'shared.mlflow_setup'
_______ TestSetupMlflowFromConfig.test_config_file_exists_azure_disabled _______
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1248: in _dot_lookup
    return getattr(thing, comp)
E   AttributeError: module 'shared' has no attribute 'mlflow_setup'

During handling of the above exception, another exception occurred:
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1431: in __enter__
    self.target = self.getter()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1618: in <lambda>
    getter = lambda: _importer(target)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1261: in _importer
    thing = _dot_lookup(thing, comp, import_path)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1250: in _dot_lookup
    __import__(import_path)
E   ModuleNotFoundError: No module named 'shared.mlflow_setup'
______________ TestSetupMlflowFromConfig.test_config_file_missing ______________
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1248: in _dot_lookup
    return getattr(thing, comp)
E   AttributeError: module 'shared' has no attribute 'mlflow_setup'

During handling of the above exception, another exception occurred:
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1376: in patched
    with self.decoration_helper(patched,
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:135: in __enter__
    return next(self.gen)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1358: in decoration_helper
    arg = exit_stack.enter_context(patching)
/opt/conda/envs/resume-ner-training/lib/python3.10/contextlib.py:492: in enter_context
    result = _cm_type.__enter__(cm)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1431: in __enter__
    self.target = self.getter()
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1618: in <lambda>
    getter = lambda: _importer(target)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1261: in _importer
    thing = _dot_lookup(thing, comp, import_path)
/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py:1250: in _dot_lookup
    __import__(import_path)
E   ModuleNotFoundError: No module named 'shared.mlflow_setup'
_ TestAzureMlNamespaceCollision.test_check_azureml_mlflow_available_with_shadowing _
tests/unit/shared/test_mlflow_setup.py:554: in test_check_azureml_mlflow_available_with_shadowing
    import shared.mlflow_setup
E   ModuleNotFoundError: No module named 'shared.mlflow_setup'
____________________________ test_full_workflow_e2e ____________________________
tests/workflows/test_full_workflow_e2e.py:356: in test_full_workflow_e2e
    from tracking.mlflow.trackers.benchmark_tracker import MLflowBenchmarkTracker
E   ModuleNotFoundError: No module named 'tracking.mlflow'
------------------------------ Captured log call -------------------------------
INFO     training.hpo.execution.local.sweep:sweep.py:637 [EARLY HASH] Computed v2 study_key_hash=1957109b1932df3e... for folder creation
INFO     infrastructure.naming.mlflow.config:config.py:358 [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
INFO     infrastructure.naming.mlflow.config:config.py:365 [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
INFO     orchestration.jobs.tracking.index.version_counter:version_counter.py:68 [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:48ad2237c71414ba63be6d5e23836962988169d56e2a8..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
INFO     orchestration.jobs.tracking.index.version_counter:version_counter.py:85 [Reserve Version] Loaded 21 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
INFO     orchestration.jobs.tracking.index.version_counter:version_counter.py:133 [Reserve Version] Found 4 allocations for counter_key (after deduplication): committed=[], reserved=[1, 2, 3, 4], expired=[], max_committed_version=0
INFO     orchestration.jobs.tracking.index.version_counter:version_counter.py:140 [Reserve Version] Allocation details: [(1, 'reserved', 'pending_2026'), (2, 'reserved', 'pending_2026'), (3, 'reserved', 'pending_2026'), (4, 'reserved', 'pending_2026')]
INFO     orchestration.jobs.tracking.index.version_counter:version_counter.py:177 [Reserve Version] Reserving next version: 5 (incremented from max_committed=0, skipped 4 reserved/expired versions)
INFO     orchestration.jobs.tracking.index.version_counter:version_counter.py:205 [Reserve Version]  Successfully reserved version 5 for counter_key resume-ner:hpo:48ad2237c71414ba63be6d5e23836962988... (run_id: pending_2026...)
INFO     training.hpo.execution.local.sweep:sweep.py:809 [HPO Setup] k_folds=None, fold_splits_file=None, k_fold config: {'enabled': True, 'n_splits': 5, 'random_seed': 42, 'shuffle': True, 'stratified': True}
INFO     common.shared.mlflow_setup:mlflow_setup.py:303 Using local tracking: sqlite:////workspaces/resume-ner-azureml/mlruns/mlflow.db
WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:1054 Using LOCAL MLflow tracking (not Azure ML)
INFO     infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:1056 To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
ERROR    infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:226 [START_SWEEP_RUN] MLflow tracking failed: Run with UUID 1eb673147e1a443faf92e853fdb86126 is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True
ERROR    infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:227 [START_SWEEP_RUN] Traceback: Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/trackers/sweep_tracker.py", line 118, in start_sweep_run
    with mlflow.start_run(run_name=run_name) as parent_run:
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/fluent.py", line 380, in start_run
    raise Exception(
Exception: Run with UUID 1eb673147e1a443faf92e853fdb86126 is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True

WARNING  infrastructure.tracking.mlflow.trackers.sweep_tracker:sweep_tracker.py:229 Continuing HPO without MLflow tracking...
INFO     training.hpo.execution.local.sweep:sweep.py:960 Setting Phase 2 tags on parent run None... (data_config=present, train_config=present)
WARNING  training.hpo.execution.local.sweep:sweep.py:446 _set_phase2_hpo_tags: parent_run_id is empty, skipping
INFO     training.hpo.execution.local.sweep:sweep.py:973  Successfully completed Phase 2 tag setting for parent run None...
INFO     training.hpo.tracking.cleanup:cleanup.py:119 [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.
INFO     training.hpo.execution.local.sweep:sweep.py:234 [Trial 0] fold_splits=None, using non-CV path
INFO     infrastructure.naming.mlflow.config:config.py:358 [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
INFO     infrastructure.naming.mlflow.config:config.py:365 [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
INFO     orchestration.jobs.tracking.index.version_counter:version_counter.py:68 [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:90e118c2e99c2340248a129c7810b54c998771a3dd3bf..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
INFO     orchestration.jobs.tracking.index.version_counter:version_counter.py:85 [Reserve Version] Loaded 22 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
INFO     orchestration.jobs.tracking.index.version_counter:version_counter.py:133 [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0
INFO     orchestration.jobs.tracking.index.version_counter:version_counter.py:177 [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)
INFO     orchestration.jobs.tracking.index.version_counter:version_counter.py:205 [Reserve Version]  Successfully reserved version 1 for counter_key resume-ner:hpo:90e118c2e99c2340248a129c7810b54c998... (run_id: pending_2026...)
INFO     training.hpo.tracking.runs:runs.py:164 [TRIAL_RUN_NO_CV] Created trial run (no CV): 91a2b981c8b4... (trial 0). Run remains RUNNING until training completes.
INFO     training.hpo.execution.local.trial:subprocess_runner.py:362 Training completed
INFO     training.hpo.execution.local.trial:trial.py:171 [TRIAL] Training completed. Objective metric 'macro-f1': 0.75
INFO     training.hpo.tracking.runs:runs.py:186 [TRIAL_RUN_NO_CV] Training completed. Marking trial run 91a2b981c8b4... as FINISHED (trial 0)
INFO     infrastructure.tracking.mlflow.lifecycle:lifecycle.py:99 Successfully terminated run 91a2b981c8b4... with status FINISHED
INFO     training.hpo.tracking.runs:runs.py:191 [TRIAL_RUN_NO_CV] Successfully marked trial run 91a2b981c8b4... as FINISHED
INFO     training.hpo.trial.callback:callback.py:91 
INFO     training.hpo.trial.callback:callback.py:94 [BEST]: trial_0
INFO     training.hpo.trial.callback:callback.py:95   Metrics: macro-f1=0.750000
INFO     training.hpo.trial.callback:callback.py:96   Params: learning_rate=3.63e-05 | batch_size=8 | dropout=0.184077 | weight_decay=0.005637
INFO     training.hpo.execution.local.sweep:sweep.py:1064 [REFIT] Starting refit training for best trial 0
WARNING  training.hpo.execution.local.sweep:sweep.py:1151 [REFIT] Could not determine trial_key_hash from best trial run tags. Refit run may not match trial run.
INFO     training.hpo.execution.local.refit:refit.py:107 [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3.631553947908713e-05, 'batch_size': 8, 'dropout': 0.1840768901656663, 'weight_decay': 0.005636906588031445}
INFO     training.hpo.execution.local.refit:refit.py:130 [REFIT] Computed trial_id='trial_0_20260115_000615', run_id='20260115_000615', trial_number=0
WARNING  training.hpo.execution.local.refit:refit.py:168 [REFIT] Computed trial_key_hash=a92cafcbb5a998a1... from trial parameters (fallback - may not match trial run hash). Trial run tags should be used as SSOT.
INFO     infrastructure.naming.mlflow.config:config.py:358 [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
INFO     infrastructure.naming.mlflow.config:config.py:365 [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
INFO     orchestration.jobs.tracking.index.version_counter:version_counter.py:68 [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:5984e92490c0ddc690bcd2af3dac114a85484d9c81527..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
INFO     orchestration.jobs.tracking.index.version_counter:version_counter.py:85 [Reserve Version] Loaded 23 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
INFO     orchestration.jobs.tracking.index.version_counter:version_counter.py:133 [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0
INFO     orchestration.jobs.tracking.index.version_counter:version_counter.py:177 [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)
INFO     orchestration.jobs.tracking.index.version_counter:version_counter.py:205 [Reserve Version]  Successfully reserved version 1 for counter_key resume-ner:hpo:5984e92490c0ddc690bcd2af3dac114a854... (run_id: pending_2026...)
INFO     training.hpo.execution.local.refit:subprocess_runner.py:362 Training completed
INFO     training.hpo.execution.local.refit:refit.py:416 [REFIT] Refit training completed. Metrics: 0.75, Checkpoint: /workspaces/resume-ner-azureml/outputs/hpo/local/distilroberta/study-1957109b/trial-a92cafcb/refit/checkpoint
INFO     training.hpo.execution.local.sweep:sweep.py:1207 [REFIT] Refit training completed. Metrics: {'macro-f1': 0.75}, Checkpoint: /workspaces/resume-ner-azureml/outputs/hpo/local/distilroberta/study-1957109b/trial-a92cafcb/refit/checkpoint, Run ID: None...
INFO     infrastructure.naming.mlflow.tags_registry:tags_registry.py:215 [Tags Registry] Config file not found at /workspaces/resume-ner-azureml/outputs/hpo/config/tags.yaml, using defaults
WARNING  training.hpo.execution.local.sweep:sweep.py:1373 [REFIT] No refit_run_id available to mark as FINISHED
INFO     training.hpo.checkpoint.cleanup:cleanup.py:341 Final cleanup: kept checkpoints for best trial 0 (metric=0.750000, CV=no, refit=no), deleted 0 non-best checkpoints
_____________ TestNotebookE2E_Core.test_hpo_sweep_execution_mocked _____________
tests/workflows/test_notebook_01_e2e.py:405: in test_hpo_sweep_execution_mocked
    from naming import get_stage_config
E   ModuleNotFoundError: No module named 'naming'
________________________ test_best_config_selection_e2e ________________________
tests/workflows/test_notebook_02_e2e.py:344: in test_best_config_selection_e2e
    conversion_output_dir = execute_conversion(
src/deployment/conversion/orchestration.py:344: in execute_conversion
    raise RuntimeError(error_msg)
E   RuntimeError: Model conversion failed with return code 1
E   
E   Stderr output (last 30 lines):
E   [conversion.execution] Output directory: '/workspaces/resume-ner-azureml/outputs/conversion/local/local/spec-aaaaaaaa_exec-bbbbbbbb/v1/conv-cd2379f5/onnx_model'
E   [conversion.execution] Using MLflow run: 088b3772c363...
E   [conversion.export] Starting ONNX export. quantize_int8=False
E   [conversion.export] Output directory created at '/workspaces/resume-ner-azureml/outputs/conversion/local/local/spec-aaaaaaaa_exec-bbbbbbbb/v1/conv-cd2379f5/onnx_model'
E   [conversion.export] Loading tokenizer and model from checkpoint directory '/tmp/pytest-of-codespace/pytest-28/test_best_config_selection_e2e0/final_training/local/distilbert/spec-aaaaaaaa_exec-bbbbbbbb/v1/checkpoint'
E   [conversion.execution] Conversion failed: stat: path should be string, bytes, os.PathLike or integer, not NoneType
E   Traceback (most recent call last):
E     File "/opt/conda/envs/resume-ner-training/lib/python3.10/runpy.py", line 196, in _run_module_as_main
E       return _run_code(code, main_globals, None,
E     File "/opt/conda/envs/resume-ner-training/lib/python3.10/runpy.py", line 86, in _run_code
E       exec(code, run_globals)
E     File "/workspaces/resume-ner-azureml/src/deployment/conversion/execution.py", line 237, in <module>
E       main()
E     File "/workspaces/resume-ner-azureml/src/deployment/conversion/execution.py", line 179, in main
E       onnx_path = export_to_onnx(
E     File "/workspaces/resume-ner-azureml/src/deployment/conversion/export.py", line 81, in export_to_onnx
E       tokenizer = AutoTokenizer.from_pretrained(checkpoint_dir, use_fast=True)
E     File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 1175, in from_pretrained
E       return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
E     File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2113, in from_pretrained
E       return cls._from_pretrained(
E     File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2151, in _from_pretrained
E       slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(
E     File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2359, in _from_pretrained
E       tokenizer = cls(*init_inputs, **init_kwargs)
E     File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/transformers/models/distilbert/tokenization_distilbert.py", line 117, in __init__
E       if not os.path.isfile(vocab_file):
E     File "/opt/conda/envs/resume-ner-training/lib/python3.10/genericpath.py", line 30, in isfile
E       st = os.stat(path)
E   TypeError: stat: path should be string, bytes, os.PathLike or integer, not NoneType
------------------------------ Captured log call -------------------------------
INFO     script.conversion.orchestration:orchestration.py:144 Output directory: /workspaces/resume-ner-azureml/outputs/conversion/local/local/spec-aaaaaaaa_exec-bbbbbbbb/v1/conv-cd2379f5
INFO     infrastructure.naming.mlflow.config:config.py:358 [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
INFO     infrastructure.naming.mlflow.config:config.py:365 [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=conversion
INFO     script.conversion.orchestration:orchestration.py:268 Created MLflow run: local_local_conversion_spec-aaaaaaaa_exec-bbbbbbbb_v1_conv-cd2379f5 (088b3772c363...)
INFO     script.conversion.orchestration:orchestration.py:278 Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /tmp/pytest-of-codespace/pytest-28/test_best_config_selection_e2e0/final_training/local/distilbert/spec-aaaaaaaa_exec-bbbbbbbb/v1/checkpoint --config-dir /workspaces/resume-ner-azureml/config --backbone local --output-dir /workspaces/resume-ner-azureml/outputs/conversion/local/local/spec-aaaaaaaa_exec-bbbbbbbb/v1/conv-cd2379f5 --opset-version 18 --run-smoke-test
WARNING  script.conversion.orchestration:orchestration.py:307 2026-01-15 00:06:18,292 - infrastructure.tracking.mlflow.compatibility - INFO - Applied Azure ML artifact compatibility patch
WARNING  script.conversion.orchestration:orchestration.py:307 [conversion.execution] Starting conversion: checkpoint='/tmp/pytest-of-codespace/pytest-28/test_best_config_selection_e2e0/final_training/local/distilbert/spec-aaaaaaaa_exec-bbbbbbbb/v1/checkpoint', backbone='local', quantize_int8=False, opset_version=18
WARNING  script.conversion.orchestration:orchestration.py:307 [conversion.execution] Resolving checkpoint directory from '/tmp/pytest-of-codespace/pytest-28/test_best_config_selection_e2e0/final_training/local/distilbert/spec-aaaaaaaa_exec-bbbbbbbb/v1/checkpoint'
WARNING  script.conversion.orchestration:orchestration.py:307 [conversion.execution] Output directory: '/workspaces/resume-ner-azureml/outputs/conversion/local/local/spec-aaaaaaaa_exec-bbbbbbbb/v1/conv-cd2379f5/onnx_model'
WARNING  script.conversion.orchestration:orchestration.py:307 [conversion.execution] Using MLflow run: 088b3772c363...
WARNING  script.conversion.orchestration:orchestration.py:307 [conversion.export] Starting ONNX export. quantize_int8=False
WARNING  script.conversion.orchestration:orchestration.py:307 [conversion.export] Output directory created at '/workspaces/resume-ner-azureml/outputs/conversion/local/local/spec-aaaaaaaa_exec-bbbbbbbb/v1/conv-cd2379f5/onnx_model'
WARNING  script.conversion.orchestration:orchestration.py:307 [conversion.export] Loading tokenizer and model from checkpoint directory '/tmp/pytest-of-codespace/pytest-28/test_best_config_selection_e2e0/final_training/local/distilbert/spec-aaaaaaaa_exec-bbbbbbbb/v1/checkpoint'
WARNING  script.conversion.orchestration:orchestration.py:307 [conversion.execution] Conversion failed: stat: path should be string, bytes, os.PathLike or integer, not NoneType
WARNING  script.conversion.orchestration:orchestration.py:307 Traceback (most recent call last):
WARNING  script.conversion.orchestration:orchestration.py:307   File "/opt/conda/envs/resume-ner-training/lib/python3.10/runpy.py", line 196, in _run_module_as_main
WARNING  script.conversion.orchestration:orchestration.py:307     return _run_code(code, main_globals, None,
WARNING  script.conversion.orchestration:orchestration.py:307   File "/opt/conda/envs/resume-ner-training/lib/python3.10/runpy.py", line 86, in _run_code
WARNING  script.conversion.orchestration:orchestration.py:307     exec(code, run_globals)
WARNING  script.conversion.orchestration:orchestration.py:307   File "/workspaces/resume-ner-azureml/src/deployment/conversion/execution.py", line 237, in <module>
WARNING  script.conversion.orchestration:orchestration.py:307     main()
WARNING  script.conversion.orchestration:orchestration.py:307   File "/workspaces/resume-ner-azureml/src/deployment/conversion/execution.py", line 179, in main
WARNING  script.conversion.orchestration:orchestration.py:307     onnx_path = export_to_onnx(
WARNING  script.conversion.orchestration:orchestration.py:307   File "/workspaces/resume-ner-azureml/src/deployment/conversion/export.py", line 81, in export_to_onnx
WARNING  script.conversion.orchestration:orchestration.py:307     tokenizer = AutoTokenizer.from_pretrained(checkpoint_dir, use_fast=True)
WARNING  script.conversion.orchestration:orchestration.py:307   File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 1175, in from_pretrained
WARNING  script.conversion.orchestration:orchestration.py:307     return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
WARNING  script.conversion.orchestration:orchestration.py:307   File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2113, in from_pretrained
WARNING  script.conversion.orchestration:orchestration.py:307     return cls._from_pretrained(
WARNING  script.conversion.orchestration:orchestration.py:307   File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2151, in _from_pretrained
WARNING  script.conversion.orchestration:orchestration.py:307     slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(
WARNING  script.conversion.orchestration:orchestration.py:307   File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2359, in _from_pretrained
WARNING  script.conversion.orchestration:orchestration.py:307     tokenizer = cls(*init_inputs, **init_kwargs)
WARNING  script.conversion.orchestration:orchestration.py:307   File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/transformers/models/distilbert/tokenization_distilbert.py", line 117, in __init__
WARNING  script.conversion.orchestration:orchestration.py:307     if not os.path.isfile(vocab_file):
WARNING  script.conversion.orchestration:orchestration.py:307   File "/opt/conda/envs/resume-ner-training/lib/python3.10/genericpath.py", line 30, in isfile
WARNING  script.conversion.orchestration:orchestration.py:307     st = os.stat(path)
WARNING  script.conversion.orchestration:orchestration.py:307 TypeError: stat: path should be string, bytes, os.PathLike or integer, not NoneType
INFO     infrastructure.tracking.mlflow.lifecycle:lifecycle.py:72 Run 088b3772c363... already has status <Mock name='mock.get_run().info.status' id='124894313470160'>, skipping termination (expected RUNNING)
=============================== warnings summary ===============================
tests/config/unit/test_fingerprint_placeholder_fallback.py::TestFingerprintPlaceholderFallback::test_placeholder_values_are_short_enough_for_naming
  /workspaces/resume-ner-azureml/tests/config/unit/test_fingerprint_placeholder_fallback.py:128: RuntimeWarning: Fingerprint computation functions not available. Using placeholder fingerprints.
    spec_fp, exec_fp = _compute_fingerprints(

tests/tracking/scripts/test_artifact_upload_manual.py::test_monkey_patch_registration
  /opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/_pytest/python.py:170: PytestReturnNotNoneWarning: Test functions should return None, but tests/tracking/scripts/test_artifact_upload_manual.py::test_monkey_patch_registration returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/tracking/scripts/test_artifact_upload_manual.py::test_artifact_upload_to_child_run
  /opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/_pytest/python.py:170: PytestReturnNotNoneWarning: Test functions should return None, but tests/tracking/scripts/test_artifact_upload_manual.py::test_artifact_upload_to_child_run returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

tests/tracking/scripts/test_artifact_upload_manual.py::test_refit_run_completion
  /opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/_pytest/python.py:170: PytestReturnNotNoneWarning: Test functions should return None, but tests/tracking/scripts/test_artifact_upload_manual.py::test_refit_run_completion returned <class 'bool'>.
  Did you mean to use `assert` instead of `return`?
  See https://docs.pytest.org/en/stable/how-to/assert.html#return-not-none for more information.
    warnings.warn(

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED tests/hpo/integration/test_refit_training.py::TestRefitCheckpointDuplicationPrevention::test_refit_prevents_duplication_only_archive_uploaded - AssertionError: Checkpoint archive SHOULD be uploaded. Found: []
assert 0 > 0
 +  where 0 = len([])
FAILED tests/integration/api/test_api.py::TestHealthEndpoint::test_model_info_loaded - assert 503 == 200
 +  where 503 = <Response [503 Service Unavailable]>.status_code
FAILED tests/integration/api/test_api.py::TestPredictEndpoint::test_predict_success - assert 503 == 200
 +  where 503 = <Response [503 Service Unavailable]>.status_code
FAILED tests/integration/api/test_api.py::TestPredictEndpoint::test_predict_batch_success - assert 503 == 200
 +  where 503 = <Response [503 Service Unavailable]>.status_code
FAILED tests/integration/api/test_api.py::TestPredictEndpoint::test_predict_batch_size_exceeded - assert 503 == 400
 +  where 503 = <Response [503 Service Unavailable]>.status_code
FAILED tests/integration/api/test_api.py::TestFileEndpoints::test_predict_file_pdf - assert 503 == 200
 +  where 503 = <Response [503 Service Unavailable]>.status_code
FAILED tests/integration/api/test_api.py::TestFileEndpoints::test_predict_file_image - assert 503 == 200
 +  where 503 = <Response [503 Service Unavailable]>.status_code
FAILED tests/selection/integration/test_artifact_acquisition_edge_cases.py::TestArtifactAcquisitionEdgeCases::test_validation_false_allows_invalid_checkpoints - AttributeError: <module 'evaluation.selection.artifact_unified.compat' from '/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/compat.py'> does not have the attribute '_validate_checkpoint'
FAILED tests/selection/integration/test_artifact_acquisition_edge_cases.py::TestArtifactAcquisitionEdgeCases::test_validation_true_rejects_invalid_checkpoints - AttributeError: <module 'evaluation.selection.artifact_unified.compat' from '/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/compat.py'> does not have the attribute '_validate_checkpoint'
FAILED tests/selection/integration/test_artifact_acquisition_logic.py::TestArtifactAcquisitionConfig::test_priority_order_local_first - AttributeError: <module 'evaluation.selection.artifact_unified.compat' from '/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/compat.py'> does not have the attribute '_validate_checkpoint'
FAILED tests/selection/integration/test_artifact_acquisition_logic.py::TestArtifactAcquisitionConfig::test_priority_order_mlflow_first - AttributeError: <module 'evaluation.selection.artifact_unified.compat' from '/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/compat.py'> does not have the attribute '_validate_checkpoint'
FAILED tests/selection/integration/test_artifact_acquisition_logic.py::TestArtifactAcquisitionConfig::test_local_validate_controls_validation - AttributeError: <module 'evaluation.selection.artifact_unified.compat' from '/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/compat.py'> does not have the attribute '_validate_checkpoint'
FAILED tests/selection/integration/test_artifact_acquisition_unified_config.py::TestConfigOptionCombinations::test_config_with_disabled_sources - assert False is True
 +  where False = ArtifactResult(request=ArtifactRequest(artifact_kind=<ArtifactKind.CHECKPOINT: 'checkpoint'>, run_id='test_run_123', backbone='distilbert', study_key_hash='study12345678', trial_key_hash='trial87654321', refit_run_id=None, experiment_name=None, metadata={}), success=False, path=None, source=None, status=None, error="Artifact not found in any configured source. Checked sources in priority order: ['local', 'drive', 'mlflow']. Request: artifact_kind=checkpoint, run_id=test_run_123..., backbone=distilbert", metadata={}).success
FAILED tests/selection/integration/test_artifact_acquisition_workflow.py::TestArtifactAcquisitionWorkflow::test_complete_workflow_with_default_config - AttributeError: <module 'evaluation.selection.artifact_unified.compat' from '/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/compat.py'> does not have the attribute '_validate_checkpoint'
FAILED tests/selection/integration/test_artifact_acquisition_workflow.py::TestArtifactAcquisitionWorkflow::test_workflow_with_custom_priority_order - AttributeError: <module 'evaluation.selection.artifact_unified.compat' from '/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/compat.py'> does not have the attribute '_validate_checkpoint'
FAILED tests/selection/integration/test_artifact_acquisition_workflow.py::TestArtifactAcquisitionWorkflow::test_workflow_with_validation_disabled - AttributeError: <module 'evaluation.selection.artifact_unified.compat' from '/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/compat.py'> does not have the attribute '_validate_checkpoint'
FAILED tests/selection/integration/test_artifact_acquisition_workflow.py::TestArtifactAcquisitionWorkflow::test_workflow_with_all_sources_enabled - ValueError: 
[ERROR] Could not acquire checkpoint for run test_run...
   Experiment: test_experiment-hpo-distilbert
   Backbone: distilbert

[TRIED] Strategies attempted:
   1. Local
   2. Drive
   3. Mlflow

[ERROR] Could not determine experiment ID: Run with id=test_run_id_123 not found
FAILED tests/selection/integration/test_artifact_acquisition_workflow.py::TestArtifactAcquisitionWorkflow::test_workflow_mlflow_fallback_to_manual - ValueError: 
[ERROR] Could not acquire checkpoint for run test_run...
   Experiment: test_experiment-hpo-distilbert
   Backbone: distilbert

[TRIED] Strategies attempted:
   1. Mlflow

[ERROR] Could not determine experiment ID: Run with id=test_run_id_123 not found
FAILED tests/shared/unit/test_drive_backup.py::TestColabMounting::test_mount_colab_drive_success - ModuleNotFoundError: No module named 'storage'
FAILED tests/shared/unit/test_drive_backup.py::TestColabMounting::test_create_colab_store_success - ModuleNotFoundError: No module named 'paths'
FAILED tests/shared/unit/test_drive_backup.py::TestColabMounting::test_create_colab_store_mount_fails - ModuleNotFoundError: No module named 'storage'
FAILED tests/shared/unit/test_drive_backup.py::TestColabMounting::test_create_colab_store_default_path - ModuleNotFoundError: No module named 'paths'
FAILED tests/tracking/unit/test_sweep_tracker_config_inference.py::TestSweepTrackerConfigInference::test_log_best_trial_id_infers_config_from_output_dir - AssertionError: assert False
 +  where False = <MagicMock name='set_tag' id='124894341639296'>.called
FAILED tests/tracking/unit/test_sweep_tracker_config_inference.py::TestSweepTrackerConfigInference::test_log_best_trial_id_finds_config_in_parent_chain - AssertionError: assert False
 +  where False = <MagicMock name='set_tag' id='124894308895040'>.called
FAILED tests/tracking/unit/test_sweep_tracker_config_inference.py::TestSweepTrackerConfigInference::test_log_best_trial_id_falls_back_to_cwd_config_when_not_found - AssertionError: assert False
 +  where False = <MagicMock name='set_tag' id='124894298178032'>.called
FAILED tests/tracking/unit/test_sweep_tracker_config_inference.py::TestSweepTrackerConfigInference::test_log_final_metrics_infers_config_correctly - AssertionError: assert False
 +  where False = <MagicMock name='set_tag' id='124894341652416'>.called
FAILED tests/tracking/unit/test_sweep_tracker_config_inference.py::TestSweepTrackerConfigInference::test_log_final_metrics_uses_hpo_output_dir_when_output_dir_none - assert 0 > 0
 +  where 0 = len([])
FAILED tests/tracking/unit/test_sweep_tracker_config_inference.py::TestSweepTrackerConfigInference::test_config_dir_not_in_outputs_directory - assert 0 > 0
 +  where 0 = len([])
FAILED tests/training/hpo/unit/test_phase2_tags.py::TestSetPhase2HpoTags::test_sets_all_tags_successfully - AttributeError: <module 'training.hpo.execution.local.sweep' from '/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py'> does not have the attribute 'TagsRegistry'
FAILED tests/training/hpo/unit/test_phase2_tags.py::TestSetPhase2HpoTags::test_handles_missing_data_config - AttributeError: <module 'training.hpo.execution.local.sweep' from '/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py'> does not have the attribute 'TagsRegistry'
FAILED tests/training/hpo/unit/test_phase2_tags.py::TestSetPhase2HpoTags::test_minimize_objective_direction - AttributeError: <module 'training.hpo.execution.local.sweep' from '/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py'> does not have the attribute 'TagsRegistry'
FAILED tests/training/hpo/unit/test_phase2_tags.py::TestSetPhase2HpoTags::test_legacy_goal_key_migration - AttributeError: <module 'training.hpo.execution.local.sweep' from '/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py'> does not have the attribute 'TagsRegistry'
FAILED tests/training/hpo/unit/test_phase2_tags.py::TestSetPhase2HpoTags::test_handles_exception_gracefully - AttributeError: <module 'training.hpo.execution.local.sweep' from '/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py'> does not have the attribute 'TagsRegistry'
FAILED tests/training/hpo/unit/test_phase2_tags.py::TestSetPhase2HpoTags::test_schema_version_1_0_fallback - AttributeError: <module 'training.hpo.execution.local.sweep' from '/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py'> does not have the attribute 'TagsRegistry'
FAILED tests/unit/api/test_extractors.py::TestPDFExtraction::test_extract_pdf_pymupdf - src.deployment.api.exceptions.TextExtractionError: Failed to extract text from PDF: Failed to open stream
FAILED tests/unit/api/test_extractors.py::TestPDFExtraction::test_extract_pdf_pdfplumber - src.deployment.api.exceptions.TextExtractionError: Failed to extract text from PDF: No /Root object! - Is this really a PDF?
FAILED tests/unit/api/test_extractors.py::TestImageExtraction::test_extract_image_easyocr - src.deployment.api.exceptions.TextExtractionError: Failed to extract text from image: EasyOCR or Pillow not installed. Install with: pip install easyocr pillow
FAILED tests/unit/api/test_extractors.py::TestImageExtraction::test_extract_image_pytesseract - src.deployment.api.exceptions.TextExtractionError: Failed to extract text from image: pytesseract or Pillow not installed. Install with: pip install pytesseract pillow
FAILED tests/unit/api/test_extractors.py::TestFileValidation::test_validate_file_success - Failed: async def functions are not natively supported.
You need to install a suitable plugin for your async framework, for example:
  - anyio
  - pytest-asyncio
  - pytest-tornasync
  - pytest-trio
  - pytest-twisted
FAILED tests/unit/api/test_extractors.py::TestFileValidation::test_validate_file_size_exceeded - Failed: async def functions are not natively supported.
You need to install a suitable plugin for your async framework, for example:
  - anyio
  - pytest-asyncio
  - pytest-tornasync
  - pytest-trio
  - pytest-twisted
FAILED tests/unit/api/test_inference.py::TestONNXInferenceEngine::test_load_model_success - AttributeError: <module 'deployment.api.inference.engine' from '/workspaces/resume-ner-azureml/src/deployment/api/inference/engine.py'> does not have the attribute 'AutoConfig'
FAILED tests/unit/api/test_inference.py::TestONNXInferenceEngine::test_predict_tokens - UnboundLocalError: local variable 'input_ids' referenced before assignment
FAILED tests/unit/api/test_inference_fixes.py::TestInferenceFixes::test_tokenization_returns_numpy_arrays - UnboundLocalError: local variable 'input_ids' referenced before assignment
FAILED tests/unit/api/test_inference_fixes.py::TestInferenceFixes::test_offset_mapping_extraction - UnboundLocalError: local variable 'input_ids' referenced before assignment
FAILED tests/unit/api/test_inference_fixes.py::TestInferenceFixes::test_entity_extraction_with_offsets - UnboundLocalError: local variable 'input_ids' referenced before assignment
FAILED tests/unit/api/test_inference_fixes.py::TestInferenceFixes::test_no_hanging_on_special_tokens - UnboundLocalError: local variable 'input_ids' referenced before assignment
FAILED tests/unit/api/test_inference_performance.py::TestInferencePerformance::test_tokenization_does_not_hang - AttributeError: <module 'deployment.api.inference.engine' from '/workspaces/resume-ner-azureml/src/deployment/api/inference/engine.py'> does not have the attribute 'AutoTokenizer'
FAILED tests/unit/api/test_inference_performance.py::TestInferencePerformance::test_entity_extraction_with_offset_mapping - AttributeError: <module 'deployment.api.inference.engine' from '/workspaces/resume-ner-azureml/src/deployment/api/inference/engine.py'> does not have the attribute 'AutoTokenizer'
FAILED tests/unit/api/test_inference_performance.py::TestInferencePerformance::test_empty_text_handling - AttributeError: <module 'deployment.api.inference.engine' from '/workspaces/resume-ner-azureml/src/deployment/api/inference/engine.py'> does not have the attribute 'AutoTokenizer'
FAILED tests/unit/api/test_inference_performance.py::TestInferencePerformance::test_special_characters_handling - AttributeError: <module 'deployment.api.inference.engine' from '/workspaces/resume-ner-azureml/src/deployment/api/inference/engine.py'> does not have the attribute 'AutoTokenizer'
FAILED tests/unit/api/test_inference_performance.py::TestInferencePerformance::test_tokenization_consistency - AttributeError: <module 'deployment.api.inference.engine' from '/workspaces/resume-ner-azureml/src/deployment/api/inference/engine.py'> does not have the attribute 'AutoTokenizer'
FAILED tests/unit/shared/test_mlflow_setup.py::TestGetLocalTrackingUri::test_local_platform - ModuleNotFoundError: No module named 'shared.mlflow_setup'
FAILED tests/unit/shared/test_mlflow_setup.py::TestGetLocalTrackingUri::test_colab_with_drive - ModuleNotFoundError: No module named 'shared.mlflow_setup'
FAILED tests/unit/shared/test_mlflow_setup.py::TestGetLocalTrackingUri::test_colab_without_drive - ModuleNotFoundError: No module named 'shared.mlflow_setup'
FAILED tests/unit/shared/test_mlflow_setup.py::TestGetLocalTrackingUri::test_kaggle_platform - ModuleNotFoundError: No module named 'shared.mlflow_setup'
FAILED tests/unit/shared/test_mlflow_setup.py::TestGetAzureMlTrackingUri::test_success - ModuleNotFoundError: No module named 'shared.mlflow_setup'
FAILED tests/unit/shared/test_mlflow_setup.py::TestGetAzureMlTrackingUri::test_import_error - ModuleNotFoundError: No module named 'shared.mlflow_setup'
FAILED tests/unit/shared/test_mlflow_setup.py::TestGetAzureMlTrackingUri::test_workspace_access_error - ModuleNotFoundError: No module named 'shared.mlflow_setup'
FAILED tests/unit/shared/test_mlflow_setup.py::TestSetupMlflowCrossPlatform::test_local_fallback_no_ml_client - ModuleNotFoundError: No module named 'shared.mlflow_setup'
FAILED tests/unit/shared/test_mlflow_setup.py::TestSetupMlflowCrossPlatform::test_azure_ml_success - ModuleNotFoundError: No module named 'shared.mlflow_setup'
FAILED tests/unit/shared/test_mlflow_setup.py::TestSetupMlflowCrossPlatform::test_azure_ml_failure_with_fallback - ModuleNotFoundError: No module named 'shared.mlflow_setup'
FAILED tests/unit/shared/test_mlflow_setup.py::TestSetupMlflowCrossPlatform::test_azure_ml_failure_no_fallback - ModuleNotFoundError: No module named 'shared.mlflow_setup'
FAILED tests/unit/shared/test_mlflow_setup.py::TestSetupMlflowCrossPlatform::test_mlflow_not_installed - ModuleNotFoundError: No module named 'shared.mlflow_setup'
FAILED tests/unit/shared/test_mlflow_setup.py::TestPlatformSpecificBehavior::test_colab_drive_mounted - ModuleNotFoundError: No module named 'shared.mlflow_setup'
FAILED tests/unit/shared/test_mlflow_setup.py::TestPlatformSpecificBehavior::test_kaggle_platform - ModuleNotFoundError: No module named 'shared.mlflow_setup'
FAILED tests/unit/shared/test_mlflow_setup.py::TestPlatformSpecificBehavior::test_local_platform - ModuleNotFoundError: No module named 'shared.mlflow_setup'
FAILED tests/unit/shared/test_mlflow_setup.py::TestCreateMlClientFromConfig::test_success_with_env_vars - ModuleNotFoundError: No module named 'shared.mlflow_setup'
FAILED tests/unit/shared/test_mlflow_setup.py::TestCreateMlClientFromConfig::test_azure_ml_disabled - ModuleNotFoundError: No module named 'shared.mlflow_setup'
FAILED tests/unit/shared/test_mlflow_setup.py::TestCreateMlClientFromConfig::test_config_missing_azure_ml_section - ModuleNotFoundError: No module named 'shared.mlflow_setup'
FAILED tests/unit/shared/test_mlflow_setup.py::TestCreateMlClientFromConfig::test_missing_credentials - ModuleNotFoundError: No module named 'shared.mlflow_setup'
FAILED tests/unit/shared/test_mlflow_setup.py::TestCreateMlClientFromConfig::test_import_error - ModuleNotFoundError: No module named 'shared.mlflow_setup'
FAILED tests/unit/shared/test_mlflow_setup.py::TestSetupMlflowFromConfig::test_config_file_exists_azure_enabled - ModuleNotFoundError: No module named 'shared.mlflow_setup'
FAILED tests/unit/shared/test_mlflow_setup.py::TestSetupMlflowFromConfig::test_config_file_exists_azure_disabled - ModuleNotFoundError: No module named 'shared.mlflow_setup'
FAILED tests/unit/shared/test_mlflow_setup.py::TestSetupMlflowFromConfig::test_config_file_missing - ModuleNotFoundError: No module named 'shared.mlflow_setup'
FAILED tests/unit/shared/test_mlflow_setup.py::TestAzureMlNamespaceCollision::test_check_azureml_mlflow_available_with_shadowing - ModuleNotFoundError: No module named 'shared.mlflow_setup'
FAILED tests/workflows/test_full_workflow_e2e.py::test_full_workflow_e2e - ModuleNotFoundError: No module named 'tracking.mlflow'
FAILED tests/workflows/test_notebook_01_e2e.py::TestNotebookE2E_Core::test_hpo_sweep_execution_mocked - ModuleNotFoundError: No module named 'naming'
FAILED tests/workflows/test_notebook_02_e2e.py::test_best_config_selection_e2e - RuntimeError: Model conversion failed with return code 1

Stderr output (last 30 lines):
[conversion.execution] Output directory: '/workspaces/resume-ner-azureml/outputs/conversion/local/local/spec-aaaaaaaa_exec-bbbbbbbb/v1/conv-cd2379f5/onnx_model'
[conversion.execution] Using MLflow run: 088b3772c363...
[conversion.export] Starting ONNX export. quantize_int8=False
[conversion.export] Output directory created at '/workspaces/resume-ner-azureml/outputs/conversion/local/local/spec-aaaaaaaa_exec-bbbbbbbb/v1/conv-cd2379f5/onnx_model'
[conversion.export] Loading tokenizer and model from checkpoint directory '/tmp/pytest-of-codespace/pytest-28/test_best_config_selection_e2e0/final_training/local/distilbert/spec-aaaaaaaa_exec-bbbbbbbb/v1/checkpoint'
[conversion.execution] Conversion failed: stat: path should be string, bytes, os.PathLike or integer, not NoneType
Traceback (most recent call last):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/workspaces/resume-ner-azureml/src/deployment/conversion/execution.py", line 237, in <module>
    main()
  File "/workspaces/resume-ner-azureml/src/deployment/conversion/execution.py", line 179, in main
    onnx_path = export_to_onnx(
  File "/workspaces/resume-ner-azureml/src/deployment/conversion/export.py", line 81, in export_to_onnx
    tokenizer = AutoTokenizer.from_pretrained(checkpoint_dir, use_fast=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 1175, in from_pretrained
    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2113, in from_pretrained
    return cls._from_pretrained(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2151, in _from_pretrained
    slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2359, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/transformers/models/distilbert/tokenization_distilbert.py", line 117, in __init__
    if not os.path.isfile(vocab_file):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/genericpath.py", line 30, in isfile
    st = os.stat(path)
TypeError: stat: path should be string, bytes, os.PathLike or integer, not NoneType
=========== 78 failed, 1256 passed, 52 skipped, 4 warnings in 35.58s ===========

============================================================
[INFO] Pytest log file: /workspaces/resume-ner-azureml/outputs/pytest_logs/pytest_20260115_000546.log
============================================================

2026-01-15 00:05:46,858 - infrastructure.tracking.mlflow.compatibility - INFO - Applied Azure ML artifact compatibility patch
2026-01-15 00:05:51,027 - evaluation.benchmarking.orchestrator - INFO - Skipping benchmarking (test data not available)
2026-01-15 00:05:51,033 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_handles_1/src/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_handles_1/checkpoint --test-data /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_handles_1/test.json --batch-sizes 1 8 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_handles_1/benchmark.json
2026-01-15 00:05:51,034 - evaluation.benchmarking.utils - ERROR - Benchmarking failed with return code 1
2026-01-15 00:05:51,038 - evaluation.benchmarking.orchestrator - INFO - Benchmarking distilbert (trial-25d03eeb)...
2026-01-15 00:05:51,051 - evaluation.benchmarking.orchestrator - WARNING - [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=f26f08fe24516179...
2026-01-15 00:05:51,053 - evaluation.benchmarking.orchestrator - INFO - [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-15 00:05:51,053 - evaluation.benchmarking.orchestrator - INFO - Benchmark completed: /tmp/pytest-of-codespace/pytest-28/test_benchmark_passes_trial_id0/outputs/benchmarking/test/benchmark.json
2026-01-15 00:05:51,054 - evaluation.benchmarking.orchestrator - INFO - Benchmarking complete. 1/1 trials benchmarked.
2026-01-15 00:05:51,057 - evaluation.benchmarking.orchestrator - INFO - Benchmarking distilbert (trial_1_20251231_161745)...
2026-01-15 00:05:51,058 - evaluation.benchmarking.orchestrator - WARNING - [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=f26f08fe24516179...
2026-01-15 00:05:51,060 - evaluation.benchmarking.orchestrator - INFO - [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-15 00:05:51,060 - evaluation.benchmarking.orchestrator - INFO - Benchmark completed: /tmp/pytest-of-codespace/pytest-28/test_benchmark_passes_trial_id1/outputs/benchmarking/test/benchmark.json
2026-01-15 00:05:51,060 - evaluation.benchmarking.orchestrator - INFO - Benchmarking complete. 1/1 trials benchmarked.
2026-01-15 00:05:51,064 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_mlflow_t0/src/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_mlflow_t0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_mlflow_t0/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_mlflow_t0/benchmark.json
2026-01-15 00:05:51,064 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Building run name: trial_id=trial-25d03eeb, root_dir=/tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_mlflow_t0, config_dir=/tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_mlflow_t0/config
2026-01-15 00:05:51,065 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Generated run name: <Mock name='create_naming_context().storage_env' id='124894361544368'>_<Mock name='create_naming_context().model' id='124894361544416'>_<Mock name='create_naming_context().process_type' id='124894361544320'>_legacy
2026-01-15 00:05:51,069 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_mlflow_t1/src/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_mlflow_t1/checkpoint --test-data /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_mlflow_t1/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_mlflow_t1/benchmark.json
2026-01-15 00:05:51,069 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Constructed trial_id from trial_key_hash: trial-25d03eeb
2026-01-15 00:05:51,070 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Building run name: trial_id=trial-25d03eeb, root_dir=/tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_mlflow_t1, config_dir=/tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_mlflow_t1/config
2026-01-15 00:05:51,070 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Generated run name: <Mock name='create_naming_context().storage_env' id='124894361593568'>_<Mock name='create_naming_context().model' id='124894361593616'>_<Mock name='create_naming_context().process_type' id='124894361593520'>_legacy
2026-01-15 00:05:51,074 - evaluation.benchmarking.orchestrator - INFO - Benchmarking distilbert (trial_0)...
2026-01-15 00:05:51,075 - evaluation.benchmarking.orchestrator - WARNING - [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=9ab6cf5dc0566079...
2026-01-15 00:05:51,077 - evaluation.benchmarking.orchestrator - INFO - [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-15 00:05:51,077 - evaluation.benchmarking.orchestrator - INFO - Benchmark completed: /tmp/pytest-of-codespace/pytest-28/test_benchmark_best_trials_use0/outputs/benchmarking/test/benchmark.json
2026-01-15 00:05:51,077 - evaluation.benchmarking.orchestrator - INFO - Benchmarking complete. 1/1 trials benchmarked.
2026-01-15 00:05:51,081 - evaluation.benchmarking.orchestrator - INFO - Benchmarking distilbert (trial_0)...
2026-01-15 00:05:51,083 - evaluation.benchmarking.orchestrator - WARNING - [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=9ab6cf5dc0566079...
2026-01-15 00:05:51,084 - evaluation.benchmarking.orchestrator - INFO - [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-15 00:05:51,084 - evaluation.benchmarking.orchestrator - INFO - Benchmark completed: /tmp/pytest-of-codespace/pytest-28/test_benchmark_best_trials_use1/outputs/benchmarking/test/benchmark.json
2026-01-15 00:05:51,084 - evaluation.benchmarking.orchestrator - INFO - Benchmarking complete. 1/1 trials benchmarked.
2026-01-15 00:05:51,088 - evaluation.benchmarking.orchestrator - INFO - Benchmarking distilbert (trial_0)...
2026-01-15 00:05:51,089 - evaluation.benchmarking.orchestrator - WARNING - [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=9ab6cf5dc0566079...
2026-01-15 00:05:51,091 - evaluation.benchmarking.orchestrator - INFO - [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-15 00:05:51,091 - evaluation.benchmarking.orchestrator - INFO - Benchmark completed: /tmp/pytest-of-codespace/pytest-28/test_benchmark_best_trials_use2/outputs/benchmarking/test/benchmark.json
2026-01-15 00:05:51,091 - evaluation.benchmarking.orchestrator - INFO - Benchmarking complete. 1/1 trials benchmarked.
2026-01-15 00:05:51,095 - evaluation.benchmarking.orchestrator - INFO - Benchmarking distilbert (trial_0)...
2026-01-15 00:05:51,096 - evaluation.benchmarking.orchestrator - WARNING - [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=9ab6cf5dc0566079...
2026-01-15 00:05:51,097 - evaluation.benchmarking.orchestrator - INFO - [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-15 00:05:51,097 - evaluation.benchmarking.orchestrator - INFO - Benchmark completed: /tmp/pytest-of-codespace/pytest-28/test_benchmark_best_trials_use3/outputs/benchmarking/test/benchmark.json
2026-01-15 00:05:51,098 - evaluation.benchmarking.orchestrator - INFO - Benchmarking complete. 1/1 trials benchmarked.
2026-01-15 00:05:51,101 - evaluation.benchmarking.orchestrator - INFO - Benchmarking distilbert (trial_0)...
2026-01-15 00:05:51,103 - evaluation.benchmarking.orchestrator - WARNING - [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=b8f03b32dbe8202f...
2026-01-15 00:05:51,104 - evaluation.benchmarking.orchestrator - INFO - [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-15 00:05:51,104 - evaluation.benchmarking.orchestrator - INFO - Benchmark completed: /tmp/pytest-of-codespace/pytest-28/test_benchmark_best_trials_use4/outputs/benchmarking/test/custom_benchmark.json
2026-01-15 00:05:51,104 - evaluation.benchmarking.orchestrator - INFO - Benchmarking complete. 1/1 trials benchmarked.
2026-01-15 00:05:51,108 - evaluation.benchmarking.orchestrator - INFO - Benchmarking distilbert (trial_0)...
2026-01-15 00:05:51,109 - evaluation.benchmarking.orchestrator - WARNING - [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=9ab6cf5dc0566079...
2026-01-15 00:05:51,110 - evaluation.benchmarking.orchestrator - INFO - [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-15 00:05:51,111 - evaluation.benchmarking.orchestrator - INFO - Benchmark completed: /tmp/pytest-of-codespace/pytest-28/test_benchmark_best_trials_use5/outputs/benchmarking/test/benchmark.json
2026-01-15 00:05:51,111 - evaluation.benchmarking.orchestrator - INFO - Benchmarking complete. 1/1 trials benchmarked.
2026-01-15 00:05:51,114 - evaluation.benchmarking.orchestrator - INFO - Benchmarking distilbert (trial_0)...
2026-01-15 00:05:51,116 - evaluation.benchmarking.orchestrator - WARNING - [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=b8f03b32dbe8202f...
2026-01-15 00:05:51,117 - evaluation.benchmarking.orchestrator - INFO - [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-15 00:05:51,117 - evaluation.benchmarking.orchestrator - INFO - Benchmark completed: /tmp/pytest-of-codespace/pytest-28/test_benchmark_best_trials_use6/outputs/benchmarking/test/custom_benchmark.json
2026-01-15 00:05:51,117 - evaluation.benchmarking.orchestrator - INFO - Benchmarking complete. 1/1 trials benchmarked.
2026-01-15 00:05:51,121 - evaluation.benchmarking.orchestrator - INFO - Benchmarking distilbert (trial_0)...
2026-01-15 00:05:51,122 - evaluation.benchmarking.orchestrator - WARNING - [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=b8f03b32dbe8202f...
2026-01-15 00:05:51,123 - evaluation.benchmarking.orchestrator - INFO - [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-15 00:05:51,123 - evaluation.benchmarking.orchestrator - INFO - Benchmark completed: /tmp/pytest-of-codespace/pytest-28/test_benchmark_best_trials_all0/outputs/benchmarking/test/custom_benchmark.json
2026-01-15 00:05:51,124 - evaluation.benchmarking.orchestrator - INFO - Benchmarking complete. 1/1 trials benchmarked.
2026-01-15 00:05:51,126 - evaluation.benchmarking.orchestrator - INFO - Benchmarking distilbert (trial_0)...
2026-01-15 00:05:51,128 - evaluation.benchmarking.orchestrator - WARNING - [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=4f4d383480ad8038...
2026-01-15 00:05:51,129 - evaluation.benchmarking.orchestrator - INFO - [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-15 00:05:51,129 - evaluation.benchmarking.orchestrator - ERROR - Benchmark failed for distilbert
2026-01-15 00:05:51,129 - evaluation.benchmarking.orchestrator - INFO - Benchmarking complete. 0/1 trials benchmarked.
2026-01-15 00:05:51,131 - evaluation.benchmarking.orchestrator - INFO - Run mode is 'force_new' - skipping idempotency check, will benchmark all champions
2026-01-15 00:05:51,133 - evaluation.benchmarking.orchestrator - INFO - Skipping distilbert - benchmark already exists (trial_key_hash=trial_hash_456...)
2026-01-15 00:05:51,142 - evaluation.benchmarking.orchestrator - WARNING - Found 1 finished benchmark run(s) by hash (fallback). Consider setting benchmark_key tag for more reliable idempotency. trial_key_hash=trial_hash_456...
2026-01-15 00:05:51,147 - evaluation.benchmarking.orchestrator - WARNING - Found 1 finished benchmark run(s) by hash (fallback). Consider setting benchmark_key tag for more reliable idempotency. trial_key_hash=trial_hash_456...
2026-01-15 00:05:51,150 - evaluation.benchmarking.orchestrator - INFO - Skipping distilbert - benchmark already exists (trial_key_hash=trial_hash_456...)
2026-01-15 00:05:51,153 - evaluation.benchmarking.orchestrator - INFO - Run mode is 'force_new' - skipping idempotency check, will benchmark all champions
2026-01-15 00:05:51,158 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_bat0/src/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_bat0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_bat0/test.json --batch-sizes 1 8 16 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_bat0/benchmark.json
2026-01-15 00:05:51,160 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_ite0/src/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_ite0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_ite0/test.json --batch-sizes 1 8 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_ite0/benchmark.json
2026-01-15 00:05:51,163 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_war0/src/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_war0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_war0/test.json --batch-sizes 1 8 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_war0/benchmark.json
2026-01-15 00:05:51,167 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_max0/src/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_max0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_max0/test.json --batch-sizes 1 8 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_max0/benchmark.json
2026-01-15 00:05:51,170 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_dev0/src/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_dev0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_dev0/test.json --batch-sizes 1 8 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_dev0/benchmark.json --device cuda
2026-01-15 00:05:51,173 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_skips_de0/src/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_skips_de0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_skips_de0/test.json --batch-sizes 1 8 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_skips_de0/benchmark.json
2026-01-15 00:05:51,175 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_out0/src/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_out0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_out0/test.json --batch-sizes 1 8 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_uses_out0/benchmark.json
2026-01-15 00:05:51,178 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_all_conf0/src/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_all_conf0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_all_conf0/test.json --batch-sizes 2 4 32 --iterations 200 --warmup 20 --max-length 256 --output /tmp/pytest-of-codespace/pytest-28/test_run_benchmarking_all_conf0/custom_benchmark.json --device cuda
2026-01-15 00:05:51,184 - evaluation.benchmarking.orchestrator - INFO - Benchmarking distilbert (trial_0)...
2026-01-15 00:05:51,185 - evaluation.benchmarking.orchestrator - WARNING - [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=9ab6cf5dc0566079...
2026-01-15 00:05:51,187 - evaluation.benchmarking.orchestrator - INFO - [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-15 00:05:51,187 - evaluation.benchmarking.orchestrator - INFO - Benchmark completed: /tmp/pytest-of-codespace/pytest-28/test_workflow_loads_config_and0/outputs/benchmarking/test/benchmark.json
2026-01-15 00:05:51,188 - evaluation.benchmarking.orchestrator - INFO - Benchmarking complete. 1/1 trials benchmarked.
2026-01-15 00:05:51,191 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-28/test_workflow_custom_config_va0/src/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-28/test_workflow_custom_config_va0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-28/test_workflow_custom_config_va0/test.json --batch-sizes 2 4 32 --iterations 200 --warmup 20 --max-length 256 --output /tmp/pytest-of-codespace/pytest-28/test_workflow_custom_config_va0/custom_benchmark.json --device cuda
2026-01-15 00:05:51,196 - evaluation.benchmarking.orchestrator - INFO - Benchmarking distilbert (trial_0)...
2026-01-15 00:05:51,197 - evaluation.benchmarking.orchestrator - WARNING - [BENCHMARK] Could not find trial run in MLflow for trial_key_hash=4f4d383480ad8038...
2026-01-15 00:05:51,199 - evaluation.benchmarking.orchestrator - INFO - [BENCHMARK] Final run IDs: trial=None..., refit=None..., sweep=None...
2026-01-15 00:05:51,199 - evaluation.benchmarking.orchestrator - INFO - Benchmark completed: /tmp/pytest-of-codespace/pytest-28/test_workflow_defaults_when_co0/outputs/benchmarking/test/benchmark.json
2026-01-15 00:05:51,199 - evaluation.benchmarking.orchestrator - INFO - Benchmarking complete. 1/1 trials benchmarked.
2026-01-15 00:05:51,255 - evaluation.benchmarking.orchestrator - INFO - Run mode is 'force_new' - skipping idempotency check, will benchmark all champions
2026-01-15 00:05:51,257 - evaluation.benchmarking.orchestrator - INFO - Skipping distilbert - benchmark already exists (trial_key_hash=trial_hash_456...)
2026-01-15 00:05:51,257 - evaluation.benchmarking.orchestrator - INFO - Skipping deberta - benchmark already exists (trial_key_hash=trial_hash_789...)
2026-01-15 00:05:51,270 - evaluation.benchmarking.orchestrator - INFO - Skipping distilbert - benchmark already exists (trial_key_hash=trial_hash_456...)
2026-01-15 00:05:51,273 - evaluation.benchmarking.orchestrator - INFO - Skipping distilbert - benchmark already exists (trial_key_hash=trial_hash_456...)
2026-01-15 00:05:51,279 - evaluation.benchmarking.orchestrator - WARNING - No run_id found for champion distilbert, skipping idempotency check
2026-01-15 00:05:51,291 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-28/test_trial_id_from_parameter_o0/src/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-28/test_trial_id_from_parameter_o0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-28/test_trial_id_from_parameter_o0/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-28/test_trial_id_from_parameter_o0/benchmark.json
2026-01-15 00:05:51,292 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Building run name: trial_id=1_20251231_161745, root_dir=/tmp/pytest-of-codespace/pytest-28/test_trial_id_from_parameter_o0, config_dir=/tmp/pytest-of-codespace/pytest-28/test_trial_id_from_parameter_o0/config
2026-01-15 00:05:51,293 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Generated run name: <Mock name='create_naming_context().storage_env' id='124894361420752'>_<Mock name='create_naming_context().model' id='124894361420608'>_<Mock name='create_naming_context().process_type' id='124894361420512'>_legacy
2026-01-15 00:05:51,299 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-28/test_trial_id_from_parameter_n0/src/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-28/test_trial_id_from_parameter_n0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-28/test_trial_id_from_parameter_n0/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-28/test_trial_id_from_parameter_n0/benchmark.json
2026-01-15 00:05:51,299 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Building run name: trial_id=trial-25d03eeb, root_dir=/tmp/pytest-of-codespace/pytest-28/test_trial_id_from_parameter_n0, config_dir=/tmp/pytest-of-codespace/pytest-28/test_trial_id_from_parameter_n0/config
2026-01-15 00:05:51,300 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Generated run name: <Mock name='create_naming_context().storage_env' id='124894361595680'>_<Mock name='create_naming_context().model' id='124894361595536'>_<Mock name='create_naming_context().process_type' id='124894361595440'>_legacy
2026-01-15 00:05:51,304 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-28/test_trial_id_extraction_from_0/src/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-28/test_trial_id_extraction_from_0/hpo/local/distilbert/study-abc123/trial_1_20251231_161745/checkpoint --test-data /tmp/pytest-of-codespace/pytest-28/test_trial_id_extraction_from_0/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-28/test_trial_id_extraction_from_0/benchmark.json
2026-01-15 00:05:51,304 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Extracted trial_id from checkpoint path: trial_1_20251231_161745 (at level 1)
2026-01-15 00:05:51,304 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Building run name: trial_id=trial_1_20251231_161745, root_dir=/tmp/pytest-of-codespace/pytest-28/test_trial_id_extraction_from_0, config_dir=/tmp/pytest-of-codespace/pytest-28/test_trial_id_extraction_from_0/config
2026-01-15 00:05:51,305 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Generated run name: <Mock name='create_naming_context().storage_env' id='124894361593568'>_<Mock name='create_naming_context().model' id='124894361593904'>_<Mock name='create_naming_context().process_type' id='124894361593616'>_legacy
2026-01-15 00:05:51,309 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-28/test_trial_id_extraction_from_1/src/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-28/test_trial_id_extraction_from_1/hpo/local/distilbert/study-abc123/trial-25d03eeb/checkpoint --test-data /tmp/pytest-of-codespace/pytest-28/test_trial_id_extraction_from_1/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-28/test_trial_id_extraction_from_1/benchmark.json
2026-01-15 00:05:51,309 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Extracted trial_id from checkpoint path: trial-25d03eeb (at level 1)
2026-01-15 00:05:51,310 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Building run name: trial_id=trial-25d03eeb, root_dir=/tmp/pytest-of-codespace/pytest-28/test_trial_id_extraction_from_1, config_dir=/tmp/pytest-of-codespace/pytest-28/test_trial_id_extraction_from_1/config
2026-01-15 00:05:51,310 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Generated run name: <Mock name='create_naming_context().storage_env' id='124894361590160'>_<Mock name='create_naming_context().model' id='124894361590208'>_<Mock name='create_naming_context().process_type' id='124894361590112'>_legacy
2026-01-15 00:05:51,314 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-28/test_trial_id_extraction_from_2/src/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-28/test_trial_id_extraction_from_2/hpo/local/distilbert/study-abc123/trial-25d03eeb/refit/checkpoint --test-data /tmp/pytest-of-codespace/pytest-28/test_trial_id_extraction_from_2/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-28/test_trial_id_extraction_from_2/benchmark.json
2026-01-15 00:05:51,314 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Extracted trial_id from checkpoint path: trial-25d03eeb (at level 2)
2026-01-15 00:05:51,314 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Building run name: trial_id=trial-25d03eeb, root_dir=/tmp/pytest-of-codespace/pytest-28/test_trial_id_extraction_from_2, config_dir=/tmp/pytest-of-codespace/pytest-28/test_trial_id_extraction_from_2/config
2026-01-15 00:05:51,315 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Generated run name: <Mock name='create_naming_context().storage_env' id='124894361127616'>_<Mock name='create_naming_context().model' id='124894361127664'>_<Mock name='create_naming_context().process_type' id='124894361127568'>_legacy
2026-01-15 00:05:51,318 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-28/test_trial_id_fallback_to_tria0/src/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-28/test_trial_id_fallback_to_tria0/checkpoint --test-data /tmp/pytest-of-codespace/pytest-28/test_trial_id_fallback_to_tria0/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-28/test_trial_id_fallback_to_tria0/benchmark.json
2026-01-15 00:05:51,318 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Constructed trial_id from trial_key_hash: trial-25d03eeb
2026-01-15 00:05:51,319 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Building run name: trial_id=trial-25d03eeb, root_dir=/tmp/pytest-of-codespace/pytest-28/test_trial_id_fallback_to_tria0, config_dir=/tmp/pytest-of-codespace/pytest-28/test_trial_id_fallback_to_tria0/config
2026-01-15 00:05:51,319 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Generated run name: <Mock name='create_naming_context().storage_env' id='124894490717840'>_<Mock name='create_naming_context().model' id='124894490715488'>_<Mock name='create_naming_context().process_type' id='124894490718416'>_legacy
2026-01-15 00:05:51,323 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /tmp/pytest-of-codespace/pytest-28/test_trial_id_parameter_overri0/src/benchmarking/cli.py --checkpoint /tmp/pytest-of-codespace/pytest-28/test_trial_id_parameter_overri0/hpo/local/distilbert/study-abc123/trial-25d03eeb/checkpoint --test-data /tmp/pytest-of-codespace/pytest-28/test_trial_id_parameter_overri0/test.json --batch-sizes 1 --iterations 100 --warmup 10 --max-length 512 --output /tmp/pytest-of-codespace/pytest-28/test_trial_id_parameter_overri0/benchmark.json
2026-01-15 00:05:51,323 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Building run name: trial_id=trial-custom123, root_dir=/tmp/pytest-of-codespace/pytest-28/test_trial_id_parameter_overri0, config_dir=/tmp/pytest-of-codespace/pytest-28/test_trial_id_parameter_overri0/config
2026-01-15 00:05:51,324 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Generated run name: <Mock name='create_naming_context().storage_env' id='124894361098976'>_<Mock name='create_naming_context().model' id='124894361099024'>_<Mock name='create_naming_context().process_type' id='124894361098928'>_legacy
2026-01-15 00:05:51,331 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_hpo_trial_workflow0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-15 00:05:51,331 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-15 00:05:51,331 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:e0fbae020bc656b7e6425344765f3df149e5f6831d2b7..., root_dir=/tmp/pytest-of-codespace/pytest-28/test_hpo_trial_workflow0, config_dir=/tmp/pytest-of-codespace/pytest-28/test_hpo_trial_workflow0/config, counter_path=/tmp/pytest-of-codespace/pytest-28/test_hpo_trial_workflow0/outputs/cache/run_name_counter.json
2026-01-15 00:05:51,332 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Loaded 0 existing allocations from /tmp/pytest-of-codespace/pytest-28/test_hpo_trial_workflow0/outputs/cache/run_name_counter.json
2026-01-15 00:05:51,332 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0
2026-01-15 00:05:51,332 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)
2026-01-15 00:05:51,332 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version]  Successfully reserved version 1 for counter_key resume-ner:hpo:e0fbae020bc656b7e6425344765f3df149e... (run_id: pending_2026...)
2026-01-15 00:05:51,339 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_final_training_workflow0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-15 00:05:51,340 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=final_training
2026-01-15 00:05:51,347 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_benchmarking_workflow0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-15 00:05:51,347 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=benchmarking
2026-01-15 00:05:51,354 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_paths_match_naming_patter0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-15 00:05:51,354 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-15 00:05:51,355 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:fa924e9d2ac5675b719f2281bdcff80badd52b78fb7ef..., root_dir=/tmp/pytest-of-codespace/pytest-28/test_paths_match_naming_patter0, config_dir=/tmp/pytest-of-codespace/pytest-28/test_paths_match_naming_patter0/config, counter_path=/tmp/pytest-of-codespace/pytest-28/test_paths_match_naming_patter0/outputs/cache/run_name_counter.json
2026-01-15 00:05:51,355 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Loaded 0 existing allocations from /tmp/pytest-of-codespace/pytest-28/test_paths_match_naming_patter0/outputs/cache/run_name_counter.json
2026-01-15 00:05:51,355 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0
2026-01-15 00:05:51,355 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)
2026-01-15 00:05:51,355 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version]  Successfully reserved version 1 for counter_key resume-ner:hpo:fa924e9d2ac5675b719f2281bdcff80badd... (run_id: pending_2026...)
2026-01-15 00:05:51,364 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_naming_patterns_from_nami0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-15 00:05:51,364 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-15 00:05:51,365 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:e0fbae020bc656b7e6425344765f3df149e5f6831d2b7..., root_dir=/tmp/pytest-of-codespace/pytest-28/test_naming_patterns_from_nami0, config_dir=/tmp/pytest-of-codespace/pytest-28/test_naming_patterns_from_nami0/config, counter_path=/tmp/pytest-of-codespace/pytest-28/test_naming_patterns_from_nami0/outputs/cache/run_name_counter.json
2026-01-15 00:05:51,365 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Loaded 0 existing allocations from /tmp/pytest-of-codespace/pytest-28/test_naming_patterns_from_nami0/outputs/cache/run_name_counter.json
2026-01-15 00:05:51,365 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0
2026-01-15 00:05:51,365 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)
2026-01-15 00:05:51,365 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version]  Successfully reserved version 1 for counter_key resume-ner:hpo:e0fbae020bc656b7e6425344765f3df149e... (run_id: pending_2026...)
2026-01-15 00:05:51,378 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_naming_conventions_consis0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-15 00:05:51,378 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-15 00:05:51,378 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:e0fbae020bc656b7e6425344765f3df149e5f6831d2b7..., root_dir=/tmp/pytest-of-codespace/pytest-28/test_naming_conventions_consis0, config_dir=/tmp/pytest-of-codespace/pytest-28/test_naming_conventions_consis0/config, counter_path=/tmp/pytest-of-codespace/pytest-28/test_naming_conventions_consis0/outputs/cache/run_name_counter.json
2026-01-15 00:05:51,379 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Loaded 0 existing allocations from /tmp/pytest-of-codespace/pytest-28/test_naming_conventions_consis0/outputs/cache/run_name_counter.json
2026-01-15 00:05:51,379 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0
2026-01-15 00:05:51,379 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)
2026-01-15 00:05:51,379 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version]  Successfully reserved version 1 for counter_key resume-ner:hpo:e0fbae020bc656b7e6425344765f3df149e... (run_id: pending_2026...)
2026-01-15 00:05:51,598 - orchestration.jobs.tracking.config.loader - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_naming_run_name_auto_incr0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-15 00:05:51,599 - orchestration.jobs.tracking.config.loader - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-15 00:05:51,601 - orchestration.jobs.tracking.config.loader - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_naming_run_name_auto_incr1/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-15 00:05:51,601 - orchestration.jobs.tracking.config.loader - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-15 00:05:51,604 - orchestration.jobs.tracking.config.loader - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_naming_run_name_auto_incr2/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-15 00:05:51,604 - orchestration.jobs.tracking.config.loader - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=benchmarking
2026-01-15 00:05:51,607 - orchestration.jobs.tracking.config.loader - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_naming_run_name_auto_incr3/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-15 00:05:51,607 - orchestration.jobs.tracking.config.loader - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
[conversion.orchestration] Output directory: /tmp/pytest-of-codespace/pytest-28/test_opset_version_passed_to_s0/outputs/conversion/test
[conversion.orchestration] Created MLflow run: test_run_name (test_run_id...)
[conversion.orchestration] Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /path/to/checkpoint --config-dir /tmp/pytest-of-codespace/pytest-28/test_opset_version_passed_to_s0/config --backbone distilbert-base-uncased --output-dir /tmp/pytest-of-codespace/pytest-28/test_opset_version_passed_to_s0/outputs/conversion/test --opset-version 19 --run-smoke-test
2026-01-15 00:05:51,917 - infrastructure.tracking.mlflow.lifecycle - WARNING - Could not ensure run test_run_id... is terminated: Run 'test_run_id' not found
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/lifecycle.py", line 130, in ensure_run_terminated
    run = client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'test_run_id' not found
[conversion.orchestration] Conversion completed. ONNX model: /tmp/pytest-of-codespace/pytest-28/test_opset_version_passed_to_s0/outputs/conversion/test/model.onnx
[conversion.orchestration] Output directory: /tmp/pytest-of-codespace/pytest-28/test_quantization_int8_adds_fl0/outputs/conversion/test
[conversion.orchestration] Created MLflow run: test_run_name (test_run_id...)
[conversion.orchestration] Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /path/to/checkpoint --config-dir /tmp/pytest-of-codespace/pytest-28/test_quantization_int8_adds_fl0/config --backbone distilbert-base-uncased --output-dir /tmp/pytest-of-codespace/pytest-28/test_quantization_int8_adds_fl0/outputs/conversion/test --opset-version 18 --quantize-int8 --run-smoke-test
2026-01-15 00:05:51,926 - infrastructure.tracking.mlflow.lifecycle - WARNING - Could not ensure run test_run_id... is terminated: Run 'test_run_id' not found
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/lifecycle.py", line 130, in ensure_run_terminated
    run = client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'test_run_id' not found
[conversion.orchestration] Conversion completed. ONNX model: /tmp/pytest-of-codespace/pytest-28/test_quantization_int8_adds_fl0/outputs/conversion/test/model_int8.onnx
[conversion.orchestration] Output directory: /tmp/pytest-of-codespace/pytest-28/test_quantization_none_no_flag0/outputs/conversion/test
[conversion.orchestration] Created MLflow run: test_run_name (test_run_id...)
[conversion.orchestration] Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /path/to/checkpoint --config-dir /tmp/pytest-of-codespace/pytest-28/test_quantization_none_no_flag0/config --backbone distilbert-base-uncased --output-dir /tmp/pytest-of-codespace/pytest-28/test_quantization_none_no_flag0/outputs/conversion/test --opset-version 18 --run-smoke-test
2026-01-15 00:05:51,934 - infrastructure.tracking.mlflow.lifecycle - WARNING - Could not ensure run test_run_id... is terminated: Run 'test_run_id' not found
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/lifecycle.py", line 130, in ensure_run_terminated
    run = client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'test_run_id' not found
[conversion.orchestration] Conversion completed. ONNX model: /tmp/pytest-of-codespace/pytest-28/test_quantization_none_no_flag0/outputs/conversion/test/model.onnx
[conversion.orchestration] Output directory: /tmp/pytest-of-codespace/pytest-28/test_run_smoke_test_true_adds_0/outputs/conversion/test
[conversion.orchestration] Created MLflow run: test_run_name (test_run_id...)
[conversion.orchestration] Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /path/to/checkpoint --config-dir /tmp/pytest-of-codespace/pytest-28/test_run_smoke_test_true_adds_0/config --backbone distilbert-base-uncased --output-dir /tmp/pytest-of-codespace/pytest-28/test_run_smoke_test_true_adds_0/outputs/conversion/test --opset-version 18 --run-smoke-test
2026-01-15 00:05:51,942 - infrastructure.tracking.mlflow.lifecycle - WARNING - Could not ensure run test_run_id... is terminated: Run 'test_run_id' not found
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/lifecycle.py", line 130, in ensure_run_terminated
    run = client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'test_run_id' not found
[conversion.orchestration] Conversion completed. ONNX model: /tmp/pytest-of-codespace/pytest-28/test_run_smoke_test_true_adds_0/outputs/conversion/test/model.onnx
[conversion.orchestration] Output directory: /tmp/pytest-of-codespace/pytest-28/test_run_smoke_test_false_no_f0/outputs/conversion/test
[conversion.orchestration] Created MLflow run: test_run_name (test_run_id...)
[conversion.orchestration] Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /path/to/checkpoint --config-dir /tmp/pytest-of-codespace/pytest-28/test_run_smoke_test_false_no_f0/config --backbone distilbert-base-uncased --output-dir /tmp/pytest-of-codespace/pytest-28/test_run_smoke_test_false_no_f0/outputs/conversion/test --opset-version 18
2026-01-15 00:05:51,949 - infrastructure.tracking.mlflow.lifecycle - WARNING - Could not ensure run test_run_id... is terminated: Run 'test_run_id' not found
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/lifecycle.py", line 130, in ensure_run_terminated
    run = client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'test_run_id' not found
[conversion.orchestration] Conversion completed. ONNX model: /tmp/pytest-of-codespace/pytest-28/test_run_smoke_test_false_no_f0/outputs/conversion/test/model.onnx
[conversion.orchestration] Output directory: /tmp/pytest-of-codespace/pytest-28/test_filename_pattern_used_in_0/outputs/conversion/test
[conversion.orchestration] Created MLflow run: test_run_name (test_run_id...)
[conversion.orchestration] Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /path/to/checkpoint --config-dir /tmp/pytest-of-codespace/pytest-28/test_filename_pattern_used_in_0/config --backbone distilbert-base-uncased --output-dir /tmp/pytest-of-codespace/pytest-28/test_filename_pattern_used_in_0/outputs/conversion/test --opset-version 18 --run-smoke-test
2026-01-15 00:05:51,957 - infrastructure.tracking.mlflow.lifecycle - WARNING - Could not ensure run test_run_id... is terminated: Run 'test_run_id' not found
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/lifecycle.py", line 130, in ensure_run_terminated
    run = client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'test_run_id' not found
[conversion.orchestration] Conversion completed. ONNX model: /tmp/pytest-of-codespace/pytest-28/test_filename_pattern_used_in_0/outputs/conversion/test/custom_fp32_model.onnx
2026-01-15 00:05:52,046 - evaluation.selection.trial_finder - INFO - Found 3 runs with stage tag for distilbert (backbone=distilbert)
2026-01-15 00:05:52,046 - evaluation.selection.trial_finder - INFO - Grouped runs for distilbert: 0 v1 group(s), 1 v2 group(s)
2026-01-15 00:05:52,047 - evaluation.selection.trial_finder - INFO - Found 1 eligible group(s) for distilbert (0 skipped due to min_trials requirement)
2026-01-15 00:05:52,048 - evaluation.selection.trial_finder - INFO - Found refit run refit-run-12... for champion trial run2... (selected latest from 1 refit run(s))
2026-01-15 00:05:52,052 - evaluation.selection.trial_finder - INFO - Found 3 runs with stage tag for distilbert (backbone=distilbert)
2026-01-15 00:05:52,052 - evaluation.selection.trial_finder - INFO - Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)
2026-01-15 00:05:52,052 - evaluation.selection.trial_finder - INFO - Found 1 eligible group(s) for distilbert (0 skipped due to min_trials requirement)
2026-01-15 00:05:52,053 - evaluation.selection.trial_finder - INFO - Found refit run refit-run-12... for champion trial run2... (selected latest from 1 refit run(s))
2026-01-15 00:05:52,056 - evaluation.selection.trial_finder - INFO - No runs found with stage='hpo_trial' for distilbert, trying legacy stage='hpo'
2026-01-15 00:05:52,057 - evaluation.selection.trial_finder - INFO - Found 0 runs with stage tag for distilbert (backbone=distilbert)
2026-01-15 00:05:52,057 - evaluation.selection.trial_finder - INFO - Grouped runs for distilbert: 0 v1 group(s), 0 v2 group(s)
2026-01-15 00:05:52,057 - evaluation.selection.trial_finder - WARNING - No valid groups found for distilbert. No trial runs found in HPO experiment 'test_hpo_experiment'. This may indicate:
  - HPO was not run for this backbone
  - Runs exist but don't have required tags (stage='hpo_trial' or 'hpo', backbone tag)
  - Runs exist but were filtered out (missing metrics, artifacts, or grouping tags)
Skipping champion selection for distilbert.
2026-01-15 00:05:52,061 - evaluation.selection.trial_finder - INFO - Found 2 runs with stage tag for distilbert (backbone=distilbert)
2026-01-15 00:05:52,062 - evaluation.selection.trial_finder - INFO - Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)
2026-01-15 00:05:52,062 - evaluation.selection.trial_finder - WARNING - Skipping group hash1... - only 2 valid trials (minimum: 3)
2026-01-15 00:05:52,062 - evaluation.selection.trial_finder - WARNING - No eligible groups for distilbert. Processed 1 group(s), but 1 were skipped due to insufficient trials (minimum: 3). Remaining groups: 0
2026-01-15 00:05:52,066 - evaluation.selection.trial_finder - INFO - Found 3 runs with stage tag for distilbert (backbone=distilbert)
2026-01-15 00:05:52,067 - evaluation.selection.trial_finder - INFO - Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)
2026-01-15 00:05:52,068 - evaluation.selection.trial_finder - WARNING - Group hash1...: 2 valid metrics, 1 missing/invalid. Missing metric runs: ['run3']
2026-01-15 00:05:52,068 - evaluation.selection.trial_finder - WARNING - Skipping group hash1... - only 2 valid trials (minimum: 3)
2026-01-15 00:05:52,068 - evaluation.selection.trial_finder - WARNING - No eligible groups for distilbert. Processed 1 group(s), but 1 were skipped due to insufficient trials (minimum: 3). Remaining groups: 0
2026-01-15 00:05:52,073 - evaluation.selection.trial_finder - INFO - Found 3 runs with stage tag for distilbert (backbone=distilbert)
2026-01-15 00:05:52,074 - evaluation.selection.trial_finder - INFO - Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)
2026-01-15 00:05:52,075 - evaluation.selection.trial_finder - WARNING - Group hash1...: 2 valid metrics, 1 missing/invalid. Missing metric runs: []
2026-01-15 00:05:52,075 - evaluation.selection.trial_finder - WARNING - Skipping group hash1... - only 2 valid trials (minimum: 3)
2026-01-15 00:05:52,075 - evaluation.selection.trial_finder - WARNING - No eligible groups for distilbert. Processed 1 group(s), but 1 were skipped due to insufficient trials (minimum: 3). Remaining groups: 0
2026-01-15 00:05:52,079 - evaluation.selection.trial_finder - INFO - Found 3 runs with stage tag for distilbert (backbone=distilbert)
2026-01-15 00:05:52,080 - evaluation.selection.trial_finder - WARNING - Artifact filter: 1 run(s) have code.artifact.available='false' (explicitly marked as unavailable)
2026-01-15 00:05:52,081 - evaluation.selection.trial_finder - WARNING - Artifact filter: 1 run(s) excluded (1 explicitly false, 0 missing/legacy allowed)
2026-01-15 00:05:52,081 - evaluation.selection.trial_finder - WARNING - Artifact filter removed 1 runs for distilbert (2 remaining). Check that runs have 'code.artifact.available' tag set to 'true'.
2026-01-15 00:05:52,081 - evaluation.selection.trial_finder - INFO - Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)
2026-01-15 00:05:52,081 - evaluation.selection.trial_finder - WARNING - Skipping group hash1... - only 2 valid trials (minimum: 3)
2026-01-15 00:05:52,081 - evaluation.selection.trial_finder - WARNING - No eligible groups for distilbert. Processed 1 group(s), but 1 were skipped due to insufficient trials (minimum: 3). Remaining groups: 0
2026-01-15 00:05:52,084 - evaluation.selection.trial_finder - INFO - Found 3 runs with stage tag for distilbert (backbone=distilbert)
2026-01-15 00:05:52,084 - evaluation.selection.trial_finder - INFO - Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)
2026-01-15 00:05:52,084 - evaluation.selection.trial_finder - INFO - Found 1 eligible group(s) for distilbert (0 skipped due to min_trials requirement)
2026-01-15 00:05:52,085 - evaluation.selection.trial_finder - INFO - Found refit run refit-run-12... for champion trial run2... (selected latest from 1 refit run(s))
2026-01-15 00:05:52,088 - evaluation.selection.trial_finder - INFO - Found 5 runs with stage tag for distilbert (backbone=distilbert)
2026-01-15 00:05:52,088 - evaluation.selection.trial_finder - INFO - Grouped runs for distilbert: 1 v1 group(s), 1 v2 group(s)
2026-01-15 00:05:52,088 - evaluation.selection.trial_finder - INFO - Found both v1 and v2 runs for distilbert. Using 2.0 groups only (never mixing versions).
2026-01-15 00:05:52,089 - evaluation.selection.trial_finder - INFO - Found 1 eligible group(s) for distilbert (0 skipped due to min_trials requirement)
2026-01-15 00:05:52,089 - evaluation.selection.trial_finder - INFO - Found refit run refit-run-12... for champion trial run3... (selected latest from 1 refit run(s))
2026-01-15 00:05:52,092 - evaluation.selection.trial_finder - INFO - Found 3 runs with stage tag for distilbert (backbone=distilbert)
2026-01-15 00:05:52,092 - evaluation.selection.trial_finder - INFO - Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)
2026-01-15 00:05:52,092 - evaluation.selection.trial_finder - INFO - Found 1 eligible group(s) for distilbert (0 skipped due to min_trials requirement)
2026-01-15 00:05:52,093 - evaluation.selection.trial_finder - INFO - Found refit run refit-run-12... for champion trial run3... (selected latest from 1 refit run(s))
2026-01-15 00:05:52,095 - evaluation.selection.trial_finder - INFO - Found 3 runs with stage tag for distilbert (backbone=distilbert)
2026-01-15 00:05:52,095 - evaluation.selection.trial_finder - INFO - Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)
2026-01-15 00:05:52,096 - evaluation.selection.trial_finder - INFO - Found 1 eligible group(s) for distilbert (0 skipped due to min_trials requirement)
2026-01-15 00:05:52,096 - evaluation.selection.trial_finder - INFO - Found refit run refit-run-12... for champion trial run2... (selected latest from 1 refit run(s))
2026-01-15 00:05:52,099 - evaluation.selection.trial_finder - INFO - Found 6 runs with stage tag for distilbert (backbone=distilbert)
2026-01-15 00:05:52,101 - evaluation.selection.trial_finder - INFO - Grouped runs for distilbert: 2 v1 group(s), 0 v2 group(s)
2026-01-15 00:05:52,101 - evaluation.selection.trial_finder - INFO - Found 2 eligible group(s) for distilbert (0 skipped due to min_trials requirement)
2026-01-15 00:05:52,102 - evaluation.selection.trial_finder - INFO - Found refit run refit-run-12... for champion trial run6... (selected latest from 1 refit run(s))
2026-01-15 00:05:52,104 - evaluation.selection.trial_finder - INFO - Found 5 runs with stage tag for distilbert (backbone=distilbert)
2026-01-15 00:05:52,105 - evaluation.selection.trial_finder - INFO - Grouped runs for distilbert: 1 v1 group(s), 0 v2 group(s)
2026-01-15 00:05:52,105 - evaluation.selection.trial_finder - INFO - Found 1 eligible group(s) for distilbert (0 skipped due to min_trials requirement)
2026-01-15 00:05:52,105 - evaluation.selection.trial_finder - INFO - Found refit run refit-run-12... for champion trial run3... (selected latest from 1 refit run(s))
2026-01-15 00:05:52,113 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_fo0/config/tags.yaml, using defaults
2026-01-15 00:05:52,123 - training.execution.mlflow_setup - INFO -  View run test_run_name at: file:///tmp/mlflow/#/experiments/314291386468007766/runs/509af58e16c246fba3d96307c6832ba4
2026-01-15 00:05:52,123 - training.execution.mlflow_setup - INFO - Created MLflow run: test_run_name (509af58e16c2...)
2026-01-15 00:05:52,124 - infrastructure.tracking.mlflow.lifecycle - INFO - Run 509af58e16c2... already terminated with status <Mock name='mock.get_run().info.status' id='124894361456016'> (expected FINISHED)
2026-01-15 00:05:52,129 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_re1/config/tags.yaml, using defaults
2026-01-15 00:05:52,136 - training.execution.mlflow_setup - INFO -  View run test_run_name at: file:///tmp/mlflow/#/experiments/314291386468007766/runs/f8c16e4c3d5c416595e2bb89699a68a0
2026-01-15 00:05:52,136 - training.execution.mlflow_setup - INFO - Created MLflow run: test_run_name (f8c16e4c3d5c...)
2026-01-15 00:05:52,137 - infrastructure.tracking.mlflow.lifecycle - INFO - Run f8c16e4c3d5c... already terminated with status <Mock name='mock.get_run().info.status' id='124894361700144'> (expected FINISHED)
2026-01-15 00:05:52,144 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_lo0/config/tags.yaml, using defaults
2026-01-15 00:05:52,151 - training.execution.mlflow_setup - INFO -  View run test_run_name at: file:///tmp/mlflow/#/experiments/314291386468007766/runs/b353d126578349999acc8a39ed05e413
2026-01-15 00:05:52,151 - training.execution.mlflow_setup - INFO - Created MLflow run: test_run_name (b353d1265783...)
2026-01-15 00:05:52,152 - infrastructure.tracking.mlflow.lifecycle - INFO - Run b353d1265783... already terminated with status <Mock name='mock.get_run().info.status' id='124894361705232'> (expected FINISHED)
2026-01-15 00:05:52,157 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_tr0/config/tags.yaml, using defaults
2026-01-15 00:05:52,164 - training.execution.mlflow_setup - INFO -  View run test_run_name at: file:///tmp/mlflow/#/experiments/314291386468007766/runs/61243bd7c32340e9b4e0b2c851de03ca
2026-01-15 00:05:52,164 - training.execution.mlflow_setup - INFO - Created MLflow run: test_run_name (61243bd7c323...)
2026-01-15 00:05:52,165 - infrastructure.tracking.mlflow.lifecycle - INFO - Run 61243bd7c323... already has status <Mock name='mock.get_run().info.status' id='124894361713968'>, skipping termination (expected RUNNING)
2026-01-15 00:05:52,169 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_ml0/config/tags.yaml, using defaults
2026-01-15 00:05:52,180 - training.execution.mlflow_setup - INFO -  View run test_run_name at: file:///tmp/mlflow/#/experiments/314291386468007766/runs/0af4a2761b074926838a53bbf33cb064
2026-01-15 00:05:52,180 - training.execution.mlflow_setup - INFO - Created MLflow run: test_run_name (0af4a2761b07...)
2026-01-15 00:05:52,181 - infrastructure.tracking.mlflow.lifecycle - INFO - Run 0af4a2761b07... already terminated with status <Mock name='mock.get_run().info.status' id='124894361698560'> (expected FINISHED)
2026-01-15 00:05:52,186 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_so0/config/tags.yaml, using defaults
2026-01-15 00:05:52,193 - training.execution.mlflow_setup - INFO -  View run test_run_name at: file:///tmp/mlflow/#/experiments/314291386468007766/runs/0eea0fa7cbf6486cabb5287adfa34d34
2026-01-15 00:05:52,194 - training.execution.mlflow_setup - INFO - Created MLflow run: test_run_name (0eea0fa7cbf6...)
2026-01-15 00:05:52,194 - infrastructure.tracking.mlflow.lifecycle - INFO - Run 0eea0fa7cbf6... already terminated with status <Mock name='mock.get_run().info.status' id='124894361705376'> (expected FINISHED)
2026-01-15 00:05:52,199 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_so1/config/tags.yaml, using defaults
2026-01-15 00:05:52,206 - training.execution.mlflow_setup - INFO -  View run test_run_name at: file:///tmp/mlflow/#/experiments/314291386468007766/runs/42cc110841cb4e9c8e9b646ad99d825f
2026-01-15 00:05:52,206 - training.execution.mlflow_setup - INFO - Created MLflow run: test_run_name (42cc110841cb...)
2026-01-15 00:05:52,207 - infrastructure.tracking.mlflow.lifecycle - INFO - Run 42cc110841cb... already terminated with status <Mock name='mock.get_run().info.status' id='124894361493488'> (expected FINISHED)
2026-01-15 00:05:52,212 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_hy0/config/tags.yaml, using defaults
2026-01-15 00:05:52,219 - training.execution.mlflow_setup - INFO -  View run test_run_name at: file:///tmp/mlflow/#/experiments/314291386468007766/runs/98d57ceda9ff41b8bdcf5a779088abab
2026-01-15 00:05:52,219 - training.execution.mlflow_setup - INFO - Created MLflow run: test_run_name (98d57ceda9ff...)
2026-01-15 00:05:52,219 - infrastructure.tracking.mlflow.lifecycle - INFO - Run 98d57ceda9ff... already terminated with status <Mock name='mock.get_run().info.status' id='124894361714496'> (expected FINISHED)
2026-01-15 00:05:52,224 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_execute_final_training_ml1/config/tags.yaml, using defaults
2026-01-15 00:05:52,230 - training.execution.mlflow_setup - INFO -  View run default_run_name at: file:///tmp/mlflow/#/experiments/805178406443610137/runs/fa407fdfa98f4e69bc3e69d1cbc7f34a
2026-01-15 00:05:52,230 - training.execution.mlflow_setup - INFO - Created MLflow run: default_run_name (fa407fdfa98f...)
2026-01-15 00:05:52,231 - infrastructure.tracking.mlflow.lifecycle - INFO - Run fa407fdfa98f... already terminated with status <Mock name='mock.get_run().info.status' id='124894361495456'> (expected FINISHED)
2026-01-15 00:05:52,236 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_logging_eval_interval_loa0/config/tags.yaml, using defaults
2026-01-15 00:05:52,243 - training.execution.mlflow_setup - INFO -  View run test_run_name at: file:///tmp/mlflow/#/experiments/314291386468007766/runs/3efa65780ee543e18a53804bc86ddd23
2026-01-15 00:05:52,243 - training.execution.mlflow_setup - INFO - Created MLflow run: test_run_name (3efa65780ee5...)
2026-01-15 00:05:52,244 - infrastructure.tracking.mlflow.lifecycle - INFO - Run 3efa65780ee5... already terminated with status <Mock name='mock.get_run().info.status' id='124894361706000'> (expected FINISHED)
2026-01-15 00:05:52,248 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_logging_save_interval_loa0/config/tags.yaml, using defaults
2026-01-15 00:05:52,255 - training.execution.mlflow_setup - INFO -  View run test_run_name at: file:///tmp/mlflow/#/experiments/314291386468007766/runs/5ca51071baa64e2a8ead5acffeca1c88
2026-01-15 00:05:52,256 - training.execution.mlflow_setup - INFO - Created MLflow run: test_run_name (5ca51071baa6...)
2026-01-15 00:05:52,256 - infrastructure.tracking.mlflow.lifecycle - INFO - Run 5ca51071baa6... already terminated with status <Mock name='mock.get_run().info.status' id='124894361496512'> (expected FINISHED)
2026-01-15 00:05:52,261 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_logging_intervals_both_lo0/config/tags.yaml, using defaults
2026-01-15 00:05:52,268 - training.execution.mlflow_setup - INFO -  View run test_run_name at: file:///tmp/mlflow/#/experiments/314291386468007766/runs/4999f8f3962e4729b64753c51e15d13b
2026-01-15 00:05:52,268 - training.execution.mlflow_setup - INFO - Created MLflow run: test_run_name (4999f8f3962e...)
2026-01-15 00:05:52,269 - infrastructure.tracking.mlflow.lifecycle - INFO - Run 4999f8f3962e... already terminated with status <Mock name='mock.get_run().info.status' id='124894361489216'> (expected FINISHED)
[I 2026-01-15 00:05:52,686] A new study created in RDB with name: test_best
[I 2026-01-15 00:05:52,737] Trial 0 finished with value: 0.7 and parameters: {}. Best is trial 0 with value: 0.7.
[I 2026-01-15 00:05:52,777] Trial 1 finished with value: 0.85 and parameters: {}. Best is trial 1 with value: 0.85.
[I 2026-01-15 00:05:52,804] Trial 2 finished with value: 0.75 and parameters: {}. Best is trial 1 with value: 0.85.
[I 2026-01-15 00:05:52,885] A new study created in RDB with name: test_extract
[I 2026-01-15 00:05:52,920] Trial 0 finished with value: 0.8 and parameters: {'learning_rate': 2.343670918697731e-05, 'batch_size': 4}. Best is trial 0 with value: 0.8.
[I 2026-01-15 00:05:53,006] A new study created in RDB with name: test_cv
[I 2026-01-15 00:05:53,036] Trial 0 finished with value: 0.8 and parameters: {}. Best is trial 0 with value: 0.8.
[I 2026-01-15 00:05:53,123] A new study created in RDB with name: test_minimize
[I 2026-01-15 00:05:53,145] Trial 0 finished with value: 0.1 and parameters: {}. Best is trial 0 with value: 0.1.
[I 2026-01-15 00:05:53,162] Trial 1 finished with value: 0.2 and parameters: {}. Best is trial 0 with value: 0.1.
[I 2026-01-15 00:05:53,178] Trial 2 finished with value: 0.15 and parameters: {}. Best is trial 0 with value: 0.1.
[I 2026-01-15 00:05:53,268] A new study created in RDB with name: test_refit
[I 2026-01-15 00:05:53,301] Trial 0 finished with value: 0.7 and parameters: {'learning_rate': 1.2919889086571561e-05}. Best is trial 0 with value: 0.7.
[I 2026-01-15 00:05:53,324] Trial 1 finished with value: 0.85 and parameters: {'learning_rate': 4.3026262761198126e-05}. Best is trial 1 with value: 0.85.
[I 2026-01-15 00:05:53,413] A new study created in RDB with name: test_empty
[I 2026-01-15 00:05:53,495] A new study created in RDB with name: test_params
[I 2026-01-15 00:05:53,536] Trial 0 finished with value: 0.8 and parameters: {'learning_rate': 2.491093664700436e-05, 'batch_size': 5, 'dropout': 0.20071624963431095, 'weight_decay': 0.0011001103428449734}. Best is trial 0 with value: 0.8.
[I 2026-01-15 00:05:53,625] A new study created in RDB with name: test_delay
[I 2026-01-15 00:05:53,658] Trial 0 finished with value: 0.5 and parameters: {}. Best is trial 0 with value: 0.5.
[I 2026-01-15 00:05:53,681] Trial 1 finished with value: 0.3 and parameters: {}. Best is trial 0 with value: 0.5.
[I 2026-01-15 00:05:53,767] A new study created in RDB with name: test_prune
[I 2026-01-15 00:05:53,799] Trial 0 finished with value: 0.8 and parameters: {}. Best is trial 0 with value: 0.8.
[I 2026-01-15 00:05:53,819] Trial 1 finished with value: 0.75 and parameters: {}. Best is trial 0 with value: 0.8.
[I 2026-01-15 00:05:53,838] Trial 2 pruned. 
[I 2026-01-15 00:05:53,922] A new study created in RDB with name: test_interval
[I 2026-01-15 00:05:53,961] Trial 0 finished with value: 0.3 and parameters: {}. Best is trial 0 with value: 0.3.
2026-01-15 00:05:53,967 - training.hpo.core.study - INFO - [HPO] Starting optimization for distilbert with checkpointing...
[I 2026-01-15 00:05:54,045] A new study created in RDB with name: hpo_test_pruner
[I 2026-01-15 00:05:54,466] A new study created in RDB with name: test_best
[I 2026-01-15 00:05:54,508] Trial 0 finished with value: 0.9 and parameters: {}. Best is trial 0 with value: 0.9.
[I 2026-01-15 00:05:54,534] Trial 1 finished with value: 0.7 and parameters: {}. Best is trial 0 with value: 0.9.
[I 2026-01-15 00:05:54,554] Trial 2 finished with value: 0.8 and parameters: {}. Best is trial 0 with value: 0.9.
2026-01-15 00:05:54,559 - training.hpo.core.study - INFO - [HPO] Starting optimization for distilbert with checkpointing...
[I 2026-01-15 00:05:54,638] A new study created in RDB with name: hpo_test_prune_resume
[I 2026-01-15 00:05:54,669] Trial 0 finished with value: 0.75 and parameters: {}. Best is trial 0 with value: 0.75.
2026-01-15 00:05:54,670 - training.hpo.core.study - INFO - [HPO] Resuming optimization for distilbert from checkpoint...
[I 2026-01-15 00:05:54,694] Using an existing study with name 'hpo_test_prune_resume' instead of creating a new one.
2026-01-15 00:05:54,711 - training.hpo.core.study - INFO - Loaded 1 existing trials (1 completed, 0 marked as failed)
[I 2026-01-15 00:05:54,793] A new study created in RDB with name: test_no_prune
[I 2026-01-15 00:05:54,819] Trial 0 finished with value: 0.5 and parameters: {}. Best is trial 0 with value: 0.5.
[I 2026-01-15 00:05:54,838] Trial 1 finished with value: 0.5 and parameters: {}. Best is trial 0 with value: 0.5.
[I 2026-01-15 00:05:54,856] Trial 2 finished with value: 0.5 and parameters: {}. Best is trial 0 with value: 0.5.
2026-01-15 00:05:54,871 - infrastructure.paths.utils - WARNING - Could not find project root with src/training/ directory after 5 levels. Using /tmp/pytest-of-codespace/pytest-28/test_training_module_not_found0 as root_dir. Config_dir: /tmp/pytest-of-codespace/pytest-28/test_training_module_not_found0/config
2026-01-15 00:05:54,875 - training.hpo.trial.metrics - ERROR - metrics.json not found at expected location: /tmp/pytest-of-codespace/pytest-28/test_missing_metrics_file0/outputs/hpo/local/distilbert/study-abc12345/trial-def67890/metrics.json. Trial output dir: /tmp/pytest-of-codespace/pytest-28/test_missing_metrics_file0/outputs/hpo/local/distilbert/study-abc12345/trial-def67890, Root dir: /tmp/pytest-of-codespace/pytest-28/test_missing_metrics_file0
2026-01-15 00:05:54,886 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_cv_trial_failure_propagat0/config/tags.yaml, using defaults
2026-01-15 00:05:54,887 - training.hpo.execution.local.cv - INFO - [CV] Created trial folder: /tmp/pytest-of-codespace/pytest-28/test_cv_trial_failure_propagat0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-d7c6e3fb (trial 0)
2026-01-15 00:05:54,891 - training.hpo.execution.local.cv - WARNING - Could not build systematic run name and tags: HPO trial run name built without study_key_hash; check study identity propagation., using fallback
2026-01-15 00:05:54,891 - training.hpo.execution.local.cv - ERROR - In v2 study folder study-abc12345 but trial_key_hash is None. study_key_hash=NO. Will attempt to compute hash.
2026-01-15 00:05:54,892 - training.hpo.execution.local.cv - WARNING - In v2 study folder but missing hashes: study_key_hash=NO, trial_key_hash=NO
2026-01-15 00:05:54,892 - training.hpo.execution.local.cv - ERROR - ERROR: In v2 study folder study-abc12345 but computed_trial_key_hash is None. study_key_hash=None, trial_params={'learning_rate': 3e-05, 'batch_size': 4, 'backbone': 'distilbert', 'trial_number': 0}. Attempting to compute trial_key_hash from trial_params...
2026-01-15 00:05:54,892 - training.hpo.execution.local.cv - ERROR - CRITICAL: Failed to compute trial_key_hash for v2 study folder study-abc12345. study_key_hash=NO, trial_params keys: ['learning_rate', 'batch_size', 'backbone', 'trial_number']. Error: Cannot compute trial_key_hash without study_key_hash
2026-01-15 00:05:54,892 - training.hpo.execution.local.cv - ERROR - Traceback: Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/cv.py", line 292, in run_training_trial_with_cv
    raise ValueError("Cannot compute trial_key_hash without study_key_hash")
ValueError: Cannot compute trial_key_hash without study_key_hash

2026-01-15 00:05:54,896 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'batch_size': 4, 'dropout': 0.2, 'weight_decay': 0.05, 'backbone': 'distilbert'}
2026-01-15 00:05:54,896 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-15 00:05:54,896 - infrastructure.paths.utils - WARNING - Could not find project root with src/training/ directory after 5 levels. Using /tmp/pytest-of-codespace/pytest-28/test_refit_subprocess_failure0 as root_dir. Config_dir: /tmp/pytest-of-codespace/pytest-28/test_refit_subprocess_failure0/config
2026-01-15 00:05:54,899 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_refit_subprocess_failure0/config/tags.yaml, using defaults
2026-01-15 00:05:54,902 - training.execution.mlflow_setup - WARNING - Could not get parent run parent_123, using experiment directly: Run 'parent_123' not found
2026-01-15 00:05:54,912 - training.execution.mlflow_setup - INFO -  View run local_distilbert_hpo_refit_legacy at: file:///tmp/mlflow/#/experiments/989272176793454112/runs/9da9b6307d3845ed9f011aa5f1080c41
2026-01-15 00:05:54,912 - training.execution.mlflow_setup - INFO - Created MLflow run: local_distilbert_hpo_refit_legacy (9da9b6307d38...)
2026-01-15 00:05:54,912 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run 9da9b6307d38... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:05:54,913 - training.hpo.execution.local.refit - ERROR - [REFIT]  Error linking refit run to trial run: Run 'parent_123' not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 743, in get_run
    run_info = self._get_run_info(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/file_store.py", line 769, in _get_run_info
    raise MlflowException(
mlflow.exceptions.MlflowException: Run 'parent_123' not found
2026-01-15 00:05:54,916 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial 0 with hyperparameters: {'backbone': 'distilbert'}
2026-01-15 00:05:54,917 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-15 00:05:54,917 - training.hpo.execution.local.refit - WARNING - Could not construct v2 refit folder, falling back to legacy: 'NoneType' object has no attribute 'mkdir'
2026-01-15 00:05:54,919 - training.hpo.core.study - INFO - [HPO] Starting optimization for distilbert with checkpointing...
[I 2026-01-15 00:05:54,996] A new study created in RDB with name: hpo_distilbert
[I 2026-01-15 00:05:55,078] A new study created in RDB with name: empty_study
[I 2026-01-15 00:05:55,183] A new study created in RDB with name: hpo_distilbert_test
2026-01-15 00:05:55,204 - training.hpo.core.study - INFO - [HPO] Resuming optimization for distilbert from checkpoint...
[I 2026-01-15 00:05:55,226] Using an existing study with name 'hpo_distilbert_test' instead of creating a new one.
2026-01-15 00:05:55,245 - training.hpo.core.study - INFO - Loaded 1 existing trials (1 completed, 0 marked as failed)
[I 2026-01-15 00:05:55,329] A new study created in RDB with name: hpo_distilbert_resume_test
2026-01-15 00:05:55,380 - training.hpo.core.study - INFO - [HPO] Resuming optimization for distilbert from checkpoint...
[I 2026-01-15 00:05:55,402] Using an existing study with name 'hpo_distilbert_resume_test' instead of creating a new one.
2026-01-15 00:05:55,419 - training.hpo.core.study - INFO - Loaded 3 existing trials (3 completed, 0 marked as failed)
[I 2026-01-15 00:05:55,507] A new study created in RDB with name: hpo_distilbert_interrupted_test
2026-01-15 00:05:55,536 - training.hpo.core.study - INFO - [HPO] Resuming optimization for distilbert from checkpoint...
[I 2026-01-15 00:05:55,560] Using an existing study with name 'hpo_distilbert_interrupted_test' instead of creating a new one.
2026-01-15 00:05:55,572 - training.hpo.core.study - WARNING - Found 1 RUNNING trials from previous session. Marking them as FAILED (interrupted).
2026-01-15 00:05:55,587 - training.hpo.core.study - INFO - Loaded 2 existing trials (1 completed, 0 marked as failed)
[I 2026-01-15 00:05:55,672] A new study created in RDB with name: hpo_distilbert_no_resume_test
2026-01-15 00:05:55,693 - training.hpo.core.study - INFO - [HPO] auto_resume=false: Creating new study 'hpo_distilbert_no_resume_test' (ignoring existing study)
2026-01-15 00:05:55,693 - training.hpo.core.study - INFO - [HPO] Starting optimization for distilbert with checkpointing...
[I 2026-01-15 00:05:55,715] Using an existing study with name 'hpo_distilbert_no_resume_test' instead of creating a new one.
[I 2026-01-15 00:05:55,813] A new study created in RDB with name: hpo_distilbert_numbering_test
2026-01-15 00:05:55,850 - training.hpo.core.study - INFO - [HPO] Resuming optimization for distilbert from checkpoint...
[I 2026-01-15 00:05:55,873] Using an existing study with name 'hpo_distilbert_numbering_test' instead of creating a new one.
2026-01-15 00:05:55,891 - training.hpo.core.study - INFO - Loaded 2 existing trials (2 completed, 0 marked as failed)
[I 2026-01-15 00:05:56,000] A new study created in RDB with name: hpo_distilbert_smoke_resume
2026-01-15 00:05:56,020 - training.hpo.core.study - INFO - [HPO] Resuming optimization for distilbert from checkpoint...
[I 2026-01-15 00:05:56,044] Using an existing study with name 'hpo_distilbert_smoke_resume' instead of creating a new one.
2026-01-15 00:05:56,061 - training.hpo.core.study - INFO - Loaded 1 existing trials (1 completed, 0 marked as failed)
[I 2026-01-15 00:05:56,151] A new study created in RDB with name: hpo_distilbert_file_test
[I 2026-01-15 00:05:56,240] A new study created in RDB with name: hpo_distilbert_persist_test
[I 2026-01-15 00:05:56,396] A new study created in RDB with name: hpo_distilbert_move_test
2026-01-15 00:05:56,460 - training.hpo.execution.local.sweep - INFO - [EARLY HASH] Computed v2 study_key_hash=01b6aaaa11c2ca02... for folder creation
2026-01-15 00:05:56,462 - training.hpo.core.study - INFO - [HPO] Starting optimization for distilbert with checkpointing...
2026-01-15 00:05:56,544 - training.hpo.execution.local.sweep - INFO - [HPO Setup] k_folds=2, fold_splits_file=None, k_fold config: {'enabled': True, 'n_splits': 2, 'random_seed': 42, 'shuffle': True, 'stratified': True}
2026-01-15 00:05:56,545 - training.hpo.execution.local.sweep - INFO - [CV Setup]  Created 2 fold splits and saved to /tmp/pytest-of-codespace/pytest-28/test_full_hpo_workflow_with_cv0/outputs/hpo/local/distilbert/study-01b6aaaa/fold_splits.json
2026/01/15 00:05:56 INFO mlflow.store.db.utils: Creating initial MLflow database tables...
2026/01/15 00:05:56 INFO mlflow.store.db.utils: Updating database tables
2026-01-15 00:05:56 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2026-01-15 00:05:56 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
2026-01-15 00:05:57 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.
2026-01-15 00:05:57 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.
2026-01-15 00:05:57,153 - common.shared.mlflow_setup - INFO - Using local tracking: sqlite:////workspaces/resume-ner-azureml/mlruns/mlflow.db
2026-01-15 00:05:57 INFO  [common.shared.mlflow_setup] Using local tracking: sqlite:////workspaces/resume-ner-azureml/mlruns/mlflow.db
2026-01-15 00:05:57,153 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Using LOCAL MLflow tracking (not Azure ML)
2026-01-15 00:05:57 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker] Using LOCAL MLflow tracking (not Azure ML)
2026-01-15 00:05:57,153 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
2026-01-15 00:05:57 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
2026-01-15 00:05:57,352 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=01b6aaaa11c2ca02...
2026-01-15 00:05:57 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=01b6aaaa11c2ca02...
2026-01-15 00:05:57,352 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=01b6aaaa11c2ca02..., study_family_hash=6a3fd9967110a5db...
2026-01-15 00:05:57 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=01b6aaaa11c2ca02..., study_family_hash=6a3fd9967110a5db...
2026-01-15 00:05:57,431 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Yielding RunHandle. run_id=dbe259b90ab7...
2026-01-15 00:05:57 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Yielding RunHandle. run_id=dbe259b90ab7...
2026-01-15 00:05:57,441 - training.hpo.execution.local.sweep - INFO - [HPO] Parent run dbe259b90ab7... is now visible in MLflow (status: RUNNING)
2026-01-15 00:05:57 INFO  [training.hpo.execution.local.sweep] [HPO] Parent run dbe259b90ab7... is now visible in MLflow (status: RUNNING)
2026-01-15 00:05:57,449 - training.hpo.execution.local.sweep - INFO - Setting Phase 2 tags on parent run dbe259b90ab7... (data_config=present, train_config=present)
2026-01-15 00:05:57 INFO  [training.hpo.execution.local.sweep] Setting Phase 2 tags on parent run dbe259b90ab7... (data_config=present, train_config=present)
2026-01-15 00:05:57,451 - training.hpo.execution.local.sweep - WARNING - Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
2026-01-15 00:05:57 WARNI [training.hpo.execution.local.sweep] Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
2026-01-15 00:05:57,452 - training.hpo.execution.local.sweep - WARNING - Could not set data fingerprint tag: 'Missing tag key: fingerprint.data'
2026-01-15 00:05:57 WARNI [training.hpo.execution.local.sweep] Could not set data fingerprint tag: 'Missing tag key: fingerprint.data'
2026-01-15 00:05:57,452 - training.hpo.execution.local.sweep - WARNING - Could not set eval fingerprint tag: 'Missing tag key: fingerprint.eval'
2026-01-15 00:05:57 WARNI [training.hpo.execution.local.sweep] Could not set eval fingerprint tag: 'Missing tag key: fingerprint.eval'
2026-01-15 00:05:57,452 - training.hpo.execution.local.sweep - WARNING - Could not set objective direction tag: 'Missing tag key: objective.direction'
2026-01-15 00:05:57 WARNI [training.hpo.execution.local.sweep] Could not set objective direction tag: 'Missing tag key: objective.direction'
2026-01-15 00:05:57,452 - training.hpo.execution.local.sweep - WARNING - Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
2026-01-15 00:05:57 WARNI [training.hpo.execution.local.sweep] Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
2026-01-15 00:05:57,452 - training.hpo.execution.local.sweep - WARNING - Could not set artifact.available tag: 'Missing tag key: artifact.available'
2026-01-15 00:05:57 WARNI [training.hpo.execution.local.sweep] Could not set artifact.available tag: 'Missing tag key: artifact.available'
2026-01-15 00:05:57,452 - training.hpo.execution.local.sweep - INFO -  Set Phase 2 tags on parent run dbe259b90ab7...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-15 00:05:57 INFO  [training.hpo.execution.local.sweep]  Set Phase 2 tags on parent run dbe259b90ab7...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-15 00:05:57,452 - training.hpo.execution.local.sweep - INFO -  Successfully completed Phase 2 tag setting for parent run dbe259b90ab7...
2026-01-15 00:05:57 INFO  [training.hpo.execution.local.sweep]  Successfully completed Phase 2 tag setting for parent run dbe259b90ab7...
2026-01-15 00:05:57,452 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Starting cleanup check: parent_run_id=dbe259b90ab7...
2026-01-15 00:05:57 INFO  [training.hpo.tracking.cleanup] [CLEANUP] Starting cleanup check: parent_run_id=dbe259b90ab7...
2026-01-15 00:05:57,457 - training.hpo.tracking.cleanup - INFO - [CLEANUP] MLflow imported successfully. Current env: local, run_key_hash: cc90dfbe1603...
2026-01-15 00:05:57 INFO  [training.hpo.tracking.cleanup] [CLEANUP] MLflow imported successfully. Current env: local, run_key_hash: cc90dfbe1603...
2026-01-15 00:05:57,459 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Retrieved experiment: 1
2026-01-15 00:05:57 INFO  [training.hpo.tracking.cleanup] [CLEANUP] Retrieved experiment: 1
2026-01-15 00:05:57,459 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Fetching all runs in experiment (may paginate for large experiments)...
2026-01-15 00:05:57 INFO  [training.hpo.tracking.cleanup] [CLEANUP] Fetching all runs in experiment (may paginate for large experiments)...
2026-01-15 00:05:57,523 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Fetched 153 total runs from experiment
2026-01-15 00:05:57 INFO  [training.hpo.tracking.cleanup] [CLEANUP] Fetched 153 total runs from experiment
2026-01-15 00:05:57,523 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Built parentchildren map: 15 parents have children
2026-01-15 00:05:57 INFO  [training.hpo.tracking.cleanup] [CLEANUP] Built parentchildren map: 15 parents have children
2026-01-15 00:05:57,588 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Status breakdown: {'RUNNING': 33, 'FAILED': 32, 'FINISHED': 88}
2026-01-15 00:05:57 INFO  [training.hpo.tracking.cleanup] [CLEANUP] Status breakdown: {'RUNNING': 33, 'FAILED': 32, 'FINISHED': 88}
2026-01-15 00:05:57,589 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Found 0 tag-based matches, 0 name-fallback matches (legacy), 0 total eligible for tagging
2026-01-15 00:05:57 INFO  [training.hpo.tracking.cleanup] [CLEANUP] Found 0 tag-based matches, 0 name-fallback matches (legacy), 0 total eligible for tagging
2026-01-15 00:05:57,589 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Found 0 orphaned child runs (RUNNING children with terminal parents)
2026-01-15 00:05:57 INFO  [training.hpo.tracking.cleanup] [CLEANUP] Found 0 orphaned child runs (RUNNING children with terminal parents)
2026-01-15 00:05:57,589 - training.hpo.tracking.cleanup - INFO - [CLEANUP] No interrupted parent runs found to tag
2026-01-15 00:05:57 INFO  [training.hpo.tracking.cleanup] [CLEANUP] No interrupted parent runs found to tag
2026-01-15 00:05:57,589 - training.hpo.tracking.cleanup - INFO - [CLEANUP] No orphaned child runs found to tag
2026-01-15 00:05:57 INFO  [training.hpo.tracking.cleanup] [CLEANUP] No orphaned child runs found to tag

  0%|          | 0/1 [00:00<?, ?it/s]2026-01-15 00:05:57,635 - training.hpo.execution.local.sweep - INFO - [Trial 0] fold_splits=present, using CV path
2026-01-15 00:05:57 INFO  [training.hpo.execution.local.sweep] [Trial 0] fold_splits=present, using CV path
2026-01-15 00:05:57,635 - training.hpo.execution.local.sweep - INFO - [Trial 0] Running 2-fold CV
2026-01-15 00:05:57 INFO  [training.hpo.execution.local.sweep] [Trial 0] Running 2-fold CV
2026-01-15 00:05:57,653 - training.hpo.execution.local.cv - INFO - [CV] Created trial folder: /tmp/pytest-of-codespace/pytest-28/test_full_hpo_workflow_with_cv0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-329f14ab (trial 0)
2026-01-15 00:05:57 INFO  [training.hpo.execution.local.cv] [CV] Created trial folder: /tmp/pytest-of-codespace/pytest-28/test_full_hpo_workflow_with_cv0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-329f14ab (trial 0)
2026-01-15 00:05:57,656 - training.hpo.execution.local.trial - INFO - Training completed
2026-01-15 00:05:57 INFO  [training.hpo.execution.local.trial] Training completed
2026-01-15 00:05:57,656 - training.hpo.execution.local.trial - INFO - [TRIAL] Training completed. Objective metric 'macro-f1': 0.75
2026-01-15 00:05:57 INFO  [training.hpo.execution.local.trial] [TRIAL] Training completed. Objective metric 'macro-f1': 0.75
2026-01-15 00:05:57,659 - training.hpo.execution.local.trial - INFO - Training completed
2026-01-15 00:05:57 INFO  [training.hpo.execution.local.trial] Training completed
2026-01-15 00:05:57,659 - training.hpo.execution.local.trial - INFO - [TRIAL] Training completed. Objective metric 'macro-f1': 0.75
2026-01-15 00:05:57 INFO  [training.hpo.execution.local.trial] [TRIAL] Training completed. Objective metric 'macro-f1': 0.75
2026-01-15 00:05:57,720 - infrastructure.tracking.mlflow.lifecycle - INFO - Successfully terminated run d93f41a3c99f... with status FINISHED
2026-01-15 00:05:57 INFO  [infrastructure.tracking.mlflow.lifecycle] Successfully terminated run d93f41a3c99f... with status FINISHED
2026-01-15 00:05:57,727 - training.hpo.trial.metrics - WARNING - Could not find v2 trial folder for trial 0 in v2 study folder study-01b6aaaa. Skipping metrics storage in trial attributes. Available folders: N/A
2026-01-15 00:05:57 WARNI [training.hpo.trial.metrics] Could not find v2 trial folder for trial 0 in v2 study folder study-01b6aaaa. Skipping metrics storage in trial attributes. Available folders: N/A
2026-01-15 00:05:57,754 - training.hpo.trial.callback - INFO - 
2026-01-15 00:05:57 INFO  [training.hpo.trial.callback] 
2026-01-15 00:05:57,754 - training.hpo.trial.callback - INFO - [BEST]: trial_0
2026-01-15 00:05:57 INFO  [training.hpo.trial.callback] [BEST]: trial_0
2026-01-15 00:05:57,754 - training.hpo.trial.callback - INFO -   Metrics: macro-f1=0.750000
2026-01-15 00:05:57 INFO  [training.hpo.trial.callback]   Metrics: macro-f1=0.750000
2026-01-15 00:05:57,754 - training.hpo.trial.callback - INFO -   Params: learning_rate=1.63e-05 | batch_size=4 | dropout=0.235039 | weight_decay=0.001692
2026-01-15 00:05:57 INFO  [training.hpo.trial.callback]   Params: learning_rate=1.63e-05 | batch_size=4 | dropout=0.235039 | weight_decay=0.001692

Best trial: 0. Best value: 0.75:   0%|          | 0/1 [00:00<?, ?it/s]
Best trial: 0. Best value: 0.75: 100%|| 1/1 [00:00<00:00,  6.02it/s]
Best trial: 0. Best value: 0.75: 100%|| 1/1 [00:00<00:00,  6.02it/s, 0.16/1200 seconds]
Best trial: 0. Best value: 0.75: 100%|| 1/1 [00:00<00:00,  6.01it/s, 0.16/1200 seconds]
2026-01-15 00:05:57,760 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=dbe259b90ab7...
2026-01-15 00:05:57 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=dbe259b90ab7...
2026-01-15 00:05:57,766 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Found 1 completed trials out of 1 total
2026-01-15 00:05:57 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Found 1 completed trials out of 1 total
2026-01-15 00:05:57,768 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging metrics: n_trials=1, n_completed_trials=1
2026-01-15 00:05:57 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Logging metrics: n_trials=1, n_completed_trials=1
2026-01-15 00:05:57,786 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=0.75
2026-01-15 00:05:57 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=0.75
2026-01-15 00:05:57,795 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging best hyperparameters: {'learning_rate': 1.628556427398078e-05, 'batch_size': 4, 'dropout': 0.2350392691346863, 'weight_decay': 0.001691803470376826}
2026-01-15 00:05:57 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Logging best hyperparameters: {'learning_rate': 1.628556427398078e-05, 'batch_size': 4, 'dropout': 0.2350392691346863, 'weight_decay': 0.001691803470376826}
2026-01-15 00:05:57,817 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Finding and logging best trial run ID...
2026-01-15 00:05:57 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Finding and logging best trial run ID...
2026-01-15 00:05:57,826 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Could not find MLflow run ID for best trial 0. Found 0 runs with matching study_key_hash and trial_number, but none belong to parent dbe259b90ab7... This may be a timing issue - trial runs may not be created/committed yet.
2026-01-15 00:05:57 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker] Could not find MLflow run ID for best trial 0. Found 0 runs with matching study_key_hash and trial_number, but none belong to parent dbe259b90ab7... This may be a timing issue - trial runs may not be created/committed yet.
2026-01-15 00:05:57,826 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Completed best trial ID logging
2026-01-15 00:05:57 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Completed best trial ID logging
2026-01-15 00:05:57,826 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Checkpoint logging enabled: False (upload_checkpoint=False)
2026-01-15 00:05:57 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Checkpoint logging enabled: False (upload_checkpoint=False)
2026-01-15 00:05:57,826 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
2026-01-15 00:05:57 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
2026-01-15 00:05:57,832 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Completed log_final_metrics successfully
2026-01-15 00:05:57 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Completed log_final_metrics successfully
2026-01-15 00:05:57,834 - training.hpo.execution.local.sweep - INFO - [REFIT] Starting refit training for best trial 0
2026-01-15 00:05:57 INFO  [training.hpo.execution.local.sweep] [REFIT] Starting refit training for best trial 0
2026-01-15 00:05:57,849 - training.hpo.execution.local.sweep - WARNING - [REFIT] Could not determine trial_key_hash from best trial run tags. Refit run may not match trial run.
2026-01-15 00:05:57 WARNI [training.hpo.execution.local.sweep] [REFIT] Could not determine trial_key_hash from best trial run tags. Refit run may not match trial run.
2026-01-15 00:05:57,850 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 1.628556427398078e-05, 'batch_size': 4, 'dropout': 0.2350392691346863, 'weight_decay': 0.001691803470376826}
2026-01-15 00:05:57 INFO  [training.hpo.execution.local.refit] [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 1.628556427398078e-05, 'batch_size': 4, 'dropout': 0.2350392691346863, 'weight_decay': 0.001691803470376826}
2026-01-15 00:05:57,851 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id='trial_0_20260115_000556', run_id='20260115_000556', trial_number=0
2026-01-15 00:05:57 INFO  [training.hpo.execution.local.refit] [REFIT] Computed trial_id='trial_0_20260115_000556', run_id='20260115_000556', trial_number=0
2026-01-15 00:05:57,851 - training.hpo.execution.local.refit - WARNING - [REFIT] Computed trial_key_hash=dbb36d6d5bb606c2... from trial parameters (fallback - may not match trial run hash). Trial run tags should be used as SSOT.
2026-01-15 00:05:57 WARNI [training.hpo.execution.local.refit] [REFIT] Computed trial_key_hash=dbb36d6d5bb606c2... from trial parameters (fallback - may not match trial run hash). Trial run tags should be used as SSOT.
2026-01-15 00:05:57,881 - training.execution.mlflow_setup - INFO -  View run local_distilbert_hpo_refit_legacy at: sqlite:////workspaces/resume-ner-azureml/mlruns/#/experiments/1/runs/2d553a1e9c0c46ffb6a72b0ac28fe45d
2026-01-15 00:05:57 INFO  [training.execution.mlflow_setup]  View run local_distilbert_hpo_refit_legacy at: sqlite:////workspaces/resume-ner-azureml/mlruns/#/experiments/1/runs/2d553a1e9c0c46ffb6a72b0ac28fe45d
2026-01-15 00:05:57,881 - training.execution.mlflow_setup - INFO - Created MLflow run: local_distilbert_hpo_refit_legacy (2d553a1e9c0c...)
2026-01-15 00:05:57 INFO  [training.execution.mlflow_setup] Created MLflow run: local_distilbert_hpo_refit_legacy (2d553a1e9c0c...)
2026-01-15 00:05:57,881 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run 2d553a1e9c0c... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:05:57 INFO  [training.hpo.execution.local.refit] [REFIT] Attempting to link refit run 2d553a1e9c0c... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:05:57,900 - training.hpo.execution.local.refit - WARNING - [REFIT]  Could not find trial run for trial_key_hash=dbb36d6d... - refit run will not be linked to trial run. Checkpoint acquisition may fail or use fallback search.
2026-01-15 00:05:57 WARNI [training.hpo.execution.local.refit] [REFIT]  Could not find trial run for trial_key_hash=dbb36d6d... - refit run will not be linked to trial run. Checkpoint acquisition may fail or use fallback search.
2026-01-15 00:05:57,914 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run 2d553a1e9c0c... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:05:57 INFO  [training.hpo.execution.local.refit] [REFIT] Attempting to link refit run 2d553a1e9c0c... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:05:57,931 - training.hpo.execution.local.refit - WARNING - [REFIT]  Could not find trial run for trial_key_hash=dbb36d6d... - refit run will not be linked to trial run. Checkpoint acquisition may fail or use fallback search.
2026-01-15 00:05:57 WARNI [training.hpo.execution.local.refit] [REFIT]  Could not find trial run for trial_key_hash=dbb36d6d... - refit run will not be linked to trial run. Checkpoint acquisition may fail or use fallback search.
2026-01-15 00:05:57,949 - training.hpo.execution.local.refit - INFO - [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-15 00:05:57 INFO  [training.hpo.execution.local.refit] [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-15 00:05:57,949 - training.hpo.execution.local.refit - INFO - [REFIT] Refit training completed. Metrics: 0.8, Checkpoint: /tmp/pytest-of-codespace/pytest-28/test_full_hpo_workflow_with_cv0/outputs/hpo/local/distilbert/study-01b6aaaa/trial-dbb36d6d/refit/checkpoint
2026-01-15 00:05:57 INFO  [training.hpo.execution.local.refit] [REFIT] Refit training completed. Metrics: 0.8, Checkpoint: /tmp/pytest-of-codespace/pytest-28/test_full_hpo_workflow_with_cv0/outputs/hpo/local/distilbert/study-01b6aaaa/trial-dbb36d6d/refit/checkpoint
2026-01-15 00:05:57,949 - training.hpo.execution.local.sweep - INFO - [REFIT] Refit training completed. Metrics: {'macro-f1': 0.8}, Checkpoint: /tmp/pytest-of-codespace/pytest-28/test_full_hpo_workflow_with_cv0/outputs/hpo/local/distilbert/study-01b6aaaa/trial-dbb36d6d/refit/checkpoint, Run ID: 2d553a1e9c0c...
2026-01-15 00:05:57 INFO  [training.hpo.execution.local.sweep] [REFIT] Refit training completed. Metrics: {'macro-f1': 0.8}, Checkpoint: /tmp/pytest-of-codespace/pytest-28/test_full_hpo_workflow_with_cv0/outputs/hpo/local/distilbert/study-01b6aaaa/trial-dbb36d6d/refit/checkpoint, Run ID: 2d553a1e9c0c...
2026-01-15 00:05:57,976 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_BEST_CHECKPOINT] Falling back to standard checkpoint search
2026-01-15 00:05:57 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_BEST_CHECKPOINT] Falling back to standard checkpoint search
2026-01-15 00:05:57,979 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Best trial checkpoint not found for trial 0. Searched in: /tmp/pytest-of-codespace/pytest-28/test_full_hpo_workflow_with_cv0/outputs/hpo/local/distilbert. Skipping MLflow checkpoint logging.
2026-01-15 00:05:57 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker] Best trial checkpoint not found for trial 0. Searched in: /tmp/pytest-of-codespace/pytest-28/test_full_hpo_workflow_with_cv0/outputs/hpo/local/distilbert. Skipping MLflow checkpoint logging.
2026-01-15 00:05:57,996 - infrastructure.tracking.mlflow.lifecycle - INFO - Successfully terminated run 2d553a1e9c0c... with status FINISHED
2026-01-15 00:05:57 INFO  [infrastructure.tracking.mlflow.lifecycle] Successfully terminated run 2d553a1e9c0c... with status FINISHED
2026-01-15 00:05:57,996 - training.hpo.execution.local.sweep - INFO - [REFIT]  Artifacts uploaded and run marked as FINISHED: 2d553a1e9c0c...
2026-01-15 00:05:57 INFO  [training.hpo.execution.local.sweep] [REFIT]  Artifacts uploaded and run marked as FINISHED: 2d553a1e9c0c...
2026-01-15 00:05:57,996 - training.hpo.checkpoint.cleanup - INFO - Final cleanup: kept checkpoints for best trial 0 (metric=0.750000, CV=no, refit=no), deleted 0 non-best checkpoints
2026-01-15 00:05:57 INFO  [training.hpo.checkpoint.cleanup] Final cleanup: kept checkpoints for best trial 0 (metric=0.750000, CV=no, refit=no), deleted 0 non-best checkpoints
2026-01-15 00:05:57,996 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Context manager exiting normally. run_id=dbe259b90ab7...
2026-01-15 00:05:57 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Context manager exiting normally. run_id=dbe259b90ab7...
2026-01-15 00:05:58,007 - training.hpo.execution.local.sweep - INFO - [EARLY HASH] Computed v2 study_key_hash=957b9a1438354884... for folder creation
2026-01-15 00:05:58 INFO  [training.hpo.execution.local.sweep] [EARLY HASH] Computed v2 study_key_hash=957b9a1438354884... for folder creation
2026-01-15 00:05:58 WARNI [infrastructure.naming.display_policy] [naming.yaml] Missing required 'run_names' section (/tmp/pytest-of-codespace/pytest-28/test_full_hpo_workflow_no_cv_n0/config/naming.yaml)
2026-01-15 00:05:58,009 - training.hpo.execution.local.sweep - INFO - [HPO Setup] k_folds=None, fold_splits_file=None, k_fold config: {'enabled': False}
2026-01-15 00:05:58 INFO  [training.hpo.execution.local.sweep] [HPO Setup] k_folds=None, fold_splits_file=None, k_fold config: {'enabled': False}
2026-01-15 00:05:58,011 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Using LOCAL MLflow tracking (not Azure ML)
2026-01-15 00:05:58 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker] Using LOCAL MLflow tracking (not Azure ML)
2026-01-15 00:05:58,011 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
2026-01-15 00:05:58,023 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=957b9a1438354884...
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=957b9a1438354884...
2026-01-15 00:05:58,023 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=957b9a1438354884..., study_family_hash=23208e5829deb236...
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=957b9a1438354884..., study_family_hash=23208e5829deb236...
2026-01-15 00:05:58,103 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Yielding RunHandle. run_id=5ed3e1062659...
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Yielding RunHandle. run_id=5ed3e1062659...
2026-01-15 00:05:58,111 - training.hpo.execution.local.sweep - INFO - [HPO] Parent run 5ed3e1062659... is now visible in MLflow (status: RUNNING)
2026-01-15 00:05:58 INFO  [training.hpo.execution.local.sweep] [HPO] Parent run 5ed3e1062659... is now visible in MLflow (status: RUNNING)
2026-01-15 00:05:58,118 - training.hpo.execution.local.sweep - INFO - Setting Phase 2 tags on parent run 5ed3e1062659... (data_config=present, train_config=present)
2026-01-15 00:05:58 INFO  [training.hpo.execution.local.sweep] Setting Phase 2 tags on parent run 5ed3e1062659... (data_config=present, train_config=present)
2026-01-15 00:05:58,121 - training.hpo.execution.local.sweep - WARNING - Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
2026-01-15 00:05:58 WARNI [training.hpo.execution.local.sweep] Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
2026-01-15 00:05:58,121 - training.hpo.execution.local.sweep - WARNING - Could not set data fingerprint tag: 'Missing tag key: fingerprint.data'
2026-01-15 00:05:58 WARNI [training.hpo.execution.local.sweep] Could not set data fingerprint tag: 'Missing tag key: fingerprint.data'
2026-01-15 00:05:58,121 - training.hpo.execution.local.sweep - WARNING - Could not set eval fingerprint tag: 'Missing tag key: fingerprint.eval'
2026-01-15 00:05:58 WARNI [training.hpo.execution.local.sweep] Could not set eval fingerprint tag: 'Missing tag key: fingerprint.eval'
2026-01-15 00:05:58,121 - training.hpo.execution.local.sweep - WARNING - Could not set objective direction tag: 'Missing tag key: objective.direction'
2026-01-15 00:05:58 WARNI [training.hpo.execution.local.sweep] Could not set objective direction tag: 'Missing tag key: objective.direction'
2026-01-15 00:05:58,121 - training.hpo.execution.local.sweep - WARNING - Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
2026-01-15 00:05:58 WARNI [training.hpo.execution.local.sweep] Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
2026-01-15 00:05:58,121 - training.hpo.execution.local.sweep - WARNING - Could not set artifact.available tag: 'Missing tag key: artifact.available'
2026-01-15 00:05:58 WARNI [training.hpo.execution.local.sweep] Could not set artifact.available tag: 'Missing tag key: artifact.available'
2026-01-15 00:05:58,121 - training.hpo.execution.local.sweep - INFO -  Set Phase 2 tags on parent run 5ed3e1062659...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-15 00:05:58 INFO  [training.hpo.execution.local.sweep]  Set Phase 2 tags on parent run 5ed3e1062659...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-15 00:05:58,121 - training.hpo.execution.local.sweep - INFO -  Successfully completed Phase 2 tag setting for parent run 5ed3e1062659...
2026-01-15 00:05:58 INFO  [training.hpo.execution.local.sweep]  Successfully completed Phase 2 tag setting for parent run 5ed3e1062659...
2026-01-15 00:05:58,121 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.
2026-01-15 00:05:58 INFO  [training.hpo.tracking.cleanup] [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.

  0%|          | 0/1 [00:00<?, ?it/s]2026-01-15 00:05:58,122 - training.hpo.execution.local.sweep - INFO - [Trial 0] fold_splits=None, using non-CV path
2026-01-15 00:05:58 INFO  [training.hpo.execution.local.sweep] [Trial 0] fold_splits=None, using non-CV path
2026-01-15 00:05:58,134 - training.hpo.tracking.runs - INFO - [TRIAL_RUN_NO_CV] Created trial run (no CV): a056ee274203... (trial 0). Run remains RUNNING until training completes.
2026-01-15 00:05:58 INFO  [training.hpo.tracking.runs] [TRIAL_RUN_NO_CV] Created trial run (no CV): a056ee274203... (trial 0). Run remains RUNNING until training completes.
2026-01-15 00:05:58,137 - training.hpo.execution.local.trial - INFO - Training completed
2026-01-15 00:05:58 INFO  [training.hpo.execution.local.trial] Training completed
2026-01-15 00:05:58,137 - training.hpo.execution.local.trial - INFO - [TRIAL] Training completed. Objective metric 'macro-f1': 0.7
2026-01-15 00:05:58 INFO  [training.hpo.execution.local.trial] [TRIAL] Training completed. Objective metric 'macro-f1': 0.7
2026-01-15 00:05:58,137 - training.hpo.tracking.runs - INFO - [TRIAL_RUN_NO_CV] Training completed. Marking trial run a056ee274203... as FINISHED (trial 0)
2026-01-15 00:05:58 INFO  [training.hpo.tracking.runs] [TRIAL_RUN_NO_CV] Training completed. Marking trial run a056ee274203... as FINISHED (trial 0)
2026-01-15 00:05:58,147 - infrastructure.tracking.mlflow.lifecycle - INFO - Successfully terminated run a056ee274203... with status FINISHED
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.lifecycle] Successfully terminated run a056ee274203... with status FINISHED
2026-01-15 00:05:58,147 - training.hpo.tracking.runs - INFO - [TRIAL_RUN_NO_CV] Successfully marked trial run a056ee274203... as FINISHED
2026-01-15 00:05:58 INFO  [training.hpo.tracking.runs] [TRIAL_RUN_NO_CV] Successfully marked trial run a056ee274203... as FINISHED
2026-01-15 00:05:58,151 - training.hpo.trial.callback - INFO - 
2026-01-15 00:05:58 INFO  [training.hpo.trial.callback] 
2026-01-15 00:05:58,151 - training.hpo.trial.callback - INFO - [BEST]: trial_0
2026-01-15 00:05:58 INFO  [training.hpo.trial.callback] [BEST]: trial_0
2026-01-15 00:05:58,152 - training.hpo.trial.callback - INFO -   Metrics: macro-f1=0.700000
2026-01-15 00:05:58 INFO  [training.hpo.trial.callback]   Metrics: macro-f1=0.700000
2026-01-15 00:05:58,152 - training.hpo.trial.callback - INFO -   Params: learning_rate=4.61e-05 | batch_size=4
2026-01-15 00:05:58 INFO  [training.hpo.trial.callback]   Params: learning_rate=4.61e-05 | batch_size=4

Best trial: 0. Best value: 0.7:   0%|          | 0/1 [00:00<?, ?it/s]
Best trial: 0. Best value: 0.7: 100%|| 1/1 [00:00<00:00, 32.68it/s, 0.03/1200 seconds]
Best trial: 0. Best value: 0.7: 100%|| 1/1 [00:00<00:00, 32.58it/s, 0.03/1200 seconds]
2026-01-15 00:05:58,152 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=5ed3e1062659...
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=5ed3e1062659...
2026-01-15 00:05:58,152 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Found 1 completed trials out of 1 total
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Found 1 completed trials out of 1 total
2026-01-15 00:05:58,152 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging metrics: n_trials=1, n_completed_trials=1
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Logging metrics: n_trials=1, n_completed_trials=1
2026-01-15 00:05:58,167 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=0.7
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=0.7
2026-01-15 00:05:58,174 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging best hyperparameters: {'learning_rate': 4.606146676803637e-05, 'batch_size': 4}
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Logging best hyperparameters: {'learning_rate': 4.606146676803637e-05, 'batch_size': 4}
2026-01-15 00:05:58,187 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Finding and logging best trial run ID...
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Finding and logging best trial run ID...
2026-01-15 00:05:58,199 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Could not find MLflow run ID for best trial 0. Found 0 runs with matching study_key_hash and trial_number, but none belong to parent 5ed3e1062659... This may be a timing issue - trial runs may not be created/committed yet.
2026-01-15 00:05:58 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker] Could not find MLflow run ID for best trial 0. Found 0 runs with matching study_key_hash and trial_number, but none belong to parent 5ed3e1062659... This may be a timing issue - trial runs may not be created/committed yet.
2026-01-15 00:05:58,199 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Completed best trial ID logging
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Completed best trial ID logging
2026-01-15 00:05:58,199 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Checkpoint logging enabled: False (upload_checkpoint=False)
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Checkpoint logging enabled: False (upload_checkpoint=False)
2026-01-15 00:05:58,199 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
2026-01-15 00:05:58,199 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Completed log_final_metrics successfully
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Completed log_final_metrics successfully
2026-01-15 00:05:58,199 - training.hpo.execution.local.sweep - INFO - [REFIT] Refit training is disabled in config
2026-01-15 00:05:58 INFO  [training.hpo.execution.local.sweep] [REFIT] Refit training is disabled in config
2026-01-15 00:05:58,199 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Context manager exiting normally. run_id=5ed3e1062659...
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Context manager exiting normally. run_id=5ed3e1062659...
2026-01-15 00:05:58,214 - training.hpo.execution.local.sweep - INFO - [EARLY HASH] Computed v2 study_key_hash=dc0c421a5056294a... for folder creation
2026-01-15 00:05:58 INFO  [training.hpo.execution.local.sweep] [EARLY HASH] Computed v2 study_key_hash=dc0c421a5056294a... for folder creation
2026-01-15 00:05:58 WARNI [infrastructure.naming.display_policy] [naming.yaml] Missing required 'run_names' section (/tmp/pytest-of-codespace/pytest-28/test_full_hpo_workflow_creates0/config/naming.yaml)
2026-01-15 00:05:58,217 - training.hpo.execution.local.sweep - INFO - [HPO Setup] k_folds=None, fold_splits_file=None, k_fold config: {'enabled': True, 'n_splits': 2, 'random_seed': 42, 'shuffle': True, 'stratified': True}
2026-01-15 00:05:58 INFO  [training.hpo.execution.local.sweep] [HPO Setup] k_folds=None, fold_splits_file=None, k_fold config: {'enabled': True, 'n_splits': 2, 'random_seed': 42, 'shuffle': True, 'stratified': True}
2026-01-15 00:05:58,220 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Using LOCAL MLflow tracking (not Azure ML)
2026-01-15 00:05:58 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker] Using LOCAL MLflow tracking (not Azure ML)
2026-01-15 00:05:58,221 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
2026-01-15 00:05:58,239 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=dc0c421a5056294a...
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=dc0c421a5056294a...
2026-01-15 00:05:58,239 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=dc0c421a5056294a..., study_family_hash=d1aa8bfffdcfdb77...
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=dc0c421a5056294a..., study_family_hash=d1aa8bfffdcfdb77...
2026-01-15 00:05:58,309 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Yielding RunHandle. run_id=ccabe8cdbbc0...
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Yielding RunHandle. run_id=ccabe8cdbbc0...
2026-01-15 00:05:58,318 - training.hpo.execution.local.sweep - INFO - [HPO] Parent run ccabe8cdbbc0... is now visible in MLflow (status: RUNNING)
2026-01-15 00:05:58 INFO  [training.hpo.execution.local.sweep] [HPO] Parent run ccabe8cdbbc0... is now visible in MLflow (status: RUNNING)
2026-01-15 00:05:58,324 - training.hpo.execution.local.sweep - INFO - Setting Phase 2 tags on parent run ccabe8cdbbc0... (data_config=present, train_config=present)
2026-01-15 00:05:58 INFO  [training.hpo.execution.local.sweep] Setting Phase 2 tags on parent run ccabe8cdbbc0... (data_config=present, train_config=present)
2026-01-15 00:05:58,326 - training.hpo.execution.local.sweep - WARNING - Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
2026-01-15 00:05:58 WARNI [training.hpo.execution.local.sweep] Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
2026-01-15 00:05:58,326 - training.hpo.execution.local.sweep - WARNING - Could not set data fingerprint tag: 'Missing tag key: fingerprint.data'
2026-01-15 00:05:58 WARNI [training.hpo.execution.local.sweep] Could not set data fingerprint tag: 'Missing tag key: fingerprint.data'
2026-01-15 00:05:58,326 - training.hpo.execution.local.sweep - WARNING - Could not set eval fingerprint tag: 'Missing tag key: fingerprint.eval'
2026-01-15 00:05:58 WARNI [training.hpo.execution.local.sweep] Could not set eval fingerprint tag: 'Missing tag key: fingerprint.eval'
2026-01-15 00:05:58,326 - training.hpo.execution.local.sweep - WARNING - Could not set objective direction tag: 'Missing tag key: objective.direction'
2026-01-15 00:05:58 WARNI [training.hpo.execution.local.sweep] Could not set objective direction tag: 'Missing tag key: objective.direction'
2026-01-15 00:05:58,326 - training.hpo.execution.local.sweep - WARNING - Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
2026-01-15 00:05:58 WARNI [training.hpo.execution.local.sweep] Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
2026-01-15 00:05:58,326 - training.hpo.execution.local.sweep - WARNING - Could not set artifact.available tag: 'Missing tag key: artifact.available'
2026-01-15 00:05:58 WARNI [training.hpo.execution.local.sweep] Could not set artifact.available tag: 'Missing tag key: artifact.available'
2026-01-15 00:05:58,326 - training.hpo.execution.local.sweep - INFO -  Set Phase 2 tags on parent run ccabe8cdbbc0...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-15 00:05:58 INFO  [training.hpo.execution.local.sweep]  Set Phase 2 tags on parent run ccabe8cdbbc0...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-15 00:05:58,326 - training.hpo.execution.local.sweep - INFO -  Successfully completed Phase 2 tag setting for parent run ccabe8cdbbc0...
2026-01-15 00:05:58 INFO  [training.hpo.execution.local.sweep]  Successfully completed Phase 2 tag setting for parent run ccabe8cdbbc0...
2026-01-15 00:05:58,326 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.
2026-01-15 00:05:58 INFO  [training.hpo.tracking.cleanup] [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.

  0%|          | 0/1 [00:00<?, ?it/s]2026-01-15 00:05:58,327 - training.hpo.execution.local.sweep - INFO - [Trial 0] fold_splits=None, using non-CV path
2026-01-15 00:05:58 INFO  [training.hpo.execution.local.sweep] [Trial 0] fold_splits=None, using non-CV path
2026-01-15 00:05:58,339 - training.hpo.tracking.runs - INFO - [TRIAL_RUN_NO_CV] Created trial run (no CV): 8297b142de4b... (trial 0). Run remains RUNNING until training completes.
2026-01-15 00:05:58 INFO  [training.hpo.tracking.runs] [TRIAL_RUN_NO_CV] Created trial run (no CV): 8297b142de4b... (trial 0). Run remains RUNNING until training completes.
2026-01-15 00:05:58,342 - training.hpo.execution.local.trial - INFO - [TRIAL] Training completed. Objective metric 'macro-f1': 0.75
2026-01-15 00:05:58 INFO  [training.hpo.execution.local.trial] [TRIAL] Training completed. Objective metric 'macro-f1': 0.75
2026-01-15 00:05:58,342 - training.hpo.tracking.runs - INFO - [TRIAL_RUN_NO_CV] Training completed. Marking trial run 8297b142de4b... as FINISHED (trial 0)
2026-01-15 00:05:58 INFO  [training.hpo.tracking.runs] [TRIAL_RUN_NO_CV] Training completed. Marking trial run 8297b142de4b... as FINISHED (trial 0)
2026-01-15 00:05:58,353 - infrastructure.tracking.mlflow.lifecycle - INFO - Successfully terminated run 8297b142de4b... with status FINISHED
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.lifecycle] Successfully terminated run 8297b142de4b... with status FINISHED
2026-01-15 00:05:58,354 - training.hpo.tracking.runs - INFO - [TRIAL_RUN_NO_CV] Successfully marked trial run 8297b142de4b... as FINISHED
2026-01-15 00:05:58 INFO  [training.hpo.tracking.runs] [TRIAL_RUN_NO_CV] Successfully marked trial run 8297b142de4b... as FINISHED
2026-01-15 00:05:58,358 - training.hpo.trial.callback - INFO - 
2026-01-15 00:05:58 INFO  [training.hpo.trial.callback] 
2026-01-15 00:05:58,358 - training.hpo.trial.callback - INFO - [BEST]: trial_0
2026-01-15 00:05:58 INFO  [training.hpo.trial.callback] [BEST]: trial_0
2026-01-15 00:05:58,358 - training.hpo.trial.callback - INFO -   Metrics: macro-f1=0.750000
2026-01-15 00:05:58 INFO  [training.hpo.trial.callback]   Metrics: macro-f1=0.750000
2026-01-15 00:05:58,358 - training.hpo.trial.callback - INFO -   Params: learning_rate=4.26e-05
2026-01-15 00:05:58 INFO  [training.hpo.trial.callback]   Params: learning_rate=4.26e-05

Best trial: 0. Best value: 0.75:   0%|          | 0/1 [00:00<?, ?it/s]
Best trial: 0. Best value: 0.75: 100%|| 1/1 [00:00<00:00, 31.63it/s, 0.03/1200 seconds]
Best trial: 0. Best value: 0.75: 100%|| 1/1 [00:00<00:00, 31.54it/s, 0.03/1200 seconds]
2026-01-15 00:05:58,358 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=ccabe8cdbbc0...
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=ccabe8cdbbc0...
2026-01-15 00:05:58,358 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Found 1 completed trials out of 1 total
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Found 1 completed trials out of 1 total
2026-01-15 00:05:58,359 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging metrics: n_trials=1, n_completed_trials=1
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Logging metrics: n_trials=1, n_completed_trials=1
2026-01-15 00:05:58,372 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=0.75
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=0.75
2026-01-15 00:05:58,380 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging best hyperparameters: {'learning_rate': 4.264636862709108e-05}
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Logging best hyperparameters: {'learning_rate': 4.264636862709108e-05}
2026-01-15 00:05:58,385 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Finding and logging best trial run ID...
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Finding and logging best trial run ID...
2026-01-15 00:05:58,394 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Could not find MLflow run ID for best trial 0. Found 0 runs with matching study_key_hash and trial_number, but none belong to parent ccabe8cdbbc0... This may be a timing issue - trial runs may not be created/committed yet.
2026-01-15 00:05:58 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker] Could not find MLflow run ID for best trial 0. Found 0 runs with matching study_key_hash and trial_number, but none belong to parent ccabe8cdbbc0... This may be a timing issue - trial runs may not be created/committed yet.
2026-01-15 00:05:58,394 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Completed best trial ID logging
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Completed best trial ID logging
2026-01-15 00:05:58,394 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Checkpoint logging enabled: False (upload_checkpoint=False)
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Checkpoint logging enabled: False (upload_checkpoint=False)
2026-01-15 00:05:58,394 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
2026-01-15 00:05:58,399 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Completed log_final_metrics successfully
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Completed log_final_metrics successfully
2026-01-15 00:05:58,400 - training.hpo.execution.local.sweep - INFO - [REFIT] Starting refit training for best trial 0
2026-01-15 00:05:58 INFO  [training.hpo.execution.local.sweep] [REFIT] Starting refit training for best trial 0
2026-01-15 00:05:58,414 - training.hpo.execution.local.sweep - WARNING - [REFIT] Could not determine trial_key_hash from best trial run tags. Refit run may not match trial run.
2026-01-15 00:05:58 WARNI [training.hpo.execution.local.sweep] [REFIT] Could not determine trial_key_hash from best trial run tags. Refit run may not match trial run.
2026-01-15 00:05:58,414 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 4.264636862709108e-05}
2026-01-15 00:05:58 INFO  [training.hpo.execution.local.refit] [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 4.264636862709108e-05}
2026-01-15 00:05:58,415 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id='trial_0_20260115_000558', run_id='20260115_000558', trial_number=0
2026-01-15 00:05:58 INFO  [training.hpo.execution.local.refit] [REFIT] Computed trial_id='trial_0_20260115_000558', run_id='20260115_000558', trial_number=0
2026-01-15 00:05:58,415 - training.hpo.execution.local.refit - WARNING - [REFIT] Computed trial_key_hash=8cf262e4857fdadc... from trial parameters (fallback - may not match trial run hash). Trial run tags should be used as SSOT.
2026-01-15 00:05:58 WARNI [training.hpo.execution.local.refit] [REFIT] Computed trial_key_hash=8cf262e4857fdadc... from trial parameters (fallback - may not match trial run hash). Trial run tags should be used as SSOT.
2026-01-15 00:05:58,441 - training.execution.mlflow_setup - INFO -  View run local_distilbert_hpo_refit_legacy at: sqlite:////workspaces/resume-ner-azureml/mlruns/#/experiments/1/runs/b1df9256bb614529b49a52502c68118a
2026-01-15 00:05:58 INFO  [training.execution.mlflow_setup]  View run local_distilbert_hpo_refit_legacy at: sqlite:////workspaces/resume-ner-azureml/mlruns/#/experiments/1/runs/b1df9256bb614529b49a52502c68118a
2026-01-15 00:05:58,441 - training.execution.mlflow_setup - INFO - Created MLflow run: local_distilbert_hpo_refit_legacy (b1df9256bb61...)
2026-01-15 00:05:58 INFO  [training.execution.mlflow_setup] Created MLflow run: local_distilbert_hpo_refit_legacy (b1df9256bb61...)
2026-01-15 00:05:58,441 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run b1df9256bb61... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:05:58 INFO  [training.hpo.execution.local.refit] [REFIT] Attempting to link refit run b1df9256bb61... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:05:58,458 - training.hpo.execution.local.refit - WARNING - [REFIT]  Could not find trial run for trial_key_hash=8cf262e4... - refit run will not be linked to trial run. Checkpoint acquisition may fail or use fallback search.
2026-01-15 00:05:58 WARNI [training.hpo.execution.local.refit] [REFIT]  Could not find trial run for trial_key_hash=8cf262e4... - refit run will not be linked to trial run. Checkpoint acquisition may fail or use fallback search.
2026-01-15 00:05:58,474 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run b1df9256bb61... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:05:58 INFO  [training.hpo.execution.local.refit] [REFIT] Attempting to link refit run b1df9256bb61... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:05:58,490 - training.hpo.execution.local.refit - WARNING - [REFIT]  Could not find trial run for trial_key_hash=8cf262e4... - refit run will not be linked to trial run. Checkpoint acquisition may fail or use fallback search.
2026-01-15 00:05:58 WARNI [training.hpo.execution.local.refit] [REFIT]  Could not find trial run for trial_key_hash=8cf262e4... - refit run will not be linked to trial run. Checkpoint acquisition may fail or use fallback search.
2026-01-15 00:05:58,496 - training.hpo.execution.local.refit - INFO - [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-15 00:05:58 INFO  [training.hpo.execution.local.refit] [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-15 00:05:58,496 - training.hpo.execution.local.refit - INFO - [REFIT] Refit training completed. Metrics: 0.75, Checkpoint: /tmp/pytest-of-codespace/pytest-28/test_full_hpo_workflow_creates0/outputs/hpo/local/distilbert/study-dc0c421a/refit/checkpoint
2026-01-15 00:05:58 INFO  [training.hpo.execution.local.refit] [REFIT] Refit training completed. Metrics: 0.75, Checkpoint: /tmp/pytest-of-codespace/pytest-28/test_full_hpo_workflow_creates0/outputs/hpo/local/distilbert/study-dc0c421a/refit/checkpoint
2026-01-15 00:05:58,496 - training.hpo.execution.local.sweep - INFO - [REFIT] Refit training completed. Metrics: {'macro-f1': 0.75}, Checkpoint: /tmp/pytest-of-codespace/pytest-28/test_full_hpo_workflow_creates0/outputs/hpo/local/distilbert/study-dc0c421a/refit/checkpoint, Run ID: b1df9256bb61...
2026-01-15 00:05:58 INFO  [training.hpo.execution.local.sweep] [REFIT] Refit training completed. Metrics: {'macro-f1': 0.75}, Checkpoint: /tmp/pytest-of-codespace/pytest-28/test_full_hpo_workflow_creates0/outputs/hpo/local/distilbert/study-dc0c421a/refit/checkpoint, Run ID: b1df9256bb61...
2026-01-15 00:05:58,518 - training.hpo.execution.local.sweep - INFO - [HPO] Skipping checkpoint logging (mlflow.log_best_checkpoint=false or not set)
2026-01-15 00:05:58 INFO  [training.hpo.execution.local.sweep] [HPO] Skipping checkpoint logging (mlflow.log_best_checkpoint=false or not set)
2026-01-15 00:05:58,518 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_full_hpo_workflow_creates0/outputs/hpo/config/tags.yaml, using defaults
2026-01-15 00:05:58 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_full_hpo_workflow_creates0/outputs/hpo/config/tags.yaml, using defaults
2026-01-15 00:05:58,533 - infrastructure.tracking.mlflow.lifecycle - INFO - Successfully terminated run b1df9256bb61... with status FINISHED
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.lifecycle] Successfully terminated run b1df9256bb61... with status FINISHED
2026-01-15 00:05:58,533 - training.hpo.execution.local.sweep - INFO - [REFIT]  Artifacts uploaded and run marked as FINISHED: b1df9256bb61...
2026-01-15 00:05:58 INFO  [training.hpo.execution.local.sweep] [REFIT]  Artifacts uploaded and run marked as FINISHED: b1df9256bb61...
2026-01-15 00:05:58,533 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Context manager exiting normally. run_id=ccabe8cdbbc0...
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Context manager exiting normally. run_id=ccabe8cdbbc0...
2026-01-15 00:05:58,544 - training.hpo.execution.local.sweep - INFO - [EARLY HASH] Computed v2 study_key_hash=957b9a1438354884... for folder creation
2026-01-15 00:05:58 INFO  [training.hpo.execution.local.sweep] [EARLY HASH] Computed v2 study_key_hash=957b9a1438354884... for folder creation
2026-01-15 00:05:58,546 - training.hpo.core.study - INFO - [HPO] Starting optimization for distilbert with checkpointing...
2026-01-15 00:05:58 INFO  [training.hpo.core.study] [HPO] Starting optimization for distilbert with checkpointing...
2026-01-15 00:05:58 WARNI [infrastructure.naming.display_policy] [naming.yaml] Missing required 'run_names' section (/tmp/pytest-of-codespace/pytest-28/test_resume_workflow_preserves0/config/naming.yaml)
2026-01-15 00:05:58,627 - training.hpo.execution.local.sweep - INFO - [HPO Setup] k_folds=None, fold_splits_file=None, k_fold config: {'enabled': False}
2026-01-15 00:05:58 INFO  [training.hpo.execution.local.sweep] [HPO Setup] k_folds=None, fold_splits_file=None, k_fold config: {'enabled': False}
2026-01-15 00:05:58,629 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Using LOCAL MLflow tracking (not Azure ML)
2026-01-15 00:05:58 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker] Using LOCAL MLflow tracking (not Azure ML)
2026-01-15 00:05:58,630 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
2026-01-15 00:05:58,641 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=957b9a1438354884...
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=957b9a1438354884...
2026-01-15 00:05:58,641 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=957b9a1438354884..., study_family_hash=23208e5829deb236...
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=957b9a1438354884..., study_family_hash=23208e5829deb236...
2026-01-15 00:05:58,716 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Yielding RunHandle. run_id=ea15a626f9ef...
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Yielding RunHandle. run_id=ea15a626f9ef...
2026-01-15 00:05:58,725 - training.hpo.execution.local.sweep - INFO - [HPO] Parent run ea15a626f9ef... is now visible in MLflow (status: RUNNING)
2026-01-15 00:05:58 INFO  [training.hpo.execution.local.sweep] [HPO] Parent run ea15a626f9ef... is now visible in MLflow (status: RUNNING)
2026-01-15 00:05:58,730 - training.hpo.execution.local.sweep - INFO - Setting Phase 2 tags on parent run ea15a626f9ef... (data_config=present, train_config=present)
2026-01-15 00:05:58 INFO  [training.hpo.execution.local.sweep] Setting Phase 2 tags on parent run ea15a626f9ef... (data_config=present, train_config=present)
2026-01-15 00:05:58,732 - training.hpo.execution.local.sweep - WARNING - Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
2026-01-15 00:05:58 WARNI [training.hpo.execution.local.sweep] Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
2026-01-15 00:05:58,732 - training.hpo.execution.local.sweep - WARNING - Could not set data fingerprint tag: 'Missing tag key: fingerprint.data'
2026-01-15 00:05:58 WARNI [training.hpo.execution.local.sweep] Could not set data fingerprint tag: 'Missing tag key: fingerprint.data'
2026-01-15 00:05:58,732 - training.hpo.execution.local.sweep - WARNING - Could not set eval fingerprint tag: 'Missing tag key: fingerprint.eval'
2026-01-15 00:05:58 WARNI [training.hpo.execution.local.sweep] Could not set eval fingerprint tag: 'Missing tag key: fingerprint.eval'
2026-01-15 00:05:58,732 - training.hpo.execution.local.sweep - WARNING - Could not set objective direction tag: 'Missing tag key: objective.direction'
2026-01-15 00:05:58 WARNI [training.hpo.execution.local.sweep] Could not set objective direction tag: 'Missing tag key: objective.direction'
2026-01-15 00:05:58,732 - training.hpo.execution.local.sweep - WARNING - Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
2026-01-15 00:05:58 WARNI [training.hpo.execution.local.sweep] Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
2026-01-15 00:05:58,732 - training.hpo.execution.local.sweep - WARNING - Could not set artifact.available tag: 'Missing tag key: artifact.available'
2026-01-15 00:05:58 WARNI [training.hpo.execution.local.sweep] Could not set artifact.available tag: 'Missing tag key: artifact.available'
2026-01-15 00:05:58,732 - training.hpo.execution.local.sweep - INFO -  Set Phase 2 tags on parent run ea15a626f9ef...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-15 00:05:58 INFO  [training.hpo.execution.local.sweep]  Set Phase 2 tags on parent run ea15a626f9ef...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-15 00:05:58,733 - training.hpo.execution.local.sweep - INFO -  Successfully completed Phase 2 tag setting for parent run ea15a626f9ef...
2026-01-15 00:05:58 INFO  [training.hpo.execution.local.sweep]  Successfully completed Phase 2 tag setting for parent run ea15a626f9ef...
2026-01-15 00:05:58,733 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.
2026-01-15 00:05:58 INFO  [training.hpo.tracking.cleanup] [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.

  0%|          | 0/1 [00:00<?, ?it/s]2026-01-15 00:05:58,749 - training.hpo.execution.local.sweep - INFO - [Trial 0] fold_splits=None, using non-CV path
2026-01-15 00:05:58 INFO  [training.hpo.execution.local.sweep] [Trial 0] fold_splits=None, using non-CV path
2026-01-15 00:05:58,761 - training.hpo.tracking.runs - INFO - [TRIAL_RUN_NO_CV] Created trial run (no CV): 6cb8372e91b5... (trial 0). Run remains RUNNING until training completes.
2026-01-15 00:05:58 INFO  [training.hpo.tracking.runs] [TRIAL_RUN_NO_CV] Created trial run (no CV): 6cb8372e91b5... (trial 0). Run remains RUNNING until training completes.
2026-01-15 00:05:58,764 - training.hpo.execution.local.trial - INFO - Training completed
2026-01-15 00:05:58 INFO  [training.hpo.execution.local.trial] Training completed
2026-01-15 00:05:58,765 - training.hpo.execution.local.trial - INFO - [TRIAL] Training completed. Objective metric 'macro-f1': 0.75
2026-01-15 00:05:58 INFO  [training.hpo.execution.local.trial] [TRIAL] Training completed. Objective metric 'macro-f1': 0.75
2026-01-15 00:05:58,765 - training.hpo.tracking.runs - INFO - [TRIAL_RUN_NO_CV] Training completed. Marking trial run 6cb8372e91b5... as FINISHED (trial 0)
2026-01-15 00:05:58 INFO  [training.hpo.tracking.runs] [TRIAL_RUN_NO_CV] Training completed. Marking trial run 6cb8372e91b5... as FINISHED (trial 0)
2026-01-15 00:05:58,775 - infrastructure.tracking.mlflow.lifecycle - INFO - Successfully terminated run 6cb8372e91b5... with status FINISHED
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.lifecycle] Successfully terminated run 6cb8372e91b5... with status FINISHED
2026-01-15 00:05:58,775 - training.hpo.tracking.runs - INFO - [TRIAL_RUN_NO_CV] Successfully marked trial run 6cb8372e91b5... as FINISHED
2026-01-15 00:05:58 INFO  [training.hpo.tracking.runs] [TRIAL_RUN_NO_CV] Successfully marked trial run 6cb8372e91b5... as FINISHED
2026-01-15 00:05:58,792 - training.hpo.trial.callback - INFO - 
2026-01-15 00:05:58 INFO  [training.hpo.trial.callback] 
2026-01-15 00:05:58,792 - training.hpo.trial.callback - INFO - [BEST]: trial_0
2026-01-15 00:05:58 INFO  [training.hpo.trial.callback] [BEST]: trial_0
2026-01-15 00:05:58,792 - training.hpo.trial.callback - INFO -   Metrics: macro-f1=0.750000
2026-01-15 00:05:58 INFO  [training.hpo.trial.callback]   Metrics: macro-f1=0.750000
2026-01-15 00:05:58,792 - training.hpo.trial.callback - INFO -   Params: learning_rate=2.87e-05 | batch_size=4
2026-01-15 00:05:58 INFO  [training.hpo.trial.callback]   Params: learning_rate=2.87e-05 | batch_size=4

Best trial: 0. Best value: 0.75:   0%|          | 0/1 [00:00<?, ?it/s]
Best trial: 0. Best value: 0.75: 100%|| 1/1 [00:00<00:00, 15.40it/s, 0.06/1200 seconds]
Best trial: 0. Best value: 0.75: 100%|| 1/1 [00:00<00:00, 15.37it/s, 0.06/1200 seconds]
2026-01-15 00:05:58,798 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=ea15a626f9ef...
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=ea15a626f9ef...
2026-01-15 00:05:58,807 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Found 1 completed trials out of 1 total
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Found 1 completed trials out of 1 total
2026-01-15 00:05:58,808 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging metrics: n_trials=1, n_completed_trials=1
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Logging metrics: n_trials=1, n_completed_trials=1
2026-01-15 00:05:58,825 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=0.75
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=0.75
2026-01-15 00:05:58,834 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging best hyperparameters: {'learning_rate': 2.871693984906391e-05, 'batch_size': 4}
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Logging best hyperparameters: {'learning_rate': 2.871693984906391e-05, 'batch_size': 4}
2026-01-15 00:05:58,845 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Finding and logging best trial run ID...
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Finding and logging best trial run ID...
2026-01-15 00:05:58,854 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Could not find MLflow run ID for best trial 0. Found 0 runs with matching study_key_hash and trial_number, but none belong to parent ea15a626f9ef... This may be a timing issue - trial runs may not be created/committed yet.
2026-01-15 00:05:58 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker] Could not find MLflow run ID for best trial 0. Found 0 runs with matching study_key_hash and trial_number, but none belong to parent ea15a626f9ef... This may be a timing issue - trial runs may not be created/committed yet.
2026-01-15 00:05:58,854 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Completed best trial ID logging
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Completed best trial ID logging
2026-01-15 00:05:58,854 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Checkpoint logging enabled: False (upload_checkpoint=False)
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Checkpoint logging enabled: False (upload_checkpoint=False)
2026-01-15 00:05:58,854 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
2026-01-15 00:05:58,854 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Completed log_final_metrics successfully
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Completed log_final_metrics successfully
2026-01-15 00:05:58,854 - training.hpo.execution.local.sweep - INFO - [REFIT] Refit training is disabled in config
2026-01-15 00:05:58 INFO  [training.hpo.execution.local.sweep] [REFIT] Refit training is disabled in config
2026-01-15 00:05:58,854 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Context manager exiting normally. run_id=ea15a626f9ef...
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Context manager exiting normally. run_id=ea15a626f9ef...
2026-01-15 00:05:58,865 - training.hpo.execution.local.sweep - INFO - [EARLY HASH] Computed v2 study_key_hash=957b9a1438354884... for folder creation
2026-01-15 00:05:58 INFO  [training.hpo.execution.local.sweep] [EARLY HASH] Computed v2 study_key_hash=957b9a1438354884... for folder creation
2026-01-15 00:05:58,865 - training.hpo.core.study - INFO - [HPO] Resuming optimization for distilbert from checkpoint...
2026-01-15 00:05:58 INFO  [training.hpo.core.study] [HPO] Resuming optimization for distilbert from checkpoint...
2026-01-15 00:05:58,904 - training.hpo.core.study - INFO - Loaded 1 existing trials (1 completed, 0 marked as failed)
2026-01-15 00:05:58 INFO  [training.hpo.core.study] Loaded 1 existing trials (1 completed, 0 marked as failed)
2026-01-15 00:05:58,906 - training.hpo.execution.local.sweep - INFO - [HPO Setup] k_folds=None, fold_splits_file=None, k_fold config: {'enabled': False}
2026-01-15 00:05:58 INFO  [training.hpo.execution.local.sweep] [HPO Setup] k_folds=None, fold_splits_file=None, k_fold config: {'enabled': False}
2026-01-15 00:05:58,910 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Using LOCAL MLflow tracking (not Azure ML)
2026-01-15 00:05:58 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker] Using LOCAL MLflow tracking (not Azure ML)
2026-01-15 00:05:58,910 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
2026-01-15 00:05:58,922 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=957b9a1438354884...
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=957b9a1438354884...
2026-01-15 00:05:58,922 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=957b9a1438354884..., study_family_hash=23208e5829deb236...
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=957b9a1438354884..., study_family_hash=23208e5829deb236...
2026-01-15 00:05:58,997 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Yielding RunHandle. run_id=07b18d66366c...
2026-01-15 00:05:58 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Yielding RunHandle. run_id=07b18d66366c...
2026-01-15 00:05:59,007 - training.hpo.execution.local.sweep - INFO - [HPO] Parent run 07b18d66366c... is now visible in MLflow (status: RUNNING)
2026-01-15 00:05:59 INFO  [training.hpo.execution.local.sweep] [HPO] Parent run 07b18d66366c... is now visible in MLflow (status: RUNNING)
2026-01-15 00:05:59,013 - training.hpo.execution.local.sweep - INFO - Setting Phase 2 tags on parent run 07b18d66366c... (data_config=present, train_config=present)
2026-01-15 00:05:59 INFO  [training.hpo.execution.local.sweep] Setting Phase 2 tags on parent run 07b18d66366c... (data_config=present, train_config=present)
2026-01-15 00:05:59,015 - training.hpo.execution.local.sweep - WARNING - Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
2026-01-15 00:05:59 WARNI [training.hpo.execution.local.sweep] Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
2026-01-15 00:05:59,015 - training.hpo.execution.local.sweep - WARNING - Could not set data fingerprint tag: 'Missing tag key: fingerprint.data'
2026-01-15 00:05:59 WARNI [training.hpo.execution.local.sweep] Could not set data fingerprint tag: 'Missing tag key: fingerprint.data'
2026-01-15 00:05:59,015 - training.hpo.execution.local.sweep - WARNING - Could not set eval fingerprint tag: 'Missing tag key: fingerprint.eval'
2026-01-15 00:05:59 WARNI [training.hpo.execution.local.sweep] Could not set eval fingerprint tag: 'Missing tag key: fingerprint.eval'
2026-01-15 00:05:59,015 - training.hpo.execution.local.sweep - WARNING - Could not set objective direction tag: 'Missing tag key: objective.direction'
2026-01-15 00:05:59 WARNI [training.hpo.execution.local.sweep] Could not set objective direction tag: 'Missing tag key: objective.direction'
2026-01-15 00:05:59,015 - training.hpo.execution.local.sweep - WARNING - Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
2026-01-15 00:05:59 WARNI [training.hpo.execution.local.sweep] Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
2026-01-15 00:05:59,015 - training.hpo.execution.local.sweep - WARNING - Could not set artifact.available tag: 'Missing tag key: artifact.available'
2026-01-15 00:05:59 WARNI [training.hpo.execution.local.sweep] Could not set artifact.available tag: 'Missing tag key: artifact.available'
2026-01-15 00:05:59,015 - training.hpo.execution.local.sweep - INFO -  Set Phase 2 tags on parent run 07b18d66366c...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-15 00:05:59 INFO  [training.hpo.execution.local.sweep]  Set Phase 2 tags on parent run 07b18d66366c...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-15 00:05:59,016 - training.hpo.execution.local.sweep - INFO -  Successfully completed Phase 2 tag setting for parent run 07b18d66366c...
2026-01-15 00:05:59 INFO  [training.hpo.execution.local.sweep]  Successfully completed Phase 2 tag setting for parent run 07b18d66366c...
2026-01-15 00:05:59,016 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.
2026-01-15 00:05:59 INFO  [training.hpo.tracking.cleanup] [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.

  0%|          | 0/1 [00:00<?, ?it/s]2026-01-15 00:05:59,034 - training.hpo.execution.local.sweep - INFO - [Trial 1] fold_splits=None, using non-CV path
2026-01-15 00:05:59 INFO  [training.hpo.execution.local.sweep] [Trial 1] fold_splits=None, using non-CV path
2026-01-15 00:05:59,046 - training.hpo.tracking.runs - INFO - [TRIAL_RUN_NO_CV] Created trial run (no CV): 463531da55ad... (trial 1). Run remains RUNNING until training completes.
2026-01-15 00:05:59 INFO  [training.hpo.tracking.runs] [TRIAL_RUN_NO_CV] Created trial run (no CV): 463531da55ad... (trial 1). Run remains RUNNING until training completes.
2026-01-15 00:05:59,049 - training.hpo.execution.local.trial - INFO - Training completed
2026-01-15 00:05:59 INFO  [training.hpo.execution.local.trial] Training completed
2026-01-15 00:05:59,049 - training.hpo.execution.local.trial - INFO - [TRIAL] Training completed. Objective metric 'macro-f1': 0.75
2026-01-15 00:05:59 INFO  [training.hpo.execution.local.trial] [TRIAL] Training completed. Objective metric 'macro-f1': 0.75
2026-01-15 00:05:59,049 - training.hpo.tracking.runs - INFO - [TRIAL_RUN_NO_CV] Training completed. Marking trial run 463531da55ad... as FINISHED (trial 1)
2026-01-15 00:05:59 INFO  [training.hpo.tracking.runs] [TRIAL_RUN_NO_CV] Training completed. Marking trial run 463531da55ad... as FINISHED (trial 1)
2026-01-15 00:05:59,059 - infrastructure.tracking.mlflow.lifecycle - INFO - Successfully terminated run 463531da55ad... with status FINISHED
2026-01-15 00:05:59 INFO  [infrastructure.tracking.mlflow.lifecycle] Successfully terminated run 463531da55ad... with status FINISHED
2026-01-15 00:05:59,059 - training.hpo.tracking.runs - INFO - [TRIAL_RUN_NO_CV] Successfully marked trial run 463531da55ad... as FINISHED
2026-01-15 00:05:59 INFO  [training.hpo.tracking.runs] [TRIAL_RUN_NO_CV] Successfully marked trial run 463531da55ad... as FINISHED
2026-01-15 00:05:59,075 - training.hpo.trial.callback - INFO - 
2026-01-15 00:05:59 INFO  [training.hpo.trial.callback] 
2026-01-15 00:05:59,075 - training.hpo.trial.callback - INFO - [Trial 1]: trial_1
2026-01-15 00:05:59 INFO  [training.hpo.trial.callback] [Trial 1]: trial_1
2026-01-15 00:05:59,075 - training.hpo.trial.callback - INFO -   Metrics: macro-f1=0.750000
2026-01-15 00:05:59 INFO  [training.hpo.trial.callback]   Metrics: macro-f1=0.750000
2026-01-15 00:05:59,075 - training.hpo.trial.callback - INFO -   Params: learning_rate=4.37e-05 | batch_size=4
2026-01-15 00:05:59 INFO  [training.hpo.trial.callback]   Params: learning_rate=4.37e-05 | batch_size=4

Best trial: 0. Best value: 0.75:   0%|          | 0/1 [00:00<?, ?it/s]
Best trial: 0. Best value: 0.75: 100%|| 1/1 [00:00<00:00, 16.64it/s, 0.06/1200 seconds]
Best trial: 0. Best value: 0.75: 100%|| 1/1 [00:00<00:00, 16.61it/s, 0.06/1200 seconds]
2026-01-15 00:05:59,077 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=07b18d66366c...
2026-01-15 00:05:59 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=07b18d66366c...
2026-01-15 00:05:59,083 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Found 2 completed trials out of 2 total
2026-01-15 00:05:59 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Found 2 completed trials out of 2 total
2026-01-15 00:05:59,084 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging metrics: n_trials=2, n_completed_trials=2
2026-01-15 00:05:59 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Logging metrics: n_trials=2, n_completed_trials=2
2026-01-15 00:05:59,101 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=0.75
2026-01-15 00:05:59 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=0.75
2026-01-15 00:05:59,110 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging best hyperparameters: {'learning_rate': 2.871693984906391e-05, 'batch_size': 4}
2026-01-15 00:05:59 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Logging best hyperparameters: {'learning_rate': 2.871693984906391e-05, 'batch_size': 4}
2026-01-15 00:05:59,120 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Finding and logging best trial run ID...
2026-01-15 00:05:59 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Finding and logging best trial run ID...
2026-01-15 00:05:59,129 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Could not find MLflow run ID for best trial 0. Found 0 runs with matching study_key_hash and trial_number, but none belong to parent 07b18d66366c... This may be a timing issue - trial runs may not be created/committed yet.
2026-01-15 00:05:59 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker] Could not find MLflow run ID for best trial 0. Found 0 runs with matching study_key_hash and trial_number, but none belong to parent 07b18d66366c... This may be a timing issue - trial runs may not be created/committed yet.
2026-01-15 00:05:59,129 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Completed best trial ID logging
2026-01-15 00:05:59 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Completed best trial ID logging
2026-01-15 00:05:59,129 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Checkpoint logging enabled: False (upload_checkpoint=False)
2026-01-15 00:05:59 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Checkpoint logging enabled: False (upload_checkpoint=False)
2026-01-15 00:05:59,129 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
2026-01-15 00:05:59 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
2026-01-15 00:05:59,129 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Completed log_final_metrics successfully
2026-01-15 00:05:59 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Completed log_final_metrics successfully
2026-01-15 00:05:59,130 - training.hpo.execution.local.sweep - INFO - [REFIT] Refit training is disabled in config
2026-01-15 00:05:59 INFO  [training.hpo.execution.local.sweep] [REFIT] Refit training is disabled in config
2026-01-15 00:05:59,130 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Context manager exiting normally. run_id=07b18d66366c...
2026-01-15 00:05:59 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Context manager exiting normally. run_id=07b18d66366c...
2026-01-15 00:05:59,145 - training.hpo.execution.local.sweep - INFO - [EARLY HASH] Computed v2 study_key_hash=42ff77fc79adab6d... for folder creation
2026-01-15 00:05:59 INFO  [training.hpo.execution.local.sweep] [EARLY HASH] Computed v2 study_key_hash=42ff77fc79adab6d... for folder creation
2026-01-15 00:05:59,147 - training.hpo.core.study - INFO - [HPO] Starting optimization for distilbert with checkpointing...
2026-01-15 00:05:59 INFO  [training.hpo.core.study] [HPO] Starting optimization for distilbert with checkpointing...
2026-01-15 00:05:59 WARNI [infrastructure.naming.display_policy] [naming.yaml] Missing required 'run_names' section (/tmp/pytest-of-codespace/pytest-28/test_resume_workflow_with_diff0/config/naming.yaml)
2026-01-15 00:05:59,232 - training.hpo.execution.local.sweep - INFO - [HPO Setup] k_folds=None, fold_splits_file=None, k_fold config: {'enabled': False}
2026-01-15 00:05:59 INFO  [training.hpo.execution.local.sweep] [HPO Setup] k_folds=None, fold_splits_file=None, k_fold config: {'enabled': False}
2026-01-15 00:05:59,235 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Using LOCAL MLflow tracking (not Azure ML)
2026-01-15 00:05:59 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker] Using LOCAL MLflow tracking (not Azure ML)
2026-01-15 00:05:59,235 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
2026-01-15 00:05:59 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
2026-01-15 00:05:59,247 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=42ff77fc79adab6d...
2026-01-15 00:05:59 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=42ff77fc79adab6d...
2026-01-15 00:05:59,247 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=42ff77fc79adab6d..., study_family_hash=ce5a7f9ff304036d...
2026-01-15 00:05:59 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=42ff77fc79adab6d..., study_family_hash=ce5a7f9ff304036d...
2026-01-15 00:05:59,323 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Yielding RunHandle. run_id=3966ecbe9196...
2026-01-15 00:05:59 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Yielding RunHandle. run_id=3966ecbe9196...
2026-01-15 00:05:59,332 - training.hpo.execution.local.sweep - INFO - [HPO] Parent run 3966ecbe9196... is now visible in MLflow (status: RUNNING)
2026-01-15 00:05:59 INFO  [training.hpo.execution.local.sweep] [HPO] Parent run 3966ecbe9196... is now visible in MLflow (status: RUNNING)
2026-01-15 00:05:59,337 - training.hpo.execution.local.sweep - INFO - Setting Phase 2 tags on parent run 3966ecbe9196... (data_config=present, train_config=present)
2026-01-15 00:05:59 INFO  [training.hpo.execution.local.sweep] Setting Phase 2 tags on parent run 3966ecbe9196... (data_config=present, train_config=present)
2026-01-15 00:05:59,340 - training.hpo.execution.local.sweep - WARNING - Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
2026-01-15 00:05:59 WARNI [training.hpo.execution.local.sweep] Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
2026-01-15 00:05:59,340 - training.hpo.execution.local.sweep - WARNING - Could not set data fingerprint tag: 'Missing tag key: fingerprint.data'
2026-01-15 00:05:59 WARNI [training.hpo.execution.local.sweep] Could not set data fingerprint tag: 'Missing tag key: fingerprint.data'
2026-01-15 00:05:59,340 - training.hpo.execution.local.sweep - WARNING - Could not set eval fingerprint tag: 'Missing tag key: fingerprint.eval'
2026-01-15 00:05:59 WARNI [training.hpo.execution.local.sweep] Could not set eval fingerprint tag: 'Missing tag key: fingerprint.eval'
2026-01-15 00:05:59,340 - training.hpo.execution.local.sweep - WARNING - Could not set objective direction tag: 'Missing tag key: objective.direction'
2026-01-15 00:05:59 WARNI [training.hpo.execution.local.sweep] Could not set objective direction tag: 'Missing tag key: objective.direction'
2026-01-15 00:05:59,340 - training.hpo.execution.local.sweep - WARNING - Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
2026-01-15 00:05:59 WARNI [training.hpo.execution.local.sweep] Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
2026-01-15 00:05:59,340 - training.hpo.execution.local.sweep - WARNING - Could not set artifact.available tag: 'Missing tag key: artifact.available'
2026-01-15 00:05:59 WARNI [training.hpo.execution.local.sweep] Could not set artifact.available tag: 'Missing tag key: artifact.available'
2026-01-15 00:05:59,340 - training.hpo.execution.local.sweep - INFO -  Set Phase 2 tags on parent run 3966ecbe9196...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-15 00:05:59 INFO  [training.hpo.execution.local.sweep]  Set Phase 2 tags on parent run 3966ecbe9196...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-15 00:05:59,341 - training.hpo.execution.local.sweep - INFO -  Successfully completed Phase 2 tag setting for parent run 3966ecbe9196...
2026-01-15 00:05:59 INFO  [training.hpo.execution.local.sweep]  Successfully completed Phase 2 tag setting for parent run 3966ecbe9196...
2026-01-15 00:05:59,341 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.
2026-01-15 00:05:59 INFO  [training.hpo.tracking.cleanup] [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.

  0%|          | 0/1 [00:00<?, ?it/s]2026-01-15 00:05:59,357 - training.hpo.execution.local.sweep - INFO - [Trial 0] fold_splits=None, using non-CV path
2026-01-15 00:05:59 INFO  [training.hpo.execution.local.sweep] [Trial 0] fold_splits=None, using non-CV path
2026-01-15 00:05:59,369 - training.hpo.tracking.runs - INFO - [TRIAL_RUN_NO_CV] Created trial run (no CV): 3786c247531d... (trial 0). Run remains RUNNING until training completes.
2026-01-15 00:05:59 INFO  [training.hpo.tracking.runs] [TRIAL_RUN_NO_CV] Created trial run (no CV): 3786c247531d... (trial 0). Run remains RUNNING until training completes.

                                     

  0%|          | 0/1 [00:00<?, ?it/s]
                                     

  0%|          | 0/1 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s]
2026-01-15 00:05:59,383 - infrastructure.tracking.mlflow.trackers.sweep_tracker - ERROR - [START_SWEEP_RUN] MLflow tracking failed: 'Mock' object is not iterable
2026-01-15 00:05:59 ERROR [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] MLflow tracking failed: 'Mock' object is not iterable
2026-01-15 00:05:59,384 - infrastructure.tracking.mlflow.trackers.sweep_tracker - ERROR - [START_SWEEP_RUN] Traceback: Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/trackers/sweep_tracker.py", line 221, in start_sweep_run
    yield handle
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py", line 1020, in run_local_hpo_sweep
    study.optimize(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/study.py", line 490, in optimize
    _optimize(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/_optimize.py", line 67, in _optimize
    _optimize_sequential(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/_optimize.py", line 164, in _optimize_sequential
    frozen_trial_id = _run_trial(study, func, catch)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/_optimize.py", line 262, in _run_trial
    raise func_err
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py", line 347, in objective
    metric_value = run_training_trial(
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/trial.py", line 212, in run_training_trial
    return executor.execute(
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/trial.py", line 149, in execute
    result = execute_training_subprocess(
  File "/workspaces/resume-ner-azureml/src/training/execution/subprocess_runner.py", line 360, in execute_training_subprocess
    filtered_stdout = _filter_debug_messages(result.stdout)
  File "/workspaces/resume-ner-azureml/src/training/execution/subprocess_runner.py", line 388, in _filter_debug_messages
    for line in lines:
TypeError: 'Mock' object is not iterable

2026-01-15 00:05:59 ERROR [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Traceback: Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/trackers/sweep_tracker.py", line 221, in start_sweep_run
    yield handle
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py", line 1020, in run_local_hpo_sweep
    study.optimize(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/study.py", line 490, in optimize
    _optimize(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/_optimize.py", line 67, in _optimize
    _optimize_sequential(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/_optimize.py", line 164, in _optimize_sequential
    frozen_trial_id = _run_trial(study, func, catch)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/_optimize.py", line 262, in _run_trial
    raise func_err
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py", line 347, in objective
    metric_value = run_training_trial(
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/trial.py", line 212, in run_training_trial
    return executor.execute(
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/trial.py", line 149, in execute
    result = execute_training_subprocess(
  File "/workspaces/resume-ner-azureml/src/training/execution/subprocess_runner.py", line 360, in execute_training_subprocess
    filtered_stdout = _filter_debug_messages(result.stdout)
  File "/workspaces/resume-ner-azureml/src/training/execution/subprocess_runner.py", line 388, in _filter_debug_messages
    for line in lines:
TypeError: 'Mock' object is not iterable

2026-01-15 00:05:59,384 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Continuing HPO without MLflow tracking...
2026-01-15 00:05:59 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker] Continuing HPO without MLflow tracking...
2026-01-15 00:05:59,384 - training.hpo.execution.local.sweep - WARNING - MLflow tracking failed: generator didn't stop after throw()
2026-01-15 00:05:59 WARNI [training.hpo.execution.local.sweep] MLflow tracking failed: generator didn't stop after throw()
2026-01-15 00:05:59,384 - training.hpo.execution.local.sweep - WARNING - Continuing HPO without MLflow tracking...
2026-01-15 00:05:59 WARNI [training.hpo.execution.local.sweep] Continuing HPO without MLflow tracking...
2026-01-15 00:05:59,390 - training.hpo.execution.local.sweep - INFO - [EARLY HASH] Computed v2 study_key_hash=42ff77fc79adab6d... for folder creation
2026-01-15 00:05:59 INFO  [training.hpo.execution.local.sweep] [EARLY HASH] Computed v2 study_key_hash=42ff77fc79adab6d... for folder creation
2026-01-15 00:05:59,391 - training.hpo.core.study - INFO - [HPO] Resuming optimization for distilbert from checkpoint...
2026-01-15 00:05:59 INFO  [training.hpo.core.study] [HPO] Resuming optimization for distilbert from checkpoint...
2026-01-15 00:05:59,431 - training.hpo.core.study - INFO - Loaded 1 existing trials (0 completed, 0 marked as failed)
2026-01-15 00:05:59 INFO  [training.hpo.core.study] Loaded 1 existing trials (0 completed, 0 marked as failed)
2026-01-15 00:05:59,432 - training.hpo.execution.local.sweep - INFO - [HPO Setup] k_folds=None, fold_splits_file=None, k_fold config: {'enabled': False}
2026-01-15 00:05:59 INFO  [training.hpo.execution.local.sweep] [HPO Setup] k_folds=None, fold_splits_file=None, k_fold config: {'enabled': False}
2026-01-15 00:05:59,434 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Using LOCAL MLflow tracking (not Azure ML)
2026-01-15 00:05:59 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker] Using LOCAL MLflow tracking (not Azure ML)
2026-01-15 00:05:59,435 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
2026-01-15 00:05:59 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
2026-01-15 00:05:59,446 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=42ff77fc79adab6d...
2026-01-15 00:05:59 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=42ff77fc79adab6d...
2026-01-15 00:05:59,446 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=42ff77fc79adab6d..., study_family_hash=ce5a7f9ff304036d...
2026-01-15 00:05:59 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=42ff77fc79adab6d..., study_family_hash=ce5a7f9ff304036d...
2026-01-15 00:05:59,523 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Yielding RunHandle. run_id=285c5e181ef7...
2026-01-15 00:05:59 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Yielding RunHandle. run_id=285c5e181ef7...
2026-01-15 00:05:59,532 - training.hpo.execution.local.sweep - INFO - [HPO] Parent run 285c5e181ef7... is now visible in MLflow (status: RUNNING)
2026-01-15 00:05:59 INFO  [training.hpo.execution.local.sweep] [HPO] Parent run 285c5e181ef7... is now visible in MLflow (status: RUNNING)
2026-01-15 00:05:59,538 - training.hpo.execution.local.sweep - INFO - Setting Phase 2 tags on parent run 285c5e181ef7... (data_config=present, train_config=present)
2026-01-15 00:05:59 INFO  [training.hpo.execution.local.sweep] Setting Phase 2 tags on parent run 285c5e181ef7... (data_config=present, train_config=present)
2026-01-15 00:05:59,540 - training.hpo.execution.local.sweep - WARNING - Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
2026-01-15 00:05:59 WARNI [training.hpo.execution.local.sweep] Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
2026-01-15 00:05:59,540 - training.hpo.execution.local.sweep - WARNING - Could not set data fingerprint tag: 'Missing tag key: fingerprint.data'
2026-01-15 00:05:59 WARNI [training.hpo.execution.local.sweep] Could not set data fingerprint tag: 'Missing tag key: fingerprint.data'
2026-01-15 00:05:59,540 - training.hpo.execution.local.sweep - WARNING - Could not set eval fingerprint tag: 'Missing tag key: fingerprint.eval'
2026-01-15 00:05:59 WARNI [training.hpo.execution.local.sweep] Could not set eval fingerprint tag: 'Missing tag key: fingerprint.eval'
2026-01-15 00:05:59,540 - training.hpo.execution.local.sweep - WARNING - Could not set objective direction tag: 'Missing tag key: objective.direction'
2026-01-15 00:05:59 WARNI [training.hpo.execution.local.sweep] Could not set objective direction tag: 'Missing tag key: objective.direction'
2026-01-15 00:05:59,540 - training.hpo.execution.local.sweep - WARNING - Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
2026-01-15 00:05:59 WARNI [training.hpo.execution.local.sweep] Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
2026-01-15 00:05:59,540 - training.hpo.execution.local.sweep - WARNING - Could not set artifact.available tag: 'Missing tag key: artifact.available'
2026-01-15 00:05:59 WARNI [training.hpo.execution.local.sweep] Could not set artifact.available tag: 'Missing tag key: artifact.available'
2026-01-15 00:05:59,540 - training.hpo.execution.local.sweep - INFO -  Set Phase 2 tags on parent run 285c5e181ef7...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-15 00:05:59 INFO  [training.hpo.execution.local.sweep]  Set Phase 2 tags on parent run 285c5e181ef7...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-15 00:05:59,540 - training.hpo.execution.local.sweep - INFO -  Successfully completed Phase 2 tag setting for parent run 285c5e181ef7...
2026-01-15 00:05:59 INFO  [training.hpo.execution.local.sweep]  Successfully completed Phase 2 tag setting for parent run 285c5e181ef7...
2026-01-15 00:05:59,540 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.
2026-01-15 00:05:59 INFO  [training.hpo.tracking.cleanup] [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.

  0%|          | 0/2 [00:00<?, ?it/s]2026-01-15 00:05:59,556 - training.hpo.execution.local.sweep - INFO - [Trial 1] fold_splits=None, using non-CV path
2026-01-15 00:05:59 INFO  [training.hpo.execution.local.sweep] [Trial 1] fold_splits=None, using non-CV path
2026-01-15 00:05:59,567 - training.hpo.tracking.runs - INFO - [TRIAL_RUN_NO_CV] Created trial run (no CV): 2cf2a8736fe8... (trial 1). Run remains RUNNING until training completes.
2026-01-15 00:05:59 INFO  [training.hpo.tracking.runs] [TRIAL_RUN_NO_CV] Created trial run (no CV): 2cf2a8736fe8... (trial 1). Run remains RUNNING until training completes.

                                     

  0%|          | 0/2 [00:00<?, ?it/s]
                                     

  0%|          | 0/2 [00:00<?, ?it/s]
  0%|          | 0/2 [00:00<?, ?it/s]
2026-01-15 00:05:59,580 - infrastructure.tracking.mlflow.trackers.sweep_tracker - ERROR - [START_SWEEP_RUN] MLflow tracking failed: 'Mock' object is not iterable
2026-01-15 00:05:59 ERROR [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] MLflow tracking failed: 'Mock' object is not iterable
2026-01-15 00:05:59,580 - infrastructure.tracking.mlflow.trackers.sweep_tracker - ERROR - [START_SWEEP_RUN] Traceback: Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/trackers/sweep_tracker.py", line 221, in start_sweep_run
    yield handle
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py", line 1012, in run_local_hpo_sweep
    study.optimize(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/study.py", line 490, in optimize
    _optimize(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/_optimize.py", line 67, in _optimize
    _optimize_sequential(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/_optimize.py", line 164, in _optimize_sequential
    frozen_trial_id = _run_trial(study, func, catch)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/_optimize.py", line 262, in _run_trial
    raise func_err
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py", line 347, in objective
    metric_value = run_training_trial(
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/trial.py", line 212, in run_training_trial
    return executor.execute(
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/trial.py", line 149, in execute
    result = execute_training_subprocess(
  File "/workspaces/resume-ner-azureml/src/training/execution/subprocess_runner.py", line 360, in execute_training_subprocess
    filtered_stdout = _filter_debug_messages(result.stdout)
  File "/workspaces/resume-ner-azureml/src/training/execution/subprocess_runner.py", line 388, in _filter_debug_messages
    for line in lines:
TypeError: 'Mock' object is not iterable

2026-01-15 00:05:59 ERROR [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Traceback: Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/trackers/sweep_tracker.py", line 221, in start_sweep_run
    yield handle
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py", line 1012, in run_local_hpo_sweep
    study.optimize(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/study.py", line 490, in optimize
    _optimize(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/_optimize.py", line 67, in _optimize
    _optimize_sequential(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/_optimize.py", line 164, in _optimize_sequential
    frozen_trial_id = _run_trial(study, func, catch)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/_optimize.py", line 262, in _run_trial
    raise func_err
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py", line 347, in objective
    metric_value = run_training_trial(
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/trial.py", line 212, in run_training_trial
    return executor.execute(
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/trial.py", line 149, in execute
    result = execute_training_subprocess(
  File "/workspaces/resume-ner-azureml/src/training/execution/subprocess_runner.py", line 360, in execute_training_subprocess
    filtered_stdout = _filter_debug_messages(result.stdout)
  File "/workspaces/resume-ner-azureml/src/training/execution/subprocess_runner.py", line 388, in _filter_debug_messages
    for line in lines:
TypeError: 'Mock' object is not iterable

2026-01-15 00:05:59,581 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Continuing HPO without MLflow tracking...
2026-01-15 00:05:59 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker] Continuing HPO without MLflow tracking...
2026-01-15 00:05:59,581 - training.hpo.execution.local.sweep - WARNING - MLflow tracking failed: generator didn't stop after throw()
2026-01-15 00:05:59 WARNI [training.hpo.execution.local.sweep] MLflow tracking failed: generator didn't stop after throw()
2026-01-15 00:05:59,581 - training.hpo.execution.local.sweep - WARNING - Continuing HPO without MLflow tracking...
2026-01-15 00:05:59 WARNI [training.hpo.execution.local.sweep] Continuing HPO without MLflow tracking...
2026-01-15 00:05:59,590 - training.hpo.execution.local.sweep - INFO - [EARLY HASH] Computed v2 study_key_hash=dc0c421a5056294a... for folder creation
2026-01-15 00:05:59 INFO  [training.hpo.execution.local.sweep] [EARLY HASH] Computed v2 study_key_hash=dc0c421a5056294a... for folder creation
2026-01-15 00:05:59,592 - training.hpo.core.study - INFO - [HPO] Starting optimization for distilbert with checkpointing...
2026-01-15 00:05:59 INFO  [training.hpo.core.study] [HPO] Starting optimization for distilbert with checkpointing...
2026-01-15 00:05:59 WARNI [infrastructure.naming.display_policy] [naming.yaml] Missing required 'run_names' section (/tmp/pytest-of-codespace/pytest-28/test_resume_workflow_with_cv0/config/naming.yaml)
2026-01-15 00:05:59,681 - training.hpo.execution.local.sweep - INFO - [HPO Setup] k_folds=2, fold_splits_file=None, k_fold config: {'enabled': True, 'n_splits': 2, 'random_seed': 42, 'shuffle': True, 'stratified': True}
2026-01-15 00:05:59 INFO  [training.hpo.execution.local.sweep] [HPO Setup] k_folds=2, fold_splits_file=None, k_fold config: {'enabled': True, 'n_splits': 2, 'random_seed': 42, 'shuffle': True, 'stratified': True}
2026-01-15 00:05:59,682 - training.hpo.execution.local.sweep - INFO - [CV Setup]  Created 2 fold splits and saved to /tmp/pytest-of-codespace/pytest-28/test_resume_workflow_with_cv0/outputs/hpo/local/distilbert/study-dc0c421a/fold_splits.json
2026-01-15 00:05:59 INFO  [training.hpo.execution.local.sweep] [CV Setup]  Created 2 fold splits and saved to /tmp/pytest-of-codespace/pytest-28/test_resume_workflow_with_cv0/outputs/hpo/local/distilbert/study-dc0c421a/fold_splits.json
2026-01-15 00:05:59,684 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Using LOCAL MLflow tracking (not Azure ML)
2026-01-15 00:05:59 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker] Using LOCAL MLflow tracking (not Azure ML)
2026-01-15 00:05:59,684 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
2026-01-15 00:05:59 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
2026-01-15 00:05:59,696 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=dc0c421a5056294a...
2026-01-15 00:05:59 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=dc0c421a5056294a...
2026-01-15 00:05:59,696 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=dc0c421a5056294a..., study_family_hash=d1aa8bfffdcfdb77...
2026-01-15 00:05:59 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=dc0c421a5056294a..., study_family_hash=d1aa8bfffdcfdb77...
2026-01-15 00:05:59,773 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Yielding RunHandle. run_id=9fb6a522c0d2...
2026-01-15 00:05:59 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Yielding RunHandle. run_id=9fb6a522c0d2...
2026-01-15 00:05:59,783 - training.hpo.execution.local.sweep - INFO - [HPO] Parent run 9fb6a522c0d2... is now visible in MLflow (status: RUNNING)
2026-01-15 00:05:59 INFO  [training.hpo.execution.local.sweep] [HPO] Parent run 9fb6a522c0d2... is now visible in MLflow (status: RUNNING)
2026-01-15 00:05:59,788 - training.hpo.execution.local.sweep - INFO - Setting Phase 2 tags on parent run 9fb6a522c0d2... (data_config=present, train_config=present)
2026-01-15 00:05:59 INFO  [training.hpo.execution.local.sweep] Setting Phase 2 tags on parent run 9fb6a522c0d2... (data_config=present, train_config=present)
2026-01-15 00:05:59,790 - training.hpo.execution.local.sweep - WARNING - Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
2026-01-15 00:05:59 WARNI [training.hpo.execution.local.sweep] Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
2026-01-15 00:05:59,790 - training.hpo.execution.local.sweep - WARNING - Could not set data fingerprint tag: 'Missing tag key: fingerprint.data'
2026-01-15 00:05:59 WARNI [training.hpo.execution.local.sweep] Could not set data fingerprint tag: 'Missing tag key: fingerprint.data'
2026-01-15 00:05:59,790 - training.hpo.execution.local.sweep - WARNING - Could not set eval fingerprint tag: 'Missing tag key: fingerprint.eval'
2026-01-15 00:05:59 WARNI [training.hpo.execution.local.sweep] Could not set eval fingerprint tag: 'Missing tag key: fingerprint.eval'
2026-01-15 00:05:59,790 - training.hpo.execution.local.sweep - WARNING - Could not set objective direction tag: 'Missing tag key: objective.direction'
2026-01-15 00:05:59 WARNI [training.hpo.execution.local.sweep] Could not set objective direction tag: 'Missing tag key: objective.direction'
2026-01-15 00:05:59,791 - training.hpo.execution.local.sweep - WARNING - Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
2026-01-15 00:05:59 WARNI [training.hpo.execution.local.sweep] Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
2026-01-15 00:05:59,791 - training.hpo.execution.local.sweep - WARNING - Could not set artifact.available tag: 'Missing tag key: artifact.available'
2026-01-15 00:05:59 WARNI [training.hpo.execution.local.sweep] Could not set artifact.available tag: 'Missing tag key: artifact.available'
2026-01-15 00:05:59,791 - training.hpo.execution.local.sweep - INFO -  Set Phase 2 tags on parent run 9fb6a522c0d2...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-15 00:05:59 INFO  [training.hpo.execution.local.sweep]  Set Phase 2 tags on parent run 9fb6a522c0d2...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-15 00:05:59,791 - training.hpo.execution.local.sweep - INFO -  Successfully completed Phase 2 tag setting for parent run 9fb6a522c0d2...
2026-01-15 00:05:59 INFO  [training.hpo.execution.local.sweep]  Successfully completed Phase 2 tag setting for parent run 9fb6a522c0d2...
2026-01-15 00:05:59,791 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.
2026-01-15 00:05:59 INFO  [training.hpo.tracking.cleanup] [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.

  0%|          | 0/1 [00:00<?, ?it/s]2026-01-15 00:05:59,807 - training.hpo.execution.local.sweep - INFO - [Trial 0] fold_splits=present, using CV path
2026-01-15 00:05:59 INFO  [training.hpo.execution.local.sweep] [Trial 0] fold_splits=present, using CV path
2026-01-15 00:05:59,807 - training.hpo.execution.local.sweep - INFO - [Trial 0] Running 2-fold CV
2026-01-15 00:05:59 INFO  [training.hpo.execution.local.sweep] [Trial 0] Running 2-fold CV

                                     

  0%|          | 0/1 [00:00<?, ?it/s]
                                     

  0%|          | 0/1 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s]
2026-01-15 00:05:59,830 - infrastructure.tracking.mlflow.trackers.sweep_tracker - ERROR - [START_SWEEP_RUN] MLflow tracking failed: In v2 study folder study-dc0c421a but trial_base_dir uses legacy naming: study-aaaaaaaa. This indicates a bug in the path construction logic. trial_base_dir=/tmp/pytest-of-codespace/pytest-28/test_resume_workflow_with_cv0/outputs/hpo/local/distilbert/study-aaaaaaaa
2026-01-15 00:05:59 ERROR [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] MLflow tracking failed: In v2 study folder study-dc0c421a but trial_base_dir uses legacy naming: study-aaaaaaaa. This indicates a bug in the path construction logic. trial_base_dir=/tmp/pytest-of-codespace/pytest-28/test_resume_workflow_with_cv0/outputs/hpo/local/distilbert/study-aaaaaaaa
2026-01-15 00:05:59,831 - infrastructure.tracking.mlflow.trackers.sweep_tracker - ERROR - [START_SWEEP_RUN] Traceback: Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/trackers/sweep_tracker.py", line 221, in start_sweep_run
    yield handle
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py", line 1020, in run_local_hpo_sweep
    study.optimize(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/study.py", line 490, in optimize
    _optimize(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/_optimize.py", line 67, in _optimize
    _optimize_sequential(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/_optimize.py", line 164, in _optimize_sequential
    frozen_trial_id = _run_trial(study, func, catch)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/_optimize.py", line 262, in _run_trial
    raise func_err
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py", line 240, in objective
    average_metric, fold_metrics = run_training_trial_with_cv(
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/cv.py", line 323, in run_training_trial_with_cv
    raise RuntimeError(
RuntimeError: In v2 study folder study-dc0c421a but trial_base_dir uses legacy naming: study-aaaaaaaa. This indicates a bug in the path construction logic. trial_base_dir=/tmp/pytest-of-codespace/pytest-28/test_resume_workflow_with_cv0/outputs/hpo/local/distilbert/study-aaaaaaaa

2026-01-15 00:05:59 ERROR [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Traceback: Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/trackers/sweep_tracker.py", line 221, in start_sweep_run
    yield handle
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py", line 1020, in run_local_hpo_sweep
    study.optimize(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/study.py", line 490, in optimize
    _optimize(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/_optimize.py", line 67, in _optimize
    _optimize_sequential(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/_optimize.py", line 164, in _optimize_sequential
    frozen_trial_id = _run_trial(study, func, catch)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/_optimize.py", line 262, in _run_trial
    raise func_err
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py", line 240, in objective
    average_metric, fold_metrics = run_training_trial_with_cv(
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/cv.py", line 323, in run_training_trial_with_cv
    raise RuntimeError(
RuntimeError: In v2 study folder study-dc0c421a but trial_base_dir uses legacy naming: study-aaaaaaaa. This indicates a bug in the path construction logic. trial_base_dir=/tmp/pytest-of-codespace/pytest-28/test_resume_workflow_with_cv0/outputs/hpo/local/distilbert/study-aaaaaaaa

2026-01-15 00:05:59,831 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Continuing HPO without MLflow tracking...
2026-01-15 00:05:59 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker] Continuing HPO without MLflow tracking...
2026-01-15 00:05:59,831 - training.hpo.execution.local.sweep - WARNING - MLflow tracking failed: generator didn't stop after throw()
2026-01-15 00:05:59 WARNI [training.hpo.execution.local.sweep] MLflow tracking failed: generator didn't stop after throw()
2026-01-15 00:05:59,831 - training.hpo.execution.local.sweep - WARNING - Continuing HPO without MLflow tracking...
2026-01-15 00:05:59 WARNI [training.hpo.execution.local.sweep] Continuing HPO without MLflow tracking...
2026-01-15 00:05:59,838 - training.hpo.execution.local.sweep - INFO - [EARLY HASH] Computed v2 study_key_hash=dc0c421a5056294a... for folder creation
2026-01-15 00:05:59 INFO  [training.hpo.execution.local.sweep] [EARLY HASH] Computed v2 study_key_hash=dc0c421a5056294a... for folder creation
2026-01-15 00:05:59,838 - training.hpo.core.study - INFO - [HPO] Resuming optimization for distilbert from checkpoint...
2026-01-15 00:05:59 INFO  [training.hpo.core.study] [HPO] Resuming optimization for distilbert from checkpoint...
2026-01-15 00:05:59,879 - training.hpo.core.study - INFO - Loaded 1 existing trials (0 completed, 0 marked as failed)
2026-01-15 00:05:59 INFO  [training.hpo.core.study] Loaded 1 existing trials (0 completed, 0 marked as failed)
2026-01-15 00:05:59,881 - training.hpo.execution.local.sweep - INFO - [HPO Setup] k_folds=2, fold_splits_file=None, k_fold config: {'enabled': True, 'n_splits': 2, 'random_seed': 42, 'shuffle': True, 'stratified': True}
2026-01-15 00:05:59 INFO  [training.hpo.execution.local.sweep] [HPO Setup] k_folds=2, fold_splits_file=None, k_fold config: {'enabled': True, 'n_splits': 2, 'random_seed': 42, 'shuffle': True, 'stratified': True}
2026-01-15 00:05:59,881 - training.hpo.execution.local.sweep - INFO - [CV Setup]  Created 2 fold splits and saved to /tmp/pytest-of-codespace/pytest-28/test_resume_workflow_with_cv0/outputs/hpo/local/distilbert/study-dc0c421a/fold_splits.json
2026-01-15 00:05:59 INFO  [training.hpo.execution.local.sweep] [CV Setup]  Created 2 fold splits and saved to /tmp/pytest-of-codespace/pytest-28/test_resume_workflow_with_cv0/outputs/hpo/local/distilbert/study-dc0c421a/fold_splits.json
2026-01-15 00:05:59,884 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Using LOCAL MLflow tracking (not Azure ML)
2026-01-15 00:05:59 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker] Using LOCAL MLflow tracking (not Azure ML)
2026-01-15 00:05:59,884 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
2026-01-15 00:05:59 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
2026-01-15 00:05:59,895 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=dc0c421a5056294a...
2026-01-15 00:05:59 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=dc0c421a5056294a...
2026-01-15 00:05:59,895 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=dc0c421a5056294a..., study_family_hash=d1aa8bfffdcfdb77...
2026-01-15 00:05:59 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=dc0c421a5056294a..., study_family_hash=d1aa8bfffdcfdb77...
2026-01-15 00:05:59,971 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Yielding RunHandle. run_id=69cf7cb4487c...
2026-01-15 00:05:59 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Yielding RunHandle. run_id=69cf7cb4487c...
2026-01-15 00:05:59,981 - training.hpo.execution.local.sweep - INFO - [HPO] Parent run 69cf7cb4487c... is now visible in MLflow (status: RUNNING)
2026-01-15 00:05:59 INFO  [training.hpo.execution.local.sweep] [HPO] Parent run 69cf7cb4487c... is now visible in MLflow (status: RUNNING)
2026-01-15 00:05:59,986 - training.hpo.execution.local.sweep - INFO - Setting Phase 2 tags on parent run 69cf7cb4487c... (data_config=present, train_config=present)
2026-01-15 00:05:59 INFO  [training.hpo.execution.local.sweep] Setting Phase 2 tags on parent run 69cf7cb4487c... (data_config=present, train_config=present)
2026-01-15 00:05:59,988 - training.hpo.execution.local.sweep - WARNING - Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
2026-01-15 00:05:59 WARNI [training.hpo.execution.local.sweep] Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
2026-01-15 00:05:59,989 - training.hpo.execution.local.sweep - WARNING - Could not set data fingerprint tag: 'Missing tag key: fingerprint.data'
2026-01-15 00:05:59 WARNI [training.hpo.execution.local.sweep] Could not set data fingerprint tag: 'Missing tag key: fingerprint.data'
2026-01-15 00:05:59,989 - training.hpo.execution.local.sweep - WARNING - Could not set eval fingerprint tag: 'Missing tag key: fingerprint.eval'
2026-01-15 00:05:59 WARNI [training.hpo.execution.local.sweep] Could not set eval fingerprint tag: 'Missing tag key: fingerprint.eval'
2026-01-15 00:05:59,989 - training.hpo.execution.local.sweep - WARNING - Could not set objective direction tag: 'Missing tag key: objective.direction'
2026-01-15 00:05:59 WARNI [training.hpo.execution.local.sweep] Could not set objective direction tag: 'Missing tag key: objective.direction'
2026-01-15 00:05:59,989 - training.hpo.execution.local.sweep - WARNING - Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
2026-01-15 00:05:59 WARNI [training.hpo.execution.local.sweep] Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
2026-01-15 00:05:59,989 - training.hpo.execution.local.sweep - WARNING - Could not set artifact.available tag: 'Missing tag key: artifact.available'
2026-01-15 00:05:59 WARNI [training.hpo.execution.local.sweep] Could not set artifact.available tag: 'Missing tag key: artifact.available'
2026-01-15 00:05:59,989 - training.hpo.execution.local.sweep - INFO -  Set Phase 2 tags on parent run 69cf7cb4487c...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-15 00:05:59 INFO  [training.hpo.execution.local.sweep]  Set Phase 2 tags on parent run 69cf7cb4487c...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-15 00:05:59,989 - training.hpo.execution.local.sweep - INFO -  Successfully completed Phase 2 tag setting for parent run 69cf7cb4487c...
2026-01-15 00:05:59 INFO  [training.hpo.execution.local.sweep]  Successfully completed Phase 2 tag setting for parent run 69cf7cb4487c...
2026-01-15 00:05:59,989 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.
2026-01-15 00:05:59 INFO  [training.hpo.tracking.cleanup] [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.

  0%|          | 0/2 [00:00<?, ?it/s]2026-01-15 00:06:00,005 - training.hpo.execution.local.sweep - INFO - [Trial 1] fold_splits=present, using CV path
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.sweep] [Trial 1] fold_splits=present, using CV path
2026-01-15 00:06:00,005 - training.hpo.execution.local.sweep - INFO - [Trial 1] Running 2-fold CV
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.sweep] [Trial 1] Running 2-fold CV

                                     

  0%|          | 0/2 [00:00<?, ?it/s]
                                     

  0%|          | 0/2 [00:00<?, ?it/s]
  0%|          | 0/2 [00:00<?, ?it/s]
2026-01-15 00:06:00,026 - infrastructure.tracking.mlflow.trackers.sweep_tracker - ERROR - [START_SWEEP_RUN] MLflow tracking failed: In v2 study folder study-dc0c421a but trial_base_dir uses legacy naming: study-aaaaaaaa. This indicates a bug in the path construction logic. trial_base_dir=/tmp/pytest-of-codespace/pytest-28/test_resume_workflow_with_cv0/outputs/hpo/local/distilbert/study-aaaaaaaa
2026-01-15 00:06:00 ERROR [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] MLflow tracking failed: In v2 study folder study-dc0c421a but trial_base_dir uses legacy naming: study-aaaaaaaa. This indicates a bug in the path construction logic. trial_base_dir=/tmp/pytest-of-codespace/pytest-28/test_resume_workflow_with_cv0/outputs/hpo/local/distilbert/study-aaaaaaaa
2026-01-15 00:06:00,027 - infrastructure.tracking.mlflow.trackers.sweep_tracker - ERROR - [START_SWEEP_RUN] Traceback: Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/trackers/sweep_tracker.py", line 221, in start_sweep_run
    yield handle
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py", line 1012, in run_local_hpo_sweep
    study.optimize(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/study.py", line 490, in optimize
    _optimize(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/_optimize.py", line 67, in _optimize
    _optimize_sequential(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/_optimize.py", line 164, in _optimize_sequential
    frozen_trial_id = _run_trial(study, func, catch)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/_optimize.py", line 262, in _run_trial
    raise func_err
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py", line 240, in objective
    average_metric, fold_metrics = run_training_trial_with_cv(
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/cv.py", line 323, in run_training_trial_with_cv
    raise RuntimeError(
RuntimeError: In v2 study folder study-dc0c421a but trial_base_dir uses legacy naming: study-aaaaaaaa. This indicates a bug in the path construction logic. trial_base_dir=/tmp/pytest-of-codespace/pytest-28/test_resume_workflow_with_cv0/outputs/hpo/local/distilbert/study-aaaaaaaa

2026-01-15 00:06:00 ERROR [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Traceback: Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/trackers/sweep_tracker.py", line 221, in start_sweep_run
    yield handle
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py", line 1012, in run_local_hpo_sweep
    study.optimize(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/study.py", line 490, in optimize
    _optimize(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/_optimize.py", line 67, in _optimize
    _optimize_sequential(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/_optimize.py", line 164, in _optimize_sequential
    frozen_trial_id = _run_trial(study, func, catch)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/_optimize.py", line 262, in _run_trial
    raise func_err
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/optuna/study/_optimize.py", line 205, in _run_trial
    value_or_values = func(trial)
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/sweep.py", line 240, in objective
    average_metric, fold_metrics = run_training_trial_with_cv(
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/cv.py", line 323, in run_training_trial_with_cv
    raise RuntimeError(
RuntimeError: In v2 study folder study-dc0c421a but trial_base_dir uses legacy naming: study-aaaaaaaa. This indicates a bug in the path construction logic. trial_base_dir=/tmp/pytest-of-codespace/pytest-28/test_resume_workflow_with_cv0/outputs/hpo/local/distilbert/study-aaaaaaaa

2026-01-15 00:06:00,027 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Continuing HPO without MLflow tracking...
2026-01-15 00:06:00 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker] Continuing HPO without MLflow tracking...
2026-01-15 00:06:00,027 - training.hpo.execution.local.sweep - WARNING - MLflow tracking failed: generator didn't stop after throw()
2026-01-15 00:06:00 WARNI [training.hpo.execution.local.sweep] MLflow tracking failed: generator didn't stop after throw()
2026-01-15 00:06:00,027 - training.hpo.execution.local.sweep - WARNING - Continuing HPO without MLflow tracking...
2026-01-15 00:06:00 WARNI [training.hpo.execution.local.sweep] Continuing HPO without MLflow tracking...
2026-01-15 00:06:00,050 - training.hpo.core.study - INFO - [HPO] Starting optimization for distilbert with checkpointing...
2026-01-15 00:06:00 INFO  [training.hpo.core.study] [HPO] Starting optimization for distilbert with checkpointing...
2026-01-15 00:06:00 WARNI [infrastructure.naming.display_policy] [naming.yaml] Unknown placeholder '{study_name}' in run_names.hpo_sweep (/tmp/pytest-of-codespace/pytest-28/test_setup_hpo_mlflow_run_crea0/config/naming.yaml)
2026-01-15 00:06:00 WARNI [infrastructure.naming.display_policy] [Naming Policy] Missing key in pattern substitution: 'study_name'
2026-01-15 00:06:00,151 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_setup_hpo_mlflow_run_crea0/config, raw_auto_inc_config={}
2026-01-15 00:06:00 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_setup_hpo_mlflow_run_crea0/config, raw_auto_inc_config={}
2026-01-15 00:06:00,151 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-15 00:06:00 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-15 00:06:00 WARNI [infrastructure.naming.display_policy] [naming.yaml] Missing required 'run_names' section (/tmp/pytest-of-codespace/pytest-28/test_setup_hpo_mlflow_run_comp0/config/naming.yaml)
2026-01-15 00:06:00 WARNI [infrastructure.naming.display_policy] [naming.yaml] Unknown placeholder '{study_name}' in run_names.hpo_sweep (/tmp/pytest-of-codespace/pytest-28/test_setup_hpo_mlflow_run_tags0/config/naming.yaml)
2026-01-15 00:06:00 WARNI [infrastructure.naming.display_policy] [Naming Policy] Missing key in pattern substitution: 'study_name'
2026-01-15 00:06:00,158 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_setup_hpo_mlflow_run_tags0/config, raw_auto_inc_config={}
2026-01-15 00:06:00 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_setup_hpo_mlflow_run_tags0/config, raw_auto_inc_config={}
2026-01-15 00:06:00,158 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-15 00:06:00 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-15 00:06:00 WARNI [infrastructure.paths.resolve] Pattern 'hpo_v2' not found in paths.yaml. Using fallback logic.
2026-01-15 00:06:00 WARNI [infrastructure.paths.resolve] Pattern 'hpo_v2' not found in paths.yaml. Using fallback logic.
2026-01-15 00:06:00 WARNI [infrastructure.paths.resolve] Pattern 'hpo_v2' not found in paths.yaml. Using fallback logic.
2026-01-15 00:06:00,235 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'batch_size': 4, 'dropout': 0.2, 'weight_decay': 0.05, 'backbone': 'distilbert'}
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'batch_size': 4, 'dropout': 0.2, 'weight_decay': 0.05, 'backbone': 'distilbert'}
2026-01-15 00:06:00,235 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-15 00:06:00 WARNI [infrastructure.paths.resolve] Pattern 'hpo_v2' not found in paths.yaml. Using fallback logic.
2026-01-15 00:06:00 WARNI [infrastructure.naming.display_policy] [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-28/test_refit_uses_best_trial_hyp0/config/naming.yaml, using empty policy
2026-01-15 00:06:00,238 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_refit_uses_best_trial_hyp0/config/tags.yaml, using defaults
2026-01-15 00:06:00 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_refit_uses_best_trial_hyp0/config/tags.yaml, using defaults
2026-01-15 00:06:00,243 - training.execution.mlflow_setup - WARNING - Could not get parent run parent_123, using experiment directly: Run with id=parent_123 not found
2026-01-15 00:06:00 WARNI [training.execution.mlflow_setup] Could not get parent run parent_123, using experiment directly: Run with id=parent_123 not found
2026-01-15 00:06:00,268 - training.execution.mlflow_setup - INFO -  View run local_distilbert_hpo_refit_legacy at: sqlite:////workspaces/resume-ner-azureml/mlruns/#/experiments/2/runs/294ee4ba557a47ddadc2bacd45721647
2026-01-15 00:06:00 INFO  [training.execution.mlflow_setup]  View run local_distilbert_hpo_refit_legacy at: sqlite:////workspaces/resume-ner-azureml/mlruns/#/experiments/2/runs/294ee4ba557a47ddadc2bacd45721647
2026-01-15 00:06:00,268 - training.execution.mlflow_setup - INFO - Created MLflow run: local_distilbert_hpo_refit_legacy (294ee4ba557a...)
2026-01-15 00:06:00 INFO  [training.execution.mlflow_setup] Created MLflow run: local_distilbert_hpo_refit_legacy (294ee4ba557a...)
2026-01-15 00:06:00,268 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run 294ee4ba557a... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Attempting to link refit run 294ee4ba557a... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:06:00,270 - training.hpo.execution.local.refit - ERROR - [REFIT]  Error linking refit run to trial run: Run with id=parent_123 not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=parent_123 not found
2026-01-15 00:06:00 ERROR [training.hpo.execution.local.refit] [REFIT]  Error linking refit run to trial run: Run with id=parent_123 not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=parent_123 not found
2026-01-15 00:06:00,271 - training.hpo.execution.local.refit - WARNING - [REFIT] Metrics file not found at /tmp/pytest-of-codespace/pytest-28/test_refit_uses_best_trial_hyp0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/metrics.json
2026-01-15 00:06:00 WARNI [training.hpo.execution.local.refit] [REFIT] Metrics file not found at /tmp/pytest-of-codespace/pytest-28/test_refit_uses_best_trial_hyp0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/metrics.json
2026-01-15 00:06:00,272 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run 294ee4ba557a... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Attempting to link refit run 294ee4ba557a... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:06:00,274 - training.hpo.execution.local.refit - ERROR - [REFIT]  Error linking refit run to trial run: Run with id=parent_123 not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=parent_123 not found
2026-01-15 00:06:00 ERROR [training.hpo.execution.local.refit] [REFIT]  Error linking refit run to trial run: Run with id=parent_123 not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=parent_123 not found
2026-01-15 00:06:00,275 - training.hpo.execution.local.refit - INFO - [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-15 00:06:00,275 - training.hpo.execution.local.refit - INFO - [REFIT] Refit training completed. Metrics: N/A, Checkpoint: /tmp/pytest-of-codespace/pytest-28/test_refit_uses_best_trial_hyp0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Refit training completed. Metrics: N/A, Checkpoint: /tmp/pytest-of-codespace/pytest-28/test_refit_uses_best_trial_hyp0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
2026-01-15 00:06:00,282 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-15 00:06:00,282 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-15 00:06:00 WARNI [infrastructure.paths.resolve] Pattern 'hpo_v2' not found in paths.yaml. Using fallback logic.
2026-01-15 00:06:00 WARNI [infrastructure.naming.display_policy] [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-28/test_refit_creates_mlflow_run0/config/naming.yaml, using empty policy
2026-01-15 00:06:00,286 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_refit_creates_mlflow_run0/config/tags.yaml, using defaults
2026-01-15 00:06:00 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_refit_creates_mlflow_run0/config/tags.yaml, using defaults
2026-01-15 00:06:00,292 - training.execution.mlflow_setup - WARNING - Could not get parent run parent_123, using experiment directly: Run with id=parent_123 not found
2026-01-15 00:06:00 WARNI [training.execution.mlflow_setup] Could not get parent run parent_123, using experiment directly: Run with id=parent_123 not found
2026-01-15 00:06:00,303 - training.execution.mlflow_setup - INFO -  View run local_distilbert_hpo_refit_legacy at: sqlite:////workspaces/resume-ner-azureml/mlruns/#/experiments/2/runs/3b8e5ea43d864721a00d4417109ff8bc
2026-01-15 00:06:00 INFO  [training.execution.mlflow_setup]  View run local_distilbert_hpo_refit_legacy at: sqlite:////workspaces/resume-ner-azureml/mlruns/#/experiments/2/runs/3b8e5ea43d864721a00d4417109ff8bc
2026-01-15 00:06:00,303 - training.execution.mlflow_setup - INFO - Created MLflow run: local_distilbert_hpo_refit_legacy (3b8e5ea43d86...)
2026-01-15 00:06:00 INFO  [training.execution.mlflow_setup] Created MLflow run: local_distilbert_hpo_refit_legacy (3b8e5ea43d86...)
2026-01-15 00:06:00,303 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run 3b8e5ea43d86... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Attempting to link refit run 3b8e5ea43d86... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:06:00,306 - training.hpo.execution.local.refit - ERROR - [REFIT]  Error linking refit run to trial run: Run with id=parent_123 not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=parent_123 not found
2026-01-15 00:06:00 ERROR [training.hpo.execution.local.refit] [REFIT]  Error linking refit run to trial run: Run with id=parent_123 not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=parent_123 not found
2026-01-15 00:06:00,307 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run 3b8e5ea43d86... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Attempting to link refit run 3b8e5ea43d86... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:06:00,308 - training.hpo.execution.local.refit - ERROR - [REFIT]  Error linking refit run to trial run: Run with id=parent_123 not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=parent_123 not found
2026-01-15 00:06:00 ERROR [training.hpo.execution.local.refit] [REFIT]  Error linking refit run to trial run: Run with id=parent_123 not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=parent_123 not found
2026-01-15 00:06:00,308 - training.hpo.execution.local.refit - INFO - [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-15 00:06:00,309 - training.hpo.execution.local.refit - INFO - [REFIT] Refit training completed. Metrics: 0.8, Checkpoint: /tmp/pytest-of-codespace/pytest-28/test_refit_creates_mlflow_run0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Refit training completed. Metrics: 0.8, Checkpoint: /tmp/pytest-of-codespace/pytest-28/test_refit_creates_mlflow_run0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
2026-01-15 00:06:00,312 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-15 00:06:00,313 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-15 00:06:00 WARNI [infrastructure.paths.resolve] Pattern 'hpo_v2' not found in paths.yaml. Using fallback logic.
2026-01-15 00:06:00 WARNI [infrastructure.naming.display_policy] [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-28/test_refit_creates_v2_output_d0/config/naming.yaml, using empty policy
2026-01-15 00:06:00,315 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_refit_creates_v2_output_d0/config/tags.yaml, using defaults
2026-01-15 00:06:00 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_refit_creates_v2_output_d0/config/tags.yaml, using defaults
2026-01-15 00:06:00,320 - training.execution.mlflow_setup - WARNING - Could not get parent run parent_123, using experiment directly: Run with id=parent_123 not found
2026-01-15 00:06:00 WARNI [training.execution.mlflow_setup] Could not get parent run parent_123, using experiment directly: Run with id=parent_123 not found
2026-01-15 00:06:00,330 - training.execution.mlflow_setup - INFO -  View run local_distilbert_hpo_refit_legacy at: sqlite:////workspaces/resume-ner-azureml/mlruns/#/experiments/2/runs/536fdabc681d424db4319fd415271287
2026-01-15 00:06:00 INFO  [training.execution.mlflow_setup]  View run local_distilbert_hpo_refit_legacy at: sqlite:////workspaces/resume-ner-azureml/mlruns/#/experiments/2/runs/536fdabc681d424db4319fd415271287
2026-01-15 00:06:00,330 - training.execution.mlflow_setup - INFO - Created MLflow run: local_distilbert_hpo_refit_legacy (536fdabc681d...)
2026-01-15 00:06:00 INFO  [training.execution.mlflow_setup] Created MLflow run: local_distilbert_hpo_refit_legacy (536fdabc681d...)
2026-01-15 00:06:00,330 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run 536fdabc681d... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Attempting to link refit run 536fdabc681d... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:06:00,332 - training.hpo.execution.local.refit - ERROR - [REFIT]  Error linking refit run to trial run: Run with id=parent_123 not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=parent_123 not found
2026-01-15 00:06:00 ERROR [training.hpo.execution.local.refit] [REFIT]  Error linking refit run to trial run: Run with id=parent_123 not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=parent_123 not found
2026-01-15 00:06:00,332 - training.hpo.execution.local.refit - WARNING - [REFIT] Metrics file not found at /tmp/pytest-of-codespace/pytest-28/test_refit_creates_v2_output_d0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/metrics.json
2026-01-15 00:06:00 WARNI [training.hpo.execution.local.refit] [REFIT] Metrics file not found at /tmp/pytest-of-codespace/pytest-28/test_refit_creates_v2_output_d0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/metrics.json
2026-01-15 00:06:00,332 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run 536fdabc681d... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Attempting to link refit run 536fdabc681d... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:06:00,334 - training.hpo.execution.local.refit - ERROR - [REFIT]  Error linking refit run to trial run: Run with id=parent_123 not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=parent_123 not found
2026-01-15 00:06:00 ERROR [training.hpo.execution.local.refit] [REFIT]  Error linking refit run to trial run: Run with id=parent_123 not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=parent_123 not found
2026-01-15 00:06:00,334 - training.hpo.execution.local.refit - INFO - [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-15 00:06:00,334 - training.hpo.execution.local.refit - INFO - [REFIT] Refit training completed. Metrics: N/A, Checkpoint: /tmp/pytest-of-codespace/pytest-28/test_refit_creates_v2_output_d0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Refit training completed. Metrics: N/A, Checkpoint: /tmp/pytest-of-codespace/pytest-28/test_refit_creates_v2_output_d0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
2026-01-15 00:06:00,338 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-15 00:06:00,338 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-15 00:06:00 WARNI [infrastructure.paths.resolve] Pattern 'hpo_v2' not found in paths.yaml. Using fallback logic.
2026-01-15 00:06:00 WARNI [infrastructure.naming.display_policy] [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-28/test_refit_reads_metrics_from_0/config/naming.yaml, using empty policy
2026-01-15 00:06:00,341 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_refit_reads_metrics_from_0/config/tags.yaml, using defaults
2026-01-15 00:06:00 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_refit_reads_metrics_from_0/config/tags.yaml, using defaults
2026-01-15 00:06:00,345 - training.execution.mlflow_setup - WARNING - Could not get parent run parent_123, using experiment directly: Run with id=parent_123 not found
2026-01-15 00:06:00 WARNI [training.execution.mlflow_setup] Could not get parent run parent_123, using experiment directly: Run with id=parent_123 not found
2026-01-15 00:06:00,355 - training.execution.mlflow_setup - INFO -  View run local_distilbert_hpo_refit_legacy at: sqlite:////workspaces/resume-ner-azureml/mlruns/#/experiments/2/runs/c18b0758cbdc458b82a689d46c8ab65e
2026-01-15 00:06:00 INFO  [training.execution.mlflow_setup]  View run local_distilbert_hpo_refit_legacy at: sqlite:////workspaces/resume-ner-azureml/mlruns/#/experiments/2/runs/c18b0758cbdc458b82a689d46c8ab65e
2026-01-15 00:06:00,355 - training.execution.mlflow_setup - INFO - Created MLflow run: local_distilbert_hpo_refit_legacy (c18b0758cbdc...)
2026-01-15 00:06:00 INFO  [training.execution.mlflow_setup] Created MLflow run: local_distilbert_hpo_refit_legacy (c18b0758cbdc...)
2026-01-15 00:06:00,355 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run c18b0758cbdc... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Attempting to link refit run c18b0758cbdc... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:06:00,357 - training.hpo.execution.local.refit - ERROR - [REFIT]  Error linking refit run to trial run: Run with id=parent_123 not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=parent_123 not found
2026-01-15 00:06:00 ERROR [training.hpo.execution.local.refit] [REFIT]  Error linking refit run to trial run: Run with id=parent_123 not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=parent_123 not found
2026-01-15 00:06:00,357 - training.hpo.execution.local.refit - WARNING - [REFIT] Metrics file not found at /tmp/pytest-of-codespace/pytest-28/test_refit_reads_metrics_from_0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/metrics.json
2026-01-15 00:06:00 WARNI [training.hpo.execution.local.refit] [REFIT] Metrics file not found at /tmp/pytest-of-codespace/pytest-28/test_refit_reads_metrics_from_0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/metrics.json
2026-01-15 00:06:00,357 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run c18b0758cbdc... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Attempting to link refit run c18b0758cbdc... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:06:00,359 - training.hpo.execution.local.refit - ERROR - [REFIT]  Error linking refit run to trial run: Run with id=parent_123 not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=parent_123 not found
2026-01-15 00:06:00 ERROR [training.hpo.execution.local.refit] [REFIT]  Error linking refit run to trial run: Run with id=parent_123 not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=parent_123 not found
2026-01-15 00:06:00,359 - training.hpo.execution.local.refit - INFO - [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-15 00:06:00,359 - training.hpo.execution.local.refit - INFO - [REFIT] Refit training completed. Metrics: N/A, Checkpoint: /tmp/pytest-of-codespace/pytest-28/test_refit_reads_metrics_from_0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Refit training completed. Metrics: N/A, Checkpoint: /tmp/pytest-of-codespace/pytest-28/test_refit_reads_metrics_from_0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
2026-01-15 00:06:00,363 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-15 00:06:00,363 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-15 00:06:00 WARNI [infrastructure.paths.resolve] Pattern 'hpo_v2' not found in paths.yaml. Using fallback logic.
2026-01-15 00:06:00 WARNI [infrastructure.naming.display_policy] [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-28/test_refit_logs_metrics_to_mlf0/config/naming.yaml, using empty policy
2026-01-15 00:06:00,366 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_refit_logs_metrics_to_mlf0/config/tags.yaml, using defaults
2026-01-15 00:06:00 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_refit_logs_metrics_to_mlf0/config/tags.yaml, using defaults
2026-01-15 00:06:00,370 - training.execution.mlflow_setup - WARNING - Could not get parent run parent_123, using experiment directly: Run with id=parent_123 not found
2026-01-15 00:06:00 WARNI [training.execution.mlflow_setup] Could not get parent run parent_123, using experiment directly: Run with id=parent_123 not found
2026-01-15 00:06:00,380 - training.execution.mlflow_setup - INFO -  View run local_distilbert_hpo_refit_legacy at: sqlite:////workspaces/resume-ner-azureml/mlruns/#/experiments/2/runs/be4b22d83e9e400e8822fa0705917be0
2026-01-15 00:06:00 INFO  [training.execution.mlflow_setup]  View run local_distilbert_hpo_refit_legacy at: sqlite:////workspaces/resume-ner-azureml/mlruns/#/experiments/2/runs/be4b22d83e9e400e8822fa0705917be0
2026-01-15 00:06:00,380 - training.execution.mlflow_setup - INFO - Created MLflow run: local_distilbert_hpo_refit_legacy (be4b22d83e9e...)
2026-01-15 00:06:00 INFO  [training.execution.mlflow_setup] Created MLflow run: local_distilbert_hpo_refit_legacy (be4b22d83e9e...)
2026-01-15 00:06:00,380 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run be4b22d83e9e... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Attempting to link refit run be4b22d83e9e... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:06:00,382 - training.hpo.execution.local.refit - ERROR - [REFIT]  Error linking refit run to trial run: Run with id=parent_123 not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=parent_123 not found
2026-01-15 00:06:00 ERROR [training.hpo.execution.local.refit] [REFIT]  Error linking refit run to trial run: Run with id=parent_123 not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=parent_123 not found
2026-01-15 00:06:00,382 - training.hpo.execution.local.refit - WARNING - [REFIT] Metrics file not found at /tmp/pytest-of-codespace/pytest-28/test_refit_logs_metrics_to_mlf0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/metrics.json
2026-01-15 00:06:00 WARNI [training.hpo.execution.local.refit] [REFIT] Metrics file not found at /tmp/pytest-of-codespace/pytest-28/test_refit_logs_metrics_to_mlf0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/metrics.json
2026-01-15 00:06:00,382 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run be4b22d83e9e... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Attempting to link refit run be4b22d83e9e... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:06:00,384 - training.hpo.execution.local.refit - ERROR - [REFIT]  Error linking refit run to trial run: Run with id=parent_123 not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=parent_123 not found
2026-01-15 00:06:00 ERROR [training.hpo.execution.local.refit] [REFIT]  Error linking refit run to trial run: Run with id=parent_123 not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=parent_123 not found
2026-01-15 00:06:00,384 - training.hpo.execution.local.refit - INFO - [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-15 00:06:00,384 - training.hpo.execution.local.refit - INFO - [REFIT] Refit training completed. Metrics: N/A, Checkpoint: /tmp/pytest-of-codespace/pytest-28/test_refit_logs_metrics_to_mlf0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Refit training completed. Metrics: N/A, Checkpoint: /tmp/pytest-of-codespace/pytest-28/test_refit_logs_metrics_to_mlf0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
2026-01-15 00:06:00,388 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-15 00:06:00,388 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-15 00:06:00 WARNI [infrastructure.paths.resolve] Pattern 'hpo_v2' not found in paths.yaml. Using fallback logic.
2026-01-15 00:06:00 WARNI [infrastructure.naming.display_policy] [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-28/test_refit_creates_checkpoint_0/config/naming.yaml, using empty policy
2026-01-15 00:06:00,391 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_refit_creates_checkpoint_0/config/tags.yaml, using defaults
2026-01-15 00:06:00 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_refit_creates_checkpoint_0/config/tags.yaml, using defaults
2026-01-15 00:06:00,395 - training.execution.mlflow_setup - WARNING - Could not get parent run parent_123, using experiment directly: Run with id=parent_123 not found
2026-01-15 00:06:00 WARNI [training.execution.mlflow_setup] Could not get parent run parent_123, using experiment directly: Run with id=parent_123 not found
2026-01-15 00:06:00,406 - training.execution.mlflow_setup - INFO -  View run local_distilbert_hpo_refit_legacy at: sqlite:////workspaces/resume-ner-azureml/mlruns/#/experiments/2/runs/221a3604f1c7439bb8142632f774bf0f
2026-01-15 00:06:00 INFO  [training.execution.mlflow_setup]  View run local_distilbert_hpo_refit_legacy at: sqlite:////workspaces/resume-ner-azureml/mlruns/#/experiments/2/runs/221a3604f1c7439bb8142632f774bf0f
2026-01-15 00:06:00,406 - training.execution.mlflow_setup - INFO - Created MLflow run: local_distilbert_hpo_refit_legacy (221a3604f1c7...)
2026-01-15 00:06:00 INFO  [training.execution.mlflow_setup] Created MLflow run: local_distilbert_hpo_refit_legacy (221a3604f1c7...)
2026-01-15 00:06:00,406 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run 221a3604f1c7... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Attempting to link refit run 221a3604f1c7... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:06:00,408 - training.hpo.execution.local.refit - ERROR - [REFIT]  Error linking refit run to trial run: Run with id=parent_123 not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=parent_123 not found
2026-01-15 00:06:00 ERROR [training.hpo.execution.local.refit] [REFIT]  Error linking refit run to trial run: Run with id=parent_123 not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=parent_123 not found
2026-01-15 00:06:00,408 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run 221a3604f1c7... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Attempting to link refit run 221a3604f1c7... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:06:00,410 - training.hpo.execution.local.refit - ERROR - [REFIT]  Error linking refit run to trial run: Run with id=parent_123 not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=parent_123 not found
2026-01-15 00:06:00 ERROR [training.hpo.execution.local.refit] [REFIT]  Error linking refit run to trial run: Run with id=parent_123 not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=parent_123 not found
2026-01-15 00:06:00,410 - training.hpo.execution.local.refit - INFO - [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-15 00:06:00,410 - training.hpo.execution.local.refit - INFO - [REFIT] Refit training completed. Metrics: 0.8, Checkpoint: /tmp/pytest-of-codespace/pytest-28/test_refit_creates_checkpoint_0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Refit training completed. Metrics: 0.8, Checkpoint: /tmp/pytest-of-codespace/pytest-28/test_refit_creates_checkpoint_0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
2026-01-15 00:06:00,414 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-15 00:06:00,414 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-15 00:06:00,414 - infrastructure.paths.utils - WARNING - Could not find project root with src/training/ directory after 5 levels. Using /tmp/pytest-of-codespace/pytest-28/test_refit_enabled_in_smoke_ya0 as root_dir. Config_dir: /tmp/pytest-of-codespace/pytest-28/test_refit_enabled_in_smoke_ya0/config
2026-01-15 00:06:00 WARNI [infrastructure.paths.utils] Could not find project root with src/training/ directory after 5 levels. Using /tmp/pytest-of-codespace/pytest-28/test_refit_enabled_in_smoke_ya0 as root_dir. Config_dir: /tmp/pytest-of-codespace/pytest-28/test_refit_enabled_in_smoke_ya0/config
2026-01-15 00:06:00 WARNI [infrastructure.paths.resolve] Pattern 'hpo_v2' not found in paths.yaml. Using fallback logic.
2026-01-15 00:06:00 WARNI [infrastructure.naming.display_policy] [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-28/test_refit_enabled_in_smoke_ya0/config/naming.yaml, using empty policy
2026-01-15 00:06:00,417 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_refit_enabled_in_smoke_ya0/config/tags.yaml, using defaults
2026-01-15 00:06:00 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_refit_enabled_in_smoke_ya0/config/tags.yaml, using defaults
2026-01-15 00:06:00,421 - training.execution.mlflow_setup - WARNING - Could not get parent run parent_123, using experiment directly: Run with id=parent_123 not found
2026-01-15 00:06:00 WARNI [training.execution.mlflow_setup] Could not get parent run parent_123, using experiment directly: Run with id=parent_123 not found
2026-01-15 00:06:00,432 - training.execution.mlflow_setup - INFO -  View run local_distilbert_hpo_refit_legacy at: sqlite:////workspaces/resume-ner-azureml/mlruns/#/experiments/2/runs/85e4f785cc70427bb41a2596c946d3b4
2026-01-15 00:06:00 INFO  [training.execution.mlflow_setup]  View run local_distilbert_hpo_refit_legacy at: sqlite:////workspaces/resume-ner-azureml/mlruns/#/experiments/2/runs/85e4f785cc70427bb41a2596c946d3b4
2026-01-15 00:06:00,432 - training.execution.mlflow_setup - INFO - Created MLflow run: local_distilbert_hpo_refit_legacy (85e4f785cc70...)
2026-01-15 00:06:00 INFO  [training.execution.mlflow_setup] Created MLflow run: local_distilbert_hpo_refit_legacy (85e4f785cc70...)
2026-01-15 00:06:00,432 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run 85e4f785cc70... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Attempting to link refit run 85e4f785cc70... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:06:00,433 - training.hpo.execution.local.refit - ERROR - [REFIT]  Error linking refit run to trial run: Run with id=parent_123 not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=parent_123 not found
2026-01-15 00:06:00 ERROR [training.hpo.execution.local.refit] [REFIT]  Error linking refit run to trial run: Run with id=parent_123 not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=parent_123 not found
2026-01-15 00:06:00,434 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run 85e4f785cc70... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Attempting to link refit run 85e4f785cc70... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:06:00,436 - training.hpo.execution.local.refit - ERROR - [REFIT]  Error linking refit run to trial run: Run with id=parent_123 not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=parent_123 not found
2026-01-15 00:06:00 ERROR [training.hpo.execution.local.refit] [REFIT]  Error linking refit run to trial run: Run with id=parent_123 not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=parent_123 not found
2026-01-15 00:06:00,436 - training.hpo.execution.local.refit - INFO - [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-15 00:06:00,436 - training.hpo.execution.local.refit - INFO - [REFIT] Refit training completed. Metrics: 0.8, Checkpoint: /tmp/pytest-of-codespace/pytest-28/test_refit_enabled_in_smoke_ya0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Refit training completed. Metrics: 0.8, Checkpoint: /tmp/pytest-of-codespace/pytest-28/test_refit_enabled_in_smoke_ya0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
2026-01-15 00:06:00,441 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-15 00:06:00,441 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-15 00:06:00 WARNI [infrastructure.paths.resolve] Pattern 'hpo_v2' not found in paths.yaml. Using fallback logic.
2026-01-15 00:06:00 WARNI [infrastructure.naming.display_policy] [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-28/test_refit_uses_full_epochs0/config/naming.yaml, using empty policy
2026-01-15 00:06:00,443 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_refit_uses_full_epochs0/config/tags.yaml, using defaults
2026-01-15 00:06:00 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_refit_uses_full_epochs0/config/tags.yaml, using defaults
2026-01-15 00:06:00,448 - training.execution.mlflow_setup - WARNING - Could not get parent run parent_123, using experiment directly: Run with id=parent_123 not found
2026-01-15 00:06:00 WARNI [training.execution.mlflow_setup] Could not get parent run parent_123, using experiment directly: Run with id=parent_123 not found
2026-01-15 00:06:00,458 - training.execution.mlflow_setup - INFO -  View run local_distilbert_hpo_refit_legacy at: sqlite:////workspaces/resume-ner-azureml/mlruns/#/experiments/2/runs/52f75ba9897740b29eb08a14390ddfbc
2026-01-15 00:06:00 INFO  [training.execution.mlflow_setup]  View run local_distilbert_hpo_refit_legacy at: sqlite:////workspaces/resume-ner-azureml/mlruns/#/experiments/2/runs/52f75ba9897740b29eb08a14390ddfbc
2026-01-15 00:06:00,458 - training.execution.mlflow_setup - INFO - Created MLflow run: local_distilbert_hpo_refit_legacy (52f75ba98977...)
2026-01-15 00:06:00 INFO  [training.execution.mlflow_setup] Created MLflow run: local_distilbert_hpo_refit_legacy (52f75ba98977...)
2026-01-15 00:06:00,458 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run 52f75ba98977... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Attempting to link refit run 52f75ba98977... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:06:00,460 - training.hpo.execution.local.refit - ERROR - [REFIT]  Error linking refit run to trial run: Run with id=parent_123 not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=parent_123 not found
2026-01-15 00:06:00 ERROR [training.hpo.execution.local.refit] [REFIT]  Error linking refit run to trial run: Run with id=parent_123 not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=parent_123 not found
2026-01-15 00:06:00,460 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run 52f75ba98977... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Attempting to link refit run 52f75ba98977... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:06:00,462 - training.hpo.execution.local.refit - ERROR - [REFIT]  Error linking refit run to trial run: Run with id=parent_123 not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=parent_123 not found
2026-01-15 00:06:00 ERROR [training.hpo.execution.local.refit] [REFIT]  Error linking refit run to trial run: Run with id=parent_123 not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=parent_123 not found
2026-01-15 00:06:00,462 - training.hpo.execution.local.refit - INFO - [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-15 00:06:00,462 - training.hpo.execution.local.refit - INFO - [REFIT] Refit training completed. Metrics: 0.8, Checkpoint: /tmp/pytest-of-codespace/pytest-28/test_refit_uses_full_epochs0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Refit training completed. Metrics: 0.8, Checkpoint: /tmp/pytest-of-codespace/pytest-28/test_refit_uses_full_epochs0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
2026-01-15 00:06:00,466 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-15 00:06:00,466 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-15 00:06:00 WARNI [infrastructure.paths.resolve] Pattern 'hpo_v2' not found in paths.yaml. Using fallback logic.
2026-01-15 00:06:00 WARNI [infrastructure.naming.display_policy] [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-28/test_refit_skips_checkpoint_fo0/config/naming.yaml, using empty policy
2026-01-15 00:06:00,469 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_refit_skips_checkpoint_fo0/config/tags.yaml, using defaults
2026-01-15 00:06:00 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_refit_skips_checkpoint_fo0/config/tags.yaml, using defaults
2026-01-15 00:06:00,474 - training.execution.mlflow_setup - WARNING - Could not get parent run parent_123, using experiment directly: Run with id=parent_123 not found
2026-01-15 00:06:00 WARNI [training.execution.mlflow_setup] Could not get parent run parent_123, using experiment directly: Run with id=parent_123 not found
2026-01-15 00:06:00,484 - training.execution.mlflow_setup - INFO -  View run local_distilbert_hpo_refit_legacy at: sqlite:////workspaces/resume-ner-azureml/mlruns/#/experiments/2/runs/a6291502c8e94d72be36831126b25736
2026-01-15 00:06:00 INFO  [training.execution.mlflow_setup]  View run local_distilbert_hpo_refit_legacy at: sqlite:////workspaces/resume-ner-azureml/mlruns/#/experiments/2/runs/a6291502c8e94d72be36831126b25736
2026-01-15 00:06:00,484 - training.execution.mlflow_setup - INFO - Created MLflow run: local_distilbert_hpo_refit_legacy (a6291502c8e9...)
2026-01-15 00:06:00 INFO  [training.execution.mlflow_setup] Created MLflow run: local_distilbert_hpo_refit_legacy (a6291502c8e9...)
2026-01-15 00:06:00,484 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run a6291502c8e9... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Attempting to link refit run a6291502c8e9... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:06:00,486 - training.hpo.execution.local.refit - ERROR - [REFIT]  Error linking refit run to trial run: Run with id=parent_123 not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=parent_123 not found
2026-01-15 00:06:00 ERROR [training.hpo.execution.local.refit] [REFIT]  Error linking refit run to trial run: Run with id=parent_123 not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=parent_123 not found
2026-01-15 00:06:00,486 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run a6291502c8e9... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Attempting to link refit run a6291502c8e9... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:06:00,488 - training.hpo.execution.local.refit - ERROR - [REFIT]  Error linking refit run to trial run: Run with id=parent_123 not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=parent_123 not found
2026-01-15 00:06:00 ERROR [training.hpo.execution.local.refit] [REFIT]  Error linking refit run to trial run: Run with id=parent_123 not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=parent_123 not found
2026-01-15 00:06:00,488 - training.hpo.execution.local.refit - INFO - [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-15 00:06:00,488 - training.hpo.execution.local.refit - INFO - [REFIT] Refit training completed. Metrics: 0.8, Checkpoint: /tmp/pytest-of-codespace/pytest-28/test_refit_skips_checkpoint_fo0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Refit training completed. Metrics: 0.8, Checkpoint: /tmp/pytest-of-codespace/pytest-28/test_refit_skips_checkpoint_fo0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
2026-01-15 00:06:00,492 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3e-05, 'backbone': 'distilbert'}
2026-01-15 00:06:00,492 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Computed trial_id='trial_0', run_id=None, trial_number=0
2026-01-15 00:06:00 WARNI [infrastructure.paths.resolve] Pattern 'hpo_v2' not found in paths.yaml. Using fallback logic.
2026-01-15 00:06:00 WARNI [infrastructure.naming.display_policy] [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-28/test_refit_prevents_duplicatio0/config/naming.yaml, using empty policy
2026-01-15 00:06:00,495 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_refit_prevents_duplicatio0/config/tags.yaml, using defaults
2026-01-15 00:06:00 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_refit_prevents_duplicatio0/config/tags.yaml, using defaults
2026-01-15 00:06:00,499 - training.execution.mlflow_setup - WARNING - Could not get parent run parent_123, using experiment directly: Run with id=parent_123 not found
2026-01-15 00:06:00 WARNI [training.execution.mlflow_setup] Could not get parent run parent_123, using experiment directly: Run with id=parent_123 not found
2026-01-15 00:06:00,510 - training.execution.mlflow_setup - INFO -  View run local_distilbert_hpo_refit_legacy at: sqlite:////workspaces/resume-ner-azureml/mlruns/#/experiments/2/runs/b76e01e4f71a434191d3892cb3c50ef3
2026-01-15 00:06:00 INFO  [training.execution.mlflow_setup]  View run local_distilbert_hpo_refit_legacy at: sqlite:////workspaces/resume-ner-azureml/mlruns/#/experiments/2/runs/b76e01e4f71a434191d3892cb3c50ef3
2026-01-15 00:06:00,510 - training.execution.mlflow_setup - INFO - Created MLflow run: local_distilbert_hpo_refit_legacy (b76e01e4f71a...)
2026-01-15 00:06:00 INFO  [training.execution.mlflow_setup] Created MLflow run: local_distilbert_hpo_refit_legacy (b76e01e4f71a...)
2026-01-15 00:06:00,510 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run b76e01e4f71a... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Attempting to link refit run b76e01e4f71a... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:06:00,511 - training.hpo.execution.local.refit - ERROR - [REFIT]  Error linking refit run to trial run: Run with id=parent_123 not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=parent_123 not found
2026-01-15 00:06:00 ERROR [training.hpo.execution.local.refit] [REFIT]  Error linking refit run to trial run: Run with id=parent_123 not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=parent_123 not found
2026-01-15 00:06:00,512 - training.hpo.execution.local.refit - INFO - [REFIT] Attempting to link refit run b76e01e4f71a... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Attempting to link refit run b76e01e4f71a... to trial run. trial_key_hash=present, hpo_parent_run_id=present
2026-01-15 00:06:00,513 - training.hpo.execution.local.refit - ERROR - [REFIT]  Error linking refit run to trial run: Run with id=parent_123 not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=parent_123 not found
2026-01-15 00:06:00 ERROR [training.hpo.execution.local.refit] [REFIT]  Error linking refit run to trial run: Run with id=parent_123 not found. This may affect checkpoint acquisition later.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/training/hpo/execution/local/refit.py", line 539, in _link_refit_to_trial_run
    parent_run = mlflow_client.get_run(hpo_parent_run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 312, in get_run
    return self._tracking_client.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=parent_123 not found
2026-01-15 00:06:00,514 - training.hpo.execution.local.refit - INFO - [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Logged metrics to MLflow (run will be marked FINISHED after artifacts are uploaded)
2026-01-15 00:06:00,514 - training.hpo.execution.local.refit - INFO - [REFIT] Refit training completed. Metrics: 0.8, Checkpoint: /tmp/pytest-of-codespace/pytest-28/test_refit_prevents_duplicatio0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
2026-01-15 00:06:00 INFO  [training.hpo.execution.local.refit] [REFIT] Refit training completed. Metrics: 0.8, Checkpoint: /tmp/pytest-of-codespace/pytest-28/test_refit_prevents_duplicatio0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
2026-01-15 00:06:00,517 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Could not set refit_completed tag: Run with id=refit_run_123 not found
2026-01-15 00:06:00 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker] Could not set refit_completed tag: Run with id=refit_run_123 not found
2026-01-15 00:06:00,517 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_BEST_CHECKPOINT] Using preferred checkpoint directory: /tmp/pytest-of-codespace/pytest-28/test_refit_prevents_duplicatio0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
2026-01-15 00:06:00 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_BEST_CHECKPOINT] Using preferred checkpoint directory: /tmp/pytest-of-codespace/pytest-28/test_refit_prevents_duplicatio0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint
2026-01-15 00:06:00,518 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - Uploading checkpoint archive to MLflow...
2026-01-15 00:06:00 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] Uploading checkpoint archive to MLflow...
2026-01-15 00:06:00,518 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - Uploading checkpoint to refit run refit_run_12 (child of parent parent_123)
2026-01-15 00:06:00 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] Uploading checkpoint to refit run refit_run_12 (child of parent parent_123)
2026-01-15 00:06:00,518 - infrastructure.tracking.mlflow.artifacts.uploader - INFO - Creating checkpoint archive from /tmp/pytest-of-codespace/pytest-28/test_refit_prevents_duplicatio0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint...
2026-01-15 00:06:00 INFO  [infrastructure.tracking.mlflow.artifacts.uploader] Creating checkpoint archive from /tmp/pytest-of-codespace/pytest-28/test_refit_prevents_duplicatio0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-bbbbbbbb/refit/checkpoint...
2026-01-15 00:06:00,519 - infrastructure.tracking.mlflow.artifacts.manager - INFO - Created checkpoint archive: /tmp/checkpoint_qlqi2wu1.tar.gz (1 files, 0.0MB)
2026-01-15 00:06:00 INFO  [infrastructure.tracking.mlflow.artifacts.manager] Created checkpoint archive: /tmp/checkpoint_qlqi2wu1.tar.gz (1 files, 0.0MB)
2026-01-15 00:06:00,519 - infrastructure.tracking.mlflow._artifacts_file - INFO - Uploading checkpoint archive (0.0MB)...
2026-01-15 00:06:00 INFO  [infrastructure.tracking.mlflow._artifacts_file] Uploading checkpoint archive (0.0MB)...
2026-01-15 00:06:00,520 - infrastructure.tracking.mlflow._artifacts_file - WARNING - Failed to upload artifact checkpoint_qlqi2wu1.tar.gz: Run with id=refit_run_123 not found
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/artifacts.py", line 89, in log_artifact_safe
    retry_with_backoff(
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/utils.py", line 64, in retry_with_backoff
    return func()
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/artifacts.py", line 77, in upload_func
    client.log_artifact(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 2533, in log_artifact
    self._tracking_client.log_artifact(run_id, local_path, artifact_path)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 667, in log_artifact
    artifact_repo = self._get_artifact_repo(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 639, in _get_artifact_repo
    run = self.get_run(resource_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=refit_run_123 not found
2026-01-15 00:06:00 WARNI [infrastructure.tracking.mlflow._artifacts_file] Failed to upload artifact checkpoint_qlqi2wu1.tar.gz: Run with id=refit_run_123 not found
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/artifacts.py", line 89, in log_artifact_safe
    retry_with_backoff(
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/utils.py", line 64, in retry_with_backoff
    return func()
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/artifacts.py", line 77, in upload_func
    client.log_artifact(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 2533, in log_artifact
    self._tracking_client.log_artifact(run_id, local_path, artifact_path)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 667, in log_artifact
    artifact_repo = self._get_artifact_repo(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 639, in _get_artifact_repo
    run = self.get_run(resource_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 119, in get_run
    return self.store.get_run(run_id)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 713, in get_run
    run = self._get_run(run_uuid=run_id, session=session, eager=True)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=refit_run_123 not found
2026-01-15 00:06:00,520 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Checkpoint upload returned False (may have been skipped)
2026-01-15 00:06:00 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker] Checkpoint upload returned False (may have been skipped)
2026-01-15 00:06:00,521 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - Marked study as complete with checkpoint uploaded (best trial: 0)
2026-01-15 00:06:00 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] Marked study as complete with checkpoint uploaded (best trial: 0)
2026-01-15 00:06:01,431 - training.hpo.core.study - WARNING - Found 1 RUNNING trials from previous session. Marking them as FAILED (interrupted).
2026-01-15 00:06:01 WARNI [training.hpo.core.study] Found 1 RUNNING trials from previous session. Marking them as FAILED (interrupted).
2026-01-15 00:06:01,569 - training.hpo.core.study - INFO - Found 1 RUNNING trials from previous session. Skipping automatic marking (via config).
2026-01-15 00:06:01 INFO  [training.hpo.core.study] Found 1 RUNNING trials from previous session. Skipping automatic marking (via config).
2026-01-15 00:06:01,607 - training.hpo.execution.local.trial - INFO - [TRIAL] Training completed. Objective metric 'macro-f1': 0.75
2026-01-15 00:06:01 INFO  [training.hpo.execution.local.trial] [TRIAL] Training completed. Objective metric 'macro-f1': 0.75
2026-01-15 00:06:01,612 - training.hpo.execution.local.trial - INFO - [TRIAL] Training completed. Objective metric 'macro-f1': 0.8
2026-01-15 00:06:01 INFO  [training.hpo.execution.local.trial] [TRIAL] Training completed. Objective metric 'macro-f1': 0.8
2026-01-15 00:06:01,617 - training.hpo.execution.local.trial - INFO - [TRIAL] Training completed. Objective metric 'macro-f1': 0.75
2026-01-15 00:06:01 INFO  [training.hpo.execution.local.trial] [TRIAL] Training completed. Objective metric 'macro-f1': 0.75
2026-01-15 00:06:01 WARNI [infrastructure.naming.display_policy] [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-28/test_trial_execution_with_cv_c0/config/naming.yaml, using empty policy
2026-01-15 00:06:01,622 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_trial_execution_with_cv_c0/config/tags.yaml, using defaults
2026-01-15 00:06:01 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_trial_execution_with_cv_c0/config/tags.yaml, using defaults
2026-01-15 00:06:01 WARNI [infrastructure.paths.resolve] Pattern 'hpo_v2' not found in paths.yaml. Using fallback logic.
2026-01-15 00:06:01,623 - training.hpo.execution.local.cv - INFO - [CV] Created trial folder: /tmp/pytest-of-codespace/pytest-28/test_trial_execution_with_cv_c0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-d7c6e3fb (trial 0)
2026-01-15 00:06:01 INFO  [training.hpo.execution.local.cv] [CV] Created trial folder: /tmp/pytest-of-codespace/pytest-28/test_trial_execution_with_cv_c0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-d7c6e3fb (trial 0)
2026-01-15 00:06:01,625 - infrastructure.tracking.mlflow.lifecycle - WARNING - Could not check run status for trial_run_12...: Run with id=trial_run_123 not found
2026-01-15 00:06:01 WARNI [infrastructure.tracking.mlflow.lifecycle] Could not check run status for trial_run_12...: Run with id=trial_run_123 not found
2026-01-15 00:06:01,626 - infrastructure.tracking.mlflow.lifecycle - WARNING - Failed to terminate run trial_run_12...: Run with id=trial_run_123 not found
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/lifecycle.py", line 98, in terminate_run_safe
    client.set_terminated(run_id, status=status)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 3506, in set_terminated
    self._tracking_client.set_terminated(run_id, status, end_time)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 797, in set_terminated
    self.store.update_run_info(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 679, in update_run_info
    run = self._get_run(run_uuid=run_id, session=session)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=trial_run_123 not found
2026-01-15 00:06:01 WARNI [infrastructure.tracking.mlflow.lifecycle] Failed to terminate run trial_run_12...: Run with id=trial_run_123 not found
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/lifecycle.py", line 98, in terminate_run_safe
    client.set_terminated(run_id, status=status)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 3506, in set_terminated
    self._tracking_client.set_terminated(run_id, status, end_time)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 797, in set_terminated
    self.store.update_run_info(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 679, in update_run_info
    run = self._get_run(run_uuid=run_id, session=session)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=trial_run_123 not found
2026-01-15 00:06:01 WARNI [infrastructure.naming.display_policy] [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-28/test_trial_execution_with_cv_c1/config/naming.yaml, using empty policy
2026-01-15 00:06:01,633 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_trial_execution_with_cv_c1/config/tags.yaml, using defaults
2026-01-15 00:06:01 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_trial_execution_with_cv_c1/config/tags.yaml, using defaults
2026-01-15 00:06:01 WARNI [infrastructure.paths.resolve] Pattern 'hpo_v2' not found in paths.yaml. Using fallback logic.
2026-01-15 00:06:01,634 - training.hpo.execution.local.cv - INFO - [CV] Created trial folder: /tmp/pytest-of-codespace/pytest-28/test_trial_execution_with_cv_c1/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-aca2e73e (trial 0)
2026-01-15 00:06:01 INFO  [training.hpo.execution.local.cv] [CV] Created trial folder: /tmp/pytest-of-codespace/pytest-28/test_trial_execution_with_cv_c1/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-aca2e73e (trial 0)
2026-01-15 00:06:01,637 - infrastructure.tracking.mlflow.lifecycle - WARNING - Could not check run status for trial_run_12...: Run with id=trial_run_123 not found
2026-01-15 00:06:01 WARNI [infrastructure.tracking.mlflow.lifecycle] Could not check run status for trial_run_12...: Run with id=trial_run_123 not found
2026-01-15 00:06:01,638 - infrastructure.tracking.mlflow.lifecycle - WARNING - Failed to terminate run trial_run_12...: Run with id=trial_run_123 not found
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/lifecycle.py", line 98, in terminate_run_safe
    client.set_terminated(run_id, status=status)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 3506, in set_terminated
    self._tracking_client.set_terminated(run_id, status, end_time)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 797, in set_terminated
    self.store.update_run_info(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 679, in update_run_info
    run = self._get_run(run_uuid=run_id, session=session)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=trial_run_123 not found
2026-01-15 00:06:01 WARNI [infrastructure.tracking.mlflow.lifecycle] Failed to terminate run trial_run_12...: Run with id=trial_run_123 not found
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/lifecycle.py", line 98, in terminate_run_safe
    client.set_terminated(run_id, status=status)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 3506, in set_terminated
    self._tracking_client.set_terminated(run_id, status, end_time)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 797, in set_terminated
    self.store.update_run_info(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 679, in update_run_info
    run = self._get_run(run_uuid=run_id, session=session)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=trial_run_123 not found
2026-01-15 00:06:01 WARNI [infrastructure.naming.display_policy] [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-28/test_trial_execution_with_cv_a0/config/naming.yaml, using empty policy
2026-01-15 00:06:01,643 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_trial_execution_with_cv_a0/config/tags.yaml, using defaults
2026-01-15 00:06:01 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_trial_execution_with_cv_a0/config/tags.yaml, using defaults
2026-01-15 00:06:01 WARNI [infrastructure.paths.resolve] Pattern 'hpo_v2' not found in paths.yaml. Using fallback logic.
2026-01-15 00:06:01,644 - training.hpo.execution.local.cv - INFO - [CV] Created trial folder: /tmp/pytest-of-codespace/pytest-28/test_trial_execution_with_cv_a0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-4a1ad078 (trial 0)
2026-01-15 00:06:01 INFO  [training.hpo.execution.local.cv] [CV] Created trial folder: /tmp/pytest-of-codespace/pytest-28/test_trial_execution_with_cv_a0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-4a1ad078 (trial 0)
2026-01-15 00:06:01,646 - infrastructure.tracking.mlflow.lifecycle - WARNING - Could not check run status for trial_run_12...: Run with id=trial_run_123 not found
2026-01-15 00:06:01 WARNI [infrastructure.tracking.mlflow.lifecycle] Could not check run status for trial_run_12...: Run with id=trial_run_123 not found
2026-01-15 00:06:01,647 - infrastructure.tracking.mlflow.lifecycle - WARNING - Failed to terminate run trial_run_12...: Run with id=trial_run_123 not found
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/lifecycle.py", line 98, in terminate_run_safe
    client.set_terminated(run_id, status=status)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 3506, in set_terminated
    self._tracking_client.set_terminated(run_id, status, end_time)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 797, in set_terminated
    self.store.update_run_info(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 679, in update_run_info
    run = self._get_run(run_uuid=run_id, session=session)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=trial_run_123 not found
2026-01-15 00:06:01 WARNI [infrastructure.tracking.mlflow.lifecycle] Failed to terminate run trial_run_12...: Run with id=trial_run_123 not found
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/lifecycle.py", line 98, in terminate_run_safe
    client.set_terminated(run_id, status=status)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 3506, in set_terminated
    self._tracking_client.set_terminated(run_id, status, end_time)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 797, in set_terminated
    self.store.update_run_info(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 679, in update_run_info
    run = self._get_run(run_uuid=run_id, session=session)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=trial_run_123 not found
2026-01-15 00:06:01 WARNI [infrastructure.naming.display_policy] [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-28/test_trial_execution_with_cv_o0/config/naming.yaml, using empty policy
2026-01-15 00:06:01,654 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_trial_execution_with_cv_o0/config/tags.yaml, using defaults
2026-01-15 00:06:01 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_trial_execution_with_cv_o0/config/tags.yaml, using defaults
2026-01-15 00:06:01 WARNI [infrastructure.paths.resolve] Pattern 'hpo_v2' not found in paths.yaml. Using fallback logic.
2026-01-15 00:06:01,654 - training.hpo.execution.local.cv - INFO - [CV] Created trial folder: /tmp/pytest-of-codespace/pytest-28/test_trial_execution_with_cv_o0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-4a1ad078 (trial 0)
2026-01-15 00:06:01 INFO  [training.hpo.execution.local.cv] [CV] Created trial folder: /tmp/pytest-of-codespace/pytest-28/test_trial_execution_with_cv_o0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-4a1ad078 (trial 0)
2026-01-15 00:06:01,656 - infrastructure.tracking.mlflow.lifecycle - WARNING - Could not check run status for trial_run_12...: Run with id=trial_run_123 not found
2026-01-15 00:06:01 WARNI [infrastructure.tracking.mlflow.lifecycle] Could not check run status for trial_run_12...: Run with id=trial_run_123 not found
2026-01-15 00:06:01,657 - infrastructure.tracking.mlflow.lifecycle - WARNING - Failed to terminate run trial_run_12...: Run with id=trial_run_123 not found
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/lifecycle.py", line 98, in terminate_run_safe
    client.set_terminated(run_id, status=status)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 3506, in set_terminated
    self._tracking_client.set_terminated(run_id, status, end_time)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 797, in set_terminated
    self.store.update_run_info(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 679, in update_run_info
    run = self._get_run(run_uuid=run_id, session=session)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=trial_run_123 not found
2026-01-15 00:06:01 WARNI [infrastructure.tracking.mlflow.lifecycle] Failed to terminate run trial_run_12...: Run with id=trial_run_123 not found
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/lifecycle.py", line 98, in terminate_run_safe
    client.set_terminated(run_id, status=status)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 3506, in set_terminated
    self._tracking_client.set_terminated(run_id, status, end_time)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 797, in set_terminated
    self.store.update_run_info(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 679, in update_run_info
    run = self._get_run(run_uuid=run_id, session=session)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=trial_run_123 not found
2026-01-15 00:06:01 WARNI [infrastructure.naming.display_policy] [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-28/test_trial_execution_with_cv_s0/config/naming.yaml, using empty policy
2026-01-15 00:06:01,662 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_trial_execution_with_cv_s0/config/tags.yaml, using defaults
2026-01-15 00:06:01 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_trial_execution_with_cv_s0/config/tags.yaml, using defaults
2026-01-15 00:06:01 WARNI [infrastructure.paths.resolve] Pattern 'hpo_v2' not found in paths.yaml. Using fallback logic.
2026-01-15 00:06:01,663 - training.hpo.execution.local.cv - INFO - [CV] Created trial folder: /tmp/pytest-of-codespace/pytest-28/test_trial_execution_with_cv_s0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-4a1ad078 (trial 0)
2026-01-15 00:06:01 INFO  [training.hpo.execution.local.cv] [CV] Created trial folder: /tmp/pytest-of-codespace/pytest-28/test_trial_execution_with_cv_s0/outputs/hpo/local/distilbert/study-aaaaaaaa/trial-4a1ad078 (trial 0)
2026-01-15 00:06:01,665 - infrastructure.tracking.mlflow.lifecycle - WARNING - Could not check run status for trial_run_12...: Run with id=trial_run_123 not found
2026-01-15 00:06:01 WARNI [infrastructure.tracking.mlflow.lifecycle] Could not check run status for trial_run_12...: Run with id=trial_run_123 not found
2026-01-15 00:06:01,666 - infrastructure.tracking.mlflow.lifecycle - WARNING - Failed to terminate run trial_run_12...: Run with id=trial_run_123 not found
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/lifecycle.py", line 98, in terminate_run_safe
    client.set_terminated(run_id, status=status)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 3506, in set_terminated
    self._tracking_client.set_terminated(run_id, status, end_time)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 797, in set_terminated
    self.store.update_run_info(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 679, in update_run_info
    run = self._get_run(run_uuid=run_id, session=session)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=trial_run_123 not found
2026-01-15 00:06:01 WARNI [infrastructure.tracking.mlflow.lifecycle] Failed to terminate run trial_run_12...: Run with id=trial_run_123 not found
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/lifecycle.py", line 98, in terminate_run_safe
    client.set_terminated(run_id, status=status)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/client.py", line 3506, in set_terminated
    self._tracking_client.set_terminated(run_id, status, end_time)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 797, in set_terminated
    self.store.update_run_info(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 679, in update_run_info
    run = self._get_run(run_uuid=run_id, session=session)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/store/tracking/sqlalchemy_store.py", line 604, in _get_run
    raise MlflowException(f"Run with id={run_uuid} not found", RESOURCE_DOES_NOT_EXIST)
mlflow.exceptions.MlflowException: Run with id=trial_run_123 not found
2026-01-15 00:06:01,743 - infrastructure.config.selection - WARNING - top_k_for_stable_score (5) > min_trials_per_group (3). Clamping top_k to 3.
2026-01-15 00:06:01 WARNI [infrastructure.config.selection] top_k_for_stable_score (5) > min_trials_per_group (3). Clamping top_k to 3.
2026-01-15 00:06:01,770 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_auto_generated_study_name0/config, raw_auto_inc_config={}
2026-01-15 00:06:01 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_auto_generated_study_name0/config, raw_auto_inc_config={}
2026-01-15 00:06:01,770 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-15 00:06:01 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-15 00:06:01,774 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_custom_study_name_include0/config, raw_auto_inc_config={}
2026-01-15 00:06:01 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_custom_study_name_include0/config, raw_auto_inc_config={}
2026-01-15 00:06:01,774 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-15 00:06:01 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-15 00:06:01,777 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_auto_generated_with_varia0/config, raw_auto_inc_config={}
2026-01-15 00:06:01 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_auto_generated_with_varia0/config, raw_auto_inc_config={}
2026-01-15 00:06:01,777 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-15 00:06:01 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-15 00:06:01,804 - common.shared.mlflow_setup - INFO - Using local tracking: sqlite:////workspaces/resume-ner-azureml/mlruns/mlflow.db
2026-01-15 00:06:01 INFO  [common.shared.mlflow_setup] Using local tracking: sqlite:////workspaces/resume-ner-azureml/mlruns/mlflow.db
2026-01-15 00:06:01,805 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=407d63530d6a4828...
2026-01-15 00:06:01 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=407d63530d6a4828...
2026-01-15 00:06:01,805 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=407d63530d6a4828..., study_family_hash=c70336361bc2d8f5...
2026-01-15 00:06:01 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=407d63530d6a4828..., study_family_hash=c70336361bc2d8f5...
2026-01-15 00:06:01,806 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Yielding RunHandle. run_id=test-run-123...
2026-01-15 00:06:01 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Yielding RunHandle. run_id=test-run-123...
2026-01-15 00:06:01,806 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Context manager exiting normally. run_id=test-run-123...
2026-01-15 00:06:01 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Context manager exiting normally. run_id=test-run-123...
2026-01-15 00:06:01,812 - common.shared.mlflow_setup - INFO - Using local tracking: sqlite:////workspaces/resume-ner-azureml/mlruns/mlflow.db
2026-01-15 00:06:01 INFO  [common.shared.mlflow_setup] Using local tracking: sqlite:////workspaces/resume-ner-azureml/mlruns/mlflow.db
2026-01-15 00:06:01,815 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Yielding RunHandle. run_id=test-run-123...
2026-01-15 00:06:01 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Yielding RunHandle. run_id=test-run-123...
2026-01-15 00:06:01,816 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Context manager exiting normally. run_id=test-run-123...
2026-01-15 00:06:01 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Context manager exiting normally. run_id=test-run-123...
2026-01-15 00:06:01,821 - common.shared.mlflow_setup - INFO - Using local tracking: sqlite:////workspaces/resume-ner-azureml/mlruns/mlflow.db
2026-01-15 00:06:01 INFO  [common.shared.mlflow_setup] Using local tracking: sqlite:////workspaces/resume-ner-azureml/mlruns/mlflow.db
2026-01-15 00:06:01,823 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=ebdca5d9f9571af3...
2026-01-15 00:06:01 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Computed v2 grouping hashes: study_key_hash=ebdca5d9f9571af3...
2026-01-15 00:06:01,823 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=ebdca5d9f9571af3..., study_family_hash=c70336361bc2d8f5...
2026-01-15 00:06:01 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Computed grouping hashes: study_key_hash=ebdca5d9f9571af3..., study_family_hash=c70336361bc2d8f5...
2026-01-15 00:06:01,824 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Yielding RunHandle. run_id=test-run-123...
2026-01-15 00:06:01 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Yielding RunHandle. run_id=test-run-123...
2026-01-15 00:06:01,824 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [START_SWEEP_RUN] Context manager exiting normally. run_id=test-run-123...
2026-01-15 00:06:01 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Context manager exiting normally. run_id=test-run-123...
2026-01-15 00:06:01,831 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - Found best trial 0 (study_key_hash + trial_number + parentRunId): trial-run-co...
2026-01-15 00:06:01 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] Found best trial 0 (study_key_hash + trial_number + parentRunId): trial-run-co...
2026-01-15 00:06:01,840 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - Found best trial 0 (study_key_hash + trial_number + parentRunId): trial-run-co...
2026-01-15 00:06:01 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] Found best trial 0 (study_key_hash + trial_number + parentRunId): trial-run-co...
2026-01-15 00:06:01,847 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Could not find MLflow run ID for best trial 0. Parent run parent-run-1... missing study_key_hash tag. This may be a timing issue - parent run tags may not be set yet.
2026-01-15 00:06:01 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker] Could not find MLflow run ID for best trial 0. Parent run parent-run-1... missing study_key_hash tag. This may be a timing issue - parent run tags may not be set yet.
2026-01-15 00:06:01,854 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Could not find MLflow run ID for best trial 0. Found 0 runs with matching study_key_hash and trial_number, but none belong to parent parent-run-1... This may be a timing issue - trial runs may not be created/committed yet.
2026-01-15 00:06:01 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker] Could not find MLflow run ID for best trial 0. Found 0 runs with matching study_key_hash and trial_number, but none belong to parent parent-run-1... This may be a timing issue - trial runs may not be created/committed yet.
2026-01-15 00:06:01,860 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Could not find MLflow run ID for best trial 0. Search failed: MLflow error. This may be a timing issue - trial runs may not be created/committed yet.
2026-01-15 00:06:01 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker] Could not find MLflow run ID for best trial 0. Search failed: MLflow error. This may be a timing issue - trial runs may not be created/committed yet.
2026-01-15 00:06:03,123 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-15 00:06:03,127 - evaluation.selection.artifact_unified.acquisition - INFO - Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_id_..., study_key_hash=N/A..., trial_key_hash=N/A...
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.acquisition] Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_id_..., study_key_hash=N/A..., trial_key_hash=N/A...
2026-01-15 00:06:03,374 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-15 00:06:03,378 - evaluation.selection.artifact_unified.acquisition - INFO - Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_id_..., study_key_hash=study_ha..., trial_key_hash=trial_ha...
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.acquisition] Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_id_..., study_key_hash=study_ha..., trial_key_hash=trial_ha...
2026-01-15 00:06:03,380 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-15 00:06:03,383 - evaluation.selection.artifact_unified.acquisition - INFO - Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_id_..., study_key_hash=study_ha..., trial_key_hash=trial_ha...
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.acquisition] Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_id_..., study_key_hash=study_ha..., trial_key_hash=trial_ha...
2026-01-15 00:06:03,388 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-15 00:06:03,391 - evaluation.selection.artifact_unified.acquisition - INFO - Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_id_..., study_key_hash=study_ha..., trial_key_hash=trial_ha...
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.acquisition] Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_id_..., study_key_hash=study_ha..., trial_key_hash=trial_ha...
2026-01-15 00:06:03,397 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-15 00:06:03,400 - evaluation.selection.artifact_unified.acquisition - INFO - Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_id_..., study_key_hash=study_ha..., trial_key_hash=trial_ha...
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.acquisition] Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_id_..., study_key_hash=study_ha..., trial_key_hash=trial_ha...
2026-01-15 00:06:03,402 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-15 00:06:03,405 - evaluation.selection.artifact_unified.acquisition - INFO - Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_id_..., study_key_hash=study_ha..., trial_key_hash=trial_ha...
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.acquisition] Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_id_..., study_key_hash=study_ha..., trial_key_hash=trial_ha...
2026-01-15 00:06:03,411 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-15 00:06:03,413 - evaluation.selection.artifact_unified.acquisition - INFO - Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_id_..., study_key_hash=study_ha..., trial_key_hash=trial_ha...
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.acquisition] Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_id_..., study_key_hash=study_ha..., trial_key_hash=trial_ha...
2026-01-15 00:06:03,419 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-15 00:06:03,422 - evaluation.selection.artifact_unified.acquisition - INFO - Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_id_..., study_key_hash=study_ha..., trial_key_hash=trial_ha...
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.acquisition] Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_id_..., study_key_hash=study_ha..., trial_key_hash=trial_ha...
2026-01-15 00:06:03,428 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-15 00:06:03,431 - evaluation.selection.artifact_unified.acquisition - INFO - Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_id_..., study_key_hash=study_ha..., trial_key_hash=trial_ha...
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.acquisition] Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_id_..., study_key_hash=study_ha..., trial_key_hash=trial_ha...
2026-01-15 00:06:03,436 - evaluation.selection.artifact_unified.acquisition - INFO - Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_123..., study_key_hash=study123..., trial_key_hash=trial876...
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.acquisition] Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_123..., study_key_hash=study123..., trial_key_hash=trial876...
2026-01-15 00:06:03,436 - evaluation.selection.artifact_unified.acquisition - INFO - Selected artifact run: artifact_run_id=refit_run_12..., trial_run_id=test_run_123...
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.acquisition] Selected artifact run: artifact_run_id=refit_run_12..., trial_run_id=test_run_123...
2026-01-15 00:06:03,437 - evaluation.selection.artifact_unified.acquisition - INFO - Artifact acquisition config for checkpoint: priority=['local'], local.enabled=True, local.validate=True, drive.enabled=True, mlflow.enabled=True
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.acquisition] Artifact acquisition config for checkpoint: priority=['local'], local.enabled=True, local.validate=True, drive.enabled=True, mlflow.enabled=True
2026-01-15 00:06:03,437 - evaluation.selection.artifact_unified.acquisition - INFO - Acquiring artifact from local: run_id=refit_run_12..., artifact_kind=checkpoint, backbone=distilbert
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.acquisition] Acquiring artifact from local: run_id=refit_run_12..., artifact_kind=checkpoint, backbone=distilbert
2026-01-15 00:06:03,437 - evaluation.selection.artifact_unified.acquisition - INFO - Copied artifact from local: /tmp/pytest-of-codespace/pytest-28/test_search_roots_used_in_loca0/checkpoint -> /tmp/pytest-of-codespace/pytest-28/test_search_roots_used_in_loca0/outputs/outputs/best_model_selection/local/distilbert/checkpoint_study123_trial876
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.acquisition] Copied artifact from local: /tmp/pytest-of-codespace/pytest-28/test_search_roots_used_in_loca0/checkpoint -> /tmp/pytest-of-codespace/pytest-28/test_search_roots_used_in_loca0/outputs/outputs/best_model_selection/local/distilbert/checkpoint_study123_trial876
2026-01-15 00:06:03,437 - evaluation.selection.artifact_unified.acquisition - INFO - Artifact validation passed: /tmp/pytest-of-codespace/pytest-28/test_search_roots_used_in_loca0/outputs/outputs/best_model_selection/local/distilbert/checkpoint_study123_trial876
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.acquisition] Artifact validation passed: /tmp/pytest-of-codespace/pytest-28/test_search_roots_used_in_loca0/outputs/outputs/best_model_selection/local/distilbert/checkpoint_study123_trial876
2026-01-15 00:06:03,442 - evaluation.selection.artifact_unified.acquisition - INFO - Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_123..., study_key_hash=study123..., trial_key_hash=trial876...
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.acquisition] Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_123..., study_key_hash=study123..., trial_key_hash=trial876...
2026-01-15 00:06:03,442 - evaluation.selection.artifact_unified.acquisition - INFO - Selected artifact run: artifact_run_id=refit_run_12..., trial_run_id=test_run_123...
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.acquisition] Selected artifact run: artifact_run_id=refit_run_12..., trial_run_id=test_run_123...
2026-01-15 00:06:03,442 - evaluation.selection.artifact_unified.acquisition - INFO - Artifact acquisition config for checkpoint: priority=['mlflow', 'local'], local.enabled=True, local.validate=True, drive.enabled=True, mlflow.enabled=True
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.acquisition] Artifact acquisition config for checkpoint: priority=['mlflow', 'local'], local.enabled=True, local.validate=True, drive.enabled=True, mlflow.enabled=True
2026-01-15 00:06:03,442 - evaluation.selection.artifact_unified.acquisition - INFO - Acquiring artifact from mlflow: run_id=refit_run_12..., artifact_kind=checkpoint, backbone=distilbert
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.acquisition] Acquiring artifact from mlflow: run_id=refit_run_12..., artifact_kind=checkpoint, backbone=distilbert
2026-01-15 00:06:03,442 - evaluation.selection.artifact_unified.acquisition - INFO - Downloading artifact from MLflow: run_id=refit_run_12..., destination=/tmp/pytest-of-codespace/pytest-28/test_artifact_kinds_priority_o0/outputs/outputs/best_model_selection/local/distilbert/checkpoint_study123_trial876
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.acquisition] Downloading artifact from MLflow: run_id=refit_run_12..., destination=/tmp/pytest-of-codespace/pytest-28/test_artifact_kinds_priority_o0/outputs/outputs/best_model_selection/local/distilbert/checkpoint_study123_trial876
2026-01-15 00:06:03,443 - evaluation.selection.artifact_unified.acquisition - ERROR - MLflow download failed: run_id=refit_run_12..., artifact_kind=checkpoint, error='Mock' object is not iterable
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/acquisition.py", line 565, in _download_from_mlflow
    artifact_paths = [a.path for a in artifacts]
TypeError: 'Mock' object is not iterable
2026-01-15 00:06:03 ERROR [evaluation.selection.artifact_unified.acquisition] MLflow download failed: run_id=refit_run_12..., artifact_kind=checkpoint, error='Mock' object is not iterable
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/evaluation/selection/artifact_unified/acquisition.py", line 565, in _download_from_mlflow
    artifact_paths = [a.path for a in artifacts]
TypeError: 'Mock' object is not iterable
2026-01-15 00:06:03,443 - evaluation.selection.artifact_unified.acquisition - ERROR - Failed to acquire artifact from mlflow
2026-01-15 00:06:03 ERROR [evaluation.selection.artifact_unified.acquisition] Failed to acquire artifact from mlflow
2026-01-15 00:06:03,446 - evaluation.selection.artifact_unified.acquisition - INFO - Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_123..., study_key_hash=study123..., trial_key_hash=trial876...
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.acquisition] Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_123..., study_key_hash=study123..., trial_key_hash=trial876...
2026-01-15 00:06:03,446 - evaluation.selection.artifact_unified.acquisition - INFO - Selected artifact run: artifact_run_id=refit_run_12..., trial_run_id=test_run_123...
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.acquisition] Selected artifact run: artifact_run_id=refit_run_12..., trial_run_id=test_run_123...
2026-01-15 00:06:03,446 - evaluation.selection.artifact_unified.acquisition - INFO - Artifact acquisition config for checkpoint: priority=['local', 'mlflow'], local.enabled=True, local.validate=True, drive.enabled=True, mlflow.enabled=True
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.acquisition] Artifact acquisition config for checkpoint: priority=['local', 'mlflow'], local.enabled=True, local.validate=True, drive.enabled=True, mlflow.enabled=True
2026-01-15 00:06:03,446 - evaluation.selection.artifact_unified.acquisition - INFO - Acquiring artifact from local: run_id=refit_run_12..., artifact_kind=checkpoint, backbone=distilbert
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.acquisition] Acquiring artifact from local: run_id=refit_run_12..., artifact_kind=checkpoint, backbone=distilbert
2026-01-15 00:06:03,447 - evaluation.selection.artifact_unified.acquisition - INFO - Copied artifact from local: /tmp/pytest-of-codespace/pytest-28/test_artifact_kinds_fallback_t0/checkpoint -> /tmp/pytest-of-codespace/pytest-28/test_artifact_kinds_fallback_t0/outputs/outputs/best_model_selection/local/distilbert/checkpoint_study123_trial876
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.acquisition] Copied artifact from local: /tmp/pytest-of-codespace/pytest-28/test_artifact_kinds_fallback_t0/checkpoint -> /tmp/pytest-of-codespace/pytest-28/test_artifact_kinds_fallback_t0/outputs/outputs/best_model_selection/local/distilbert/checkpoint_study123_trial876
2026-01-15 00:06:03,447 - evaluation.selection.artifact_unified.acquisition - INFO - Artifact validation passed: /tmp/pytest-of-codespace/pytest-28/test_artifact_kinds_fallback_t0/outputs/outputs/best_model_selection/local/distilbert/checkpoint_study123_trial876
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.acquisition] Artifact validation passed: /tmp/pytest-of-codespace/pytest-28/test_artifact_kinds_fallback_t0/outputs/outputs/best_model_selection/local/distilbert/checkpoint_study123_trial876
2026-01-15 00:06:03,452 - evaluation.selection.artifact_unified.acquisition - INFO - Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_123..., study_key_hash=study123..., trial_key_hash=trial876...
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.acquisition] Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_123..., study_key_hash=study123..., trial_key_hash=trial876...
2026-01-15 00:06:03,452 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_config_with_disabled_sour0/config/tags.yaml, using defaults
2026-01-15 00:06:03 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_config_with_disabled_sour0/config/tags.yaml, using defaults
2026-01-15 00:06:03,452 - evaluation.selection.artifact_unified.acquisition - INFO - Selected artifact run: artifact_run_id=test_run_123..., trial_run_id=test_run_123...
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.acquisition] Selected artifact run: artifact_run_id=test_run_123..., trial_run_id=test_run_123...
2026-01-15 00:06:03,453 - evaluation.selection.artifact_unified.acquisition - INFO - Artifact acquisition config for checkpoint: priority=['local', 'drive', 'mlflow'], local.enabled=True, local.validate=True, drive.enabled=False, mlflow.enabled=False
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.acquisition] Artifact acquisition config for checkpoint: priority=['local', 'drive', 'mlflow'], local.enabled=True, local.validate=True, drive.enabled=False, mlflow.enabled=False
2026-01-15 00:06:03,453 - evaluation.selection.artifact_unified.acquisition - ERROR - Artifact not found in any configured source. Checked sources in priority order: ['local', 'drive', 'mlflow']. Request: artifact_kind=checkpoint, run_id=test_run_123..., backbone=distilbert
2026-01-15 00:06:03 ERROR [evaluation.selection.artifact_unified.acquisition] Artifact not found in any configured source. Checked sources in priority order: ['local', 'drive', 'mlflow']. Request: artifact_kind=checkpoint, run_id=test_run_123..., backbone=distilbert
2026-01-15 00:06:03,806 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-15 00:06:03,810 - evaluation.selection.artifact_unified.acquisition - INFO - Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_id_..., study_key_hash=study_ha..., trial_key_hash=trial_ha...
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.acquisition] Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_id_..., study_key_hash=study_ha..., trial_key_hash=trial_ha...
2026-01-15 00:06:03,823 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-15 00:06:03,826 - evaluation.selection.artifact_unified.acquisition - INFO - Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_id_..., study_key_hash=study_ha..., trial_key_hash=trial_ha...
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.acquisition] Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_id_..., study_key_hash=study_ha..., trial_key_hash=trial_ha...
2026-01-15 00:06:03,831 - evaluation.selection.artifact_unified.compat - INFO - Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.compat] Using search_roots from config: ['artifacts', 'best_model_selection'] (from artifact_acquisition.yaml: no, using defaults)
2026-01-15 00:06:03,834 - evaluation.selection.artifact_unified.acquisition - INFO - Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_id_..., study_key_hash=study_ha..., trial_key_hash=trial_ha...
2026-01-15 00:06:03 INFO  [evaluation.selection.artifact_unified.acquisition] Starting artifact acquisition: artifact_kind=checkpoint, backbone=distilbert, run_id=test_run_id_..., study_key_hash=study_ha..., trial_key_hash=trial_ha...
2026-01-15 00:06:03,847 - evaluation.selection.cache - INFO - Saved best model selection cache: /tmp/pytest-of-codespace/pytest-28/test_cache_dual_file_strategy_0/outputs/cache/best_model_selection/latest_best_model_selection.json
2026-01-15 00:06:03 INFO  [evaluation.selection.cache] Saved best model selection cache: /tmp/pytest-of-codespace/pytest-28/test_cache_dual_file_strategy_0/outputs/cache/best_model_selection/latest_best_model_selection.json
2026-01-15 00:06:03,851 - evaluation.selection.cache - INFO - Using cached best model selection: run_id=run-123...
2026-01-15 00:06:03 INFO  [evaluation.selection.cache] Using cached best model selection: run_id=run-123...
2026-01-15 00:06:03,860 - evaluation.selection.cache - INFO - Using cached best model selection: run_id=run-123...
2026-01-15 00:06:03 INFO  [evaluation.selection.cache] Using cached best model selection: run_id=run-123...
2026-01-15 00:06:03,861 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-15 00:06:03,861 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-15 00:06:03,862 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 1
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 1
2026-01-15 00:06:03,862 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-15 00:06:03,862 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.70, Latency=0.30
2026-01-15 00:06:03,862 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from config file, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from config file, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-15 00:06:03,862 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-15 00:06:03,862 - evaluation.selection.mlflow_selection - INFO - Found 0 finished benchmark runs
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Found 0 finished benchmark runs
2026-01-15 00:06:03,862 - evaluation.selection.mlflow_selection - INFO - Found 0 benchmark runs with required metrics and grouping tags
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Found 0 benchmark runs with required metrics and grouping tags
2026-01-15 00:06:03,862 - evaluation.selection.mlflow_selection - WARNING - No valid benchmark runs found
2026-01-15 00:06:03 WARNI [evaluation.selection.mlflow_selection] No valid benchmark runs found
2026-01-15 00:06:03,864 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-15 00:06:03,864 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-15 00:06:03,864 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 1
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 1
2026-01-15 00:06:03,864 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-15 00:06:03,864 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.70, Latency=0.30
2026-01-15 00:06:03,864 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: median (from config file, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: median (from config file, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-15 00:06:03,864 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-15 00:06:03,864 - evaluation.selection.mlflow_selection - INFO - Found 0 finished benchmark runs
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Found 0 finished benchmark runs
2026-01-15 00:06:03,864 - evaluation.selection.mlflow_selection - INFO - Found 0 benchmark runs with required metrics and grouping tags
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Found 0 benchmark runs with required metrics and grouping tags
2026-01-15 00:06:03,864 - evaluation.selection.mlflow_selection - WARNING - No valid benchmark runs found
2026-01-15 00:06:03 WARNI [evaluation.selection.mlflow_selection] No valid benchmark runs found
2026-01-15 00:06:03,866 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-15 00:06:03,866 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-15 00:06:03,866 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 1
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 1
2026-01-15 00:06:03,866 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-15 00:06:03,866 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.70, Latency=0.30
2026-01-15 00:06:03,866 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: mean (from config file, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: mean (from config file, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-15 00:06:03,866 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-15 00:06:03,866 - evaluation.selection.mlflow_selection - INFO - Found 0 finished benchmark runs
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Found 0 finished benchmark runs
2026-01-15 00:06:03,866 - evaluation.selection.mlflow_selection - INFO - Found 0 benchmark runs with required metrics and grouping tags
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Found 0 benchmark runs with required metrics and grouping tags
2026-01-15 00:06:03,866 - evaluation.selection.mlflow_selection - WARNING - No valid benchmark runs found
2026-01-15 00:06:03 WARNI [evaluation.selection.mlflow_selection] No valid benchmark runs found
2026-01-15 00:06:03,877 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-15 00:06:03,877 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-15 00:06:03,877 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 1
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 1
2026-01-15 00:06:03,877 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-15 00:06:03,877 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.80, Latency=0.20
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.80, Latency=0.20
2026-01-15 00:06:03,877 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-15 00:06:03,877 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-15 00:06:03,877 - evaluation.selection.mlflow_selection - INFO - Found 0 finished benchmark runs
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Found 0 finished benchmark runs
2026-01-15 00:06:03,877 - evaluation.selection.mlflow_selection - INFO - Found 0 benchmark runs with required metrics and grouping tags
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Found 0 benchmark runs with required metrics and grouping tags
2026-01-15 00:06:03,877 - evaluation.selection.mlflow_selection - WARNING - No valid benchmark runs found
2026-01-15 00:06:03 WARNI [evaluation.selection.mlflow_selection] No valid benchmark runs found
2026-01-15 00:06:03,879 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-15 00:06:03,879 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-15 00:06:03,879 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 1
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 1
2026-01-15 00:06:03,879 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-15 00:06:03,879 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.70, Latency=0.30
2026-01-15 00:06:03,879 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-15 00:06:03,879 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-15 00:06:03,879 - evaluation.selection.mlflow_selection - INFO - Found 0 finished benchmark runs
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Found 0 finished benchmark runs
2026-01-15 00:06:03,879 - evaluation.selection.mlflow_selection - INFO - Found 0 benchmark runs with required metrics and grouping tags
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Found 0 benchmark runs with required metrics and grouping tags
2026-01-15 00:06:03,879 - evaluation.selection.mlflow_selection - WARNING - No valid benchmark runs found
2026-01-15 00:06:03 WARNI [evaluation.selection.mlflow_selection] No valid benchmark runs found
2026-01-15 00:06:03,881 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-15 00:06:03,881 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-15 00:06:03,881 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 1
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 1
2026-01-15 00:06:03,881 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-15 00:06:03,881 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.70, Latency=0.30
2026-01-15 00:06:03,881 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-15 00:06:03,881 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-15 00:06:03,881 - evaluation.selection.mlflow_selection - INFO - Found 0 finished benchmark runs
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Found 0 finished benchmark runs
2026-01-15 00:06:03,881 - evaluation.selection.mlflow_selection - INFO - Found 0 benchmark runs with required metrics and grouping tags
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Found 0 benchmark runs with required metrics and grouping tags
2026-01-15 00:06:03,881 - evaluation.selection.mlflow_selection - WARNING - No valid benchmark runs found
2026-01-15 00:06:03 WARNI [evaluation.selection.mlflow_selection] No valid benchmark runs found
2026-01-15 00:06:03,883 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-15 00:06:03,883 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-15 00:06:03,884 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 1
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 1
2026-01-15 00:06:03,884 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-15 00:06:03,884 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.70, Latency=0.30
2026-01-15 00:06:03,884 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: median (from config file, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: median (from config file, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-15 00:06:03,884 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-15 00:06:03,884 - evaluation.selection.mlflow_selection - INFO - Found 0 finished benchmark runs
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Found 0 finished benchmark runs
2026-01-15 00:06:03,884 - evaluation.selection.mlflow_selection - INFO - Found 0 benchmark runs with required metrics and grouping tags
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Found 0 benchmark runs with required metrics and grouping tags
2026-01-15 00:06:03,884 - evaluation.selection.mlflow_selection - WARNING - No valid benchmark runs found
2026-01-15 00:06:03 WARNI [evaluation.selection.mlflow_selection] No valid benchmark runs found
2026-01-15 00:06:03,887 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-15 00:06:03,888 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-15 00:06:03,888 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 2
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 2
2026-01-15 00:06:03,888 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-15 00:06:03,888 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.70, Latency=0.30
2026-01-15 00:06:03,888 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-15 00:06:03,888 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-15 00:06:03,888 - evaluation.selection.mlflow_selection - INFO - Found 0 finished benchmark runs
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Found 0 finished benchmark runs
2026-01-15 00:06:03,888 - evaluation.selection.mlflow_selection - INFO - Found 0 benchmark runs with required metrics and grouping tags
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Found 0 benchmark runs with required metrics and grouping tags
2026-01-15 00:06:03,888 - evaluation.selection.mlflow_selection - WARNING - No valid benchmark runs found
2026-01-15 00:06:03 WARNI [evaluation.selection.mlflow_selection] No valid benchmark runs found
2026-01-15 00:06:03,890 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-15 00:06:03,890 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-15 00:06:03,890 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 2
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 2
2026-01-15 00:06:03,890 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-15 00:06:03,890 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.70, Latency=0.30
2026-01-15 00:06:03,891 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-15 00:06:03,891 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-15 00:06:03,891 - evaluation.selection.mlflow_selection - INFO - Found 0 finished benchmark runs
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Found 0 finished benchmark runs
2026-01-15 00:06:03,891 - evaluation.selection.mlflow_selection - INFO - Found 0 benchmark runs with required metrics and grouping tags
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Found 0 benchmark runs with required metrics and grouping tags
2026-01-15 00:06:03,891 - evaluation.selection.mlflow_selection - WARNING - No valid benchmark runs found
2026-01-15 00:06:03 WARNI [evaluation.selection.mlflow_selection] No valid benchmark runs found
2026-01-15 00:06:03,894 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-15 00:06:03,894 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-15 00:06:03,894 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 2
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 2
2026-01-15 00:06:03,894 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-15 00:06:03,894 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.70, Latency=0.30
2026-01-15 00:06:03,894 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-15 00:06:03,894 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-15 00:06:03,895 - evaluation.selection.mlflow_selection - INFO - Found 1 finished benchmark runs
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Found 1 finished benchmark runs
2026-01-15 00:06:03,895 - evaluation.selection.mlflow_selection - INFO - Found 1 benchmark runs with required metrics and grouping tags
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Found 1 benchmark runs with required metrics and grouping tags
2026-01-15 00:06:03,895 - evaluation.selection.mlflow_selection - INFO - Preloading trial and refit runs from HPO experiments...
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Preloading trial and refit runs from HPO experiments...
2026-01-15 00:06:03,895 - evaluation.selection.mlflow_selection - INFO - Built trial lookup with 0 unique (study_hash, trial_hash) pairs
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Built trial lookup with 0 unique (study_hash, trial_hash) pairs
2026-01-15 00:06:03,895 - evaluation.selection.mlflow_selection - INFO - Built refit lookup with 0 unique (study_hash, trial_hash) pairs
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Built refit lookup with 0 unique (study_hash, trial_hash) pairs
2026-01-15 00:06:03,895 - evaluation.selection.mlflow_selection - WARNING - No trial runs found in HPO experiments
2026-01-15 00:06:03 WARNI [evaluation.selection.mlflow_selection] No trial runs found in HPO experiments
2026-01-15 00:06:03,897 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-15 00:06:03,897 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-15 00:06:03,897 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 2
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 2
2026-01-15 00:06:03,897 - evaluation.selection.mlflow_selection - INFO -   Objective metric: accuracy
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Objective metric: accuracy
2026-01-15 00:06:03,897 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.50, Latency=0.50
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.50, Latency=0.50
2026-01-15 00:06:03,898 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-15 00:06:03,898 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-15 00:06:03,898 - evaluation.selection.mlflow_selection - INFO - Found 0 finished benchmark runs
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Found 0 finished benchmark runs
2026-01-15 00:06:03,898 - evaluation.selection.mlflow_selection - INFO - Found 0 benchmark runs with required metrics and grouping tags
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Found 0 benchmark runs with required metrics and grouping tags
2026-01-15 00:06:03,898 - evaluation.selection.mlflow_selection - WARNING - No valid benchmark runs found
2026-01-15 00:06:03 WARNI [evaluation.selection.mlflow_selection] No valid benchmark runs found
2026-01-15 00:06:03,902 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-15 00:06:03,902 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-15 00:06:03,902 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 2
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 2
2026-01-15 00:06:03,902 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-15 00:06:03,902 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.70, Latency=0.30
2026-01-15 00:06:03,902 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-15 00:06:03,902 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-15 00:06:03,902 - evaluation.selection.mlflow_selection - INFO - Found 1 finished benchmark runs
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Found 1 finished benchmark runs
2026-01-15 00:06:03,902 - evaluation.selection.mlflow_selection - INFO - Found 1 benchmark runs with required metrics and grouping tags
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Found 1 benchmark runs with required metrics and grouping tags
2026-01-15 00:06:03,903 - evaluation.selection.mlflow_selection - INFO - Preloading trial and refit runs from HPO experiments...
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Preloading trial and refit runs from HPO experiments...
2026-01-15 00:06:03,903 - evaluation.selection.mlflow_selection - INFO - Built trial lookup with 1 unique (study_hash, trial_hash) pairs
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Built trial lookup with 1 unique (study_hash, trial_hash) pairs
2026-01-15 00:06:03,903 - evaluation.selection.mlflow_selection - INFO - Built refit lookup with 1 unique (study_hash, trial_hash) pairs
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Built refit lookup with 1 unique (study_hash, trial_hash) pairs
2026-01-15 00:06:03,903 - evaluation.selection.mlflow_selection - INFO - Grouped 1 benchmark runs into 1 unique groups (by study_key_hash, trial_key_hash, benchmark_key)
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Grouped 1 benchmark runs into 1 unique groups (by study_key_hash, trial_key_hash, benchmark_key)
2026-01-15 00:06:03,903 - evaluation.selection.mlflow_selection - INFO - Found 1 candidate(s) with both benchmark and training metrics
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Found 1 candidate(s) with both benchmark and training metrics
2026-01-15 00:06:03,903 - evaluation.selection.mlflow_selection - INFO - Best model selected: backbone=distilbert, f1=0.7500, latency=5.00ms, composite=0.3000
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Best model selected: backbone=distilbert, f1=0.7500, latency=5.00ms, composite=0.3000
2026-01-15 00:06:03,906 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-15 00:06:03,906 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-15 00:06:03,906 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 2
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 2
2026-01-15 00:06:03,906 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-15 00:06:03,906 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.70, Latency=0.30
2026-01-15 00:06:03,906 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-15 00:06:03,906 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-15 00:06:03,906 - evaluation.selection.mlflow_selection - INFO - Found 0 finished benchmark runs
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Found 0 finished benchmark runs
2026-01-15 00:06:03,906 - evaluation.selection.mlflow_selection - INFO - Found 0 benchmark runs with required metrics and grouping tags
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Found 0 benchmark runs with required metrics and grouping tags
2026-01-15 00:06:03,906 - evaluation.selection.mlflow_selection - WARNING - No valid benchmark runs found
2026-01-15 00:06:03 WARNI [evaluation.selection.mlflow_selection] No valid benchmark runs found
2026-01-15 00:06:03,908 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-15 00:06:03,908 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-15 00:06:03,908 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 2
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 2
2026-01-15 00:06:03,908 - evaluation.selection.mlflow_selection - INFO -   Objective metric: accuracy
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Objective metric: accuracy
2026-01-15 00:06:03,908 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.50, Latency=0.50
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.50, Latency=0.50
2026-01-15 00:06:03,909 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-15 00:06:03,909 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-15 00:06:03,909 - evaluation.selection.mlflow_selection - INFO - Found 0 finished benchmark runs
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Found 0 finished benchmark runs
2026-01-15 00:06:03,909 - evaluation.selection.mlflow_selection - INFO - Found 0 benchmark runs with required metrics and grouping tags
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Found 0 benchmark runs with required metrics and grouping tags
2026-01-15 00:06:03,909 - evaluation.selection.mlflow_selection - WARNING - No valid benchmark runs found
2026-01-15 00:06:03 WARNI [evaluation.selection.mlflow_selection] No valid benchmark runs found
2026-01-15 00:06:03,916 - evaluation.selection.cache - INFO - Using cached best model selection: run_id=test_run_id_...
2026-01-15 00:06:03 INFO  [evaluation.selection.cache] Using cached best model selection: run_id=test_run_id_...
2026-01-15 00:06:03,925 - evaluation.selection.cache - INFO - Saved best model selection cache: latest.json
2026-01-15 00:06:03 INFO  [evaluation.selection.cache] Saved best model selection cache: latest.json
2026-01-15 00:06:03,930 - evaluation.selection.cache - INFO - Using cached best model selection: run_id=test_run_id_...
2026-01-15 00:06:03 INFO  [evaluation.selection.cache] Using cached best model selection: run_id=test_run_id_...
2026-01-15 00:06:03,948 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-15 00:06:03,948 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-15 00:06:03,948 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 2
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 2
2026-01-15 00:06:03,948 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-15 00:06:03,948 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.70, Latency=0.30
2026-01-15 00:06:03,948 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-15 00:06:03,948 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-15 00:06:03,948 - evaluation.selection.mlflow_selection - INFO - Found 1 finished benchmark runs
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Found 1 finished benchmark runs
2026-01-15 00:06:03,948 - evaluation.selection.mlflow_selection - INFO - Found 0 benchmark runs with required metrics and grouping tags
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Found 0 benchmark runs with required metrics and grouping tags
2026-01-15 00:06:03,948 - evaluation.selection.mlflow_selection - WARNING - No valid benchmark runs found
2026-01-15 00:06:03 WARNI [evaluation.selection.mlflow_selection] No valid benchmark runs found
2026-01-15 00:06:03,950 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-15 00:06:03,950 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-15 00:06:03,950 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 2
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 2
2026-01-15 00:06:03,950 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-15 00:06:03,950 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.70, Latency=0.30
2026-01-15 00:06:03,950 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-15 00:06:03,950 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-15 00:06:03,950 - evaluation.selection.mlflow_selection - INFO - Found 0 finished benchmark runs
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Found 0 finished benchmark runs
2026-01-15 00:06:03,950 - evaluation.selection.mlflow_selection - INFO - Found 0 benchmark runs with required metrics and grouping tags
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Found 0 benchmark runs with required metrics and grouping tags
2026-01-15 00:06:03,950 - evaluation.selection.mlflow_selection - WARNING - No valid benchmark runs found
2026-01-15 00:06:03 WARNI [evaluation.selection.mlflow_selection] No valid benchmark runs found
2026-01-15 00:06:03,955 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-15 00:06:03,956 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-15 00:06:03,956 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 2
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 2
2026-01-15 00:06:03,956 - evaluation.selection.mlflow_selection - INFO -   Objective metric: macro-f1
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Objective metric: macro-f1
2026-01-15 00:06:03,956 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.70, Latency=0.30
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.70, Latency=0.30
2026-01-15 00:06:03,956 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-15 00:06:03,956 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-15 00:06:03,956 - evaluation.selection.mlflow_selection - INFO - Found 0 finished benchmark runs
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Found 0 finished benchmark runs
2026-01-15 00:06:03,956 - evaluation.selection.mlflow_selection - INFO - Found 0 benchmark runs with required metrics and grouping tags
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Found 0 benchmark runs with required metrics and grouping tags
2026-01-15 00:06:03,956 - evaluation.selection.mlflow_selection - WARNING - No valid benchmark runs found
2026-01-15 00:06:03 WARNI [evaluation.selection.mlflow_selection] No valid benchmark runs found
2026-01-15 00:06:03,959 - evaluation.selection.mlflow_selection - INFO - Finding best model from MLflow
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Finding best model from MLflow
2026-01-15 00:06:03,960 - evaluation.selection.mlflow_selection - INFO -   Benchmark experiment: test_experiment-benchmark
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Benchmark experiment: test_experiment-benchmark
2026-01-15 00:06:03,960 - evaluation.selection.mlflow_selection - INFO -   HPO experiments: 2
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   HPO experiments: 2
2026-01-15 00:06:03,960 - evaluation.selection.mlflow_selection - INFO -   Objective metric: accuracy
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Objective metric: accuracy
2026-01-15 00:06:03,960 - evaluation.selection.mlflow_selection - INFO -   Composite weights: F1=0.50, Latency=0.50
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Composite weights: F1=0.50, Latency=0.50
2026-01-15 00:06:03,960 - evaluation.selection.mlflow_selection - INFO -   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection]   Latency aggregation: latest (from default, applied when multiple benchmark runs exist with same benchmark_key)
2026-01-15 00:06:03,960 - evaluation.selection.mlflow_selection - INFO - Querying benchmark runs...
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Querying benchmark runs...
2026-01-15 00:06:03,960 - evaluation.selection.mlflow_selection - INFO - Found 0 finished benchmark runs
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Found 0 finished benchmark runs
2026-01-15 00:06:03,960 - evaluation.selection.mlflow_selection - INFO - Found 0 benchmark runs with required metrics and grouping tags
2026-01-15 00:06:03 INFO  [evaluation.selection.mlflow_selection] Found 0 benchmark runs with required metrics and grouping tags
2026-01-15 00:06:03,960 - evaluation.selection.mlflow_selection - WARNING - No valid benchmark runs found
2026-01-15 00:06:03 WARNI [evaluation.selection.mlflow_selection] No valid benchmark runs found
2026-01-15 00:06:03,965 - evaluation.selection.cache - INFO - Using cached best model selection: run_id=test_run_id_...
2026-01-15 00:06:03 INFO  [evaluation.selection.cache] Using cached best model selection: run_id=test_run_id_...
2026-01-15 00:06:04 WARNI [infrastructure.paths.resolve] Pattern 'final_training_v2' not found in paths.yaml. Using fallback logic.
2026-01-15 00:06:04 WARNI [infrastructure.paths.resolve] Pattern 'final_training_v2' not found in paths.yaml. Using fallback logic.
2026-01-15 00:06:04 WARNI [infrastructure.paths.resolve] Pattern 'conversion_v2' not found in paths.yaml. Using fallback logic.
2026-01-15 00:06:04 WARNI [infrastructure.paths.resolve] Pattern 'final_training_v2' not found in paths.yaml. Using fallback logic.
2026-01-15 00:06:04 WARNI [infrastructure.paths.resolve] Pattern 'final_training_v2' not found in paths.yaml. Using fallback logic.
2026-01-15 00:06:04,653 - infrastructure.tracking.mlflow.trackers.benchmark_tracker - INFO - [START_BENCHMARK_RUN] Building tags: context=None, study_key_hash=None..., trial_key_hash=None..., context.model=None, context.process_type=None
2026-01-15 00:06:04 INFO  [infrastructure.tracking.mlflow.trackers.benchmark_tracker] [START_BENCHMARK_RUN] Building tags: context=None, study_key_hash=None..., trial_key_hash=None..., context.model=None, context.process_type=None
2026-01-15 00:06:04,653 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_benchmark_tracking_enable0/config/tags.yaml, using defaults
2026-01-15 00:06:04 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_benchmark_tracking_enable0/config/tags.yaml, using defaults
2026-01-15 00:06:04,654 - infrastructure.tracking.mlflow.trackers.benchmark_tracker - INFO - [START_BENCHMARK_RUN] Built tags, grouping tags present: study_key_hash=False, trial_key_hash=False, code.model=unknown, code.stage=unknown
2026-01-15 00:06:04 INFO  [infrastructure.tracking.mlflow.trackers.benchmark_tracker] [START_BENCHMARK_RUN] Built tags, grouping tags present: study_key_hash=False, trial_key_hash=False, code.model=unknown, code.stage=unknown
2026-01-15 00:06:04,654 - infrastructure.tracking.mlflow.trackers.benchmark_tracker - WARNING - [START_BENCHMARK_RUN] No valid parent run IDs found. Received: trial=None, refit=None, sweep=None. These may be timestamps or None - will query MLflow if trial_key_hash available.
2026-01-15 00:06:04 WARNI [infrastructure.tracking.mlflow.trackers.benchmark_tracker] [START_BENCHMARK_RUN] No valid parent run IDs found. Received: trial=None, refit=None, sweep=None. These may be timestamps or None - will query MLflow if trial_key_hash available.
2026-01-15 00:06:04,690 - infrastructure.tracking.mlflow.trackers.benchmark_tracker - INFO - [Benchmark Tracker] MLflow tracking disabled for benchmark stage (tracking.benchmark.enabled=false)
2026-01-15 00:06:04 INFO  [infrastructure.tracking.mlflow.trackers.benchmark_tracker] [Benchmark Tracker] MLflow tracking disabled for benchmark stage (tracking.benchmark.enabled=false)
2026-01-15 00:06:04,696 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_training_tracking_enabled0/config/tags.yaml, using defaults
2026-01-15 00:06:04 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_training_tracking_enabled0/config/tags.yaml, using defaults
2026-01-15 00:06:04,710 - infrastructure.tracking.mlflow.trackers.training_tracker - INFO - [Training Tracker] MLflow tracking disabled for training stage (tracking.training.enabled=false)
2026-01-15 00:06:04 INFO  [infrastructure.tracking.mlflow.trackers.training_tracker] [Training Tracker] MLflow tracking disabled for training stage (tracking.training.enabled=false)
2026-01-15 00:06:04,717 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_conversion_tracking_enabl0/config/tags.yaml, using defaults
2026-01-15 00:06:04 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_conversion_tracking_enabl0/config/tags.yaml, using defaults
2026-01-15 00:06:04,732 - infrastructure.tracking.mlflow.trackers.conversion_tracker - INFO - [Conversion Tracker] MLflow tracking disabled for conversion stage (tracking.conversion.enabled=false)
2026-01-15 00:06:04 INFO  [infrastructure.tracking.mlflow.trackers.conversion_tracker] [Conversion Tracker] MLflow tracking disabled for conversion stage (tracking.conversion.enabled=false)
2026-01-15 00:06:04,738 - infrastructure.tracking.mlflow.trackers.benchmark_tracker - INFO - [START_BENCHMARK_RUN] Building tags: context=None, study_key_hash=None..., trial_key_hash=None..., context.model=None, context.process_type=None
2026-01-15 00:06:04 INFO  [infrastructure.tracking.mlflow.trackers.benchmark_tracker] [START_BENCHMARK_RUN] Building tags: context=None, study_key_hash=None..., trial_key_hash=None..., context.model=None, context.process_type=None
2026-01-15 00:06:04,738 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_benchmark_log_artifacts_d0/config/tags.yaml, using defaults
2026-01-15 00:06:04 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_benchmark_log_artifacts_d0/config/tags.yaml, using defaults
2026-01-15 00:06:04,738 - infrastructure.tracking.mlflow.trackers.benchmark_tracker - INFO - [START_BENCHMARK_RUN] Built tags, grouping tags present: study_key_hash=False, trial_key_hash=False, code.model=unknown, code.stage=unknown
2026-01-15 00:06:04 INFO  [infrastructure.tracking.mlflow.trackers.benchmark_tracker] [START_BENCHMARK_RUN] Built tags, grouping tags present: study_key_hash=False, trial_key_hash=False, code.model=unknown, code.stage=unknown
2026-01-15 00:06:04,739 - infrastructure.tracking.mlflow.trackers.benchmark_tracker - WARNING - [START_BENCHMARK_RUN] No valid parent run IDs found. Received: trial=None, refit=None, sweep=None. These may be timestamps or None - will query MLflow if trial_key_hash available.
2026-01-15 00:06:04 WARNI [infrastructure.tracking.mlflow.trackers.benchmark_tracker] [START_BENCHMARK_RUN] No valid parent run IDs found. Received: trial=None, refit=None, sweep=None. These may be timestamps or None - will query MLflow if trial_key_hash available.
2026-01-15 00:06:04,797 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_training_log_checkpoint_d0/config/tags.yaml, using defaults
2026-01-15 00:06:04 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_training_log_checkpoint_d0/config/tags.yaml, using defaults
2026-01-15 00:06:04,810 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_training_log_metrics_json0/config/tags.yaml, using defaults
2026-01-15 00:06:04 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_training_log_metrics_json0/config/tags.yaml, using defaults
2026-01-15 00:06:04,818 - infrastructure.tracking.mlflow.artifacts.uploader - INFO - Creating checkpoint archive from /tmp/pytest-of-codespace/pytest-28/test_training_log_metrics_json0/outputs/final_training/checkpoint...
2026-01-15 00:06:04 INFO  [infrastructure.tracking.mlflow.artifacts.uploader] Creating checkpoint archive from /tmp/pytest-of-codespace/pytest-28/test_training_log_metrics_json0/outputs/final_training/checkpoint...
2026-01-15 00:06:04,818 - infrastructure.tracking.mlflow.artifacts.manager - INFO - Created checkpoint archive: /tmp/checkpoint_z050at68.tar.gz (0 files, 0.0MB)
2026-01-15 00:06:04 INFO  [infrastructure.tracking.mlflow.artifacts.manager] Created checkpoint archive: /tmp/checkpoint_z050at68.tar.gz (0 files, 0.0MB)
2026-01-15 00:06:04,819 - infrastructure.tracking.mlflow._artifacts_file - INFO - Uploading checkpoint archive (0.0MB)...
2026-01-15 00:06:04 INFO  [infrastructure.tracking.mlflow._artifacts_file] Uploading checkpoint archive (0.0MB)...
2026-01-15 00:06:04,825 - infrastructure.tracking.mlflow._artifacts_file - INFO - Successfully uploaded checkpoint archive: checkpoint_z050at68.tar.gz
2026-01-15 00:06:04 INFO  [infrastructure.tracking.mlflow._artifacts_file] Successfully uploaded checkpoint archive: checkpoint_z050at68.tar.gz
2026-01-15 00:06:04,825 - infrastructure.tracking.mlflow.artifacts.uploader - INFO - Successfully uploaded checkpoint archive: 0 files (0.0MB) for trial 0
2026-01-15 00:06:04 INFO  [infrastructure.tracking.mlflow.artifacts.uploader] Successfully uploaded checkpoint archive: 0 files (0.0MB) for trial 0
2026-01-15 00:06:04,831 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_conversion_log_onnx_model0/config/tags.yaml, using defaults
2026-01-15 00:06:04 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_conversion_log_onnx_model0/config/tags.yaml, using defaults
2026-01-15 00:06:04,857 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_conversion_log_conversion0/config/tags.yaml, using defaults
2026-01-15 00:06:04 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_conversion_log_conversion0/config/tags.yaml, using defaults
2026-01-15 00:06:04 INFO  [tracking.mlflow.artifacts] Uploading checkpoint archive (0.0MB)...
2026-01-15 00:06:04 INFO  [tracking.mlflow.artifacts] Successfully uploaded checkpoint archive: tmpbvp1hulb.tar.gz
2026-01-15 00:06:04 INFO  [tracking.mlflow.artifacts] Uploading checkpoint archive (0.0MB)...
2026-01-15 00:06:04 INFO  [tracking.mlflow.artifacts] Successfully uploaded checkpoint archive: tmpnm2zpkss.tar.gz
2026-01-15 00:06:04,933 - infrastructure.tracking.mlflow.lifecycle - INFO - Successfully terminated run refit-run-id... with status FINISHED
2026-01-15 00:06:04 INFO  [infrastructure.tracking.mlflow.lifecycle] Successfully terminated run refit-run-id... with status FINISHED
2026-01-15 00:06:04,936 - infrastructure.tracking.mlflow.lifecycle - INFO - Successfully terminated run refit-run-id... with status FAILED
2026-01-15 00:06:04 INFO  [infrastructure.tracking.mlflow.lifecycle] Successfully terminated run refit-run-id... with status FAILED
2026-01-15 00:06:04,938 - infrastructure.tracking.mlflow.lifecycle - INFO - Run refit-run-id... already has status FINISHED, skipping termination (expected RUNNING)
2026-01-15 00:06:04 INFO  [infrastructure.tracking.mlflow.lifecycle] Run refit-run-id... already has status FINISHED, skipping termination (expected RUNNING)
2026-01-15 00:06:04,972 - orchestration.jobs.tracking.config.loader - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_naming_run_name_config0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-15 00:06:04 INFO  [orchestration.jobs.tracking.config.loader] [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_naming_run_name_config0/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-15 00:06:04,972 - orchestration.jobs.tracking.config.loader - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-15 00:06:04 INFO  [orchestration.jobs.tracking.config.loader] [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-15 00:06:04 WARNI [tracking.mlflow.artifacts] Failed to upload artifact tmp5iuhak91: Upload failed
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/artifacts.py", line 89, in log_artifact_safe
    retry_with_backoff(
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/utils.py", line 64, in retry_with_backoff
    return func()
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/artifacts.py", line 84, in upload_func
    mlflow.log_artifact(
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: Upload failed
2026-01-15 00:06:04 INFO  [tracking.mlflow.artifacts] Uploading 2 files from /tmp/tmprm5d57j7...
2026-01-15 00:06:04 INFO  [tracking.mlflow.artifacts] Successfully uploaded 2 files from tmprm5d57j7 (run_id=active)
2026-01-15 00:06:04 INFO  [tracking.mlflow.artifacts] Uploading checkpoint archive (0.0MB)...
2026-01-15 00:06:04 INFO  [tracking.mlflow.artifacts] Successfully uploaded checkpoint archive: tmp1p9jhb0t.tar.gz
2026-01-15 00:06:05,001 - infrastructure.tracking.mlflow.lifecycle - INFO - Successfully terminated run test-run-id-... with status FINISHED
2026-01-15 00:06:05 INFO  [infrastructure.tracking.mlflow.lifecycle] Successfully terminated run test-run-id-... with status FINISHED
2026-01-15 00:06:05,003 - infrastructure.tracking.mlflow.lifecycle - INFO - Run test-run-id-... already has status FINISHED, skipping termination (expected RUNNING)
2026-01-15 00:06:05 INFO  [infrastructure.tracking.mlflow.lifecycle] Run test-run-id-... already has status FINISHED, skipping termination (expected RUNNING)
2026-01-15 00:06:05,005 - infrastructure.tracking.mlflow.lifecycle - INFO - Successfully terminated run test-run-id-... with status FINISHED
2026-01-15 00:06:05 INFO  [infrastructure.tracking.mlflow.lifecycle] Successfully terminated run test-run-id-... with status FINISHED
2026-01-15 00:06:05,009 - infrastructure.tracking.mlflow.lifecycle - INFO - Successfully terminated run test-run-id-... with status FINISHED
2026-01-15 00:06:05 INFO  [infrastructure.tracking.mlflow.lifecycle] Successfully terminated run test-run-id-... with status FINISHED
2026-01-15 00:06:05,011 - infrastructure.tracking.mlflow.lifecycle - INFO - Successfully terminated run test-run-id-... with status FINISHED
2026-01-15 00:06:05 INFO  [infrastructure.tracking.mlflow.lifecycle] Successfully terminated run test-run-id-... with status FINISHED
2026-01-15 00:06:05,016 - infrastructure.tracking.mlflow.runs - INFO - Created new experiment: new-experiment (exp-456)
2026-01-15 00:06:05 INFO  [infrastructure.tracking.mlflow.runs] Created new experiment: new-experiment (exp-456)
2026-01-15 00:06:05 WARNI [infrastructure.paths.resolve] Failed to load paths.yaml config: [Errno 13] Permission denied: '/root/config/paths.yaml'. Using fallback logic.
2026-01-15 00:06:05 WARNI [infrastructure.paths.resolve] Failed to load paths.yaml config: [Errno 13] Permission denied: '/root/config/paths.yaml'. Using fallback logic.
2026-01-15 00:06:05 WARNI [infrastructure.paths.resolve] Failed to load paths.yaml config: [Errno 13] Permission denied: '/root/config/paths.yaml'. Using fallback logic.
2026-01-15 00:06:05 WARNI [infrastructure.paths.resolve] Failed to load paths.yaml config: [Errno 13] Permission denied: '/root/config/paths.yaml'. Using fallback logic.
2026-01-15 00:06:05 WARNI [infrastructure.paths.resolve] Failed to load paths.yaml config: [Errno 13] Permission denied: '/root/config/paths.yaml'. Using fallback logic.
2026-01-15 00:06:05 WARNI [infrastructure.paths.resolve] Failed to load paths.yaml config: [Errno 13] Permission denied: '/root/config/paths.yaml'. Using fallback logic.
2026-01-15 00:06:05,052 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_hpo_trial_run_name0/config, raw_auto_inc_config={}
2026-01-15 00:06:05 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_hpo_trial_run_name0/config, raw_auto_inc_config={}
2026-01-15 00:06:05,052 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-15 00:06:05 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-15 00:06:05,056 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_hpo_trial_fold_run_name0/config, raw_auto_inc_config={}
2026-01-15 00:06:05 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_hpo_trial_fold_run_name0/config, raw_auto_inc_config={}
2026-01-15 00:06:05,056 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo_trial_fold
2026-01-15 00:06:05 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo_trial_fold
2026-01-15 00:06:05 WARNI [infrastructure.naming.display_policy] [Naming Policy] No pattern for process_type: hpo_sweep
2026-01-15 00:06:05,060 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_hpo_refit_run_name0/config, raw_auto_inc_config={}
2026-01-15 00:06:05 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_hpo_refit_run_name0/config, raw_auto_inc_config={}
2026-01-15 00:06:05,060 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-15 00:06:05 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-15 00:06:05,063 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_hpo_sweep_run_name0/config, raw_auto_inc_config={}
2026-01-15 00:06:05 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_hpo_sweep_run_name0/config, raw_auto_inc_config={}
2026-01-15 00:06:05,063 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-15 00:06:05 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-15 00:06:05 WARNI [infrastructure.naming.display_policy] [Naming Policy] Missing key in pattern substitution: 'study_hash'
2026-01-15 00:06:05,066 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_run_name_max_length0/config, raw_auto_inc_config={}
2026-01-15 00:06:05 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_run_name_max_length0/config, raw_auto_inc_config={}
2026-01-15 00:06:05,066 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-15 00:06:05 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-15 00:06:05 WARNI [infrastructure.naming.display_policy] [Naming Policy] Missing key in pattern substitution: 'study_hash'
2026-01-15 00:06:05,069 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_run_name_forbidden_chars_0/config, raw_auto_inc_config={}
2026-01-15 00:06:05 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Loading from config_dir=/tmp/pytest-of-codespace/pytest-28/test_run_name_forbidden_chars_0/config, raw_auto_inc_config={}
2026-01-15 00:06:05,069 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-15 00:06:05 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Validated config: {'enabled': False, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-15 00:06:05,086 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-15 00:06:05 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-15 00:06:05,086 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=final_training
2026-01-15 00:06:05 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=final_training
2026-01-15 00:06:05 ERROR [orchestration.jobs.tracking.naming.policy] [Naming Policy] Run name exceeds max length (256): aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa... (length: 300)
2026-01-15 00:06:05 ERROR [orchestration.jobs.tracking.naming.policy] [Naming Policy] Run name contains forbidden characters ['/']: local/distilbert/hpo_trial...
2026-01-15 00:06:05 WARNI [orchestration.jobs.tracking.naming.policy] [Naming Policy] Run name exceeds recommended length (150): aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa... (length: 200)
2026-01-15 00:06:05,155 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Could not find MLflow run ID for best trial 0. Found 1 runs with matching study_key_hash and trial_number, but none belong to parent parent-run-i... This may be a timing issue - trial runs may not be created/committed yet.
2026-01-15 00:06:05 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker] Could not find MLflow run ID for best trial 0. Found 1 runs with matching study_key_hash and trial_number, but none belong to parent parent-run-i... This may be a timing issue - trial runs may not be created/committed yet.
2026-01-15 00:06:05,168 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Could not find MLflow run ID for best trial 0. Found 1 runs with matching study_key_hash and trial_number, but none belong to parent parent-run-i... This may be a timing issue - trial runs may not be created/committed yet.
2026-01-15 00:06:05 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker] Could not find MLflow run ID for best trial 0. Found 1 runs with matching study_key_hash and trial_number, but none belong to parent parent-run-i... This may be a timing issue - trial runs may not be created/committed yet.
2026-01-15 00:06:05,571 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Could not find MLflow run ID for best trial 0. Found 1 runs with matching study_key_hash and trial_number, but none belong to parent parent-run-i... This may be a timing issue - trial runs may not be created/committed yet.
2026-01-15 00:06:05 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker] Could not find MLflow run ID for best trial 0. Found 1 runs with matching study_key_hash and trial_number, but none belong to parent parent-run-i... This may be a timing issue - trial runs may not be created/committed yet.
2026-01-15 00:06:05,582 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=parent-run-i...
2026-01-15 00:06:05 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=parent-run-i...
2026-01-15 00:06:05,582 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Found 0 completed trials out of 1 total
2026-01-15 00:06:05 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Found 0 completed trials out of 1 total
2026-01-15 00:06:05,583 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging metrics: n_trials=1, n_completed_trials=0
2026-01-15 00:06:05 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Logging metrics: n_trials=1, n_completed_trials=0
2026-01-15 00:06:05,583 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=0.5
2026-01-15 00:06:05 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=0.5
2026-01-15 00:06:05,583 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging best hyperparameters: {'learning_rate': 0.0001}
2026-01-15 00:06:05 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Logging best hyperparameters: {'learning_rate': 0.0001}
2026-01-15 00:06:05,583 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Finding and logging best trial run ID...
2026-01-15 00:06:05 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Finding and logging best trial run ID...
2026-01-15 00:06:05,583 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Could not find MLflow run ID for best trial 0. Found 1 runs with matching study_key_hash and trial_number, but none belong to parent parent-run-i... This may be a timing issue - trial runs may not be created/committed yet.
2026-01-15 00:06:05 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker] Could not find MLflow run ID for best trial 0. Found 1 runs with matching study_key_hash and trial_number, but none belong to parent parent-run-i... This may be a timing issue - trial runs may not be created/committed yet.
2026-01-15 00:06:05,583 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Completed best trial ID logging
2026-01-15 00:06:05 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Completed best trial ID logging
2026-01-15 00:06:05,584 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Checkpoint logging enabled: None (upload_checkpoint=True)
2026-01-15 00:06:05 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Checkpoint logging enabled: None (upload_checkpoint=True)
2026-01-15 00:06:05,584 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
2026-01-15 00:06:05 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
2026-01-15 00:06:05,584 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Completed log_final_metrics successfully
2026-01-15 00:06:05 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Completed log_final_metrics successfully
2026-01-15 00:06:05,594 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=parent-run-i...
2026-01-15 00:06:05 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Starting log_final_metrics for parent_run_id=parent-run-i...
2026-01-15 00:06:05,595 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Found 0 completed trials out of 1 total
2026-01-15 00:06:05 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Found 0 completed trials out of 1 total
2026-01-15 00:06:05,595 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging metrics: n_trials=1, n_completed_trials=0
2026-01-15 00:06:05 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Logging metrics: n_trials=1, n_completed_trials=0
2026-01-15 00:06:05,595 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=0.5
2026-01-15 00:06:05 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Logging best trial metrics: macro-f1=0.5
2026-01-15 00:06:05,595 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Logging best hyperparameters: {'learning_rate': 0.0001}
2026-01-15 00:06:05 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Logging best hyperparameters: {'learning_rate': 0.0001}
2026-01-15 00:06:05,595 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Finding and logging best trial run ID...
2026-01-15 00:06:05 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Finding and logging best trial run ID...
2026-01-15 00:06:05,595 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Could not find MLflow run ID for best trial 0. Found 1 runs with matching study_key_hash and trial_number, but none belong to parent parent-run-i... This may be a timing issue - trial runs may not be created/committed yet.
2026-01-15 00:06:05 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker] Could not find MLflow run ID for best trial 0. Found 1 runs with matching study_key_hash and trial_number, but none belong to parent parent-run-i... This may be a timing issue - trial runs may not be created/committed yet.
2026-01-15 00:06:05,596 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Completed best trial ID logging
2026-01-15 00:06:05 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Completed best trial ID logging
2026-01-15 00:06:05,596 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Checkpoint logging enabled: None (upload_checkpoint=True)
2026-01-15 00:06:05 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Checkpoint logging enabled: None (upload_checkpoint=True)
2026-01-15 00:06:05,596 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
2026-01-15 00:06:05 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Checkpoint logging deferred (will upload after refit if available)
2026-01-15 00:06:05,596 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - [LOG_FINAL_METRICS] Completed log_final_metrics successfully
2026-01-15 00:06:05 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] [LOG_FINAL_METRICS] Completed log_final_metrics successfully
2026-01-15 00:06:05,606 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Could not find MLflow run ID for best trial 0. Found 1 runs with matching study_key_hash and trial_number, but none belong to parent parent-run-i... This may be a timing issue - trial runs may not be created/committed yet.
2026-01-15 00:06:05 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker] Could not find MLflow run ID for best trial 0. Found 1 runs with matching study_key_hash and trial_number, but none belong to parent parent-run-i... This may be a timing issue - trial runs may not be created/committed yet.
2026-01-15 00:06:05,621 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_load_tags_registry_fallba0/config/tags.yaml, using defaults
2026-01-15 00:06:05 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_load_tags_registry_fallba0/config/tags.yaml, using defaults
2026-01-15 00:06:05,642 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_build_mlflow_tags_minimal0/config/tags.yaml, using defaults
2026-01-15 00:06:05 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_build_mlflow_tags_minimal0/config/tags.yaml, using defaults
2026-01-15 00:06:05,645 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_build_mlflow_tags_hpo_pro0/config/tags.yaml, using defaults
2026-01-15 00:06:05 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_build_mlflow_tags_hpo_pro0/config/tags.yaml, using defaults
2026-01-15 00:06:05,647 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_build_mlflow_tags_hpo_ref0/config/tags.yaml, using defaults
2026-01-15 00:06:05 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_build_mlflow_tags_hpo_ref0/config/tags.yaml, using defaults
2026-01-15 00:06:05,649 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_build_mlflow_tags_benchma0/config/tags.yaml, using defaults
2026-01-15 00:06:05 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_build_mlflow_tags_benchma0/config/tags.yaml, using defaults
2026-01-15 00:06:05,652 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_build_mlflow_tags_final_t0/config/tags.yaml, using defaults
2026-01-15 00:06:05 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_build_mlflow_tags_final_t0/config/tags.yaml, using defaults
2026-01-15 00:06:05,654 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_build_mlflow_tags_convers0/config/tags.yaml, using defaults
2026-01-15 00:06:05 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_build_mlflow_tags_convers0/config/tags.yaml, using defaults
2026-01-15 00:06:05,656 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_build_mlflow_tags_optiona0/config/tags.yaml, using defaults
2026-01-15 00:06:05 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_build_mlflow_tags_optiona0/config/tags.yaml, using defaults
2026-01-15 00:06:05,672 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_get_tag_key_falls_back_to0/config/tags.yaml, using defaults
2026-01-15 00:06:05 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_get_tag_key_falls_back_to0/config/tags.yaml, using defaults
2026-01-15 00:06:05,674 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_get_tag_key_raises_when_m0/config/tags.yaml, using defaults
2026-01-15 00:06:05 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_get_tag_key_raises_when_m0/config/tags.yaml, using defaults
2026-01-15 00:06:05,677 - infrastructure.naming.mlflow.tags_registry - WARNING - [Tags Registry] Failed to load config from /tmp/pytest-of-codespace/pytest-28/test_get_tag_key_handles_regis0/config/tags.yaml: mapping values are not allowed here
  in "/tmp/pytest-of-codespace/pytest-28/test_get_tag_key_handles_regis0/config/tags.yaml", line 1, column 14. Using defaults.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/naming/mlflow/tags_registry.py", line 206, in load_tags_registry
    loaded_data = load_yaml(config_path)
  File "/workspaces/resume-ner-azureml/src/common/shared/yaml_utils.py", line 26, in load_yaml
    return yaml.safe_load(handle)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/__init__.py", line 125, in safe_load
    return load(stream, SafeLoader)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/composer.py", line 36, in get_single_node
    document = self.compose_document()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/composer.py", line 55, in compose_document
    node = self.compose_node(None, None)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/composer.py", line 84, in compose_node
    node = self.compose_mapping_node(anchor)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/composer.py", line 127, in compose_mapping_node
    while not self.check_event(MappingEndEvent):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/parser.py", line 98, in check_event
    self.current_event = self.state()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/parser.py", line 428, in parse_block_mapping_key
    if self.check_token(KeyToken):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/scanner.py", line 116, in check_token
    self.fetch_more_tokens()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/scanner.py", line 223, in fetch_more_tokens
    return self.fetch_value()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/scanner.py", line 577, in fetch_value
    raise ScannerError(None, None,
yaml.scanner.ScannerError: mapping values are not allowed here
  in "/tmp/pytest-of-codespace/pytest-28/test_get_tag_key_handles_regis0/config/tags.yaml", line 1, column 14
2026-01-15 00:06:05 WARNI [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Failed to load config from /tmp/pytest-of-codespace/pytest-28/test_get_tag_key_handles_regis0/config/tags.yaml: mapping values are not allowed here
  in "/tmp/pytest-of-codespace/pytest-28/test_get_tag_key_handles_regis0/config/tags.yaml", line 1, column 14. Using defaults.
Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/naming/mlflow/tags_registry.py", line 206, in load_tags_registry
    loaded_data = load_yaml(config_path)
  File "/workspaces/resume-ner-azureml/src/common/shared/yaml_utils.py", line 26, in load_yaml
    return yaml.safe_load(handle)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/__init__.py", line 125, in safe_load
    return load(stream, SafeLoader)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/__init__.py", line 81, in load
    return loader.get_single_data()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/constructor.py", line 49, in get_single_data
    node = self.get_single_node()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/composer.py", line 36, in get_single_node
    document = self.compose_document()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/composer.py", line 55, in compose_document
    node = self.compose_node(None, None)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/composer.py", line 84, in compose_node
    node = self.compose_mapping_node(anchor)
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/composer.py", line 127, in compose_mapping_node
    while not self.check_event(MappingEndEvent):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/parser.py", line 98, in check_event
    self.current_event = self.state()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/parser.py", line 428, in parse_block_mapping_key
    if self.check_token(KeyToken):
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/scanner.py", line 116, in check_token
    self.fetch_more_tokens()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/scanner.py", line 223, in fetch_more_tokens
    return self.fetch_value()
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/yaml/scanner.py", line 577, in fetch_value
    raise ScannerError(None, None,
yaml.scanner.ScannerError: mapping values are not allowed here
  in "/tmp/pytest-of-codespace/pytest-28/test_get_tag_key_handles_regis0/config/tags.yaml", line 1, column 14
2026-01-15 00:06:05,687 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_load_tags_registry_missin0/config/tags.yaml, using defaults
2026-01-15 00:06:05 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /tmp/pytest-of-codespace/pytest-28/test_load_tags_registry_missin0/config/tags.yaml, using defaults
2026-01-15 00:06:05,708 - training.hpo.execution.local.sweep - WARNING - Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
2026-01-15 00:06:05 WARNI [training.hpo.execution.local.sweep] Could not set schema_version tag: 'Missing tag key: study.key_schema_version'
2026-01-15 00:06:05,708 - training.hpo.execution.local.sweep - WARNING - Could not set objective direction tag: 'Missing tag key: objective.direction'
2026-01-15 00:06:05 WARNI [training.hpo.execution.local.sweep] Could not set objective direction tag: 'Missing tag key: objective.direction'
2026-01-15 00:06:05,708 - training.hpo.execution.local.sweep - WARNING - Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
2026-01-15 00:06:05 WARNI [training.hpo.execution.local.sweep] Could not set objective direction tag (fallback): 'Missing tag key: objective.direction'
2026-01-15 00:06:05,708 - training.hpo.execution.local.sweep - WARNING - Could not set artifact.available tag: 'Missing tag key: artifact.available'
2026-01-15 00:06:05 WARNI [training.hpo.execution.local.sweep] Could not set artifact.available tag: 'Missing tag key: artifact.available'
2026-01-15 00:06:05,708 - training.hpo.execution.local.sweep - INFO -  Set Phase 2 tags on parent run parent-run-1...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-15 00:06:05 INFO  [training.hpo.execution.local.sweep]  Set Phase 2 tags on parent run parent-run-1...: schema_version=2.0, data_fp=set, eval_fp=set, objective_direction=maximize
2026-01-15 00:06:05 WARNI [infrastructure.naming.display_policy] [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-28/test_priority_1_use_provided_h0/workspace/config/naming.yaml, using empty policy
2026-01-15 00:06:05 WARNI [infrastructure.naming.display_policy] [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-28/test_priority_2_get_from_paren0/workspace/config/naming.yaml, using empty policy
2026-01-15 00:06:05 WARNI [infrastructure.naming.display_policy] [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-28/test_priority_3_compute_v2_fro0/workspace/config/naming.yaml, using empty policy
2026-01-15 00:06:05 WARNI [infrastructure.naming.display_policy] [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-28/test_hash_consistency_with_par0/workspace/config/naming.yaml, using empty policy
2026-01-15 00:06:05 WARNI [infrastructure.naming.display_policy] [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-28/test_missing_train_config_stil0/workspace/config/naming.yaml, using empty policy
2026-01-15 00:06:05 WARNI [infrastructure.naming.display_policy] [Naming Policy] Policy file not found at /tmp/pytest-of-codespace/pytest-28/test_error_handling_parent_run0/workspace/config/naming.yaml, using empty policy
2026-01-15 00:06:05,831 - training.hpo.execution.local.sweep - WARNING - _set_phase2_hpo_tags: parent_run_id is empty, skipping
2026-01-15 00:06:05 WARNI [training.hpo.execution.local.sweep] _set_phase2_hpo_tags: parent_run_id is empty, skipping
2026-01-15 00:06:05,833 - training.hpo.execution.local.sweep - WARNING - _set_phase2_hpo_tags: parent_run_id is empty, skipping
2026-01-15 00:06:05 WARNI [training.hpo.execution.local.sweep] _set_phase2_hpo_tags: parent_run_id is empty, skipping
2026-01-15 00:06:08 ERROR [src.deployment.api.inference.engine] Token decoding failed: 'input_ids'
2026-01-15 00:06:08 ERROR [src.deployment.api.inference.engine] Token decoding failed: 'input_ids'
2026-01-15 00:06:09 ERROR [src.deployment.api.inference.engine] Token decoding failed: 'input_ids'
2026-01-15 00:06:09 ERROR [src.deployment.api.inference.engine] Token decoding failed: 'input_ids'
2026-01-15 00:06:09 ERROR [src.deployment.api.inference.engine] Token decoding failed: 'input_ids'
  [Training] MLflow run detected: 1eb673147e1a...
2026-01-15 00:06:14,287 - infrastructure.tracking.mlflow.artifacts.uploader - WARNING - Checkpoint directory does not exist: /tmp/pytest-of-codespace/pytest-28/test_train_model_basic0/output/checkpoint
2026-01-15 00:06:14 WARNI [infrastructure.tracking.mlflow.artifacts.uploader] Checkpoint directory does not exist: /tmp/pytest-of-codespace/pytest-28/test_train_model_basic0/output/checkpoint
  [Training]  Checkpoint directory does not exist: /tmp/pytest-of-codespace/pytest-28/test_train_model_basic0/output/checkpoint
  [Training] MLflow run detected: 1eb673147e1a...
2026-01-15 00:06:14,292 - infrastructure.tracking.mlflow.artifacts.uploader - WARNING - Checkpoint directory does not exist: /tmp/pytest-of-codespace/pytest-28/test_train_model_with_fold_spl0/output/checkpoint
2026-01-15 00:06:14 WARNI [infrastructure.tracking.mlflow.artifacts.uploader] Checkpoint directory does not exist: /tmp/pytest-of-codespace/pytest-28/test_train_model_with_fold_spl0/output/checkpoint
  [Training]  Checkpoint directory does not exist: /tmp/pytest-of-codespace/pytest-28/test_train_model_with_fold_spl0/output/checkpoint
  [Training] MLflow run detected: 1eb673147e1a...
2026-01-15 00:06:14,296 - infrastructure.tracking.mlflow.artifacts.uploader - WARNING - Checkpoint directory does not exist: /tmp/pytest-of-codespace/pytest-28/test_train_model_use_all_data0/output/checkpoint
2026-01-15 00:06:14 WARNI [infrastructure.tracking.mlflow.artifacts.uploader] Checkpoint directory does not exist: /tmp/pytest-of-codespace/pytest-28/test_train_model_use_all_data0/output/checkpoint
  [Training]  Checkpoint directory does not exist: /tmp/pytest-of-codespace/pytest-28/test_train_model_use_all_data0/output/checkpoint
Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2026/01/15 00:06:15 INFO mlflow.tracking.fluent: Experiment with name 'resume_ner_baseline-training' does not exist. Creating a new experiment.
2026-01-15 00:06:15,590 - training.hpo.execution.local.sweep - INFO - [EARLY HASH] Computed v2 study_key_hash=1957109b1932df3e... for folder creation
2026-01-15 00:06:15 INFO  [training.hpo.execution.local.sweep] [EARLY HASH] Computed v2 study_key_hash=1957109b1932df3e... for folder creation
2026-01-15 00:06:15,590 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-15 00:06:15 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-15 00:06:15,590 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-15 00:06:15 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-15 00:06:15,591 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:48ad2237c71414ba63be6d5e23836962988169d56e2a8..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-15 00:06:15 INFO  [orchestration.jobs.tracking.index.version_counter] [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:48ad2237c71414ba63be6d5e23836962988169d56e2a8..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-15 00:06:15,591 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Loaded 21 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-15 00:06:15 INFO  [orchestration.jobs.tracking.index.version_counter] [Reserve Version] Loaded 21 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-15 00:06:15,591 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Found 4 allocations for counter_key (after deduplication): committed=[], reserved=[1, 2, 3, 4], expired=[], max_committed_version=0
2026-01-15 00:06:15 INFO  [orchestration.jobs.tracking.index.version_counter] [Reserve Version] Found 4 allocations for counter_key (after deduplication): committed=[], reserved=[1, 2, 3, 4], expired=[], max_committed_version=0
2026-01-15 00:06:15,591 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Allocation details: [(1, 'reserved', 'pending_2026'), (2, 'reserved', 'pending_2026'), (3, 'reserved', 'pending_2026'), (4, 'reserved', 'pending_2026')]
2026-01-15 00:06:15 INFO  [orchestration.jobs.tracking.index.version_counter] [Reserve Version] Allocation details: [(1, 'reserved', 'pending_2026'), (2, 'reserved', 'pending_2026'), (3, 'reserved', 'pending_2026'), (4, 'reserved', 'pending_2026')]
2026-01-15 00:06:15,591 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Reserving next version: 5 (incremented from max_committed=0, skipped 4 reserved/expired versions)
2026-01-15 00:06:15 INFO  [orchestration.jobs.tracking.index.version_counter] [Reserve Version] Reserving next version: 5 (incremented from max_committed=0, skipped 4 reserved/expired versions)
2026-01-15 00:06:15,591 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version]  Successfully reserved version 5 for counter_key resume-ner:hpo:48ad2237c71414ba63be6d5e23836962988... (run_id: pending_2026...)
2026-01-15 00:06:15 INFO  [orchestration.jobs.tracking.index.version_counter] [Reserve Version]  Successfully reserved version 5 for counter_key resume-ner:hpo:48ad2237c71414ba63be6d5e23836962988... (run_id: pending_2026...)
2026-01-15 00:06:15,592 - training.hpo.execution.local.sweep - INFO - [HPO Setup] k_folds=None, fold_splits_file=None, k_fold config: {'enabled': True, 'n_splits': 5, 'random_seed': 42, 'shuffle': True, 'stratified': True}
2026-01-15 00:06:15 INFO  [training.hpo.execution.local.sweep] [HPO Setup] k_folds=None, fold_splits_file=None, k_fold config: {'enabled': True, 'n_splits': 5, 'random_seed': 42, 'shuffle': True, 'stratified': True}
2026-01-15 00:06:15,595 - common.shared.mlflow_setup - INFO - Using local tracking: sqlite:////workspaces/resume-ner-azureml/mlruns/mlflow.db
2026-01-15 00:06:15 INFO  [common.shared.mlflow_setup] Using local tracking: sqlite:////workspaces/resume-ner-azureml/mlruns/mlflow.db
2026-01-15 00:06:15,595 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Using LOCAL MLflow tracking (not Azure ML)
2026-01-15 00:06:15 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker] Using LOCAL MLflow tracking (not Azure ML)
2026-01-15 00:06:15,595 - infrastructure.tracking.mlflow.trackers.sweep_tracker - INFO - To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
2026-01-15 00:06:15 INFO  [infrastructure.tracking.mlflow.trackers.sweep_tracker] To use Azure ML, ensure:
  1. config/mlflow.yaml has azure_ml.enabled: true
  2. Environment variables are set: AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP
  3. Azure ML SDK is installed: pip install azure-ai-ml azure-identity azureml-mlflow
2026-01-15 00:06:15,595 - infrastructure.tracking.mlflow.trackers.sweep_tracker - ERROR - [START_SWEEP_RUN] MLflow tracking failed: Run with UUID 1eb673147e1a443faf92e853fdb86126 is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True
2026-01-15 00:06:15 ERROR [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] MLflow tracking failed: Run with UUID 1eb673147e1a443faf92e853fdb86126 is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True
2026-01-15 00:06:15,596 - infrastructure.tracking.mlflow.trackers.sweep_tracker - ERROR - [START_SWEEP_RUN] Traceback: Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/trackers/sweep_tracker.py", line 118, in start_sweep_run
    with mlflow.start_run(run_name=run_name) as parent_run:
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/fluent.py", line 380, in start_run
    raise Exception(
Exception: Run with UUID 1eb673147e1a443faf92e853fdb86126 is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True

2026-01-15 00:06:15 ERROR [infrastructure.tracking.mlflow.trackers.sweep_tracker] [START_SWEEP_RUN] Traceback: Traceback (most recent call last):
  File "/workspaces/resume-ner-azureml/src/infrastructure/tracking/mlflow/trackers/sweep_tracker.py", line 118, in start_sweep_run
    with mlflow.start_run(run_name=run_name) as parent_run:
  File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/mlflow/tracking/fluent.py", line 380, in start_run
    raise Exception(
Exception: Run with UUID 1eb673147e1a443faf92e853fdb86126 is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True

2026-01-15 00:06:15,596 - infrastructure.tracking.mlflow.trackers.sweep_tracker - WARNING - Continuing HPO without MLflow tracking...
2026-01-15 00:06:15 WARNI [infrastructure.tracking.mlflow.trackers.sweep_tracker] Continuing HPO without MLflow tracking...
2026-01-15 00:06:15,596 - training.hpo.execution.local.sweep - INFO - Setting Phase 2 tags on parent run None... (data_config=present, train_config=present)
2026-01-15 00:06:15 INFO  [training.hpo.execution.local.sweep] Setting Phase 2 tags on parent run None... (data_config=present, train_config=present)
2026-01-15 00:06:15,596 - training.hpo.execution.local.sweep - WARNING - _set_phase2_hpo_tags: parent_run_id is empty, skipping
2026-01-15 00:06:15 WARNI [training.hpo.execution.local.sweep] _set_phase2_hpo_tags: parent_run_id is empty, skipping
2026-01-15 00:06:15,596 - training.hpo.execution.local.sweep - INFO -  Successfully completed Phase 2 tag setting for parent run None...
2026-01-15 00:06:15 INFO  [training.hpo.execution.local.sweep]  Successfully completed Phase 2 tag setting for parent run None...
2026-01-15 00:06:15,597 - training.hpo.tracking.cleanup - INFO - [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.
2026-01-15 00:06:15 INFO  [training.hpo.tracking.cleanup] [CLEANUP] Automatic interrupted-run cleanup disabled (via config). Clean up via UI if needed.

  0%|          | 0/1 [00:00<?, ?it/s]2026-01-15 00:06:15,603 - training.hpo.execution.local.sweep - INFO - [Trial 0] fold_splits=None, using non-CV path
2026-01-15 00:06:15 INFO  [training.hpo.execution.local.sweep] [Trial 0] fold_splits=None, using non-CV path
2026-01-15 00:06:15,609 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-15 00:06:15 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-15 00:06:15,609 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-15 00:06:15 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-15 00:06:15,609 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:90e118c2e99c2340248a129c7810b54c998771a3dd3bf..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-15 00:06:15 INFO  [orchestration.jobs.tracking.index.version_counter] [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:90e118c2e99c2340248a129c7810b54c998771a3dd3bf..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-15 00:06:15,610 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Loaded 22 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-15 00:06:15 INFO  [orchestration.jobs.tracking.index.version_counter] [Reserve Version] Loaded 22 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-15 00:06:15,610 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0
2026-01-15 00:06:15 INFO  [orchestration.jobs.tracking.index.version_counter] [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0
2026-01-15 00:06:15,610 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)
2026-01-15 00:06:15 INFO  [orchestration.jobs.tracking.index.version_counter] [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)
2026-01-15 00:06:15,611 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version]  Successfully reserved version 1 for counter_key resume-ner:hpo:90e118c2e99c2340248a129c7810b54c998... (run_id: pending_2026...)
2026-01-15 00:06:15 INFO  [orchestration.jobs.tracking.index.version_counter] [Reserve Version]  Successfully reserved version 1 for counter_key resume-ner:hpo:90e118c2e99c2340248a129c7810b54c998... (run_id: pending_2026...)
2026-01-15 00:06:15,623 - training.hpo.tracking.runs - INFO - [TRIAL_RUN_NO_CV] Created trial run (no CV): 91a2b981c8b4... (trial 0). Run remains RUNNING until training completes.
2026-01-15 00:06:15 INFO  [training.hpo.tracking.runs] [TRIAL_RUN_NO_CV] Created trial run (no CV): 91a2b981c8b4... (trial 0). Run remains RUNNING until training completes.
2026-01-15 00:06:15,626 - training.hpo.execution.local.trial - INFO - Training completed
2026-01-15 00:06:15 INFO  [training.hpo.execution.local.trial] Training completed
2026-01-15 00:06:15,626 - training.hpo.execution.local.trial - INFO - [TRIAL] Training completed. Objective metric 'macro-f1': 0.75
2026-01-15 00:06:15 INFO  [training.hpo.execution.local.trial] [TRIAL] Training completed. Objective metric 'macro-f1': 0.75
2026-01-15 00:06:15,626 - training.hpo.tracking.runs - INFO - [TRIAL_RUN_NO_CV] Training completed. Marking trial run 91a2b981c8b4... as FINISHED (trial 0)
2026-01-15 00:06:15 INFO  [training.hpo.tracking.runs] [TRIAL_RUN_NO_CV] Training completed. Marking trial run 91a2b981c8b4... as FINISHED (trial 0)
2026-01-15 00:06:15,635 - infrastructure.tracking.mlflow.lifecycle - INFO - Successfully terminated run 91a2b981c8b4... with status FINISHED
2026-01-15 00:06:15 INFO  [infrastructure.tracking.mlflow.lifecycle] Successfully terminated run 91a2b981c8b4... with status FINISHED
2026-01-15 00:06:15,636 - training.hpo.tracking.runs - INFO - [TRIAL_RUN_NO_CV] Successfully marked trial run 91a2b981c8b4... as FINISHED
2026-01-15 00:06:15 INFO  [training.hpo.tracking.runs] [TRIAL_RUN_NO_CV] Successfully marked trial run 91a2b981c8b4... as FINISHED
2026-01-15 00:06:15,636 - training.hpo.trial.callback - INFO - 
2026-01-15 00:06:15 INFO  [training.hpo.trial.callback] 
2026-01-15 00:06:15,636 - training.hpo.trial.callback - INFO - [BEST]: trial_0
2026-01-15 00:06:15 INFO  [training.hpo.trial.callback] [BEST]: trial_0
2026-01-15 00:06:15,636 - training.hpo.trial.callback - INFO -   Metrics: macro-f1=0.750000
2026-01-15 00:06:15 INFO  [training.hpo.trial.callback]   Metrics: macro-f1=0.750000
2026-01-15 00:06:15,636 - training.hpo.trial.callback - INFO -   Params: learning_rate=3.63e-05 | batch_size=8 | dropout=0.184077 | weight_decay=0.005637
2026-01-15 00:06:15 INFO  [training.hpo.trial.callback]   Params: learning_rate=3.63e-05 | batch_size=8 | dropout=0.184077 | weight_decay=0.005637

Best trial: 0. Best value: 0.75:   0%|          | 0/1 [00:00<?, ?it/s]
Best trial: 0. Best value: 0.75: 100%|| 1/1 [00:00<00:00, 25.14it/s, 0.04/28800 seconds]
Best trial: 0. Best value: 0.75: 100%|| 1/1 [00:00<00:00, 25.08it/s, 0.04/28800 seconds]
2026-01-15 00:06:15,637 - training.hpo.execution.local.sweep - INFO - [REFIT] Starting refit training for best trial 0
2026-01-15 00:06:15 INFO  [training.hpo.execution.local.sweep] [REFIT] Starting refit training for best trial 0
2026-01-15 00:06:15,638 - training.hpo.execution.local.sweep - WARNING - [REFIT] Could not determine trial_key_hash from best trial run tags. Refit run may not match trial run.
2026-01-15 00:06:15 WARNI [training.hpo.execution.local.sweep] [REFIT] Could not determine trial_key_hash from best trial run tags. Refit run may not match trial run.
2026-01-15 00:06:15,638 - training.hpo.execution.local.refit - INFO - [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3.631553947908713e-05, 'batch_size': 8, 'dropout': 0.1840768901656663, 'weight_decay': 0.005636906588031445}
2026-01-15 00:06:15 INFO  [training.hpo.execution.local.refit] [REFIT] Starting refit training for trial 0 with hyperparameters: {'learning_rate': 3.631553947908713e-05, 'batch_size': 8, 'dropout': 0.1840768901656663, 'weight_decay': 0.005636906588031445}
2026-01-15 00:06:15,638 - training.hpo.execution.local.refit - INFO - [REFIT] Computed trial_id='trial_0_20260115_000615', run_id='20260115_000615', trial_number=0
2026-01-15 00:06:15 INFO  [training.hpo.execution.local.refit] [REFIT] Computed trial_id='trial_0_20260115_000615', run_id='20260115_000615', trial_number=0
2026-01-15 00:06:15,638 - training.hpo.execution.local.refit - WARNING - [REFIT] Computed trial_key_hash=a92cafcbb5a998a1... from trial parameters (fallback - may not match trial run hash). Trial run tags should be used as SSOT.
2026-01-15 00:06:15 WARNI [training.hpo.execution.local.refit] [REFIT] Computed trial_key_hash=a92cafcbb5a998a1... from trial parameters (fallback - may not match trial run hash). Trial run tags should be used as SSOT.
2026-01-15 00:06:15,658 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-15 00:06:15 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-15 00:06:15,658 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-15 00:06:15 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=hpo
2026-01-15 00:06:15,658 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:5984e92490c0ddc690bcd2af3dac114a85484d9c81527..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-15 00:06:15 INFO  [orchestration.jobs.tracking.index.version_counter] [Reserve Version] Starting reservation: counter_key=resume-ner:hpo:5984e92490c0ddc690bcd2af3dac114a85484d9c81527..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-15 00:06:15,659 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Loaded 23 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-15 00:06:15 INFO  [orchestration.jobs.tracking.index.version_counter] [Reserve Version] Loaded 23 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-15 00:06:15,659 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0
2026-01-15 00:06:15 INFO  [orchestration.jobs.tracking.index.version_counter] [Reserve Version] Found 0 allocations for counter_key (after deduplication): committed=[], reserved=[], expired=[], max_committed_version=0
2026-01-15 00:06:15,659 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)
2026-01-15 00:06:15 INFO  [orchestration.jobs.tracking.index.version_counter] [Reserve Version] Reserving next version: 1 (incremented from max_committed=0, skipped 0 reserved/expired versions)
2026-01-15 00:06:15,659 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version]  Successfully reserved version 1 for counter_key resume-ner:hpo:5984e92490c0ddc690bcd2af3dac114a854... (run_id: pending_2026...)
2026-01-15 00:06:15 INFO  [orchestration.jobs.tracking.index.version_counter] [Reserve Version]  Successfully reserved version 1 for counter_key resume-ner:hpo:5984e92490c0ddc690bcd2af3dac114a854... (run_id: pending_2026...)
2026-01-15 00:06:15,660 - training.hpo.execution.local.refit - INFO - Training completed
2026-01-15 00:06:15 INFO  [training.hpo.execution.local.refit] Training completed
2026-01-15 00:06:15,660 - training.hpo.execution.local.refit - INFO - [REFIT] Refit training completed. Metrics: 0.75, Checkpoint: /workspaces/resume-ner-azureml/outputs/hpo/local/distilroberta/study-1957109b/trial-a92cafcb/refit/checkpoint
2026-01-15 00:06:15 INFO  [training.hpo.execution.local.refit] [REFIT] Refit training completed. Metrics: 0.75, Checkpoint: /workspaces/resume-ner-azureml/outputs/hpo/local/distilroberta/study-1957109b/trial-a92cafcb/refit/checkpoint
2026-01-15 00:06:15,660 - training.hpo.execution.local.sweep - INFO - [REFIT] Refit training completed. Metrics: {'macro-f1': 0.75}, Checkpoint: /workspaces/resume-ner-azureml/outputs/hpo/local/distilroberta/study-1957109b/trial-a92cafcb/refit/checkpoint, Run ID: None...
2026-01-15 00:06:15 INFO  [training.hpo.execution.local.sweep] [REFIT] Refit training completed. Metrics: {'macro-f1': 0.75}, Checkpoint: /workspaces/resume-ner-azureml/outputs/hpo/local/distilroberta/study-1957109b/trial-a92cafcb/refit/checkpoint, Run ID: None...
2026-01-15 00:06:15,660 - infrastructure.naming.mlflow.tags_registry - INFO - [Tags Registry] Config file not found at /workspaces/resume-ner-azureml/outputs/hpo/config/tags.yaml, using defaults
2026-01-15 00:06:15 INFO  [infrastructure.naming.mlflow.tags_registry] [Tags Registry] Config file not found at /workspaces/resume-ner-azureml/outputs/hpo/config/tags.yaml, using defaults
2026-01-15 00:06:15,660 - training.hpo.execution.local.sweep - WARNING - [REFIT] No refit_run_id available to mark as FINISHED
2026-01-15 00:06:15 WARNI [training.hpo.execution.local.sweep] [REFIT] No refit_run_id available to mark as FINISHED
2026-01-15 00:06:15,660 - training.hpo.checkpoint.cleanup - INFO - Final cleanup: kept checkpoints for best trial 0 (metric=0.750000, CV=no, refit=no), deleted 0 non-best checkpoints
2026-01-15 00:06:15 INFO  [training.hpo.checkpoint.cleanup] Final cleanup: kept checkpoints for best trial 0 (metric=0.750000, CV=no, refit=no), deleted 0 non-best checkpoints
2026/01/15 00:06:15 INFO mlflow.tracking.fluent: Experiment with name 'test-experiment' does not exist. Creating a new experiment.
2026-01-15 00:06:15,779 - common.shared.mlflow_setup - INFO - Using local tracking: sqlite:////workspaces/resume-ner-azureml/mlruns/mlflow.db
2026-01-15 00:06:15 INFO  [common.shared.mlflow_setup] Using local tracking: sqlite:////workspaces/resume-ner-azureml/mlruns/mlflow.db
2026-01-15 00:06:15,779 - evaluation.benchmarking.orchestrator - INFO - Benchmarking distilbert (test456test456te)...
2026-01-15 00:06:15 INFO  [evaluation.benchmarking.orchestrator] Benchmarking distilbert (test456test456te)...
2026-01-15 00:06:15,779 - evaluation.benchmarking.orchestrator - INFO - [BENCHMARK] Final run IDs: trial=None..., refit=trial-run-12..., sweep=None...
2026-01-15 00:06:15 INFO  [evaluation.benchmarking.orchestrator] [BENCHMARK] Final run IDs: trial=None..., refit=trial-run-12..., sweep=None...
2026-01-15 00:06:15,779 - evaluation.benchmarking.utils - INFO - Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /workspaces/resume-ner-azureml/src/evaluation/benchmarking/cli.py --checkpoint /workspaces/resume-ner-azureml/outputs/hpo/local/distilbert/study-test123/trial-test456/refit/checkpoint --test-data /tmp/pytest-of-codespace/pytest-28/test_benchmarking_execution_mo0/dataset_tiny/seed0/test.json --batch-sizes 1 --iterations 10 --warmup 10 --max-length 512 --output /workspaces/resume-ner-azureml/outputs/benchmarking/local/distilbert/study-df9d920c/trial-test456t/bench-fcfe0227/benchmark.json
2026-01-15 00:06:15 INFO  [evaluation.benchmarking.utils] Running benchmark script: /opt/conda/envs/resume-ner-training/bin/python -u /workspaces/resume-ner-azureml/src/evaluation/benchmarking/cli.py --checkpoint /workspaces/resume-ner-azureml/outputs/hpo/local/distilbert/study-test123/trial-test456/refit/checkpoint --test-data /tmp/pytest-of-codespace/pytest-28/test_benchmarking_execution_mo0/dataset_tiny/seed0/test.json --batch-sizes 1 --iterations 10 --warmup 10 --max-length 512 --output /workspaces/resume-ner-azureml/outputs/benchmarking/local/distilbert/study-df9d920c/trial-test456t/bench-fcfe0227/benchmark.json
2026-01-15 00:06:15,780 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Building run name: trial_id=test456test456te, root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config
2026-01-15 00:06:15 INFO  [evaluation.benchmarking.utils] [Benchmark Run Name] Building run name: trial_id=test456test456te, root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config
2026-01-15 00:06:15,780 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-15 00:06:15 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-15 00:06:15,780 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=benchmarking
2026-01-15 00:06:15 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=benchmarking
2026-01-15 00:06:15,780 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Starting reservation: counter_key=resume-ner:benchmarking:8c84880a0e34997930dfb162ad3883bad187..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-15 00:06:15 INFO  [orchestration.jobs.tracking.index.version_counter] [Reserve Version] Starting reservation: counter_key=resume-ner:benchmarking:8c84880a0e34997930dfb162ad3883bad187..., root_dir=/workspaces/resume-ner-azureml, config_dir=/workspaces/resume-ner-azureml/config, counter_path=/workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-15 00:06:15,780 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Loaded 24 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-15 00:06:15 INFO  [orchestration.jobs.tracking.index.version_counter] [Reserve Version] Loaded 24 existing allocations from /workspaces/resume-ner-azureml/outputs/cache/run_name_counter.json
2026-01-15 00:06:15,780 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Found 4 allocations for counter_key (after deduplication): committed=[], reserved=[1, 2, 3, 4], expired=[], max_committed_version=0
2026-01-15 00:06:15 INFO  [orchestration.jobs.tracking.index.version_counter] [Reserve Version] Found 4 allocations for counter_key (after deduplication): committed=[], reserved=[1, 2, 3, 4], expired=[], max_committed_version=0
2026-01-15 00:06:15,780 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Allocation details: [(1, 'reserved', 'pending_2026'), (2, 'reserved', 'pending_2026'), (3, 'reserved', 'pending_2026'), (4, 'reserved', 'pending_2026')]
2026-01-15 00:06:15 INFO  [orchestration.jobs.tracking.index.version_counter] [Reserve Version] Allocation details: [(1, 'reserved', 'pending_2026'), (2, 'reserved', 'pending_2026'), (3, 'reserved', 'pending_2026'), (4, 'reserved', 'pending_2026')]
2026-01-15 00:06:15,780 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version] Reserving next version: 5 (incremented from max_committed=0, skipped 4 reserved/expired versions)
2026-01-15 00:06:15 INFO  [orchestration.jobs.tracking.index.version_counter] [Reserve Version] Reserving next version: 5 (incremented from max_committed=0, skipped 4 reserved/expired versions)
2026-01-15 00:06:15,781 - orchestration.jobs.tracking.index.version_counter - INFO - [Reserve Version]  Successfully reserved version 5 for counter_key resume-ner:benchmarking:8c84880a0e34997930dfb162ad... (run_id: pending_2026...)
2026-01-15 00:06:15 INFO  [orchestration.jobs.tracking.index.version_counter] [Reserve Version]  Successfully reserved version 5 for counter_key resume-ner:benchmarking:8c84880a0e34997930dfb162ad... (run_id: pending_2026...)
2026-01-15 00:06:15,781 - evaluation.benchmarking.utils - INFO - [Benchmark Run Name] Generated run name: local_distilbert_benchmark_study-df9d920c_trial-test456t_bench-fcfe0227_5
2026-01-15 00:06:15 INFO  [evaluation.benchmarking.utils] [Benchmark Run Name] Generated run name: local_distilbert_benchmark_study-df9d920c_trial-test456t_bench-fcfe0227_5
2026-01-15 00:06:15,781 - infrastructure.tracking.mlflow.trackers.benchmark_tracker - WARNING - MLflow tracking failed: Run with UUID 1eb673147e1a443faf92e853fdb86126 is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True
2026-01-15 00:06:15 WARNI [infrastructure.tracking.mlflow.trackers.benchmark_tracker] MLflow tracking failed: Run with UUID 1eb673147e1a443faf92e853fdb86126 is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True
2026-01-15 00:06:15,781 - infrastructure.tracking.mlflow.trackers.benchmark_tracker - WARNING - Continuing benchmarking without MLflow tracking...
2026-01-15 00:06:15 WARNI [infrastructure.tracking.mlflow.trackers.benchmark_tracker] Continuing benchmarking without MLflow tracking...
2026-01-15 00:06:15,786 - infrastructure.tracking.mlflow.trackers.benchmark_tracker - WARNING - Could not log benchmark results to MLflow: Changing param values is not allowed. Param with key='benchmark_batch_sizes' was already logged with value='[1, 8]' for run ID='1eb673147e1a443faf92e853fdb86126'. Attempted logging new value '[1]'.

The cause of this error is typically due to repeated calls
to an individual run_id event logging.

Incorrect Example:
---------------------------------------
with mlflow.start_run():
    mlflow.log_param("depth", 3)
    mlflow.log_param("depth", 5)
---------------------------------------

Which will throw an MlflowException for overwriting a
logged parameter.

Correct Example:
---------------------------------------
with mlflow.start_run():
    with mlflow.start_run(nested=True):
        mlflow.log_param("depth", 3)
    with mlflow.start_run(nested=True):
        mlflow.log_param("depth", 5)
---------------------------------------

Which will create a new nested run for each individual
model and prevent parameter key collisions within the
tracking store.
2026-01-15 00:06:15 WARNI [infrastructure.tracking.mlflow.trackers.benchmark_tracker] Could not log benchmark results to MLflow: Changing param values is not allowed. Param with key='benchmark_batch_sizes' was already logged with value='[1, 8]' for run ID='1eb673147e1a443faf92e853fdb86126'. Attempted logging new value '[1]'.

The cause of this error is typically due to repeated calls
to an individual run_id event logging.

Incorrect Example:
---------------------------------------
with mlflow.start_run():
    mlflow.log_param("depth", 3)
    mlflow.log_param("depth", 5)
---------------------------------------

Which will throw an MlflowException for overwriting a
logged parameter.

Correct Example:
---------------------------------------
with mlflow.start_run():
    with mlflow.start_run(nested=True):
        mlflow.log_param("depth", 3)
    with mlflow.start_run(nested=True):
        mlflow.log_param("depth", 5)
---------------------------------------

Which will create a new nested run for each individual
model and prevent parameter key collisions within the
tracking store.
2026-01-15 00:06:15,786 - evaluation.benchmarking.orchestrator - INFO - Benchmark completed: /workspaces/resume-ner-azureml/outputs/benchmarking/local/distilbert/study-df9d920c/trial-test456t/bench-fcfe0227/benchmark.json
2026-01-15 00:06:15 INFO  [evaluation.benchmarking.orchestrator] Benchmark completed: /workspaces/resume-ner-azureml/outputs/benchmarking/local/distilbert/study-df9d920c/trial-test456t/bench-fcfe0227/benchmark.json
2026-01-15 00:06:15,786 - evaluation.benchmarking.orchestrator - INFO - Benchmarking complete. 1/1 trials benchmarked.
2026-01-15 00:06:15 INFO  [evaluation.benchmarking.orchestrator] Benchmarking complete. 1/1 trials benchmarked.
[conversion.orchestration] Output directory: /workspaces/resume-ner-azureml/outputs/conversion/local/local/spec-aaaaaaaa_exec-bbbbbbbb/v1/conv-cd2379f5
2026-01-15 00:06:15 INFO  [script.conversion.orchestration] Output directory: /workspaces/resume-ner-azureml/outputs/conversion/local/local/spec-aaaaaaaa_exec-bbbbbbbb/v1/conv-cd2379f5
2026-01-15 00:06:15,851 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-15 00:06:15 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Loading from config_dir=/workspaces/resume-ner-azureml/config, raw_auto_inc_config={'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}
2026-01-15 00:06:15,851 - infrastructure.naming.mlflow.config - INFO - [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=conversion
2026-01-15 00:06:15 INFO  [infrastructure.naming.mlflow.config] [Auto-Increment Config] Validated config: {'enabled': True, 'processes': {'hpo': True, 'benchmarking': True}, 'format': '{base}.{version}'}, process_type=conversion
[conversion.orchestration] Created MLflow run: local_local_conversion_spec-aaaaaaaa_exec-bbbbbbbb_v1_conv-cd2379f5 (088b3772c363...)
2026-01-15 00:06:15 INFO  [script.conversion.orchestration] Created MLflow run: local_local_conversion_spec-aaaaaaaa_exec-bbbbbbbb_v1_conv-cd2379f5 (088b3772c363...)
[conversion.orchestration] Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /tmp/pytest-of-codespace/pytest-28/test_best_config_selection_e2e0/final_training/local/distilbert/spec-aaaaaaaa_exec-bbbbbbbb/v1/checkpoint --config-dir /workspaces/resume-ner-azureml/config --backbone local --output-dir /workspaces/resume-ner-azureml/outputs/conversion/local/local/spec-aaaaaaaa_exec-bbbbbbbb/v1/conv-cd2379f5 --opset-version 18 --run-smoke-test
2026-01-15 00:06:15 INFO  [script.conversion.orchestration] Running conversion: /opt/conda/envs/resume-ner-training/bin/python -m deployment.conversion.execution --checkpoint-path /tmp/pytest-of-codespace/pytest-28/test_best_config_selection_e2e0/final_training/local/distilbert/spec-aaaaaaaa_exec-bbbbbbbb/v1/checkpoint --config-dir /workspaces/resume-ner-azureml/config --backbone local --output-dir /workspaces/resume-ner-azureml/outputs/conversion/local/local/spec-aaaaaaaa_exec-bbbbbbbb/v1/conv-cd2379f5 --opset-version 18 --run-smoke-test
[conversion.orchestration] 2026-01-15 00:06:18,292 - infrastructure.tracking.mlflow.compatibility - INFO - Applied Azure ML artifact compatibility patch
2026-01-15 00:06:18 WARNI [script.conversion.orchestration] 2026-01-15 00:06:18,292 - infrastructure.tracking.mlflow.compatibility - INFO - Applied Azure ML artifact compatibility patch
[conversion.orchestration] [conversion.execution] Starting conversion: checkpoint='/tmp/pytest-of-codespace/pytest-28/test_best_config_selection_e2e0/final_training/local/distilbert/spec-aaaaaaaa_exec-bbbbbbbb/v1/checkpoint', backbone='local', quantize_int8=False, opset_version=18
2026-01-15 00:06:21 WARNI [script.conversion.orchestration] [conversion.execution] Starting conversion: checkpoint='/tmp/pytest-of-codespace/pytest-28/test_best_config_selection_e2e0/final_training/local/distilbert/spec-aaaaaaaa_exec-bbbbbbbb/v1/checkpoint', backbone='local', quantize_int8=False, opset_version=18
[conversion.orchestration] [conversion.execution] Resolving checkpoint directory from '/tmp/pytest-of-codespace/pytest-28/test_best_config_selection_e2e0/final_training/local/distilbert/spec-aaaaaaaa_exec-bbbbbbbb/v1/checkpoint'
2026-01-15 00:06:21 WARNI [script.conversion.orchestration] [conversion.execution] Resolving checkpoint directory from '/tmp/pytest-of-codespace/pytest-28/test_best_config_selection_e2e0/final_training/local/distilbert/spec-aaaaaaaa_exec-bbbbbbbb/v1/checkpoint'
[conversion.orchestration] [conversion.execution] Output directory: '/workspaces/resume-ner-azureml/outputs/conversion/local/local/spec-aaaaaaaa_exec-bbbbbbbb/v1/conv-cd2379f5/onnx_model'
2026-01-15 00:06:21 WARNI [script.conversion.orchestration] [conversion.execution] Output directory: '/workspaces/resume-ner-azureml/outputs/conversion/local/local/spec-aaaaaaaa_exec-bbbbbbbb/v1/conv-cd2379f5/onnx_model'
[conversion.orchestration] [conversion.execution] Using MLflow run: 088b3772c363...
2026-01-15 00:06:21 WARNI [script.conversion.orchestration] [conversion.execution] Using MLflow run: 088b3772c363...
[conversion.orchestration] [conversion.export] Starting ONNX export. quantize_int8=False
2026-01-15 00:06:21 WARNI [script.conversion.orchestration] [conversion.export] Starting ONNX export. quantize_int8=False
[conversion.orchestration] [conversion.export] Output directory created at '/workspaces/resume-ner-azureml/outputs/conversion/local/local/spec-aaaaaaaa_exec-bbbbbbbb/v1/conv-cd2379f5/onnx_model'
2026-01-15 00:06:21 WARNI [script.conversion.orchestration] [conversion.export] Output directory created at '/workspaces/resume-ner-azureml/outputs/conversion/local/local/spec-aaaaaaaa_exec-bbbbbbbb/v1/conv-cd2379f5/onnx_model'
[conversion.orchestration] [conversion.export] Loading tokenizer and model from checkpoint directory '/tmp/pytest-of-codespace/pytest-28/test_best_config_selection_e2e0/final_training/local/distilbert/spec-aaaaaaaa_exec-bbbbbbbb/v1/checkpoint'
2026-01-15 00:06:21 WARNI [script.conversion.orchestration] [conversion.export] Loading tokenizer and model from checkpoint directory '/tmp/pytest-of-codespace/pytest-28/test_best_config_selection_e2e0/final_training/local/distilbert/spec-aaaaaaaa_exec-bbbbbbbb/v1/checkpoint'
[conversion.orchestration] [conversion.execution] Conversion failed: stat: path should be string, bytes, os.PathLike or integer, not NoneType
2026-01-15 00:06:21 WARNI [script.conversion.orchestration] [conversion.execution] Conversion failed: stat: path should be string, bytes, os.PathLike or integer, not NoneType
[conversion.orchestration] Traceback (most recent call last):
2026-01-15 00:06:21 WARNI [script.conversion.orchestration] Traceback (most recent call last):
[conversion.orchestration]   File "/opt/conda/envs/resume-ner-training/lib/python3.10/runpy.py", line 196, in _run_module_as_main
2026-01-15 00:06:21 WARNI [script.conversion.orchestration]   File "/opt/conda/envs/resume-ner-training/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[conversion.orchestration]     return _run_code(code, main_globals, None,
2026-01-15 00:06:21 WARNI [script.conversion.orchestration]     return _run_code(code, main_globals, None,
[conversion.orchestration]   File "/opt/conda/envs/resume-ner-training/lib/python3.10/runpy.py", line 86, in _run_code
2026-01-15 00:06:21 WARNI [script.conversion.orchestration]   File "/opt/conda/envs/resume-ner-training/lib/python3.10/runpy.py", line 86, in _run_code
[conversion.orchestration]     exec(code, run_globals)
2026-01-15 00:06:21 WARNI [script.conversion.orchestration]     exec(code, run_globals)
[conversion.orchestration]   File "/workspaces/resume-ner-azureml/src/deployment/conversion/execution.py", line 237, in <module>
2026-01-15 00:06:21 WARNI [script.conversion.orchestration]   File "/workspaces/resume-ner-azureml/src/deployment/conversion/execution.py", line 237, in <module>
[conversion.orchestration]     main()
2026-01-15 00:06:21 WARNI [script.conversion.orchestration]     main()
[conversion.orchestration]   File "/workspaces/resume-ner-azureml/src/deployment/conversion/execution.py", line 179, in main
2026-01-15 00:06:21 WARNI [script.conversion.orchestration]   File "/workspaces/resume-ner-azureml/src/deployment/conversion/execution.py", line 179, in main
[conversion.orchestration]     onnx_path = export_to_onnx(
2026-01-15 00:06:21 WARNI [script.conversion.orchestration]     onnx_path = export_to_onnx(
[conversion.orchestration]   File "/workspaces/resume-ner-azureml/src/deployment/conversion/export.py", line 81, in export_to_onnx
2026-01-15 00:06:21 WARNI [script.conversion.orchestration]   File "/workspaces/resume-ner-azureml/src/deployment/conversion/export.py", line 81, in export_to_onnx
[conversion.orchestration]     tokenizer = AutoTokenizer.from_pretrained(checkpoint_dir, use_fast=True)
2026-01-15 00:06:21 WARNI [script.conversion.orchestration]     tokenizer = AutoTokenizer.from_pretrained(checkpoint_dir, use_fast=True)
[conversion.orchestration]   File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 1175, in from_pretrained
2026-01-15 00:06:21 WARNI [script.conversion.orchestration]   File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 1175, in from_pretrained
[conversion.orchestration]     return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
2026-01-15 00:06:21 WARNI [script.conversion.orchestration]     return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
[conversion.orchestration]   File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2113, in from_pretrained
2026-01-15 00:06:21 WARNI [script.conversion.orchestration]   File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2113, in from_pretrained
[conversion.orchestration]     return cls._from_pretrained(
2026-01-15 00:06:21 WARNI [script.conversion.orchestration]     return cls._from_pretrained(
[conversion.orchestration]   File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2151, in _from_pretrained
2026-01-15 00:06:21 WARNI [script.conversion.orchestration]   File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2151, in _from_pretrained
[conversion.orchestration]     slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(
2026-01-15 00:06:21 WARNI [script.conversion.orchestration]     slow_tokenizer = (cls.slow_tokenizer_class)._from_pretrained(
[conversion.orchestration]   File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2359, in _from_pretrained
2026-01-15 00:06:21 WARNI [script.conversion.orchestration]   File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 2359, in _from_pretrained
[conversion.orchestration]     tokenizer = cls(*init_inputs, **init_kwargs)
2026-01-15 00:06:21 WARNI [script.conversion.orchestration]     tokenizer = cls(*init_inputs, **init_kwargs)
[conversion.orchestration]   File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/transformers/models/distilbert/tokenization_distilbert.py", line 117, in __init__
2026-01-15 00:06:21 WARNI [script.conversion.orchestration]   File "/opt/conda/envs/resume-ner-training/lib/python3.10/site-packages/transformers/models/distilbert/tokenization_distilbert.py", line 117, in __init__
[conversion.orchestration]     if not os.path.isfile(vocab_file):
2026-01-15 00:06:21 WARNI [script.conversion.orchestration]     if not os.path.isfile(vocab_file):
[conversion.orchestration]   File "/opt/conda/envs/resume-ner-training/lib/python3.10/genericpath.py", line 30, in isfile
2026-01-15 00:06:21 WARNI [script.conversion.orchestration]   File "/opt/conda/envs/resume-ner-training/lib/python3.10/genericpath.py", line 30, in isfile
[conversion.orchestration]     st = os.stat(path)
2026-01-15 00:06:21 WARNI [script.conversion.orchestration]     st = os.stat(path)
[conversion.orchestration] TypeError: stat: path should be string, bytes, os.PathLike or integer, not NoneType
2026-01-15 00:06:21 WARNI [script.conversion.orchestration] TypeError: stat: path should be string, bytes, os.PathLike or integer, not NoneType
2026-01-15 00:06:22,330 - infrastructure.tracking.mlflow.lifecycle - INFO - Run 088b3772c363... already has status <Mock name='mock.get_run().info.status' id='124894313470160'>, skipping termination (expected RUNNING)
2026-01-15 00:06:22 INFO  [infrastructure.tracking.mlflow.lifecycle] Run 088b3772c363... already has status <Mock name='mock.get_run().info.status' id='124894313470160'>, skipping termination (expected RUNNING)

ERROR conda.cli.main_run:execute(127): `conda run pytest tests/ -v --tb=short` failed. (See above for error)

